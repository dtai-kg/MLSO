@prefix : <http://w3id.org/mlso/vocab/ml_algorithm/> .
@prefix dcterms: <http://purl.org/dc/terms/> .
@prefix owl: <http://www.w3.org/2002/07/owl#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix skos: <http://www.w3.org/2004/02/skos/core#> .

<http://w3id.org/mlso/vocab/ml_algorithm> a owl:Ontology .

<http://w3id.org/mlso/vocab/ml_algorithm/(2+1)DConvolution> a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1711.11248v3> ;
    rdfs:seeAlso <https://github.com/pytorch/vision/blob/9e474c3c46c0871838c021093c67a9c7eb1863ea/torchvision/models/video/resnet.py#L36> ;
    skos:definition "A **(2+1)D Convolution** is a type of [convolution](https://paperswithcode.com/method/convolution) used for action recognition convolutional neural networks, with a spatiotemporal volume. As opposed to applying a [3D Convolution](https://paperswithcode.com/method/3d-convolution) over the entire volume, which can be computationally expensive and lead to overfitting, a (2+1)D convolution splits computation into two convolutions: a spatial 2D convolution followed by a temporal 1D convolution." ;
    skos:prefLabel "(2+1)D Convolution" .

:1-bitAdam a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2102.02888v2> ;
    skos:definition "**1-bit Adam** is a [stochastic optimization](https://paperswithcode.com/methods/category/stochastic-optimization) technique that is a variant of [ADAM](https://paperswithcode.com/method/adam) with error-compensated 1-bit compression, based on finding that Adam's variance term becomes stable at an early stage. First vanilla Adam is used for a few epochs as a warm-up. After the warm-up stage, the compression stage starts and we stop updating the variance term $\\mathbf{v}$ and use it as a fixed precondition. At the compression stage, we communicate based on the momentum applied with error-compensated 1-bit compression. The momentums are quantized into 1-bit representation (the sign of each element). Accompanying the vector, a scaling factor is computed as $\\frac{\\text { magnitude of compensated gradient }}{\\text { magnitude of quantized gradient }}$. This scaling factor ensures that the compressed momentum has the same magnitude as the uncompressed momentum. This 1-bit compression could reduce the communication cost by $97 \\%$ and $94 \\%$ compared to the original float 32 and float 16 training, respectively." ;
    skos:prefLabel "1-bit Adam" .

:1-bitLAMB a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2104.06069v2> ;
    skos:definition """**1-bit LAMB** is a communication-efficient stochastic optimization technique which introduces a novel way to support adaptive layerwise learning rates even when communication is compressed. Learning from the insights behind [1-bit Adam](https://paperswithcode.com/method/1-bit-adam), it is a a 2-stage algorithm which uses [LAMB](https://paperswithcode.com/method/lamb) (warmup stage) to “pre-condition” a communication compressed momentum SGD algorithm (compression stage). At compression stage where original LAMB algorithm cannot be used to update the layerwise learning rates, 1-bit LAMB employs a novel way to adaptively scale layerwise learning rates based on information from both warmup and compression stages. As a result, 1-bit LAMB is able to achieve large batch optimization (LAMB)’s convergence speed under compressed communication.\r
\r
There are two major differences between 1-bit LAMB and the original LAMB:\r
\r
- During compression stage, 1-bit LAMB updates the layerwise learning rate based on a novel “reconstructed gradient” based on the compressed momentum. This makes 1-bit LAMB compatible with error compensation and be able to keep track of the training dynamic under compression.\r
- 1-bit LAMB also introduces extra stabilized soft thresholds when updating layerwise learning rate at compression stage, which makes training more stable under compression.""" ;
    skos:prefLabel "1-bit LAMB" .

:1DCNN a skos:Concept ;
    dcterms:source <https://www.researchgate.net/publication/348288032_Convolutional_Neural_Network_and_Rule-Based_Algorithms_for_Classifying_12-lead_ECGs> ;
    skos:altLabel "1-Dimensional Convolutional Neural Networks" ;
    skos:definition "1D Convolutional Neural Networks are similar to well known and more established 2D Convolutional Neural Networks. 1D Convolutional Neural Networks are used mainly used on text and 1D signals." ;
    skos:prefLabel "1D CNN" .

:1cycle a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1803.09820v2> ;
    skos:altLabel "1cycle learning rate scheduling policy" ;
    skos:definition "" ;
    skos:prefLabel "1cycle" .

:1x1Convolution a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1312.4400v3> ;
    skos:definition """A **1 x 1 Convolution** is a [convolution](https://paperswithcode.com/method/convolution) with some special properties in that it can be used for dimensionality reduction, efficient low dimensional embeddings, and applying non-linearity after convolutions. It maps an input pixel with all its channels to an output pixel which can be squeezed to a desired output depth. It can be viewed as an [MLP](https://paperswithcode.com/method/feedforward-network) looking at a particular pixel location.\r
\r
Image Credit: [http://deeplearning.ai](http://deeplearning.ai)""" ;
    skos:prefLabel "1x1 Convolution" .

:2DDWT a skos:Concept ;
    dcterms:source <https://openreview.net/forum?id=tBoSm4hUWV> ;
    skos:altLabel "2D Discrete Wavelet Transform" ;
    skos:definition "" ;
    skos:prefLabel "2D DWT" .

:3-Augment a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2204.07118v1> ;
    skos:definition "" ;
    skos:prefLabel "3-Augment" .

:3DConvolution a skos:Concept ;
    rdfs:seeAlso <https://github.com/pytorch/pytorch/blob/73642d9425a358b51a683cf6f95852d06cba1096/torch/nn/modules/conv.py#L421> ;
    skos:definition """A **3D Convolution** is a type of [convolution](https://paperswithcode.com/method/convolution) where the kernel slides in 3 dimensions as opposed to 2 dimensions with 2D convolutions. One example use case is medical imaging where a model is constructed using 3D image slices. Additionally video based data has an additional temporal dimension over images making it suitable for this module. \r
\r
Image: Lung nodule detection based on 3D convolutional neural networks, Fan et al""" ;
    skos:prefLabel "3D Convolution" .

:3DDynamicSceneGraph a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2101.06894v3> ;
    skos:definition "**3D Dynamic Scene Graph**, or **DSG**, is a representation that captures metric and semantic aspects of a dynamic environment. A DSG is a layered graph where nodes represent spatial concepts at different levels of abstraction, and edges represent spatio-temporal relations among nodes." ;
    skos:prefLabel "3D Dynamic Scene Graph" .

:3DIS a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2007.05405v1> ;
    skos:altLabel "3-dimensional interaction space" ;
    skos:definition """A **trainable 3D interaction space** aims to captures the associations between the triplet components and helps model the recognition of multiple triplets in the same frame.\r
\r
Source: [Nwoye et al.](https://arxiv.org/pdf/2007.05405v1.pdf)\r
\r
Image source: [Nwoye et al.](https://arxiv.org/pdf/2007.05405v1.pdf)""" ;
    skos:prefLabel "3DIS" .

:3DResNet-RS a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2109.01696v1> ;
    skos:definition """**3D ResNet-RS** is an architecture and scaling strategy for 3D ResNets for video recognition. The key additions are:\r
\r
- **3D ResNet-D stem**: The [ResNet-D](https://paperswithcode.com/method/resnet-d) stem is adapted to 3D inputs by using three consecutive [3D convolutional layers](https://paperswithcode.com/method/3d-convolution). The first convolutional layer employs a temporal kernel size of 5 while the remaining two convolutional layers employ a temporal kernel size of 1.\r
\r
- **3D Squeeze-and-Excitation**:  [Squeeze-and-Excite](https://paperswithcode.com/method/squeeze-and-excitation-block) is adapted to spatio-temporal inputs by using a 3D [global average pooling](https://paperswithcode.com/method/global-average-pooling) operation for the squeeze operation. A SE ratio of 0.25 is applied in each 3D bottleneck block for all experiments.\r
\r
- **Self-gating**: A self-gating module is used in each 3D bottleneck block after the SE module.""" ;
    skos:prefLabel "3D ResNet-RS" .

:3DSA a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2009.04532v3> ;
    skos:altLabel "3 Dimensional Soft Attention" ;
    skos:definition "" ;
    skos:prefLabel "3D SA" .

:3DSSD a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2002.10187v1> ;
    skos:definition "**3DSSD** is a point-based 3D single stage object detection detector. In this paradigm, all upsampling layers and refinement stage, which are indispensable in all existing point-based methods, are abandoned to reduce the large computation cost. The authors propose a fusion sampling strategy in the downsampling process to make detection on less representative points feasible. A delicate box prediction network including a candidate generation layer, an anchor-free regression head with a 3D center-ness assignment strategy is designed to meet the needs of accuracy and speed." ;
    skos:prefLabel "3DSSD" .

<http://w3id.org/mlso/vocab/ml_algorithm/4DA*> a skos:Concept ;
    dcterms:source <https://doi.org/10.1109/ACCESS.2020.3026193> ;
    skos:altLabel "Four-dimensional A-star" ;
    skos:definition "The aim of 4D A* is to find the shortest path between two four-dimensional (4D) nodes of a 4D search space - a starting node and a target node - as long as there is a path. It achieves both optimality and completeness. The former is because the path is shortest possible, and the latter because if the solution exists the algorithm is guaranteed to find it." ;
    skos:prefLabel "4D A*" .

:A2C a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1602.01783v2> ;
    skos:definition """**A2C**, or **Advantage Actor Critic**, is a synchronous version of the [A3C](https://paperswithcode.com/method/a3c) policy gradient method. As an alternative to the asynchronous implementation of A3C, A2C is a synchronous, deterministic implementation that waits for each actor to finish its segment of experience before updating, averaging over all of the actors. This more effectively uses GPUs due to larger batch sizes.\r
\r
Image Credit: [OpenAI Baselines](https://openai.com/blog/baselines-acktr-a2c/)""" ;
    skos:prefLabel "A2C" .

:A3C a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1602.01783v2> ;
    skos:definition """**A3C**, **Asynchronous Advantage Actor Critic**, is a policy gradient algorithm in reinforcement learning that maintains a policy $\\pi\\left(a\\_{t}\\mid{s}\\_{t}; \\theta\\right)$ and an estimate of the value\r
function $V\\left(s\\_{t}; \\theta\\_{v}\\right)$. It operates in the forward view and uses a mix of $n$-step returns to update both the policy and the value-function. The policy and the value function are updated after every $t\\_{\\text{max}}$ actions or when a terminal state is reached. The update performed by the algorithm can be seen as $\\nabla\\_{\\theta{'}}\\log\\pi\\left(a\\_{t}\\mid{s\\_{t}}; \\theta{'}\\right)A\\left(s\\_{t}, a\\_{t}; \\theta, \\theta\\_{v}\\right)$ where $A\\left(s\\_{t}, a\\_{t}; \\theta, \\theta\\_{v}\\right)$ is an estimate of the advantage function given by:\r
\r
$$\\sum^{k-1}\\_{i=0}\\gamma^{i}r\\_{t+i} + \\gamma^{k}V\\left(s\\_{t+k}; \\theta\\_{v}\\right) - V\\left(s\\_{t}; \\theta\\_{v}\\right)$$\r
\r
where $k$ can vary from state to state and is upper-bounded by $t\\_{max}$.\r
\r
The critics in A3C learn the value function while multiple actors are trained in parallel and get synced with global parameters every so often. The gradients are accumulated as part of training for stability - this is like parallelized stochastic gradient descent.\r
\r
Note that while the parameters $\\theta$ of the policy and $\\theta\\_{v}$ of the value function are shown as being separate for generality, we always share some of the parameters in practice. We typically use a convolutional neural network that has one [softmax](https://paperswithcode.com/method/softmax) output for the policy $\\pi\\left(a\\_{t}\\mid{s}\\_{t}; \\theta\\right)$ and one linear output for the value function $V\\left(s\\_{t}; \\theta\\_{v}\\right)$, with all non-output layers shared.""" ;
    skos:prefLabel "A3C" .

:ABC a skos:Concept ;
    dcterms:source <https://www.umass.edu/nanofabrics/publication/accelerating-simulation-based-inference-emerging-ai-hardware> ;
    skos:altLabel "Approximate Bayesian Computation" ;
    skos:definition """Class of methods in Bayesian Statistics where the posterior distribution is approximated over a rejection scheme on simulations because the likelihood function is intractable.\r
\r
Different parameters get sampled and simulated. Then a distance function is calculated to measure the quality of the simulation compared to data from real observations. Only simulations that fall below a certain threshold get accepted.\r
\r
Image source: [Kulkarni et al.](https://www.umass.edu/nanofabrics/sites/default/files/PDF_0.pdf)""" ;
    skos:prefLabel "ABC" .

:ABCNet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2002.10200v2> ;
    skos:altLabel "Adaptive Bezier-Curve Network" ;
    skos:definition "**Adaptive Bezier-Curve Network**, or **ABCNet**, is an end-to-end framework for arbitrarily-shaped scene text spotting. It adaptively fits arbitrary-shaped text by a parameterized bezier curve. It also utilizes a feature alignment layer, [BezierAlign](https://paperswithcode.com/method/bezieralign), to calculate convolutional features of text instances in curved shapes. These features are then passed to a light-weight recognition head." ;
    skos:prefLabel "ABCNet" .

:ACER a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1611.01224v2> ;
    skos:definition """**ACER**, or **Actor Critic with Experience Replay**, is an actor-critic deep reinforcement learning agent with [experience replay](https://paperswithcode.com/method/experience-replay). It can be seen as an off-policy extension of [A3C](https://paperswithcode.com/method/a3c), where the off-policy estimator is made feasible by:\r
\r
- Using [Retrace](https://paperswithcode.com/method/retrace) Q-value estimation.\r
- Using truncated importance sampling with bias correction.\r
- Using a trust region policy optimization method.\r
- Using a [stochastic dueling network](https://paperswithcode.com/method/stochastic-dueling-network) architecture.""" ;
    skos:prefLabel "ACER" .

:ACGPN a skos:Concept ;
    dcterms:source <http://openaccess.thecvf.com/content_CVPR_2020/html/Yang_Towards_Photo-Realistic_Virtual_Try-On_by_Adaptively_Generating-Preserving_Image_Content_CVPR_2020_paper.html> ;
    skos:altLabel "Adaptive Content Generating and Preserving Network" ;
    skos:definition """**ACGPN**, or **Adaptive Content Generating and Preserving Network**, is a [generative adversarial network](https://www.paperswithcode.com/method/category/generative-adversarial-network) for virtual try-on clothing applications. \r
\r
In Step I, the Semantic Generation Module (SGM) takes the target clothing image $\\mathcal{T}\\_{c}$, the pose map $\\mathcal{M}\\_{p}$, and the fused body part mask $\\mathcal{M}^{F}$ as the input to predict the semantic layout and to output the synthesized body part mask $\\mathcal{M}^{S}\\_{\\omega}$ and the target clothing mask $\\mathcal{M}^{S\\_{c}$.\r
\r
In Step II, the Clothes Warping Module (CWM) warps the target clothing image to $\\mathcal{T}^{R}\\_{c}$ according to the predicted semantic layout, where a second-order difference constraint is introduced to stabilize the warping process. \r
\r
In Steps III and IV, the Content Fusion Module (CFM) first produces the composited body part mask $\\mathcal{M}^{C}\\_{\\omega}$ using the original clothing mask $\\mathcal{M}\\_{c}$, the synthesized clothing mask $\\mathcal{M}^{S}\\_{c}$, the body part mask $\\mathcal{M}\\_{\\omega}$, and the synthesized body part mask $\\mathcal{M}\\_{\\omega}^{S}$, and then exploits a fusion network to generate the try-on images $\\mathcal{I}^{S}$ by utilizing the information $\\mathcal{T}^{R}\\_{c}$, $\\mathcal{M}^{S}\\_{c}$, and the body part image $I\\_{\\omega}$ from previous steps.""" ;
    skos:prefLabel "ACGPN" .

:ACTKR a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1708.05144v2> ;
    skos:definition """**ACKTR**, or **Actor Critic with Kronecker-factored Trust Region**, is an actor-critic method for reinforcement learning that applies [trust region optimization](https://paperswithcode.com/method/trpo) using a recently proposed Kronecker-factored approximation to the curvature. The method extends the framework of natural policy gradient and optimizes both the actor and the critic using Kronecker-factored approximate\r
curvature (K-FAC) with trust region.""" ;
    skos:prefLabel "ACTKR" .

:ADELE a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2110.03740v2> ;
    skos:altLabel "Adaptive Early-Learning Correction" ;
    skos:definition "Adaptive Early-Learning Correction for Segmentation from Noisy Annotations" ;
    skos:prefLabel "ADELE" .

:ADMM a skos:Concept ;
    skos:altLabel "Alternating Direction Method of Multipliers" ;
    skos:definition """The **alternating direction method of multipliers** (**ADMM**) is an algorithm that solves convex optimization problems by breaking them into smaller pieces, each of which are then easier to handle. It takes the form of a decomposition-coordination procedure, in which the solutions to small\r
local subproblems are coordinated to find a solution to a large global problem. ADMM can be viewed as an attempt to blend the benefits of dual decomposition and augmented Lagrangian methods for constrained optimization. It turns out to be equivalent or closely related to many other algorithms\r
as well, such as Douglas-Rachford splitting from numerical analysis, Spingarn’s method of partial inverses, Dykstra’s alternating projections method, Bregman iterative algorithms for l1 problems in signal processing, proximal methods, and many others.\r
\r
Text Source: [https://stanford.edu/~boyd/papers/pdf/admm_distr_stats.pdf](https://stanford.edu/~boyd/papers/pdf/admm_distr_stats.pdf)\r
\r
Image Source: [here](https://www.slideshare.net/derekcypang/alternating-direction)""" ;
    skos:prefLabel "ADMM" .

:AE a skos:Concept ;
    skos:altLabel "Autoencoders" ;
    skos:definition """An **autoencoder** is a type of artificial neural network used to learn efficient data codings in an unsupervised manner. The aim of an autoencoder is to learn a representation (encoding) for a set of data, typically for dimensionality reduction, by training the network to ignore signal “noise”. Along with the reduction side, a reconstructing side is learnt, where the autoencoder tries to generate from the reduced encoding a representation as close as possible to its original input, hence its name. \r
\r
Extracted from: [Wikipedia](https://en.wikipedia.org/wiki/Autoencoder)\r
\r
Image source: [Wikipedia](https://en.wikipedia.org/wiki/Autoencoder#/media/File:Autoencoder_schema.png)""" ;
    skos:prefLabel "AE" .

:AEDA a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2108.13230v1> ;
    skos:altLabel "An Easier Data Augmentation" ;
    skos:definition "**AEDA**, or **An Easier Data Augmentation**, is a type of data augmentation technique for text classification which includes only the insertion of various punctuation marks into the input sequence. AEDA preserves all the input information and does not mislead the network since it keeps the word order intact while changing their positions in that the words are shifted to the right." ;
    skos:prefLabel "AEDA" .

:AGCN a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1801.03226v1> ;
    skos:altLabel "Adaptive Graph Convolutional Neural Networks" ;
    skos:definition """AGCN is a novel spectral graph convolution network that feed on original data of diverse graph structures.\r
\r
Image credit: [Adaptive Graph Convolutional Neural Networks](https://arxiv.org/pdf/1801.03226.pdf)""" ;
    skos:prefLabel "AGCN" .

:AHAF a skos:Concept ;
    dcterms:source <https://doi.org/10.20535/SRIT.2308-8893.2022.1.07> ;
    skos:altLabel "Adaptive Hybrid Activation Function" ;
    skos:definition "Trainable activation function as a sigmoid-based generalization of ReLU, Swish and SiLU." ;
    skos:prefLabel "AHAF" .

:ALAE a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2004.04467v1> ;
    skos:altLabel "Adversarial Latent Autoencoder" ;
    skos:definition "**ALAE**, or **Adversarial Latent Autoencoder**, is a type of autoencoder that attempts to overcome some of the limitations of[ generative adversarial networks](https://paperswithcode.com/paper/generative-adversarial-networks). The architecture allows the latent distribution to be learned from data to address entanglement (A). The output data distribution is learned with an adversarial strategy (B). Thus, we retain the generative properties of GANs, as well as the ability to build on the recent advances in this area. For instance, we can include independent sources of stochasticity, which have proven essential for generating image details, or can leverage recent improvements on GAN loss functions, regularization, and hyperparameters tuning. Finally, to implement (A) and (B), AE reciprocity is imposed in the latent space (C). Therefore, we can avoid using reconstruction losses based on simple $\\mathcal{l}\\){2}$ norm that operates in data space, where they are often suboptimal, like for the image space. Since it works on the latent space, rather than autoencoding the data space, the approach is named Adversarial Latent Autoencoder (ALAE)." ;
    skos:prefLabel "ALAE" .

:ALBEF a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2107.07651v2> ;
    skos:definition "ALBEF introduces a contrastive loss to align the image and text representations before fusing them through cross-modal attention. This enables more grounded vision and language representation learning. ALBEF also doesn't require bounding box annotations. The model consists of an image encode, a text encoder, and a multimodal encoder. The image-text contrastive loss helps to align the unimodal representations of an image-text pair before fusion. The image-text matching loss and a masked language modeling loss are applied to learn multimodal interactions between image and text. In addition, momentum distillation is used to generate pseudo-targets. This improves learning with noisy data." ;
    skos:prefLabel "ALBEF" .

:ALBERT a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1909.11942v6> ;
    skos:definition """**ALBERT** is a [Transformer](https://paperswithcode.com/method/transformer) architecture based on [BERT](https://paperswithcode.com/method/bert) but with much fewer parameters. It achieves this through two parameter reduction techniques. The first is a factorized embeddings parameterization. By decomposing the large vocabulary embedding matrix into two small matrices, the size of the hidden layers is separated from the size of vocabulary embedding. This makes it easier to grow the hidden size without significantly increasing the parameter size of the vocabulary embeddings. The second technique is cross-layer parameter sharing. This technique prevents the parameter from growing with the depth of the network. \r
\r
Additionally, ALBERT utilises a self-supervised loss for sentence-order prediction (SOP). SOP primary focuses on inter-sentence coherence and is designed to address the ineffectiveness of the next sentence prediction (NSP) loss proposed in the original BERT.""" ;
    skos:prefLabel "ALBERT" .

:ALCN a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1809.09533v3> ;
    rdfs:seeAlso <https://github.com/btekgit/FocusingNeuron/blob/7510c72763dc2458a35fa7a116bdf5745f9ef376/Kfocusing.py#L19> ;
    skos:altLabel "Adaptive Locally Connected Neuron" ;
    skos:definition """The **Adaptive Locally Connected Neuron (ALCN)** is a topology aware, and locally adaptive -focusing neuron:\r
\r
$$a =  f\\:\\Bigg( \\sum_{i=1}^{m} w_{i}\\phi\\left( \\tau\\left(i\\right),\\Theta\\right) x_{i} + b \\Bigg) %f\\:\\Bigg(\\mathbf{X(W \\circ \\Phi) + b} \\Bigg) $$""" ;
    skos:prefLabel "ALCN" .

:ALDA a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2001.01046v1> ;
    skos:definition "**Adversarial-Learned Loss for Domain Adaptation** is a method for domain adaptation that combines adversarial learning with self-training. Specifically, the domain discriminator has to produce different corrected labels for different domains, while the feature generator aims to confuse the domain discriminator. The adversarial process finally leads to a proper confusion matrix on the target domain. In this way, ALDA takes the strengths of domain-adversarial learning and self-training based methods." ;
    skos:prefLabel "ALDA" .

:ALDEN a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2108.10687v1> ;
    skos:definition """**ALDEN**, or **Active Learning with DivErse iNterpretations**, is an active learning approach for text classification. With local interpretations in DNNs, ALDEN identifies linearly separable regions of samples. Then, it selects samples according to their diversity of local interpretations and queries their labels.\r
\r
Specifically, we first calculate the local interpretations in DNN for each sample as the gradient backpropagated from the final\r
predictions to the input features. Then, we use the most diverse interpretation of words in a sample to measure its diverseness. Accordingly, we select unlabeled samples with the maximally diverse interpretations for labeling and retrain the model with these\r
labeled samples.""" ;
    skos:prefLabel "ALDEN" .

:ALI a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1606.00704v3> ;
    rdfs:seeAlso <https://github.com/IshmaelBelghazi/ALI> ;
    skos:altLabel "Adversarially Learned Inference" ;
    skos:definition """**Adversarially Learned Inference (ALI)** is a generative modelling approach that casts the learning of both an inference machine (or encoder) and a deep directed generative model (or decoder) in an GAN-like adversarial framework. A discriminator is trained to discriminate joint samples of the data and the corresponding latent variable from the encoder (or approximate posterior) from joint samples from the decoder while in opposition, the encoder and the decoder are trained together to fool the discriminator. Not is the discriminator asked to distinguish synthetic samples from real data, but it is required it to distinguish between two joint distributions over the data space and the latent variables.\r
\r
An ALI differs from a [GAN](https://paperswithcode.com/method/gan) in two ways:\r
\r
- The generator has two components: the encoder, $G\\_{z}\\left(\\mathbf{x}\\right)$, which maps data samples $x$ to $z$-space, and the decoder $G\\_{x}\\left(\\mathbf{z}\\right)$, which maps samples from the prior $p\\left(\\mathbf{z}\\right)$ (a source of noise) to the input space.\r
- The discriminator is trained to distinguish between joint pairs $\\left(\\mathbf{x}, \\tilde{\\mathbf{z}} = G\\_{\\mathbf{x}}\\left(\\mathbf{x}\\right)\\right)$ and $\\left(\\tilde{\\mathbf{x}} =\r
G\\_{x}\\left(\\mathbf{z}\\right), \\mathbf{z}\\right)$, as opposed to marginal samples $\\mathbf{x} \\sim q\\left(\\mathbf{x}\\right)$ and $\\tilde{\\mathbf{x}} ∼ p\\left(\\mathbf{x}\\right)$.""" ;
    skos:prefLabel "ALI" .

:ALIGN a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2102.05918v2> ;
    skos:definition "In the ALIGN method, visual and language representations are jointly trained from noisy image alt-text data. The image and text encoders are learned via contrastive loss (formulated as normalized softmax) that pushes the embeddings of the matched image-text pair together and pushing those of non-matched image-text pair apart. The model learns to align visual and language representations of the image and text pairs using the contrastive loss. The representations can be used for vision-only or vision-language task transfer. Without any fine-tuning, ALIGN powers zero-shot visual classification and cross-modal search including image-to-text search, text-to image search and even search with joint image+text queries." ;
    skos:prefLabel "ALIGN" .

:ALIS a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2104.06954v1> ;
    skos:altLabel "Aligning Latent and Image Spaces" ;
    skos:definition "An infinite image generator which is based on a patch-wise, periodically equivariant generator." ;
    skos:prefLabel "ALIS" .

:ALP-GMM a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1910.07224v1> ;
    skos:altLabel "Absolute Learning Progress and Gaussian Mixture Models for Automatic Curriculum Learning" ;
    skos:definition "ALP-GMM is is an algorithm that learns to generate a learning curriculum for black box reinforcement learning agents, whereby it sequentially samples parameters controlling a stochastic procedural generation of tasks or environments." ;
    skos:prefLabel "ALP-GMM" .

:ALQandAMQ a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2010.12460v1> ;
    rdfs:seeAlso <https://github.com/tabrizian/learning-to-quantize> ;
    skos:altLabel "Gradient Quantization with Adaptive Levels/Multiplier" ;
    skos:definition "Many communication-efficient variants of [SGD](https://paperswithcode.com/method/sgd) use gradient quantization schemes. These schemes are often heuristic and fixed over the course of training. We empirically observe that the statistics of gradients of deep models change during the training. Motivated by this observation, we introduce two adaptive quantization schemes, ALQ and AMQ. In both schemes, processors update their compression schemes in parallel by efficiently computing sufficient statistics of a parametric distribution. We improve the validation accuracy by almost 2% on CIFAR-10 and 1% on ImageNet in challenging low-cost communication setups. Our adaptive methods are also significantly more robust to the choice of hyperparameters." ;
    skos:prefLabel "ALQ and AMQ" .

:ALS a skos:Concept ;
    dcterms:source <http://proceedings.mlr.press/v123/kim20a.html> ;
    skos:altLabel "Adaptive Label Smoothing" ;
    skos:definition "" ;
    skos:prefLabel "ALS" .

:ALiBi a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2108.12409v2> ;
    skos:altLabel "Attention with Linear Biases" ;
    skos:definition """**ALiBi**, or **Attention with Linear Biases**, is a [positioning method](https://paperswithcode.com/methods/category/position-embeddings) that allows [Transformer](https://paperswithcode.com/methods/category/transformers) language models to consume, at inference time, sequences which are longer than the ones they were trained on. \r
\r
ALiBi does this without using actual position embeddings. Instead, computing the attention between a certain key and query, ALiBi penalizes the attention value that that query can assign to the key depending on how far away the key and query are. So when a key and query are close by, the penalty is very low, and when they are far away, the penalty is very high. \r
\r
This method was motivated by the simple reasoning that words that are close-by matter much more than ones that are  far away.\r
\r
This method is as fast as the sinusoidal or absolute embedding methods (the fastest positioning methods there are). It outperforms those methods and Rotary embeddings when evaluating sequences that are longer than the ones the model was trained on (this is known as extrapolation).""" ;
    skos:prefLabel "ALiBi" .

:AM a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1803.08475v3> ;
    skos:altLabel "Attention Model" ;
    skos:definition "" ;
    skos:prefLabel "AM" .

:AMP a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2010.04925v4> ;
    rdfs:seeAlso <https://github.com/hiyouga/AMP-Regularizer> ;
    skos:altLabel "Adversarial Model Perturbation" ;
    skos:definition "Based on the understanding that the flat local minima of the empirical risk cause the model to generalize better. Adversarial Model Perturbation (AMP) improves generalization via minimizing the **AMP loss**, which is obtained from the empirical risk by applying the **worst** norm-bounded perturbation on each point in the parameter space." ;
    skos:prefLabel "AMP" .

:AMSBound a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1902.09843v1> ;
    rdfs:seeAlso <https://github.com/jettify/pytorch-optimizer/blob/155246597d66dd774156599be0f07a8c6f7758aa/torch_optimizer/adabound.py#L11> ;
    skos:definition """**AMSBound** is a variant of the [AMSGrad](https://paperswithcode.com/method/amsgrad) stochastic optimizer which is designed to be more robust to extreme learning rates. Dynamic bounds are employed on learning rates, where the lower and upper bound are initialized as zero and infinity respectively, and they both smoothly converge to a constant final step size. AMSBound can be regarded as an adaptive method at the beginning of training, and it gradually and smoothly transforms to [SGD](https://paperswithcode.com/method/sgd) (or with momentum) as time step increases. \r
\r
$$ g\\_{t} = \\nabla{f}\\_{t}\\left(x\\_{t}\\right) $$\r
\r
$$ m\\_{t} = \\beta\\_{1t}m\\_{t-1} + \\left(1-\\beta\\_{1t}\\right)g\\_{t} $$\r
\r
$$ v\\_{t} = \\beta\\_{2}v\\_{t-1} + \\left(1-\\beta\\_{2}\\right)g\\_{t}^{2}$$\r
\r
$$ \\hat{v}\\_{t} = \\max\\left(\\hat{v}\\_{t-1}, v\\_{t}\\right) \\text{ and } V\\_{t} = \\text{diag}\\left(\\hat{v}\\_{t}\\right) $$\r
\r
$$ \\eta = \\text{Clip}\\left(\\alpha/\\sqrt{V\\_{t}}, \\eta\\_{l}\\left(t\\right), \\eta\\_{u}\\left(t\\right)\\right) \\text{ and } \\eta\\_{t} = \\eta/\\sqrt{t} $$\r
\r
$$ x\\_{t+1} = \\Pi\\_{\\mathcal{F}, \\text{diag}\\left(\\eta\\_{t}^{-1}\\right)}\\left(x\\_{t} - \\eta\\_{t} \\odot m\\_{t} \\right) $$\r
\r
Where $\\alpha$ is the initial step size, and $\\eta_{l}$ and $\\eta_{u}$ are the lower and upper bound functions respectively.""" ;
    skos:prefLabel "AMSBound" .

:AMSGrad a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1904.09237v1> ;
    rdfs:seeAlso <https://github.com/pytorch/pytorch/blob/b7bda236d18815052378c88081f64935427d7716/torch/optim/adam.py#L6> ;
    skos:definition """**AMSGrad** is a stochastic optimization method that seeks to fix a convergence issue with [Adam](https://paperswithcode.com/method/adam) based optimizers. AMSGrad uses the maximum of past squared gradients \r
$v\\_{t}$ rather than the exponential average to update the parameters:\r
\r
$$m\\_{t} = \\beta\\_{1}m\\_{t-1} + \\left(1-\\beta\\_{1}\\right)g\\_{t} $$\r
\r
$$v\\_{t} = \\beta\\_{2}v\\_{t-1} + \\left(1-\\beta\\_{2}\\right)g\\_{t}^{2}$$\r
\r
$$ \\hat{v}\\_{t} = \\max\\left(\\hat{v}\\_{t-1}, v\\_{t}\\right) $$\r
\r
$$\\theta\\_{t+1} = \\theta\\_{t} - \\frac{\\eta}{\\sqrt{\\hat{v}_{t}} + \\epsilon}m\\_{t}$$""" ;
    skos:prefLabel "AMSGrad" .

:APPO a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2006.11751v2> ;
    skos:altLabel "Asynchronous Proximal Policy Optimization" ;
    skos:definition "" ;
    skos:prefLabel "APPO" .

:ARCH a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2004.04572v2> ;
    skos:altLabel "Animatable Reconstruction of Clothed Humans" ;
    skos:definition "**Animatable Reconstruction of Clothed Humans** is an end-to-end framework for accurate reconstruction of animation-ready 3D clothed humans from a monocular image. ARCH is a learned pose-aware model that produces detailed 3D rigged full-body human avatars from a single unconstrained RGB image. A Semantic Space and a Semantic Deformation Field are created using a parametric 3D body estimator. They allow the transformation of 2D/3D clothed humans into a canonical space, reducing ambiguities in geometry caused by pose variations and occlusions in training data. Detailed surface geometry and appearance are learned using an implicit function representation with spatial local features." ;
    skos:prefLabel "ARCH" .

:ARM-Net a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2107.01830v1> ;
    skos:definition "ARM-Net is an adaptive relation modeling network tailored for structured data, and a lightweight framework ARMOR based on ARM-Net for relational data analytics. The key idea is to model feature interactions with cross features selectively and dynamically, by first transforming the input features into exponential space, and then determining the interaction order and interaction weights adaptively for each cross feature. The authors propose a novel sparse attention mechanism to dynamically generate the interaction weights given the input tuple, so that we can explicitly model cross features of arbitrary orders with noisy features filtered selectively. Then during model inference, ARM-Net can specify the cross features being used for each prediction for higher accuracy and better interpretability." ;
    skos:prefLabel "ARM-Net" .

:ARMA a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1901.01343v7> ;
    skos:altLabel "ARMA GNN" ;
    skos:definition "The ARMA GNN layer implements a rational graph filter with a recursive approximation." ;
    skos:prefLabel "ARMA" .

:ARShoe a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2108.10515v1> ;
    skos:definition "**ARShoe** is a multi-branch network for pose estimation and segmentation tackling the \"try-on\" problem for augmented reality shoes. Consisting of an encoder and a decoder, the multi-branch network is trained to predict keypoints [heatmap](https://paperswithcode.com/method/heatmap) (heatmap), [PAFs](https://paperswithcode.com/method/pafs) heatmap (pafmap), and segmentation results (segmap) simultaneously. Post processes are then performed for a smooth and realistic virtual try-on." ;
    skos:prefLabel "ARShoe" .

:ARiA a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1805.08878v1> ;
    skos:altLabel "Adaptive Richard's Curve Weighted Activation" ;
    skos:definition "This work introduces a novel activation unit that can be efficiently employed in deep neural nets (DNNs) and performs significantly better than the traditional Rectified Linear Units ([ReLU](https://paperswithcode.com/method/relu)). The function developed is a two parameter version of the specialized Richard's Curve and we call it Adaptive Richard's Curve weighted Activation (ARiA). This function is non-monotonous, analogous to the newly introduced [Swish](https://paperswithcode.com/method/swish), however allows a precise control over its non-monotonous convexity by varying the hyper-parameters. We first demonstrate the mathematical significance of the two parameter ARiA followed by its application to benchmark problems such as MNIST, CIFAR-10 and CIFAR-100, where we compare the performance with ReLU and Swish units. Our results illustrate a significantly superior performance on all these datasets, making ARiA a potential replacement for ReLU and other activations in DNNs." ;
    skos:prefLabel "ARiA" .

:ASAF a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2006.13258v6> ;
    skos:altLabel "Adaptive Spline Activation Function" ;
    skos:definition """Stefano Guarnieri, Francesco Piazza, and Aurelio Uncini \r
"Multilayer Feedforward Networks with Adaptive Spline Activation Function," \r
 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 10, NO. 3, MAY 1999\r
\r
Abstract — In this paper, a new adaptive spline activation function neural network (ASNN) is presented. Due to the ASNN’s high representation capabilities, networks with a small number of interconnections can be trained to solve both pattern recognition and data processing real-time problems. The main idea is to use a Catmull–Rom cubic spline as the neuron’s activation function, which ensures a simple structure suitable for both software and hardware implementation. Experimental results demonstrate improvements in terms of generalization capability\r
and of learning speed in both pattern recognition and data processing tasks.\r
Index Terms— Adaptive activation functions, function shape autotuning, generalization, generalized sigmoidal functions, multilayer\r
perceptron, neural networks, spline neural networks.""" ;
    skos:prefLabel "ASAF" .

:ASFF a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1911.09516v2> ;
    rdfs:seeAlso <https://github.com/ruinmessi/ASFF/blob/09f558863ae297663ec476f52a90c0cf6881415e/models/network_blocks.py#L267> ;
    skos:altLabel "Adaptively Spatial Feature Fusion" ;
    skos:definition """**ASFF**, or **Adaptively Spatial Feature Fusion**, is a method for pyramidal feature fusion. It learns the way to spatially filter conflictive information to suppress inconsistency across different feature scales, thus improving the scale-invariance of features. \r
\r
ASFF enables the network to directly learn how to spatially filter features at other levels so that only useful information is kept for combination. For the features at a certain level, features of other levels are first integrated and resized into the same resolution and then trained to find the optimal fusion. At each spatial location, features at different levels are fused adaptively, *i.e.*, some features may be filter out as they carry contradictory information at this location and some may dominate with more discriminative clues. ASFF offers several advantages: (1) as the operation of searching the optimal fusion is differential, it can be conveniently learned in back-propagation; (2) it is agnostic to the backbone model and it is applied to single-shot detectors that have a feature pyramid structure; and (3) its implementation is simple and the increased computational cost is marginal.\r
\r
Let $\\mathbf{x}_{ij}^{n\\rightarrow l}$ denote the feature vector at the position $(i,j)$ on the feature maps resized from level $n$ to level $l$. Following a feature resizing stage, we fuse the features at the corresponding level $l$ as follows:\r
\r
$$\r
\\mathbf{y}\\_{ij}^l = \\alpha^l_{ij} \\cdot \\mathbf{x}\\_{ij}^{1\\rightarrow l} + \\beta^l_{ij} \\cdot \\mathbf{x}\\_{ij}^{2\\rightarrow l} +\\gamma^l\\_{ij} \\cdot \\mathbf{x}\\_{ij}^{3\\rightarrow l},\r
$$\r
\r
where $\\mathbf{y}\\_{ij}^l$ implies the $(i,j)$-th vector of the output feature maps $\\mathbf{y}^l$ among channels. $\\alpha^l\\_{ij}$, $\\beta^l\\_{ij}$ and $\\gamma^l\\_{ij}$ refer to the spatial importance weights for the feature maps at three different levels to level $l$, which are adaptively learned by the network. Note that $\\alpha^l\\_{ij}$, $\\beta^l\\_{ij}$ and $\\gamma^l\\_{ij}$ can be simple scalar variables, which are shared across all the channels. Inspired by acnet, we force $\\alpha^l\\_{ij}+\\beta^l\\_{ij}+\\gamma^l\\_{ij}=1$ and $\\alpha^l\\_{ij},\\beta^l\\_{ij},\\gamma^l\\_{ij} \\in [0,1]$, and \r
\r
$$\r
	\\alpha^l_{ij} = \\frac{e^{\\lambda^l\\_{\\alpha\\_{ij}}}}{e^{\\lambda^l\\_{\\alpha_{ij}}} + e^{\\lambda^l\\_{\\beta_{ij}\r
		}} + e^{\\lambda^l\\_{\\gamma_{ij}}}}.\r
$$\r
\r
Here $\\alpha^l\\_{ij}$, $\\beta^l\\_{ij}$ and $\\gamma^l\\_{ij}$ are defined by using the [softmax](https://paperswithcode.com/method/softmax) function with $\\lambda^l\\_{\\alpha_{ij}}$, $\\lambda^l\\_{\\beta_{ij}}$ and $\\lambda^l\\_{\\gamma_{ij}}$ as control parameters respectively. We use $1\\times1$ [convolution](https://paperswithcode.com/method/convolution) layers to compute the weight scalar maps $\\mathbf{\\lambda}^l_\\alpha$, $\\mathbf{\\lambda}^l\\_\\beta$ and $\\mathbf{\\lambda}^l\\_\\gamma$ from $\\mathbf{x}^{1\\rightarrow l}$, $\\mathbf{x}^{2\\rightarrow l}$ and $\\mathbf{x}^{3\\rightarrow l}$ respectively, and they can thus be learned through standard back-propagation.\r
\r
With this method, the features at all the levels are adaptively aggregated at each scale. The outputs are used for object detection following the same pipeline of [YOLOv3](https://paperswithcode.com/method/yolov3).""" ;
    skos:prefLabel "ASFF" .

:ASLFeat a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2003.10071v2> ;
    skos:definition "**ASLFeat** is a convolutional neural network for learning local features that uses deformable convolutional networks to densely estimate and apply local transformation. It also takes advantage of the inherent feature hierarchy to restore spatial resolution and low-level details for accurate keypoint localization. Finally, it uses a peakiness measurement to relate feature responses and derive more indicative detection scores." ;
    skos:prefLabel "ASLFeat" .

:ASPP a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1606.00915v2> ;
    skos:altLabel "Atrous Spatial Pyramid Pooling" ;
    skos:definition "**Atrous Spatial Pyramid Pooling (ASPP)** is a semantic segmentation module for resampling a given feature layer at multiple rates prior to [convolution](https://paperswithcode.com/method/convolution). This amounts to probing the original image with multiple filters that have complementary effective fields of view, thus capturing objects as well as useful image context at multiple scales. Rather than actually resampling features, the mapping is implemented using multiple parallel atrous convolutional layers with different sampling rates." ;
    skos:prefLabel "ASPP" .

:ASU a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2010.05365v1> ;
    skos:altLabel "Amplifying Sine Unit: An Oscillatory Activation Function for Deep Neural Networks to Recover Nonlinear Oscillations Efficiently" ;
    skos:definition "2023" ;
    skos:prefLabel "ASU" .

:ASVI a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2002.00643v3> ;
    skos:altLabel "Automatic Structured Variational Inference" ;
    skos:definition "**Automatic Structured Variational Inference (ASVI)** is a fully automated method for constructing structured variational families, inspired by the closed-form update in conjugate Bayesian models. These convex-update families incorporate the forward pass of the input probabilistic program and can therefore capture complex statistical dependencies. Convex-update families have the same space and time complexity as the input probabilistic program and are therefore tractable for a very large family of models including both continuous and discrete variables." ;
    skos:prefLabel "ASVI" .

:ATMO a skos:Concept ;
    dcterms:source <https://www.mdpi.com/1999-4893/14/6/186> ;
    skos:altLabel "AdapTive Meta Optimizer" ;
    skos:definition """This method combines multiple optimization techniques like [ADAM](https://paperswithcode.com/method/adam) and [SGD](https://paperswithcode.com/method/sgd) or PADAM. This method can be applied to any couple of optimizers.\r
\r
Image credit: [Combining Optimization Methods Using an Adaptive Meta Optimizer](https://www.mdpi.com/1999-4893/14/6/186)""" ;
    skos:prefLabel "ATMO" .

:ATSS a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1912.02424v4> ;
    rdfs:seeAlso <https://github.com/sfzhang15/ATSS/blob/79dfb28bd18c931dd75a3ca2c63d32f5e4b1626a/atss_core/modeling/rpn/atss/atss.py#L194> ;
    skos:altLabel "Adaptive Training Sample Selection" ;
    skos:definition """**Adaptive Training Sample Selection**, or **ATSS**, is a method to automatically select positive and negative samples according to statistical characteristics of object. It bridges the gap between anchor-based and anchor-free detectors. \r
\r
For each ground-truth box $g$ on the image, we first find out its candidate positive samples. As described in Line $3$ to $6$, on each pyramid level, we select $k$ anchor boxes whose center are closest to the center of $g$ based on L2 distance. Supposing there are $\\mathcal{L}$ feature pyramid levels, the ground-truth box $g$ will have $k\\times\\mathcal{L}$ candidate positive samples. After that, we compute the IoU between these candidates and the ground-truth $g$ as $\\mathcal{D}_g$ in Line $7$, whose mean and standard deviation are computed as $m_g$ and $v_g$ in Line $8$ and Line $9$. With these statistics, the IoU threshold for this ground-truth $g$ is obtained as $t_g=m_g+v_g$ in Line $10$. Finally, we select these candidates whose IoU are greater than or equal to the threshold $t_g$ as final positive samples in Line $11$ to $15$. \r
\r
Notably ATSS also limits the positive samples' center to the ground-truth box as shown in Line $12$. Besides, if an anchor box is assigned to multiple ground-truth boxes, the one with the highest IoU will be selected. The rest are negative samples.""" ;
    skos:prefLabel "ATSS" .

:AUCC a skos:Concept ;
    skos:altLabel "Area Under the ROC Curve for Clustering" ;
    skos:definition "The area under the receiver operating characteristics (ROC) Curve, referred to as AUC, is a well-known performance measure in the supervised learning domain. Due to its compelling features, it has been employed in a number of studies to evaluate and compare the performance of different classifiers. In this work, we explore AUC as a performance measure in the unsupervised learning domain, more specifically, in the context of cluster analysis. In particular, we elaborate on the use of AUC as an internal/relative measure of clustering quality, which we refer to as Area Under the Curve for Clustering (AUCC). We show that the AUCC of a given candidate clustering solution has an expected value under a null model of random clustering solutions, regardless of the size of the dataset and, more importantly, regardless of the number or the (im)balance of clusters under evaluation. In addition, we elaborate on the fact that, in the context of internal/relative clustering validation as we consider, AUCC is actually a linear transformation of the Gamma criterion from Baker and Hubert (1975), for which we also formally derive a theoretical expected value for chance clusterings. We also discuss the computational complexity of these criteria and show that, while an ordinary implementation of Gamma can be computationally prohibitive and impractical for most real applications of cluster analysis, its equivalence with AUCC actually unveils a much more efficient algorithmic procedure. Our theoretical findings are supported by experimental results. These results show that, in addition to an effective and robust quantitative evaluation provided by AUCC, visual inspection of the ROC curves themselves can be useful to further assess a candidate clustering solution from a broader, qualitative perspective as well." ;
    skos:prefLabel "AUCC" .

:AUCOResNet a skos:Concept ;
    dcterms:source <https://www.sciencedirect.com/science/article/pii/S0031320322001376> ;
    rdfs:seeAlso <https://github.com/vincenzodentamaro/aucoresnet> ;
    skos:altLabel "Auditory Cortex ResNet" ;
    skos:definition "The Auditory Cortex ResNet, briefly AUCO ResNet, is proposed and tested. It is a deep neural network architecture especially designed for audio classification trained end-to-end. It is inspired by the architectural organization of rat's auditory cortex, containing also innovations 2 and 3. The network outperforms the state-of-the-art accuracies on a reference audio benchmark dataset without any kind of preprocessing, imbalanced data handling and, most importantly, any kind of data augmentation." ;
    skos:prefLabel "AUCO ResNet" .

:AVSlowFast a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2001.08740v2> ;
    skos:altLabel "Audiovisual SlowFast Network" ;
    skos:definition "**Audiovisual SlowFast Network**, or **AVSlowFast**, is an architecture for integrated audiovisual perception. AVSlowFast has Slow and Fast visual pathways that are integrated with a Faster Audio pathway to model vision and sound in a unified representation. Audio and visual features are fused at multiple layers, enabling audio to contribute to the formation of hierarchical audiovisual concepts. To overcome training difficulties that arise from different learning dynamics for audio and visual modalities, [DropPathway](https://paperswithcode.com/method/droppathway) is used, which randomly drops the Audio pathway during training as an effective regularization technique. Inspired by prior studies in neuroscience, hierarchical audiovisual synchronization is performed to learn joint audiovisual features." ;
    skos:prefLabel "AVSlowFast" .

:AWARE a skos:Concept ;
    skos:altLabel "Attentive Walk-Aggregating Graph Neural Network" ;
    skos:definition "We propose to theoretically and empirically examine the effect of incorporating weighting schemes into walk-aggregating GNNs. To this end, we propose a simple, interpretable, and end-to-end supervised GNN model, called AWARE (Attentive Walk-Aggregating GRaph Neural NEtwork), for graph-level prediction. AWARE aggregates the walk information by means of weighting schemes at distinct levels (vertex-, walk-, and graph-level) in a principled manner. By virtue of the incorporated weighting schemes at these different levels, AWARE can emphasize the information important for prediction while diminishing the irrelevant ones—leading to representations that can improve learning performance." ;
    skos:prefLabel "AWARE" .

:AWD-LSTM a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1708.02182v1> ;
    skos:altLabel "ASGD Weight-Dropped LSTM" ;
    skos:definition "**ASGD Weight-Dropped LSTM**, or **AWD-LSTM**, is a type of recurrent neural network that employs [DropConnect](https://paperswithcode.com/method/dropconnect) for regularization, as well as [NT-ASGD](https://paperswithcode.com/method/nt-asgd) for optimization - non-monotonically triggered averaged [SGD](https://paperswithcode.com/method/sgd) - which returns an average of last iterations of weights. Additional regularization techniques employed include variable length backpropagation sequences, [variational dropout](https://paperswithcode.com/method/variational-dropout), [embedding dropout](https://paperswithcode.com/method/embedding-dropout), [weight tying](https://paperswithcode.com/method/weight-tying), independent embedding/hidden size, [activation regularization](https://paperswithcode.com/method/activation-regularization) and [temporal activation regularization](https://paperswithcode.com/method/temporal-activation-regularization)." ;
    skos:prefLabel "AWD-LSTM" .

:AbsolutePositionEncodings a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1706.03762v7> ;
    skos:definition """**Absolute Position Encodings** are a type of position embeddings for [[Transformer](https://paperswithcode.com/method/transformer)-based models] where positional encodings are added to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension $d\\_{model}$ as the embeddings, so that the two can be summed. In the original implementation, sine and cosine functions of different frequencies are used:\r
\r
$$ \\text{PE}\\left(pos, 2i\\right) = \\sin\\left(pos/10000^{2i/d\\_{model}}\\right) $$\r
\r
$$ \\text{PE}\\left(pos, 2i+1\\right) = \\cos\\left(pos/10000^{2i/d\\_{model}}\\right) $$\r
\r
where $pos$ is the position and $i$ is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from $2\\pi$ to $10000 \\dot 2\\pi$. This function was chosen because the authors hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset $k$,  $\\text{PE}\\_{pos+k}$ can be represented as a linear function of $\\text{PE}\\_{pos}$.\r
\r
Image Source: [D2L.ai](https://d2l.ai/chapter_attention-mechanisms/self-attention-and-positional-encoding.html)""" ;
    skos:prefLabel "Absolute Position Encodings" .

:AccoMontage a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2108.11213v1> ;
    skos:definition "**AccoMontage** is a model for accompaniment arrangement, a type of music generation task involving intertwined constraints of melody, harmony, texture, and music structure. AccoMontage generates piano accompaniments for folk/pop songs based on a lead sheet (i.e. a melody with chord progression). It first retrieves phrase montages from a database while recombining them structurally using dynamic programming. Second, chords of the retrieved phrases are manipulated to match the lead sheet via style transfer. Lastly, the system offers controls over the generation process. In contrast to pure deep learning approaches, AccoMontage uses a hybrid pathway, in which rule-based optimization and deep learning are both leveraged." ;
    skos:prefLabel "AccoMontage" .

:Accordion a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2010.16248v1> ;
    skos:definition "**Accordion** is a gradient communication scheduling algorithm that is generic across models while imposing low computational overheads. Accordion inspects the change in the gradient norms to detect critical regimes and adjusts the communication schedule dynamically. Accordion works for both adjusting the gradient compression rate or the batch size without additional parameter tuning." ;
    skos:prefLabel "Accordion" .

:AccumulatingEligibilityTrace a skos:Concept ;
    skos:definition """An **Accumulating Eligibility Trace** is a type of [eligibility trace](https://paperswithcode.com/method/eligibility-trace) where the trace increments in an accumulative way. For the memory vector $\\textbf{e}\\_{t} \\in \\mathbb{R}^{b} \\geq \\textbf{0}$:\r
\r
$$\\mathbf{e\\_{0}} = \\textbf{0}$$\r
\r
$$\\textbf{e}\\_{t} = \\nabla{\\hat{v}}\\left(S\\_{t}, \\mathbf{\\theta}\\_{t}\\right) + \\gamma\\lambda\\textbf{e}\\_{t}$$""" ;
    skos:prefLabel "Accumulating Eligibility Trace" .

:Accuracy-RobustnessArea\(ARA\) a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1906.02896v2> ;
    skos:altLabel "Accuracy-Robustness Area" ;
    skos:definition "In the space of adversarial perturbation against classifier accuracy, the ARA is the area between a classifier's curve and the straight line defined by a naive classifier's maximum accuracy. Intuitively, the ARA measures a combination of the classifier’s predictive power and its ability to overcome an adversary. Importantly, when contrasted against existing robustness metrics, the ARA takes into account the classifier’s performance against all adversarial examples, without  bounding them by some arbitrary $\\epsilon$." ;
    skos:prefLabel "Accuracy-Robustness Area (ARA)" .

:ActivationNormalization a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1807.03039v2> ;
    rdfs:seeAlso <https://github.com/openai/glow/blob/1f1352977cb1b21c7c0aa83b08efb24dfc216663/tfops.py#L70> ;
    skos:definition "**Activation Normalization** is a type of normalization used for flow-based generative models; specifically it was introduced in the [GLOW](https://paperswithcode.com/method/glow) architecture. An ActNorm layer performs an affine transformation of the activations using a scale and bias parameter per channel, similar to [batch normalization](https://paperswithcode.com/method/batch-normalization). These parameters are initialized such that the post-actnorm activations per-channel have zero mean and unit variance given an initial minibatch of data. This is a form of data dependent initilization. After initialization, the scale and bias are treated as regular trainable parameters that are independent of the data." ;
    skos:prefLabel "Activation Normalization" .

:ActivationRegularization a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1708.01009v1> ;
    rdfs:seeAlso <https://github.com/salesforce/awd-lstm-lm/blob/32fcb42562aeb5c7e6c9dec3f2a3baaaf68a5cb5/main.py#L200> ;
    skos:definition """**Activation Regularization (AR)**, or $L\\_{2}$ activation regularization, is regularization performed on activations as opposed to weights. It is usually used in conjunction with [RNNs](https://paperswithcode.com/methods/category/recurrent-neural-networks). It is defined as:\r
\r
$$\\alpha{L}\\_{2}\\left(m\\circ{h\\_{t}}\\right) $$\r
\r
where $m$ is a [dropout](https://paperswithcode.com/method/dropout) mask used by later parts of the model, $L\\_{2}$ is the $L\\_{2}$ norm, and $h_{t}$ is the output of an RNN at timestep $t$, and $\\alpha$ is a scaling coefficient. \r
\r
When applied to the output of a dense layer, AR penalizes activations that are substantially away from 0, encouraging activations to remain small.""" ;
    skos:prefLabel "Activation Regularization" .

:ActiveConvolution a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1703.09076v1> ;
    rdfs:seeAlso <https://github.com/jyh2986/Active-Convolution> ;
    skos:definition "An **Active Convolution** is a type of [convolution](https://paperswithcode.com/method/convolution) which does not have a fixed shape of the receptive field, and can be used to take more diverse forms of receptive fields for convolutions. Its shape can be learned through backpropagation during training. It can be seen as a generalization of convolution; it can define not only all conventional convolutions, but also convolutions with fractional pixel coordinates. We can freely change the shape of the convolution, which provides greater freedom to form CNN structures. Second, the shape of the convolution is learned while training and there is no need to tune it by hand" ;
    skos:prefLabel "Active Convolution" .

:AdaBound a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1902.09843v1> ;
    rdfs:seeAlso <https://github.com/Luolc/AdaBound/blob/2e928c3007a2fc44af0e4c97e343e1fed6986e44/adabound/adabound.py#L6> ;
    skos:definition """**AdaBound** is a variant of the [Adam](https://paperswithcode.com/method/adabound) stochastic optimizer which is designed to be more robust to extreme learning rates. Dynamic bounds are employed on learning rates, where the lower and upper bound are initialized as zero and infinity respectively, and they both smoothly converge to a constant final step size. AdaBound can be regarded as an adaptive method at the beginning of training, and thereafter it gradually and smoothly transforms to [SGD](https://paperswithcode.com/method/sgd) (or with momentum) as the time step increases. \r
\r
$$ g\\_{t} = \\nabla{f}\\_{t}\\left(x\\_{t}\\right) $$\r
\r
$$ m\\_{t} = \\beta\\_{1t}m\\_{t-1} + \\left(1-\\beta\\_{1t}\\right)g\\_{t} $$\r
\r
$$ v\\_{t} = \\beta\\_{2}v\\_{t-1} + \\left(1-\\beta\\_{2}\\right)g\\_{t}^{2} \\text{ and } V\\_{t} = \\text{diag}\\left(v\\_{t}\\right) $$\r
\r
$$ \\hat{\\eta}\\_{t} = \\text{Clip}\\left(\\alpha/\\sqrt{V\\_{t}}, \\eta\\_{l}\\left(t\\right), \\eta\\_{u}\\left(t\\right)\\right) \\text{ and } \\eta\\_{t} = \\hat{\\eta}\\_{t}/\\sqrt{t} $$\r
\r
$$ x\\_{t+1} = \\Pi\\_{\\mathcal{F}, \\text{diag}\\left(\\eta\\_{t}^{-1}\\right)}\\left(x\\_{t} - \\eta\\_{t} \\odot m\\_{t} \\right) $$\r
\r
Where $\\alpha$ is the initial step size, and $\\eta_{l}$ and $\\eta_{u}$ are the lower and upper bound functions respectively.""" ;
    skos:prefLabel "AdaBound" .

:AdaDelta a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1212.5701v1> ;
    rdfs:seeAlso <https://github.com/pytorch/pytorch/blob/b7bda236d18815052378c88081f64935427d7716/torch/optim/adadelta.py#L6> ;
    skos:definition """**AdaDelta** is a stochastic optimization technique that allows for per-dimension learning rate method for [SGD](https://paperswithcode.com/method/sgd). It is an extension of [Adagrad](https://paperswithcode.com/method/adagrad) that seeks to reduce its aggressive, monotonically decreasing learning rate. Instead of accumulating all past squared gradients, Adadelta restricts the window of accumulated past gradients to a fixed size $w$.\r
\r
Instead of inefficiently storing $w$ previous squared gradients, the sum of gradients is recursively defined as a decaying average of all past squared gradients. The running average $E\\left[g^{2}\\right]\\_{t}$ at time step $t$ then depends only on the previous average and current gradient:\r
\r
$$E\\left[g^{2}\\right]\\_{t} = \\gamma{E}\\left[g^{2}\\right]\\_{t-1} + \\left(1-\\gamma\\right)g^{2}\\_{t}$$\r
\r
Usually $\\gamma$ is set to around $0.9$. Rewriting SGD updates in terms of the parameter update vector:\r
\r
$$ \\Delta\\theta_{t} = -\\eta\\cdot{g\\_{t, i}}$$\r
$$\\theta\\_{t+1}  = \\theta\\_{t} + \\Delta\\theta_{t}$$\r
\r
AdaDelta takes the form:\r
\r
$$ \\Delta\\theta_{t} = -\\frac{\\eta}{\\sqrt{E\\left[g^{2}\\right]\\_{t} + \\epsilon}}g_{t} $$\r
\r
The main advantage of AdaDelta is that we do not need to set a default learning rate.""" ;
    skos:prefLabel "AdaDelta" .

:AdaGPR a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2108.10636v3> ;
    skos:definition "**AdaGPR** is an adaptive, layer-wise graph [convolution](https://paperswithcode.com/method/convolution) model. AdaGPR applies adaptive generalized Pageranks at each layer of a [GCNII](https://paperswithcode.com/method/gcnii) model by learning to predict the coefficients of generalized Pageranks using sparse solvers." ;
    skos:prefLabel "AdaGPR" .

:AdaGrad a skos:Concept ;
    rdfs:seeAlso <https://github.com/Dawn-Of-Eve/nadir/blob/main/src/nadir/adagrad.py> ;
    skos:definition """**AdaGrad** is a stochastic optimization method that adapts the learning rate to the parameters. It performs smaller updates for parameters associated with frequently occurring features, and larger updates for parameters associated with infrequently occurring features. In its update rule, Adagrad modifies the general learning rate $\\eta$ at each time step $t$ for every parameter $\\theta\\_{i}$ based on the past gradients for $\\theta\\_{i}$: \r
\r
$$ \\theta\\_{t+1, i} = \\theta\\_{t, i} - \\frac{\\eta}{\\sqrt{G\\_{t, ii} + \\epsilon}}g\\_{t, i} $$\r
\r
The benefit of AdaGrad is that it eliminates the need to manually tune the learning rate; most leave it at a default value of $0.01$. Its main weakness is the accumulation of the squared gradients in the denominator. Since every added term is positive, the accumulated sum keeps growing during training, causing the learning rate to shrink and becoming infinitesimally small.\r
\r
Image: [Alec Radford](https://twitter.com/alecrad)""" ;
    skos:prefLabel "AdaGrad" .

:AdaHessian a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2006.00719v3> ;
    skos:altLabel "ADAHESSIAN" ;
    skos:definition "AdaHessian achieves new state-of-the-art results by a large margin as compared to other adaptive optimization methods, including variants of [ADAM](https://paperswithcode.com/method/adam). In particular, we perform extensive tests on CV, NLP, and recommendation system tasks and find that AdaHessian: (i) achieves 1.80%/1.45% higher accuracy on ResNets20/32 on Cifar10, and 5.55% higher accuracy on ImageNet as compared to ADAM; (ii) outperforms ADAMW for transformers by 0.27/0.33 BLEU score on IWSLT14/WMT14 and 1.8/1.0 PPL on PTB/Wikitext-103; and (iii) achieves 0.032% better score than [AdaGrad](https://paperswithcode.com/method/adagrad) for DLRM on the Criteo Ad Kaggle dataset. Importantly, we show that the cost per iteration of AdaHessian is comparable to first-order methods, and that it exhibits robustness towards its hyperparameters." ;
    skos:prefLabel "AdaHessian" .

:AdaMod a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1910.12249v1> ;
    rdfs:seeAlso <https://github.com/jettify/pytorch-optimizer/blob/155246597d66dd774156599be0f07a8c6f7758aa/torch_optimizer/adamod.py#L11> ;
    skos:definition """**AdaMod** is a stochastic optimizer that restricts adaptive learning rates with adaptive and momental upper bounds. The dynamic learning rate bounds are based on the exponential moving averages of the adaptive learning rates themselves, which smooth out unexpected large learning rates and stabilize the training of deep neural networks.\r
\r
\r
The weight updates are performed as:\r
\r
\r
$$ g\\_{t} = \\nabla{f}\\_{t}\\left(\\theta\\_{t-1}\\right) $$\r
\r
$$ m\\_{t} = \\beta\\_{1}m\\_{t-1} + \\left(1-\\beta\\_{1}\\right)g\\_{t} $$\r
\r
$$ v\\_{t} = \\beta\\_{2}v\\_{t-1} + \\left(1-\\beta\\_{2}\\right)g\\_{t}^{2} $$\r
\r
$$ \\hat{m}\\_{t} = m\\_{t} / \\left(1 - \\beta^{t}\\_{1}\\right)$$\r
\r
$$ \\hat{v}\\_{t} = v\\_{t} / \\left(1 - \\beta^{t}\\_{2}\\right)$$\r
\r
$$ \\eta\\_{t} = \\alpha\\_{t} / \\left(\\sqrt{\\hat{v}\\_{t}} + \\epsilon\\right) $$\r
\r
$$ s\\_{t} = \\beta\\_{3}s\\_{t-1} + (1-\\beta\\_{3})\\eta\\_{t} $$\r
\r
$$ \\hat{\\eta}\\_{t} = \\text{min}\\left(\\eta\\_{t}, s\\_{t}\\right) $$\r
\r
$$ \\theta\\_{t} = \\theta\\_{t-1} - \\hat{\\eta}\\_{t}\\hat{m}\\_{t} $$""" ;
    skos:prefLabel "AdaMod" .

:AdaRNN a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2108.04443v2> ;
    skos:definition "**AdaRNN** is an adaptive [RNN](https://paperswithcode.com/methods/category/recurrent-neural-networks) that learns an adaptive model through two modules: [Temporal Distribution Characterization](https://paperswithcode.com/method/temporal-distribution-characterization) (TDC) and [Temporal Distribution Matching](https://paperswithcode.com/method/temporal-distribution-matching) (TDM) algorithms. Firstly, to better characterize the distribution information in time-series, TDC splits the training data into $K$ most diverse periods that have a large distribution gap inspired by the principle of maximum entropy. After that, a temporal distribution matching (TDM) algorithm is used to dynamically reduce distribution divergence using a [RNN](https://paperswithcode.com/methods/category/recurrent-neural-networks)-based model." ;
    skos:prefLabel "AdaRNN" .

:AdaShift a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1810.00143v4> ;
    rdfs:seeAlso <https://github.com/MichaelKonobeev/adashift/blob/bf86b021d42e922078a39246770f0f875300a6f3/adashift/optimizers.py#L8> ;
    skos:definition """**AdaShift** is a type of adaptive stochastic optimizer that decorrelates $v\\_{t}$ and $g\\_{t}$ in [Adam](https://paperswithcode.com/method/adam) by temporal shifting, i.e., using temporally shifted gradient $g\\_{t−n}$ to calculate $v\\_{t}$. The authors argue that an inappropriate correlation between gradient $g\\_{t}$ and the second-moment term $v\\_{t}$ exists in Adam, which results in a large gradient being likely to have a small step size while a small gradient may have a large step size. The authors argue that such biased step sizes are the fundamental cause of non-convergence of Adam.\r
\r
The AdaShift updates, based on the idea of temporal independence between gradients, are as follows:\r
\r
$$ g\\_{t} = \\nabla{f\\_{t}}\\left(\\theta\\_{t}\\right) $$\r
\r
$$ m\\_{t} = \\sum^{n-1}\\_{i=0}\\beta^{i}\\_{1}g\\_{t-i}/\\sum^{n-1}\\_{i=0}\\beta^{i}\\_{1} $$\r
\r
Then for $i=1$ to $M$:\r
\r
$$ v\\_{t}\\left[i\\right] = \\beta\\_{2}v\\_{t-1}\\left[i\\right] + \\left(1-\\beta\\_{2}\\right)\\phi\\left(g^{2}\\_{t-n}\\left[i\\right]\\right) $$\r
\r
$$ \\theta\\_{t}\\left[i\\right] = \\theta\\_{t-1}\\left[i\\right] - \\alpha\\_{t}/\\sqrt{v\\_{t}\\left[i\\right]}\\cdot{m\\_{t}\\left[i\\right]} $$""" ;
    skos:prefLabel "AdaShift" .

:AdaSmooth a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2204.00825v1> ;
    skos:altLabel "Adaptive Smooth Optimizer" ;
    skos:definition """**AdaSmooth** is a stochastic optimization technique that allows for per-dimension learning rate method for [SGD](https://paperswithcode.com/method/sgd). It is an extension of [Adagrad](https://paperswithcode.com/method/adagrad) and [AdaDelta](https://paperswithcode.com/method/adadelta) that seek to reduce its aggressive, monotonically decreasing learning rate. Instead of accumulating all past squared gradients, Adadelta restricts the window of accumulated past gradients to a fixed size $w$ while AdaSmooth adaptively selects the size of the window.\r
\r
Given the window size  $M$, the effective ratio is calculated by \r
\r
$$e_t  = \\frac{s_t}{n_t}= \\frac{| x_t -  x_{t-M}|}{\\sum_{i=0}^{M-1} | x_{t-i} -  x_{t-1-i}|}\\\\\r
= \\frac{| \\sum_{i=0}^{M-1} \\Delta x_{t-1-i}|}{\\sum_{i=0}^{M-1} | \\Delta x_{t-1-i}|}.$$\r
\r
Given the effective ratio, the scaled smoothing constant is obtained by:\r
\r
$$c_t =  ( \\rho_2- \\rho_1) \\times e_t   + (1-\\rho_2),$$\r
\r
The running average $E\\left[g^{2}\\right]\\_{t}$ at time step $t$ then depends only on the previous average and current gradient:\r
\r
$$ E\\left[g^{2}\\right]\\_{t} = c_t^2 \\odot g_{t}^2  +  \\left(1-c_t^2 \\right)\\odot E[g^2]_{t-1} $$\r
\r
Usually $\\rho_1$ is set to around $0.5$ and $\\rho_2$ is set to around 0.99. The update step the follows:\r
\r
$$ \\Delta x_t = -\\frac{\\eta}{\\sqrt{E\\left[g^{2}\\right]\\_{t} + \\epsilon}} \\odot  g_{t}, $$\r
\r
which is incorporated into the final update:\r
\r
$$x_{t+1} = x_{t} + \\Delta x_t.$$\r
\r
The main advantage of AdaSmooth is its faster convergence rate and insensitivity to hyperparameters.""" ;
    skos:prefLabel "AdaSmooth" .

:AdaSqrt a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1912.09926v1> ;
    skos:definition """**AdaSqrt** is a stochastic optimization technique that is motivated by the observation that methods like [Adagrad](https://paperswithcode.com/method/adagrad) and [Adam](https://paperswithcode.com/method/adam) can be viewed as relaxations of [Natural Gradient Descent](https://paperswithcode.com/method/natural-gradient-descent).\r
\r
The updates are performed as follows:\r
\r
$$ t \\leftarrow t + 1 $$\r
\r
$$ \\alpha\\_{t} \\leftarrow \\sqrt{t} $$\r
\r
$$ g\\_{t} \\leftarrow \\nabla\\_{\\theta}f\\left(\\theta\\_{t-1}\\right) $$\r
\r
$$ S\\_{t} \\leftarrow S\\_{t-1} + g\\_{t}^{2} $$\r
\r
$$ \\theta\\_{t+1} \\leftarrow \\theta\\_{t} + \\eta\\frac{\\alpha\\_{t}g\\_{t}}{S\\_{t} + \\epsilon} $$""" ;
    skos:prefLabel "AdaSqrt" .

:Adabelief a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2010.07468v5> ;
    skos:definition "" ;
    skos:prefLabel "Adabelief" .

:Adafactor a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1804.04235v1> ;
    rdfs:seeAlso <https://github.com/DeadAt0m/adafactor-pytorch/blob/561e627239c29c0be11256171a795b49e0404098/adafactor.py#L7> ;
    skos:definition """**Adafactor** is a stochastic optimization method based on [Adam](https://paperswithcode.com/method/adam) that reduces memory usage while retaining the empirical benefits of adaptivity. This is achieved through maintaining a factored representation of the squared gradient accumulator across training steps. Specifically, by tracking moving averages of the row and column sums of the squared gradients for matrix-valued variables, we are able to reconstruct a low-rank approximation of the exponentially smoothed accumulator at each training step that is optimal with respect to the generalized Kullback-Leibler divergence. For an $n \\times m$ matrix, this reduces the memory requirements from $O(n m)$ to $O(n + m)$. \r
\r
Instead of defining the optimization algorithm in terms of absolute step sizes {$\\alpha_t$}$\\_{t=1}^T$, the authors define the optimization algorithm in terms of relative step sizes {$\\rho_t$}$\\_{t=1}^T$, which get multiplied by the scale of the parameters. The scale of a parameter vector or matrix is defined as the root-mean-square of its components, lower-bounded by a small constant $\\epsilon_2$.  The reason for this lower bound is to allow zero-initialized parameters to escape 0. \r
\r
Proposed hyperparameters are: $\\epsilon\\_{1} = 10^{-30}$, $\\epsilon\\_{2} = 10^{-3}$, $d=1$, $p\\_{t} = \\min\\left(10^{-2}, \\frac{1}{\\sqrt{t}}\\right)$, $\\hat{\\beta}\\_{2\\_{t}} = 1 - t^{-0.8}$.""" ;
    skos:prefLabel "Adafactor" .

:AdamW a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1711.05101v3> ;
    rdfs:seeAlso <https://github.com/pytorch/pytorch/blob/b7bda236d18815052378c88081f64935427d7716/torch/optim/adamw.py#L6> ;
    skos:definition """**AdamW** is a stochastic optimization method that modifies the typical implementation of weight decay in [Adam](https://paperswithcode.com/method/adam), by decoupling [weight decay](https://paperswithcode.com/method/weight-decay) from the gradient update. To see this, $L\\_{2}$ regularization in Adam is usually implemented with the below modification where $w\\_{t}$ is the rate of the weight decay at time $t$:\r
\r
$$ g\\_{t} = \\nabla{f\\left(\\theta\\_{t}\\right)} + w\\_{t}\\theta\\_{t}$$\r
\r
while AdamW adjusts the weight decay term to appear in the gradient update:\r
\r
$$ \\theta\\_{t+1, i} = \\theta\\_{t, i} - \\eta\\left(\\frac{1}{\\sqrt{\\hat{v}\\_{t} + \\epsilon}}\\cdot{\\hat{m}\\_{t}} + w\\_{t, i}\\theta\\_{t, i}\\right), \\forall{t}$$""" ;
    skos:prefLabel "AdamW" .

:Adapter a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2101.03289v5> ;
    skos:definition "" ;
    skos:prefLabel "Adapter" .

:AdaptiveBins a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2011.14141v1> ;
    skos:altLabel "Adaptive Bins" ;
    skos:definition "" ;
    skos:prefLabel "AdaptiveBins" .

:AdaptiveDropout a skos:Concept ;
    dcterms:source <http://papers.nips.cc/paper/5032-adaptive-dropout-for-training-deep-neural-networks> ;
    rdfs:seeAlso <https://github.com/mabirck/adaptative-dropout-pytorch/blob/9c81ba607c1d9c4c9845ccaf4c7b2413d89c50c1/layers.py#L5> ;
    skos:definition """**Adaptive Dropout** is a regularization technique that extends dropout by allowing the dropout probability to be different for different units. The intuition is that there may be hidden units that can individually make confident predictions for the presence or absence of an important feature or combination of features. [Dropout](https://paperswithcode.com/method/dropout) will ignore this confidence and drop the unit out 50% of the time. \r
\r
Denote the activity of unit $j$ in a deep neural network by $a\\_{j}$ and assume that its inputs are {$a\\_{i}: i < j$}. In dropout, $a\\_{j}$ is randomly set to zero with probability 0.5. Let $m\\_{j}$ be a binary variable that is used to mask, the activity $a\\_{j}$, so that its value is:\r
\r
$$ a\\_{j} = m\\_{j}g \\left( \\sum\\_{i: i<j}w\\_{j, i}a\\_{i} \\right)$$\r
\r
where $w\\_{j,i}$ is the weight from unit $i$ to unit $j$ and $g\\left(·\\right)$ is the activation function and $a\\_{0} = 1$ accounts for biases. Whereas in standard dropout, $m\\_{j}$ is Bernoulli with probability $0.5$, adaptive dropout uses adaptive dropout probabilities that depends on input activities:\r
\r
$$ P\\left(m\\_{j} = 1\\mid{\\{a\\_{i}: i < j\\}}\\right) = f \\left( \\sum\\_{i: i<j}\\pi{\\_{j, i}a\\_{i}} \\right) $$\r
\r
where $\\pi\\_{j, i}$ is the weight from unit $i$ to unit $j$ in the standout network or the adaptive dropout network; $f(·)$ is a sigmoidal function. Here 'standout' refers to a binary belief network is that is overlaid on a neural network as part of the overall regularization technique.""" ;
    skos:prefLabel "Adaptive Dropout" .

:AdaptiveFeaturePooling a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1803.01534v4> ;
    rdfs:seeAlso <https://github.com/ShuLiu1993/PANet/blob/2644d5ad6ae98c2bf58df45c8792c019b1d7b2b9/lib/model/roi_pooling/functions/roi_pool.py#L6> ;
    skos:definition """**Adaptive Feature Pooling** pools features from all levels for each proposal in object detection and fuses them for the following prediction. For each proposal, we map them to different feature levels. Following the idea of [Mask R-CNN](https://paperswithcode.com/method/adaptive-feature-pooling), [RoIAlign](https://paperswithcode.com/method/roi-align) is used to pool feature grids from each level. Then a fusion operation (element-wise max or sum) is utilized to fuse feature grids from different levels.\r
\r
The motivation for this technique is that in an [FPN](https://paperswithcode.com/method/fpn) we assign proposals to different feature levels based on the size of proposals, which could be suboptimal if images with small differences are assigned to different levels, or if the importance of features is not strongly correlated to their level which they belong.""" ;
    skos:prefLabel "Adaptive Feature Pooling" .

:AdaptiveInputRepresentations a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1809.10853v3> ;
    skos:definition "**Adaptive Input Embeddings** extend the [adaptive softmax](https://paperswithcode.com/method/adaptive-softmax) to input word representations. The factorization assigns more capacity to frequent words and reduces the capacity for less frequent words with the benefit of reducing overfitting to rare words." ;
    skos:prefLabel "Adaptive Input Representations" .

:AdaptiveInstanceNormalization a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1703.06868v2> ;
    rdfs:seeAlso <https://github.com/naoto0804/pytorch-AdaIN/blob/5eb7f9f1091cdb98bf76d775a50388805a1f0cca/function.py#L15> ;
    skos:definition """**Adaptive Instance Normalization** is a normalization method that aligns the mean and variance of the content features with those of the style features. \r
\r
[Instance Normalization](https://paperswithcode.com/method/instance-normalization) normalizes the input to a single style specified by the affine parameters. Adaptive Instance Normaliation is an extension. In AdaIN, we receive a content input $x$ and a style input $y$, and we simply align the channel-wise mean and variance of $x$ to match those of $y$. Unlike [Batch Normalization](https://paperswithcode.com/method/batch-normalization), Instance Normalization or [Conditional Instance Normalization](https://paperswithcode.com/method/conditional-instance-normalization), AdaIN has no learnable affine parameters. Instead, it adaptively computes the affine parameters from the style input:\r
\r
$$\r
\\textrm{AdaIN}(x, y)= \\sigma(y)\\left(\\frac{x-\\mu(x)}{\\sigma(x)}\\right)+\\mu(y)\r
$$""" ;
    skos:prefLabel "Adaptive Instance Normalization" .

:AdaptiveLoss a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1701.03077v10> ;
    skos:altLabel "Adaptive Robust Loss" ;
    skos:definition "The Robust Loss is a generalization of the Cauchy/Lorentzian, Geman-McClure, Welsch/Leclerc, generalized Charbonnier, Charbonnier/pseudo-Huber/L1-L2, and L2 loss functions. By introducing robustness as a continuous parameter, the loss function allows algorithms built around robust loss minimization to be generalized, which improves performance on basic vision tasks such as registration and clustering. Interpreting the loss as the negative log of a univariate density yields a general probability distribution that includes normal and Cauchy distributions as special cases. This probabilistic interpretation enables the training of neural networks in which the robustness of the loss automatically adapts itself during training, which improves performance on learning-based tasks such as generative image synthesis and unsupervised monocular depth estimation, without requiring any manual parameter tuning." ;
    skos:prefLabel "Adaptive Loss" .

:AdaptiveMasking a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1905.07799v2> ;
    rdfs:seeAlso <https://github.com/facebookresearch/adaptive-span/blob/349f7a27a822803239b7a6ad9df06e1175275a64/adaptive_span.py#L17> ;
    skos:definition """**Adaptive Masking** is a type of attention mechanism that allows a model to learn its own context size to attend over. For each head in [Multi-Head Attention](https://paperswithcode.com/method/multi-head-attention), a masking function is added to control for the span of the attention. A masking function is a non-increasing function that maps a\r
distance to a value in $\\left[0, 1\\right]$. Adaptive masking takes the following soft masking function $m\\_{z}$ parametrized by a real value $z$ in $\\left[0, S\\right]$:\r
\r
$$ m\\_{z}\\left(x\\right) = \\min\\left[\\max\\left[\\frac{1}{R}\\left(R+z-x\\right), 0\\right], 1\\right] $$\r
\r
where $R$ is a hyper-parameter that controls its softness. The shape of this piecewise function as a function of the distance. This soft masking function is inspired by [Jernite et al. (2017)](https://arxiv.org/abs/1611.06188). The attention weights from are then computed on the masked span:\r
\r
$$ a\\_{tr} = \\frac{m\\_{z}\\left(t-r\\right)\\exp\\left(s\\_{tr}\\right)}{\\sum^{t-1}\\_{q=t-S}m\\_{z}\\left(t-q\\right)\\exp\\left(s\\_{tq}\\right)}$$\r
\r
A $\\mathcal{l}\\_{1}$ penalization is added on the parameters $z\\_{i}$ for each attention head $i$ of the model to the loss function:\r
\r
$$ L = - \\log{P}\\left(w\\_{1}, \\dots, w\\_{T}\\right) + \\frac{\\lambda}{M}\\sum\\_{i}z\\_{i} $$\r
\r
where $\\lambda > 0$ is the regularization hyperparameter, and $M$ is the number of heads in each\r
layer. This formulation is differentiable in the parameters $z\\_{i}$, and learnt jointly with the rest of the model.""" ;
    skos:prefLabel "Adaptive Masking" .

:AdaptiveNMS a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1904.03629v1> ;
    skos:definition "**Adaptive Non-Maximum Suppression** is a non-maximum suppression algorithm that applies a dynamic suppression threshold to an instance according to the target density. The motivation is to find an NMS algorithm that works well for pedestrian detection in a crowd. Intuitively, a high NMS threshold keeps more crowded instances while a low NMS threshold wipes out more false positives. The adaptive-NMS thus applies a dynamic suppression strategy, where the threshold rises as instances gather and occlude each other and decays when instances appear separately. To this end, an auxiliary and learnable sub-network is designed to predict the adaptive NMS threshold for each instance." ;
    skos:prefLabel "Adaptive NMS" .

:AdaptiveSoftmax a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1609.04309v3> ;
    rdfs:seeAlso <https://github.com/rosinality/adaptive-softmax-pytorch/blob/2f02ca68d34f83867bea385c4be23a5392823f1e/adasoft.py#L6> ;
    skos:definition """**Adaptive Softmax** is a speedup technique for the computation of probability distributions over words. The adaptive [softmax](https://paperswithcode.com/method/softmax) is inspired by the class-based [hierarchical softmax](https://paperswithcode.com/method/hierarchical-softmax), where the word classes are built to minimize the computation time. Adaptive softmax achieves efficiency by explicitly taking into account the computation time of matrix-multiplication on parallel systems and combining it with a few important observations, namely keeping a shortlist of frequent words in the root node\r
and reducing the capacity of rare words.""" ;
    skos:prefLabel "Adaptive Softmax" .

:AdaptiveSpanTransformer a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1905.07799v2> ;
    skos:definition """The **Adaptive Attention Span Transformer** is a Transformer that utilises an improvement to the self-attention layer called [adaptive masking](https://paperswithcode.com/method/adaptive-masking) that allows the model to choose its own context size. This results in a network where each attention layer gathers information on their own context. This allows for scaling to input sequences of more than 8k tokens.\r
\r
Their proposals are based on the observation that, with the dense attention of a traditional [Transformer](https://paperswithcode.com/method/transformer), each attention head shares the same attention span $S$ (attending over the full context). But many attention heads can specialize to more local context (others look at the longer sequence). This motivates the need for a variant of self-attention that allows the model to choose its own context size (adaptive masking - see components).""" ;
    skos:prefLabel "Adaptive Span Transformer" .

:AdaptivelySparseTransformer a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1909.00015v2> ;
    skos:definition "The **Adaptively Sparse Transformer** is a type of [Transformer](https://paperswithcode.com/method/transformer)." ;
    skos:prefLabel "Adaptively Sparse Transformer" .

:AdditiveAttention a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1409.0473v7> ;
    rdfs:seeAlso <https://github.com/intelpro/trajectory/blob/967131ca5c16af5f6ab09fe724eae4077e1e4596/trajectron/model/components/additive_attention.py#L6> ;
    skos:definition """**Additive Attention**, also known as **Bahdanau Attention**, uses a one-hidden layer feed-forward network to calculate the attention alignment score:\r
\r
$$f_{att}\\left(\\textbf{h}_{i}, \\textbf{s}\\_{j}\\right) = v\\_{a}^{T}\\tanh\\left(\\textbf{W}\\_{a}\\left[\\textbf{h}\\_{i};\\textbf{s}\\_{j}\\right]\\right)$$\r
\r
where $\\textbf{v}\\_{a}$ and $\\textbf{W}\\_{a}$ are learned attention parameters. Here $\\textbf{h}$ refers to the hidden states for the encoder, and $\\textbf{s}$ is the hidden states for the decoder. The function above is thus a type of alignment score function. We can use a matrix of alignment scores to show the correlation between source and target words, as the Figure to the right shows.\r
\r
Within a neural network, once we have the alignment scores, we calculate the final scores using a [softmax](https://paperswithcode.com/method/softmax) function of these alignment scores (ensuring it sums to 1).""" ;
    skos:prefLabel "Additive Attention" .

:AdvProp a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1911.09665v2> ;
    skos:definition "**AdvProp** is an adversarial training scheme which treats adversarial examples as additional examples, to prevent overfitting. Key to the method is the usage of a separate auxiliary batch norm for adversarial examples, as they have different underlying distributions to normal examples." ;
    skos:prefLabel "AdvProp" .

:AdversarialColorEnhancement a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2002.01008v3> ;
    skos:definition "**Adversarial Color Enhancement** is an approach to generating unrestricted adversarial images by optimizing a color filter via gradient descent." ;
    skos:prefLabel "Adversarial Color Enhancement" .

:AdversarialSoftAdvantageFitting\(ASAF\) a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2006.13258v6> ;
    skos:definition "" ;
    skos:prefLabel "Adversarial Soft Advantage Fitting (ASAF)" .

:AdversarialSolarization a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2308.12661v1> ;
    skos:definition "" ;
    skos:prefLabel "Adversarial Solarization" .

:AffCorrs a skos:Concept ;
    rdfs:seeAlso <https://github.com/RPL-CS-UCL/UCL-AffCorrs> ;
    skos:altLabel "Affordance Correspondence" ;
    skos:definition "Method for one-shot visual search of object parts / one-shot semantic part correspondence. Given a single reference image of an object with annotated affordance regions, it segments semantically corresponding parts within a target scene. AffCorrs is used to find corresponding affordances both for intra- and inter-class one-shot part segmentation." ;
    skos:prefLabel "AffCorrs" .

:AffineCoupling a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1410.8516v6> ;
    rdfs:seeAlso <https://github.com/paultsw/nice_pytorch/blob/15cfc543fc3dc81ee70398b8dfc37b67269ede95/nice/layers.py#L109> ;
    skos:definition """**Affine Coupling** is a method for implementing a normalizing flow (where we stack a sequence of invertible bijective transformation functions). Affine coupling is one of these bijective transformation functions. Specifically, it is an example of a reversible transformation where the forward function, the reverse function and the log-determinant are computationally efficient. For the forward function, we split the input dimension into two parts:\r
\r
$$ \\mathbf{x}\\_{a}, \\mathbf{x}\\_{b} = \\text{split}\\left(\\mathbf{x}\\right) $$\r
\r
The second part stays the same $\\mathbf{x}\\_{b} = \\mathbf{y}\\_{b}$, while the first part  $\\mathbf{x}\\_{a}$ undergoes an affine transformation, where the parameters for this transformation are learnt using the second part $\\mathbf{x}\\_{b}$ being put through a neural network. Together we have:\r
\r
$$ \\left(\\log{\\mathbf{s}, \\mathbf{t}}\\right) = \\text{NN}\\left(\\mathbf{x}\\_{b}\\right) $$\r
\r
$$ \\mathbf{s} = \\exp\\left(\\log{\\mathbf{s}}\\right) $$\r
\r
$$ \\mathbf{y}\\_{a} = \\mathbf{s} \\odot \\mathbf{x}\\_{a} + \\mathbf{t}  $$\r
\r
$$ \\mathbf{y}\\_{b} = \\mathbf{x}\\_{b} $$\r
\r
$$ \\mathbf{y} = \\text{concat}\\left(\\mathbf{y}\\_{a}, \\mathbf{y}\\_{b}\\right) $$\r
\r
Image: [GLOW](https://paperswithcode.com/method/glow)""" ;
    skos:prefLabel "Affine Coupling" .

:AffineOperator a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2105.03404v2> ;
    skos:definition """The **Affine Operator** is an affine transformation layer introduced in the [ResMLP](https://paperswithcode.com/method/resmlp) architecture. This replaces [layer normalization](https://paperswithcode.com/method/layer-normalization), as in [Transformer based networks](https://paperswithcode.com/methods/category/transformers), which is possible since in the ResMLP, there are no [self-attention layers](https://paperswithcode.com/method/scaled) which makes training more stable - hence allowing a more simple affine transformation.\r
\r
The affine operator is defined as:\r
\r
$$ \\operatorname{Aff}_{\\mathbf{\\alpha}, \\mathbf{\\beta}}(\\mathbf{x})=\\operatorname{Diag}(\\mathbf{\\alpha}) \\mathbf{x}+\\mathbf{\\beta} $$\r
\r
where $\\alpha$ and $\\beta$ are learnable weight vectors. This operation only rescales and shifts the input element-wise. This operation has several advantages over other normalization operations: first, as opposed to Layer Normalization, it has no cost at inference time, since it can absorbed in the adjacent linear layer. Second, as opposed to [BatchNorm](https://paperswithcode.com/method/batch-normalization) and Layer Normalization, the Aff operator does not depend on batch statistics.""" ;
    skos:prefLabel "Affine Operator" .

:AggMo a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1804.00325v3> ;
    rdfs:seeAlso <https://github.com/AtheMathmo/AggMo/blob/f91730d1c3d8398abb19ab3331e08d39cd5df390/aggmo.py#L5> ;
    skos:definition """**Aggregated Momentum (AggMo)** is a variant of the [classical momentum](https://paperswithcode.com/method/sgd-with-momentum) stochastic optimizer which maintains several velocity vectors with different $\\beta$ parameters. AggMo averages the velocity vectors when updating the parameters. It resolves the problem of choosing a momentum parameter by taking a linear combination of multiple momentum buffers. Each of $K$ momentum buffers have a different discount factor $\\beta \\in \\mathbb{R}^{K}$, and these are averaged for the update. The update rule is:\r
\r
$$ \\textbf{v}\\_{t}^{\\left(i\\right)} = \\beta^{(i)}\\textbf{v}\\_{t-1}^{\\left(i\\right)} - \\nabla\\_{\\theta}f\\left(\\mathbf{\\theta}\\_{t-1}\\right) $$\r
\r
$$ \\mathbf{\\theta\\_{t}} = \\mathbf{\\theta\\_{t-1}} + \\frac{\\gamma\\_{t}}{K}\\sum^{K}\\_{i=1}\\textbf{v}\\_{t}^{\\left(i\\right)} $$\r
\r
where $v^{\\left(i\\right)}_{0}$ for each $i$. The vector $\\mathcal{\\beta} = \\left[\\beta^{(1)}, \\ldots, \\beta^{(K)}\\right]$ is the dampening factor.""" ;
    skos:prefLabel "AggMo" .

:AgglomerativeContextualDecomposition a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1806.05337v2> ;
    rdfs:seeAlso <https://github.com/csinva/hierarchical-dnn-interpretations> ;
    skos:definition "**Agglomerative Contextual Decomposition (ACD)** is an interpretability method that produces hierarchical interpretations for a single prediction made by a neural network, by scoring interactions and building them into a tree. Given a prediction from a trained neural network, ACD produces a hierarchical clustering of the input features, along with the contribution of each cluster to the final prediction. This hierarchy is optimized to identify clusters of features that the DNN learned are predictive." ;
    skos:prefLabel "Agglomerative Contextual Decomposition" .

:AggregatedLearning a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2001.03955v3> ;
    skos:definition "**Aggregated Learning (AgrLearn)** is a vector-quantization approach to learning neural network classifiers. It builds on an equivalence between IB learning and IB quantization and exploits the power of vector quantization, which is well known in information theory." ;
    skos:prefLabel "Aggregated Learning" .

:AgingEvolution a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1802.01548v7> ;
    skos:definition """**Aging Evolution**, or **Regularized Evolution**, is an evolutionary algorithm for [neural architecture search](https://paperswithcode.com/method/neural-architecture-search). Whereas in tournament selection, the best architectures are kept, in aging evolution we associate each genotype with an age, and bias the tournament selection to choose\r
the younger genotypes. In the context of architecture search, aging evolution allows us to explore the search space more, instead of zooming in on good models too early, as non-aging evolution would.""" ;
    skos:prefLabel "Aging Evolution" .

:AlexNet a skos:Concept ;
    dcterms:source <http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks> ;
    rdfs:seeAlso <https://github.com/dansuh17/alexnet-pytorch/blob/d0c1b1c52296ffcbecfbf5b17e1d1685b4ca6744/model.py#L40> ;
    skos:definition "**AlexNet** is a classic convolutional neural network architecture. It consists of convolutions, [max pooling](https://paperswithcode.com/method/max-pooling) and dense layers as the basic building blocks. Grouped convolutions are used in order to fit the model across two GPUs." ;
    skos:prefLabel "AlexNet" .

:AlignPS a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2109.00211v1> ;
    skos:altLabel "Feature-Aligned Person Search Network" ;
    skos:definition "**AlignPS**, or **Feature-Aligned Person Search Network**, is an anchor-free framework for efficient person search. The model employs the typical architecture of an anchor-free detection model (i.e., [FCOS](https://paperswithcode.com/method/fcos)). An aligned feature aggregation (AFA) module is designed to make the model focus more on the re-id subtask. Specifically, AFA reshapes some building blocks of [FPN](https://paperswithcode.com/method/fpn) to overcome the issues of region and scale misalignment in re-id feature learning. A [deformable convolution](https://paperswithcode.com/method/deformable-convolution) is exploited to make the re-id embeddings adaptively aligned with the foreground regions. A feature fusion scheme is designed to better aggregate features from different FPN levels, which makes the re-id features more robust to scale variations. The training procedures of re-id and detection are also optimized to place more emphasis on generating robust re-id embeddings." ;
    skos:prefLabel "AlignPS" .

:All-AttentionLayer a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1907.01470v1> ;
    skos:definition "An **All-Attention Layer** is an attention module and layer for transformers that merges the self-attention and feedforward sublayers into a single unified attention layer. As opposed to the two-step mechanism of the [Transformer](https://paperswithcode.com/method/transformer) layer, it directly builds its representation from the context and a persistent memory block without going through a feedforward transformation. The additional persistent memory block stores, in the form of key-value vectors, information that does not depend on the context. In terms of parameters, these persistent key-value vectors replace the feedforward sublayer." ;
    skos:prefLabel "All-Attention Layer" .

:AlphaFold a skos:Concept ;
    dcterms:source <https://www.nature.com/articles/s41586-021-03819-2> ;
    skos:definition """AlphaFold is a deep learning based algorithm for accurate protein structure prediction. AlphaFold incorporates physical and biological knowledge about protein structure, leveraging multi-sequence alignments, into the design of the deep learning algorithm.\r
\r
Description from: [Highly accurate protein structure prediction with AlphaFold](https://paperswithcode.com/paper/highly-accurate-protein-structure-prediction)\r
\r
Image credit: [DeepMind](https://deepmind.com/blog/article/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology)""" ;
    skos:prefLabel "AlphaFold" .

:AlphaStar a skos:Concept ;
    rdfs:seeAlso <https://github.com/opendilab/DI-star> ;
    skos:altLabel "DeepMind AlphaStar" ;
    skos:definition """**AlphaStar** is a reinforcement learning agent for tackling the game of Starcraft II. It learns a policy $\\pi\\_{\\theta}\\left(a\\_{t}\\mid{s\\_{t}}, z\\right) = P\\left[a\\_{t}\\mid{s\\_{t}}, z\\right]$ using a neural network for parameters $\\theta$ that receives observations $s\\_{t} = \\left(o\\_{1:t}, a\\_{1:t-1}\\right)$ as inputs and chooses actions as outputs. Additionally, the policy conditions on a statistic $z$ that summarizes a strategy sampled from human data such as a build order [1].\r
\r
AlphaStar uses numerous types of architecture to incorporate different types of features. Observations of player and enemy units are processed with a [Transformer](https://paperswithcode.com/method/transformer). Scatter connections are used to integrate spatial and non-spatial information. The temporal sequence of observations is processed by a core [LSTM](https://paperswithcode.com/method/lstm). Minimap features are extracted with a Residual Network. To manage the combinatorial action space, the agent uses an autoregressive policy and a recurrent [pointer network](https://paperswithcode.com/method/pointer-net).\r
\r
The agent is trained first with supervised learning from human replays. Parameters are subsequently trained using reinforcement learning that maximizes the win rate against opponents.  The RL algorithm is based on a policy-gradient algorithm similar to actor-critic. Updates are performed asynchronously and off-policy. To deal with this, a combination of $TD\\left(\\lambda\\right)$ and [V-trace](https://paperswithcode.com/method/v-trace) are used, as well as a new self-imitation algorithm (UPGO).\r
\r
Lastly, to address game-theoretic challenges, AlphaStar is trained with league training to try to approximate a fictitious self-play (FSP) setting which avoids cycles by computing a best response against a uniform mixture of all previous policies. The league of potential opponents includes a diverse range of agents, including policies from current and previous agents.\r
\r
Image Credit: [Yekun Chai](https://ychai.uk/notes/2019/07/21/RL/DRL/Decipher-AlphaStar-on-StarCraft-II/)\r
\r
####  References\r
1. Chai, Yekun. "AlphaStar: Grandmaster level in StarCraft II Explained." (2019).  [https://ychai.uk/notes/2019/07/21/RL/DRL/Decipher-AlphaStar-on-StarCraft-II/](https://ychai.uk/notes/2019/07/21/RL/DRL/Decipher-AlphaStar-on-StarCraft-II/)\r
\r
#### Code Implementation\r
1. https://github.com/opendilab/DI-star""" ;
    skos:prefLabel "AlphaStar" .

:AlphaZero a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1712.01815v1> ;
    skos:definition "**AlphaZero** is a reinforcement learning agent for playing board games such as Go, chess, and shogi. " ;
    skos:prefLabel "AlphaZero" .

:AltCLIP a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2211.06679v2> ;
    skos:definition "In this work, we present a conceptually simple and effective method to train a strong bilingual multimodal representation model. Starting from the pretrained multimodal representation model CLIP released by OpenAI, we switched its text encoder with a pretrained multilingual text encoder XLM-R, and aligned both languages and image representations by a two-stage training schema consisting of teacher learning and contrastive learning. We validate our method through evaluations of a wide range of tasks. We set new state-of-the-art performances on a bunch of tasks including ImageNet-CN, Flicker30k- CN, and COCO-CN. Further, we obtain very close performances with CLIP on almost all tasks, suggesting that one can simply alter the text encoder in CLIP for extended capabilities such as multilingual understanding. Our models and code are available at https://github.com/FlagAI-Open/FlagAI." ;
    skos:prefLabel "AltCLIP" .

:AltDiffusion a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2211.06679v2> ;
    skos:definition "In this work, we present a conceptually simple and effective method to train a strong bilingual multimodal representation model. Starting from the pretrained multimodal representation model CLIP released by OpenAI, we switched its text encoder with a pretrained multilingual text encoder XLM-R, and aligned both languages and image representations by a two-stage training schema consisting of teacher learning and contrastive learning. We validate our method through evaluations of a wide range of tasks. We set new state-of-the-art performances on a bunch of tasks including ImageNet-CN, Flicker30k- CN, and COCO-CN. Further, we obtain very close performances with CLIP on almost all tasks, suggesting that one can simply alter the text encoder in CLIP for extended capabilities such as multilingual understanding. Our models and code are available at https://github.com/FlagAI-Open/FlagAI." ;
    skos:prefLabel "AltDiffusion" .

:AlterNet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2202.06709v4> ;
    skos:definition "" ;
    skos:prefLabel "AlterNet" .

:AmoebaNet a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1802.01548v7> ;
    rdfs:seeAlso <https://github.com/tensorflow/tpu/blob/d35e48588cfdab24aaddd045e37d0e967166931f/models/official/amoeba_net/amoeba_net_model.py#L218> ;
    skos:definition "**AmoebaNet** is a convolutional neural network found through regularized evolution architecture search. The search space is NASNet, which specifies a space of image classifiers with a fixed outer structure: a feed-forward stack of [Inception-like modules](https://paperswithcode.com/method/inception-module) called cells. The discovered architecture is shown to the right." ;
    skos:prefLabel "AmoebaNet" .

:AnnealingSNNL a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2006.04535v1> ;
    skos:altLabel "Soft Nearest Neighbor Loss with Annealing Temperature" ;
    skos:definition "" ;
    skos:prefLabel "Annealing SNNL" .

:Anti-AliasDownsampling a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1904.11486v2> ;
    skos:definition "**Anti-Alias Downsampling (AA)** aims to improve the shift-equivariance of deep networks. Max-pooling is inherently composed of two operations. The first operation is to densely evaluate the max operator and second operation is naive subsampling. AA is proposed as a low-pass filter between them to achieve practical anti-aliasing in any existing strided layer such as strided [convolution](https://paperswithcode.com/method/convolution). The smoothing factor can be adjusted by changing the blur kernel filter size, where a larger filter size results in increased blur." ;
    skos:prefLabel "Anti-Alias Downsampling" .

:AnycostGAN a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2103.03243v1> ;
    skos:definition "**Anycost GAN** is a type of generative adversarial network for image synthesis and editing. Given an input image, we project it into the latent space with encoder $E$ and backward optimization. We can modify the latent code with user input to edit the image. During editing, a sub-generator of small cost is used for fast and interactive preview; during idle time, the full cost generator renders the final, high-quality output. The outputs from the full and sub-generators are visually consistent during projection and editing." ;
    skos:prefLabel "Anycost GAN" .

:Ape-X a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1803.00933v1> ;
    skos:definition """**Ape-X** is a distributed architecture for deep reinforcement learning. The algorithm decouples acting from learning: the actors interact with their own instances of the environment by selecting actions according to a shared neural network, and accumulate the resulting experience in a shared [experience replay](https://paperswithcode.com/method/experience-replay) memory; the learner replays samples of experience and updates the neural network. The architecture relies on [prioritized experience replay](https://paperswithcode.com/method/prioritized-experience-replay) to focus only on the most significant data generated by the actors.\r
\r
In contrast to Gorila, Ape-X uses a shared, centralized replay memory, and instead of sampling\r
uniformly, it prioritizes, to sample the most useful data more often. All communications are batched with the centralized replay, increasing the efficiency and throughput at the cost of some latency. \r
And by learning off-policy, Ape-X has the ability to combine data from many distributed actors, by giving the different actors different exploration policies, broadening the diversity of the experience they jointly encounter.""" ;
    skos:prefLabel "Ape-X" .

:Ape-XDPG a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1803.00933v1> ;
    skos:definition "**Ape-X DPG** combines [DDPG](https://paperswithcode.com/method/ddpg) with distributed [prioritized experience replay](https://paperswithcode.com/method/prioritized-experience-replay) through the [Ape-X](https://paperswithcode.com/method/ape-x) architecture." ;
    skos:prefLabel "Ape-X DPG" .

:Ape-XDQN a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1803.00933v1> ;
    skos:definition "**Ape-X DQN** is a variant of a [DQN](https://paperswithcode.com/method/dqn) with some components of [Rainbow-DQN](https://paperswithcode.com/method/rainbow-dqn) that utilizes distributed [prioritized experience replay](https://paperswithcode.com/method/prioritized-experience-replay) through the [Ape-X](https://paperswithcode.com/method/ape-x) architecture." ;
    skos:prefLabel "Ape-X DQN" .

:Apollo a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2009.13586v6> ;
    skos:altLabel "Adaptive Parameter-wise Diagonal Quasi-Newton Method" ;
    skos:definition "Please enter a description about the method here" ;
    skos:prefLabel "Apollo" .

:ArcFace a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1801.07698v4> ;
    rdfs:seeAlso <https://github.com/ronghuaiyang/arcface-pytorch/blob/47ace80b128042cd8d2efd408f55c5a3e156b032/models/metrics.py#L10> ;
    skos:altLabel "Additive Angular Margin Loss" ;
    skos:definition """**ArcFace**, or **Additive Angular Margin Loss**, is a loss function used in face recognition tasks. The [softmax](https://paperswithcode.com/method/softmax) is traditionally used in these tasks. However, the softmax loss function does not explicitly optimise the feature embedding to enforce higher similarity for intraclass samples and diversity for inter-class samples, which results in a performance gap for deep face recognition under large intra-class appearance variations. \r
\r
The ArcFace loss transforms the logits $W^{T}\\_{j}x\\_{i} = || W\\_{j} || \\text{ } || x\\_{i} || \\cos\\theta\\_{j}$,\r
where $\\theta\\_{j}$ is the angle between the weight $W\\_{j}$ and the feature $x\\_{i}$. The individual weight $ || W\\_{j} || = 1$ is fixed by $l\\_{2}$ normalization. The embedding feature $ ||x\\_{i} ||$ is fixed by $l\\_{2}$ normalization and re-scaled to $s$. The normalisation step on features and weights makes the predictions only depend on the angle between the feature and the weight. The learned embedding\r
features are thus distributed on a hypersphere with a radius of $s$. Finally, an additive angular margin penalty $m$ is added between $x\\_{i}$ and $W\\_{y\\_{i}}$ to simultaneously enhance the intra-class compactness and inter-class discrepancy. Since the proposed additive angular margin penalty is\r
equal to the geodesic distance margin penalty in the normalised hypersphere, the method is named ArcFace:\r
\r
$$ L\\_{3} = -\\frac{1}{N}\\sum^{N}\\_{i=1}\\log\\frac{e^{s\\left(\\cos\\left(\\theta\\_{y\\_{i}} + m\\right)\\right)}}{e^{s\\left(\\cos\\left(\\theta\\_{y\\_{i}} + m\\right)\\right)} + \\sum^{n}\\_{j=1, j \\neq y\\_{i}}e^{s\\cos\\theta\\_{j}}} $$\r
\r
The authors select face images from 8 different identities containing enough samples (around 1,500 images/class) to train 2-D feature embedding networks with the softmax and ArcFace loss, respectively. As the Figure shows, the softmax loss provides roughly separable feature embedding\r
but produces noticeable ambiguity in decision boundaries, while the proposed ArcFace loss can obviously enforce a more evident gap between the nearest classes.\r
\r
Other alternatives to enforce intra-class compactness and inter-class distance include [Supervised Contrastive Learning](https://arxiv.org/abs/2004.11362).""" ;
    skos:prefLabel "ArcFace" .

:Assemble-ResNet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2001.06268v2> ;
    rdfs:seeAlso <https://github.com/clovaai/assembled-cnn/blob/4651444d17f2560516adf812db3e38951ed137ab/nets/resnet_model.py#L166> ;
    skos:definition "**Assemble-ResNet** is a modification to the [ResNet](https://paperswithcode.com/method/resnet) architecture with several tweaks including using [ResNet-D](https://paperswithcode.com/method/resnet-d), channel attention, [anti-alias downsampling](https://paperswithcode.com/method/anti-alias-downsampling), and Big Little Networks." ;
    skos:prefLabel "Assemble-ResNet" .

:AssociativeLSTM a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1602.03032v2> ;
    skos:definition """An **Associative LSTM** combines an [LSTM](https://paperswithcode.com/method/lstm) with ideas from Holographic Reduced Representations (HRRs) to enable key-value storage of data. HRRs use a “binding” operator to implement key-value\r
binding between two vectors (the key and its associated content). They natively implement associative arrays; as a byproduct, they can also easily implement stacks, queues, or lists.""" ;
    skos:prefLabel "Associative LSTM" .

:AsynchronousInteractionAggregation a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2004.07485v1> ;
    skos:definition "**Asynchronous Interaction Aggregation**, or **AIA**, is a network that leverages different interactions to boost action detection. There are two key designs in it: one is the Interaction Aggregation structure (IA) adopting a uniform paradigm to model and integrate multiple types of interaction; the other is the Asynchronous Memory Update algorithm (AMU) that enables us to achieve better performance by modeling very long-term interaction dynamically." ;
    skos:prefLabel "Asynchronous Interaction Aggregation" .

:AttLWB a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2011.09055v2> ;
    skos:altLabel "Attentional Liquid Warping Block" ;
    skos:definition "**Attentional Liquid Warping Block**, or **AttLWB**, is a module for human image synthesis GANs that propagates the source information - such as texture, style, color and face identity - in both image and feature spaces to the synthesized reference. It firstly learns similarities of the global features among all multiple sources features, and then it fuses the multiple sources features by a linear combination of the learned similarities and the multiple sources in the feature spaces. Finally, to better propagate the source identity (style, color, and texture) into the global stream, the fused source features are warped to the global stream by [Spatially-Adaptive Normalization](https://paperswithcode.com/method/spade) (SPADE)." ;
    skos:prefLabel "AttLWB" .

:Attention-augmentedConvolution a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1904.09925v5> ;
    rdfs:seeAlso <https://github.com/leaderj1001/Attention-Augmented-Conv2d/blob/1ce94a3072c2d9aabe258313b3a17c974d987411/AA-Wide-ResNet/attention_augmented_conv.py#L9> ;
    skos:definition """**Attention-augmented Convolution** is a type of [convolution](https://paperswithcode.com/method/convolution) with a two-dimensional relative self-attention mechanism that can replace convolutions as a stand-alone computational primitive for image classification. It employs [scaled-dot product attention](https://paperswithcode.com/method/scaled) and [multi-head attention](https://paperswithcode.com/method/multi-head-attention) as with [Transformers](https://paperswithcode.com/method/transformer).\r
\r
It works by concatenating convolutional and attentional feature map. To see this, consider an original convolution operator with kernel size $k$, $F\\_{in}$ input filters and $F\\_{out}$ output filters. The corresponding attention augmented convolution can be written as"\r
\r
$$\\text{AAConv}\\left(X\\right) = \\text{Concat}\\left[\\text{Conv}(X), \\text{MHA}(X)\\right] $$\r
\r
$X$ originates from an input tensor of shape $\\left(H, W, F\\_{in}\\right)$. This is flattened to become $X \\in \\mathbb{R}^{HW \\times F\\_{in}}$ which is passed into a multi-head attention module, as well as a convolution (see above).\r
\r
Similarly to the convolution, the attention augmented convolution 1) is equivariant to translation and 2) can readily operate on inputs of different spatial dimensions.""" ;
    skos:prefLabel "Attention-augmented Convolution" .

:AttentionDropout a skos:Concept ;
    rdfs:seeAlso <https://github.com/huggingface/transformers/blob/4dc65591b5c61d75c3ef3a2a883bf1433e08fc45/src/transformers/modeling_tf_bert.py#L271> ;
    skos:definition """**Attention Dropout** is a type of [dropout](https://paperswithcode.com/method/dropout) used in attention-based architectures, where elements are randomly dropped out of the [softmax](https://paperswithcode.com/method/softmax) in the attention equation. For example, for scaled-dot product attention, we would drop elements from the first term:\r
\r
$$ {\\text{Attention}}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^{T}}{\\sqrt{d_k}}\\right)V $$""" ;
    skos:prefLabel "Attention Dropout" .

:AttentionFeatureFilters a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2207.12496v2> ;
    skos:definition "An attention mechanism for content-based filtering of multi-level features. For example, recurrent features obtained by forward and backward passes of a bidirectional RNN block can be combined using attention feature filters, with unprocessed input features/embeddings as queries and recurrent features as keys/values." ;
    skos:prefLabel "Attention Feature Filters" .

:AttentionFreeTransformer a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2105.14103v2> ;
    skos:definition """**Attention Free Transformer**, or **AFT**, is an efficient variant of a [multi-head attention module](https://paperswithcode.com/method/multi-head-attention) that eschews [dot product self attention](https://paperswithcode.com/method/scaled). In an AFT layer, the key and value are first combined with a set of learned position biases, the result of which is multiplied with the query in an element-wise fashion. This new operation has a memory complexity linear w.r.t. both the context size and the dimension of features, making it compatible to both large input and model sizes.\r
\r
Given the input $X$, AFT first linearly transforms them into $Q=X W^{Q}, K=X W^{K}, V=X W^{V}$, then performs following operation:\r
\r
$$\r
Y=f(X) ; Y\\_{t}=\\sigma\\_{q}\\left(Q\\_{t}\\right) \\odot \\frac{\\sum\\_{t^{\\prime}=1}^{T} \\exp \\left(K\\_{t^{\\prime}}+w\\_{t, t^{\\prime}}\\right) \\odot V\\_{t^{\\prime}}}{\\sum\\_{t^{\\prime}=1}^{T} \\exp \\left(K\\_{t^{\\prime}}+w\\_{t, t^{\\prime}}\\right)}\r
$$\r
\r
where $\\odot$ is the element-wise product; $\\sigma\\_{q}$ is the nonlinearity applied to the query with default being sigmoid; $w \\in R^{T \\times T}$ is the learned pair-wise position biases.\r
\r
Explained in words, for each target position $t$, AFT performs a weighted average of values, the result of which is combined with the query with element-wise multiplication. In particular, the weighting is simply composed of the keys and a set of learned pair-wise position biases. This provides the immediate advantage of not needing to compute and store the expensive attention matrix, while maintaining the global interactions between query and values as MHA does.""" ;
    skos:prefLabel "Attention Free Transformer" .

:AttentionGate a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1804.03999v3> ;
    skos:definition """Attention gate focuses on targeted regions while suppressing feature activations in irrelevant regions.\r
Given the input feature map $X$ and the gating signal $G\\in \\mathbb{R}^{C'\\times H\\times W}$ which is collected at a coarse scale and contains contextual information, the attention gate uses additive attention to obtain the gating coefficient. Both the input $X$ and the gating signal are first linearly mapped to an $\\mathbb{R}^{F\\times H\\times W}$ dimensional space, and then the output is squeezed in the channel domain to produce a spatial attention weight map $ S \\in \\mathbb{R}^{1\\times H\\times W}$. The overall process can be written as\r
\\begin{align}\r
    S &= \\sigma(\\varphi(\\delta(\\phi_x(X)+\\phi_g(G))))\r
\\end{align}\r
\\begin{align}\r
    Y &= S X\r
\\end{align}\r
where $\\varphi$, $\\phi_x$ and $\\phi_g$ are linear transformations implemented as $1\\times 1$ convolutions. \r
\r
The attention gate guides the model's attention to important regions while suppressing feature activation in unrelated areas. It substantially enhances the representational power of the model without a significant increase in computing cost or number of model parameters due to its lightweight design. It is general and modular, making it simple to use in various CNN models.""" ;
    skos:prefLabel "Attention Gate" .

:AttentionMesh a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2006.10962v1> ;
    skos:definition "**Attention Mesh** is a neural network architecture for 3D face mesh prediction that uses attention to semantically meaningful regions. Specifically region-specific heads are employed that transform the feature maps with spatial transformers." ;
    skos:prefLabel "Attention Mesh" .

:AttentionalLiquidWarpingGAN a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2011.09055v2> ;
    skos:definition "**Attentional Liquid Warping GAN** is a type of generative adversarial network for human image synthesis that utilizes a [AttLWB](https://paperswithcode.com/method/attlwb) block, which is a 3D body mesh recovery module that disentangles pose and shape. To preserve the source information, such as texture, style, color, and face identity, the Attentional Liquid Warping GAN with AttLWB propagates the source information in both image and feature spaces to the synthesized reference." ;
    skos:prefLabel "Attentional Liquid Warping GAN" .

:AttentiveNormalization a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1908.01259v3> ;
    rdfs:seeAlso <https://github.com/iVMCL/AttentiveNorm_Detection/blob/92e0084473c0a0a751b5786f3c62848eb2ba0e95/mmdet/models/backbones/aognet/operator_basic.py#L155> ;
    skos:definition "**Attentive Normalization** generalizes the common affine transformation component in the vanilla feature normalization. Instead of learning a single affine transformation, AN learns a mixture of affine transformations and utilizes their weighted-sum as the final affine transformation applied to re-calibrate features in an instance-specific way. The weights are learned by leveraging feature attention." ;
    skos:prefLabel "Attentive Normalization" .

:Attribute2Font a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2005.07865v1> ;
    skos:definition "**Attribute2Font** is a model that automatically creates fonts by synthesizing visually pleasing glyph images according to user-specified attributes and their corresponding values. Specifically, Attribute2Font is trained to perform font style transfer between any two fonts conditioned on their attribute values. After training, the model can generate glyph images in accordance with an arbitrary set of font attribute values. A unit named Attribute Attention Module is designed to make those generated glyph images better embody the prominent font attributes. A semi-supervised learning scheme is also introduced to exploit a large number of unlabeled fonts" ;
    skos:prefLabel "Attribute2Font" .

:AugMix a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1912.02781v2> ;
    rdfs:seeAlso <https://github.com/google-research/augmix/blob/7c84885fe064435b4b7a8b596b937f0a879e458c/augment_and_mix.py#L40> ;
    skos:definition "AugMix mixes augmented images through linear interpolations. Consequently it is like [Mixup](https://paperswithcode.com/method/mixup) but instead mixes augmented versions of the same image." ;
    skos:prefLabel "AugMix" .

:AugmentedSBERT a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2010.08240v2> ;
    skos:definition "**Augmented SBERT** is a data augmentation strategy for pairwise sentence scoring that uses a [BERT](https://paperswithcode.com/method/bert) cross-encoder to improve the performance for the [SBERT](https://paperswithcode.com/method/sbert) bi-encoders. Given a pre-trained, well-performing crossencoder, we sample sentence pairs according to a certain sampling strategy and label these using the cross-encoder. We call these weakly labeled examples the silver dataset and they will be merged with the gold training dataset. We then train the bi-encoder on this extended training dataset." ;
    skos:prefLabel "Augmented SBERT" .

:Auto-Classifier a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2009.01573v1> ;
    skos:definition "" ;
    skos:prefLabel "Auto-Classifier" .

:AutoAugment a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1805.09501v3> ;
    skos:definition """**AutoAugment** is an automated approach to find data augmentation policies from data. It formulates the problem of finding the best augmentation policy as a discrete search problem. It consists of two components: a search algorithm and a search space. \r
\r
At a high level, the search algorithm (implemented as a controller RNN) samples a data augmentation policy $S$, which has information about what image processing operation to use, the probability of using the operation in each batch, and the magnitude of the operation. The policy $S$ is used to train a neural network with a fixed architecture, whose validation accuracy $R$ is sent back to update the controller. Since $R$ is not differentiable, the controller will be updated by policy gradient methods. \r
\r
The operations used are from PIL, a popular Python image library: all functions in PIL that accept an image as input and output an image. It additionally uses two other augmentation techniques: [Cutout](https://paperswithcode.com/method/cutout) and SamplePairing. The operations searched over are ShearX/Y, TranslateX/Y, Rotate, AutoContrast, Invert, Equalize, Solarize, Posterize, Contrast, Color, Brightness, Sharpness, Cutout and Sample Pairing.""" ;
    skos:prefLabel "AutoAugment" .

:AutoDropout a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2101.01761v1> ;
    skos:definition """**AutoDropout** automates the process of designing [dropout](https://paperswithcode.com/method/dropout) patterns using a [Transformer](https://paperswithcode.com/method/transformer) based controller. In this method, a controller learns to generate a dropout pattern at every channel and layer of a target network, such as a [ConvNet](https://paperswithcode.com/methods/category/convolutional-neural-networks) or a Transformer. The target network is then trained with the dropped-out pattern, and its resulting validation performance is used as a signal for the controller to learn from. The resulting pattern is applied to a convolutional output channel, which is a common building block of image recognition models.\r
\r
The controller network generates the tokens to describe the configurations of the dropout pattern. The tokens are generated like words in a language model. For every layer in a ConvNet, a group of 8 tokens need to be made to create a dropout pattern. These 8 tokens are generated sequentially. In the figure above, size, stride, and repeat indicate the size and the tiling of the pattern; rotate, shear_x, and shear_y specify the geometric transformations of the pattern; share_c is a binary deciding whether a pattern is applied to all $C$ channels; and residual is a binary deciding whether the pattern is applied to the residual branch as well. If we need $L$ dropout patterns, the controller will generate $8L$ decisions.""" ;
    skos:prefLabel "AutoDropout" .

:AutoEncoder a skos:Concept ;
    dcterms:source <https://science.sciencemag.org/content/313/5786/504> ;
    rdfs:seeAlso <https://github.com/L1aoXingyu/pytorch-beginner/blob/9c86be785c7c318a09cf29112dd1f1a58613239b/08-AutoEncoder/simple_autoencoder.py#L38> ;
    skos:definition """An **Autoencoder** is a bottleneck architecture that turns a high-dimensional input into a latent low-dimensional code (encoder), and then performs a reconstruction of the input with this latent code (the decoder).\r
\r
Image: [Michael Massi](https://en.wikipedia.org/wiki/Autoencoder#/media/File:Autoencoder_schema.png)""" ;
    skos:prefLabel "AutoEncoder" .

:AutoGAN a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1908.03835v1> ;
    skos:definition "[Neural architecture search](https://paperswithcode.com/method/neural-architecture-search) (NAS) has witnessed prevailing success in image classification and (very recently) segmentation tasks. In this paper, we present the first preliminary study on introducing the NAS algorithm to generative adversarial networks (GANs), dubbed AutoGAN. The marriage of NAS and GANs faces its unique challenges. We define the search space for the generator architectural variations and use an RNN controller to guide the search, with parameter sharing and dynamic-resetting to accelerate the process. Inception score is adopted as the reward, and a multi-level search strategy is introduced to perform NAS in a progressive way." ;
    skos:prefLabel "AutoGAN" .

:AutoGL a skos:Concept ;
    dcterms:source <https://www.nature.com/articles/s42256-022-00501-8> ;
    skos:altLabel "Automated Graph Learning" ;
    skos:definition "Automated graph learning is a method that aims at discovering the best hyper-parameter and neural architecture configuration for different graph tasks/data without manual design." ;
    skos:prefLabel "AutoGL" .

:AutoInt a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1810.11921v2> ;
    skos:definition "**AutoInt** is a deep tabular learning method that models high-order feature interactions of input features. AutoInt can be applied to both numerical and categorical input features. Specifically, both the numerical and categorical features are mapped into the same low-dimensional space. Afterwards, a multi-head self-attentive neural network with residual connections is proposed to explicitly model the feature interactions in the low-dimensional space. With different layers of the multi-head self-attentive neural networks, different orders of feature combinations of input features can be modeled." ;
    skos:prefLabel "AutoInt" .

:AutoML-Zero a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2003.03384v2> ;
    skos:definition """**AutoML-Zero** is an AutoML technique that aims to search a fine-grained space simultaneously for the model, optimization procedure, initialization, and so on, permitting much less human-design and even allowing the discovery of non-neural network algorithms. It represents ML algorithms as computer programs comprised of three component functions, Setup, Predict, and Learn, that performs initialization, prediction and learning. The instructions in these functions apply basic mathematical operations on a small memory. The operation and memory addresses used by each instruction are free parameters in the search space, as is the size of the component functions. While this reduces expert design, the consequent sparsity means that [random search](https://paperswithcode.com/method/random-search) cannot make enough progress. To overcome this difficulty, the authors use small proxy tasks and migration techniques to build an optimized infrastructure capable of searching through 10,000 models/second/cpu core.\r
\r
Evolutionary methods can find solutions in the AutoML-Zero search space despite its enormous\r
size and sparsity. The authors show that by randomly modifying the programs and periodically selecting the best performing ones on given tasks/datasets, AutoML-Zero discovers reasonable algorithms. They start from empty programs and using data labeled by “teacher” neural networks with random weights, and demonstrate  evolution can discover neural networks trained by gradient descent. Following this, they minimize bias toward known algorithms by switching to binary classification tasks extracted from CIFAR-10 and allowing a larger set of possible operations. This discovers interesting techniques like multiplicative interactions, normalized gradient and weight averaging. Finally, they show it is possible for evolution to adapt the algorithm to the type of task provided. For example, [dropout](https://paperswithcode.com/method/dropout)-like operations emerge when the task needs regularization and learning rate decay appears when the task requires faster convergence.""" ;
    skos:prefLabel "AutoML-Zero" .

:AutoSmart a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2109.04115v1> ;
    skos:definition "**AutoSmart** is AutoML framework for temporal relational data. The framework includes automatic data processing, table merging, feature engineering, and model tuning, integrated with a time&memory control unit." ;
    skos:prefLabel "AutoSmart" .

:AutoSync a skos:Concept ;
    dcterms:source <http://proceedings.neurips.cc/paper/2020/hash/0a2298a72858d90d5c4b4fee954b6896-Abstract.html> ;
    skos:definition "**AutoSync** is a pipeline for automatically optimizing synchronization strategies, given model structures and resource specifications, in data-parallel distributed machine learning. By factorizing the synchronization strategy with respect to each trainable building block of a DL model, we can construct a valid and large strategy space spanned by multiple factors. AutoSync efficiently navigates the space and locates the optimal strategy. AutoSync leverages domain knowledge about synchronization systems to reduce the search space, and is equipped with a domain adaptive simulator, which combines principled communication modeling and data-driven ML models, to estimate the runtime of strategy proposals without launching real distributed execution." ;
    skos:prefLabel "AutoSync" .

:AutoTinyBERT a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2107.13686v1> ;
    skos:definition "**AutoTinyBERT** is a an efficient [BERT](https://paperswithcode.com/method/bert) variant found through neural architecture search. Specifically, one-shot learning is used to obtain a big Super Pretrained Language Model (SuperPLM), where the objectives of pre-training or task-agnostic BERT distillation are used.  Then, given a specific latency constraint, an evolutionary algorithm is run on the SuperPLM to search optimal architectures. Finally, we extract the corresponding sub-models based on the optimal architectures and further train these models." ;
    skos:prefLabel "AutoTinyBERT" .

:AuxiliaryBatchNormalization a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1911.09665v2> ;
    rdfs:seeAlso <https://github.com/rwightman/pytorch-image-models/blob/e0685dd41585a75385c2f62352669cf5e1df278f/timm/models/layers/split_batchnorm.py#L1> ;
    skos:definition "**Auxiliary Batch Normalization** is a type of regularization used in adversarial training schemes. The idea is that adversarial examples should have a separate [batch normalization](https://paperswithcode.com/method/batch-normalization) components to the clean examples, as they have different underlying statistics." ;
    skos:prefLabel "Auxiliary Batch Normalization" .

:AuxiliaryClassifier a skos:Concept ;
    skos:definition "**Auxiliary Classifiers** are type of architectural component that seek to improve the convergence of very deep networks. They are classifier heads we attach to layers before the end of the network. The motivation is to push useful gradients to the lower layers to make them immediately useful and improve the convergence during training by combatting the vanishing gradient problem. They are notably used in the Inception family of convolutional neural networks." ;
    skos:prefLabel "Auxiliary Classifier" .

:AveragePooling a skos:Concept ;
    skos:definition """**Average Pooling** is a pooling operation that calculates the average value for patches of a feature map, and uses it to create a downsampled (pooled) feature map. It is usually used after a convolutional layer. It adds a small amount of translation invariance - meaning translating the image by a small amount does not significantly affect the values of most pooled outputs. It extracts features more smoothly than [Max Pooling](https://paperswithcode.com/method/max-pooling), whereas max pooling extracts more pronounced features like edges.\r
\r
Image Source: [here](https://www.researchgate.net/figure/Illustration-of-Max-Pooling-and-Average-Pooling-Figure-2-above-shows-an-example-of-max_fig2_333593451)""" ;
    skos:prefLabel "Average Pooling" .

:AxialAttention a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1912.12180v1> ;
    skos:definition """**Axial Attention** is a simple generalization of self-attention that naturally aligns with the multiple dimensions of the tensors in both the encoding and the decoding settings. It was first proposed in [CCNet](https://paperswithcode.com/method/ccnet) [1] named as criss-cross attention, which harvests the contextual information of all the pixels on its criss-cross path. By taking a further recurrent operation, each pixel can finally capture the full-image dependencies. Ho et al [2] extents CCNet to process multi-dimensional data.  The proposed structure of the layers allows for the vast majority of the context to be computed in parallel during decoding without introducing any independence assumptions. It serves as the basic building block for developing self-attention-based autoregressive models for high-dimensional data tensors, e.g., Axial Transformers. It has been applied in [AlphaFold](https://paperswithcode.com/method/alphafold) [3] for interpreting protein sequences.\r
\r
[1] Zilong Huang, Xinggang Wang, Lichao Huang, Chang Huang, Yunchao Wei, Wenyu Liu. CCNet: Criss-Cross Attention for Semantic Segmentation. ICCV, 2019.\r
\r
[2] Jonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, Tim Salimans. arXiv:1912.12180\r
\r
[3] Jumper J, Evans R, Pritzel A, Green T, Figurnov M, Ronneberger O, Tunyasuvunakool K, Bates R, Žídek A, Potapenko A, Bridgland A. Highly accurate protein structure prediction with AlphaFold. Nature. 2021 Jul 15:1-1.""" ;
    skos:prefLabel "Axial Attention" .

:BAGUA a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2107.01499v4> ;
    skos:definition "**BAGUA** is a communication framework whose design goal is to provide a system abstraction that is both flexible and modular to support state-of-the-art system relaxation techniques of distributed training. The abstraction goes beyond parameter server and Allreduce paradigms, and provides a collection of MPI-style collective operations to facilitate communications with different precision and centralization strategies." ;
    skos:prefLabel "BAGUA" .

:BAM a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1807.06514v2> ;
    skos:altLabel "Bottleneck Attention Module" ;
    skos:definition """Park et al. proposed the bottleneck attention module (BAM), aiming\r
to efficiently improve the representational capability of networks. \r
It uses dilated convolution to enlarge the receptive field of the spatial attention sub-module, and build a bottleneck structure as suggested  by ResNet to save computational cost.\r
\r
For a given input feature map $X$, BAM infers the channel attention $s_c \\in \\mathbb{R}^C$ and spatial attention $s_s\\in \\mathbb{R}^{H\\times W}$ in two parallel streams, then sums the two attention maps after resizing both branch outputs to $\\mathbb{R}^{C\\times H \\times W}$. The channel attention branch, like an SE block, applies global average pooling to the feature map to aggregate global information, and then uses an MLP with channel dimensionality reduction. In order to utilize contextual information effectively, the spatial attention branch combines a bottleneck structure and dilated convolutions. Overall, BAM can be written as\r
\\begin{align}\r
    s_c &= \\text{BN}(W_2(W_1\\text{GAP}(X)+b_1)+b_2)\r
\\end{align}\r
\r
\\begin{align}\r
    s_s &= BN(Conv_2^{1 \\times 1}(DC_2^{3\\times 3}(DC_1^{3 \\times 3}(Conv_1^{1 \\times 1}(X))))) \r
\\end{align}\r
\\begin{align}\r
    s &= \\sigma(\\text{Expand}(s_s)+\\text{Expand}(s_c)) \r
\\end{align}\r
\\begin{align}\r
    Y &= s X+X\r
\\end{align}\r
where $W_i$, $b_i$ denote  weights and biases of fully connected layers respectively, $Conv_{1}^{1\\times 1}$ and $Conv_{2}^{1\\times 1}$ are convolution layers  used for channel reduction. $DC_i^{3\\times 3}$ denotes a dilated convolution with $3\\times 3$ kernel,  applied to utilize contextual information effectively. $\\text{Expand}$ expands the attention maps $s_s$ and $s_c$ to $\\mathbb{R}^{C\\times H\\times W}$.\r
\r
BAM can emphasize or suppress features in both spatial and channel dimensions, as well as improving the representational power. Dimensional reduction applied to both channel and spatial attention branches enables it to be integrated with any convolutional neural network with little extra computational cost. However, although dilated convolutions enlarge the receptive field effectively, it still fails to capture long-range contextual information as well as encoding cross-domain relationships.""" ;
    skos:prefLabel "BAM" .

:BART a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1910.13461v1> ;
    skos:definition "**BART** is a [denoising autoencoder](https://paperswithcode.com/method/denoising-autoencoder) for pretraining sequence-to-sequence models. It is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard [Transformer](https://paperswithcode.com/method/transformer)-based neural machine translation architecture. It uses a standard seq2seq/NMT architecture with a bidirectional encoder (like [BERT](https://paperswithcode.com/method/bert)) and a left-to-right decoder (like [GPT](https://paperswithcode.com/method/gpt)). This means the encoder's attention mask is fully visible, like BERT, and the decoder's attention mask is causal, like [GPT2](https://paperswithcode.com/method/gpt-2)." ;
    skos:prefLabel "BART" .

:BASE a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2111.12880v1> ;
    skos:altLabel "Balanced Selection" ;
    skos:definition "" ;
    skos:prefLabel "BASE" .

:BASNet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2101.04704v2> ;
    skos:altLabel "Boundary-Aware Segmentation Network" ;
    skos:definition """**BASNet**, or **Boundary-Aware Segmentation Network**, is an image segmentation architecture that consists of a predict-refine architecture and a hybrid loss. The proposed BASNet comprises a predict-refine architecture and a hybrid loss, for highly accurate image segmentation.  The predict-refine architecture consists of a densely supervised encoder-decoder network and a residual \r
 refinement module, which are respectively used to predict and refine a segmentation probability map. The hybrid loss is a combination of the binary cross entropy, structural similarity and intersection-over-union losses, which guide the network to learn three-level (i.e., pixel-, patch- and map- level) hierarchy representations.""" ;
    skos:prefLabel "BASNet" .

:BERT a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1810.04805v2> ;
    rdfs:seeAlso <https://github.com/google-research/bert> ;
    skos:definition """**BERT**, or Bidirectional Encoder Representations from Transformers, improves upon standard [Transformers](http://paperswithcode.com/method/transformer) by removing the unidirectionality constraint by using a *masked language model* (MLM) pre-training objective. The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked word based only on its context. Unlike left-to-right language model pre-training, the MLM objective enables the representation to fuse the left and the right context, which allows us to pre-train a deep bidirectional Transformer. In addition to the masked language model, BERT uses a *next sentence prediction* task that jointly pre-trains text-pair representations. \r
\r
There are two steps in BERT: *pre-training* and *fine-tuning*. During pre-training, the model is trained on unlabeled data over different pre-training tasks. For fine-tuning, the BERT model is first initialized with the pre-trained parameters, and all of the parameters are fine-tuned using labeled data from the downstream tasks. Each downstream task has separate fine-tuned models, even though they\r
are initialized with the same pre-trained parameters.""" ;
    skos:prefLabel "BERT" .

:BIDeN a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2108.11364v3> ;
    skos:altLabel "Blind Image Decomposition Network" ;
    skos:definition """**BIDeN**, or **Blind Image Decomposition Network**, is a model for blind image decomposition, which requires separating a superimposed image into constituent underlying images in a blind setting, that is, both the source components involved in mixing as well as the mixing mechanism are unknown.  For example, rain may consist of multiple components, such as rain streaks, raindrops, snow, and haze. \r
\r
The Figure shows an example where $N = 4, L = 2, x = {a, b, c, d}$, and $I = {1, 3}$. $a, c$ are selected then passed to the mixing function $f$, and outputs the mixed input image $z$, which is $f\\left(a, c\\right)$ here. The generator consists of an encoder $E$ with three branches and multiple heads $H$. $\\bigotimes$ denotes the concatenation operation. Depth and receptive field of each branch is different to capture multiple scales of features. Each specified head points to the corresponding source component, and the number of heads varies with the maximum number of source components N. All reconstructed images $\\left(a', c'\\right)$ and their corresponding real images $\\left(a, c\\right)$ are sent to an unconditional discriminator. The discriminator also predicts the source components of the input image $z$. The outputs from other heads $\\left(b', d'\\right)$ do not contribute to the optimization.""" ;
    skos:prefLabel "BIDeN" .

:BIMAN a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2003.03172v3> ;
    skos:definition "**BIMAN**, or **Bot Identification by commit Message, commit Association, and author Name**, is a technique to detect bots that commit code. It is comprised of three methods that consider independent aspects of the commits made by a particular author: 1) Commit Message: Identify if commit messages are being generated from templates; 2) Commit Association: Predict if an author is a bot using a random forest model, with features related to files and projects associated with the commits as predictors; and 3) Author Name: Match author’s name and email to common bot patterns." ;
    skos:prefLabel "BIMAN" .

:BLANC a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2002.09836v2> ;
    skos:definition "**BLANC** is an automatic estimation approach for document summary quality. The goal is to measure the functional performance of a summary with an objective, reproducible, and fully automated method. BLANC achieves this by measuring the performance boost gained by a pre-trained language model with access to a document summary while carrying out its language understanding task on the document's text." ;
    skos:prefLabel "BLANC" .

:BLIP a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2201.12086v2> ;
    skos:altLabel "BLIP: Bootstrapping Language-Image Pre-training" ;
    skos:definition "Vision-Language Pre-training (VLP) has advanced the performance for many vision-language tasks. However, most existing pre-trained models only excel in either understanding-based tasks or generation-based tasks. Furthermore, performance improvement has been largely achieved by scaling up the dataset with noisy image-text pairs collected from the web, which is a suboptimal source of supervision. In this paper, we propose BLIP, a new VLP framework which transfers flexibly to both vision-language understanding and generation tasks. BLIP effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. We achieve state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval (+2.7% in average recall@1), image captioning (+2.8% in CIDEr), and VQA (+1.6% in VQA score). BLIP also demonstrates strong generalization ability when directly transferred to video-language tasks in a zero-shot manner. Code, models, and datasets are released at https://github.com/salesforce/BLIP." ;
    skos:prefLabel "BLIP" .

:BLOOM a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2211.05100v4> ;
    skos:definition """**BLOOM** is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of\r
sources in 46 natural and 13 programming languages (59 in total).""" ;
    skos:prefLabel "BLOOM" .

:BLOOMZ a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2211.01786v2> ;
    skos:definition "**BLOOMZ** is a Multitask prompted finetuning (MTF) variant of BLOOM." ;
    skos:prefLabel "BLOOMZ" .

:BP-Transformer a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1911.04070v1> ;
    skos:definition """The **BP-Transformer (BPT)** is a type of [Transformer](https://paperswithcode.com/method/transformer) that is motivated by the need to find a better balance between capability and computational complexity for self-attention. The architecture partitions the input sequence into different multi-scale spans via binary partitioning (BP). It incorporates an inductive bias of attending the context information from fine-grain to coarse-grain as the relative distance increases. The farther the context information is, the coarser its representation is.\r
BPT can be regard as graph neural network, whose nodes are the multi-scale spans. A token node can attend the smaller-scale span for the closer context and the larger-scale span for the longer distance context. The representations of nodes are updated with [Graph Self-Attention](https://paperswithcode.com/method/graph-self-attention).""" ;
    skos:prefLabel "BP-Transformer" .

:BPE a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1508.07909v5> ;
    skos:altLabel "Byte Pair Encoding" ;
    skos:definition """**Byte Pair Encoding**, or **BPE**, is a subword segmentation algorithm that encodes rare and unknown words as sequences of subword units. The intuition is that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations).\r
\r
[Lei Mao](https://leimao.github.io/blog/Byte-Pair-Encoding/) has a detailed blog post that explains how this works.""" ;
    skos:prefLabel "BPE" .

:BRepNet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2104.00706v2> ;
    skos:definition "**BRepNet** is a neural network for CAD applications. It is designed to operate directly on B-rep data structures, avoiding the need to approximate the model as meshes or point clouds. BRepNet defines convolutional kernels with respect to oriented coedges in the data structure. In the neighborhood of each coedge, a small collection of faces, edges and coedges can be identified and patterns in the feature vectors from these entities detected by specific learnable parameters." ;
    skos:prefLabel "BRepNet" .

:BS-Net a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2006.04603v3> ;
    skos:definition """**BS-Net** is an architecture for COVID-19 severity prediction based on clinical data from different modalities. The architecture comprises 1) a shared multi-task feature extraction backbone, 2) a lung segmentation branch, 3) an original registration mechanism that acts as a ”multi-resolution feature alignment” block operating on the encoding backbone , and 4) a multi-regional classification part for the final six-valued score estimation. \r
\r
All these blocks act together in the final training thanks to a loss specifically crated for this task. This loss guarantees also performance robustness, comprising a differentiable version of the target discrete metric. The learning phase operates in a weakly-supervised fashion. This is due to the fact that difficulties and pitfalls in the visual interpretation of the disease signs on CXRs (spanning from subtle findings to heavy lung impairment), and the lack of detailed localization information, produces unavoidable inter-rater variability among radiologists in assigning scores.\r
\r
Specifically the architectural details are:\r
\r
- The input image is processed with a convolutional backbone; the authors opt for a [ResNet](https://paperswithcode.com/method/resnet)-18.\r
- Segmentation is performed by a nested version of [U-Net](https://paperswithcode.com/method/u-net) (U-Net++).\r
- Alignment is estimated through the segmentation probability map produced by the U-Net++ decoder, which is achieved through a [spatial transformer network](https://paperswithcode.com/method/spatial-transformer) -- able to estimate the spatial transform matrix in order to center, rotate, and correctly zoom the lungs. After alignment at various scales, features are forward to a [ROIPool](https://paperswithcode.com/method/roi-pooling). \r
- The alignment block is pre-trained on the synthetic alignment dataset in a weakly-supervised setting, using a Dice loss.\r
- The scoring head uses [FPNs](https://paperswithcode.com/method/fpn) for the combination of multi-scale feature maps. The multiresolution feature aligner produces input feature maps that are well focused on the specific area of interest. Eventually, the output of the FPN layer flows in a series of convolutional blocks to retrieve the output map. The classification is performed by a final [Global Average Pooling](https://paperswithcode.com/method/global-average-pooling) layer and a [SoftMax](https://paperswithcode.com/method/softmax) activation.\r
- The Loss function used for training is a sparse categorical cross entropy (SCCE) with a (differentiable) mean absolute error contribution.""" ;
    skos:prefLabel "BS-Net" .

:BTF a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2203.05550v3> ;
    skos:altLabel "Back to the Feature" ;
    skos:definition "" ;
    skos:prefLabel "BTF" .

:BTmPG a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2109.01862v1> ;
    skos:definition """**BTmPG**, or **Back-Translation guided multi-round Paraphrase Generation**, is a multi-round paraphrase generation method that leverages back-translation to guide paraphrase model during training and generates paraphrases in a multiround process. The model regards paraphrase generation as a monolingual translation task. Given a paraphrase pair $\\left(S\\_{0}, P\\right)$, which $S\\_{0}$ is the original/source sentence and $P$ is the target paraphrase given in the dataset. In the first round generation, we send $S\\_{0}$ into a paraphrase model to generate a paraphrase $S\\_{1}$. In the second round generation, we use the $S\\_{1}$ as the input of the model to generate a new paraphrase $S\\_{2}$. And so forth, in the $i$-th round generation, we send $S\\_{i−1}$ into the paraphrase model to generate $S\\_{i}$.\r
.""" ;
    skos:prefLabel "BTmPG" .

:BYOL a skos:Concept ;
    dcterms:source <http://proceedings.neurips.cc/paper/2020/hash/f3ada80d5c4ee70142b17b8192b2958e-Abstract.html> ;
    skos:altLabel "Bootstrap Your Own Latent" ;
    skos:definition """BYOL (Bootstrap Your Own Latent) is a new approach to self-supervised learning. BYOL’s goal is to learn a representation $y_θ$ which can then be used for downstream tasks. BYOL uses two neural networks to learn: the online and target networks. The online network is defined by a set of weights $θ$ and is comprised of three stages: an encoder $f_θ$, a projector $g_θ$ and a predictor $q_θ$. The target network has the same architecture\r
as the online network, but uses a different set of weights $ξ$. The target network provides the regression\r
targets to train the online network, and its parameters $ξ$ are an exponential moving average of the\r
online parameters $θ$.\r
\r
Given the architecture diagram on the right, BYOL minimizes a similarity loss between $q_θ(z_θ)$ and $sg(z'{_ξ})$, where $θ$ are the trained weights, $ξ$ are an exponential moving average of $θ$ and $sg$ means stop-gradient. At the end of training, everything but $f_θ$ is discarded, and $y_θ$ is used as the image representation.\r
\r
Source: [Bootstrap Your Own Latent - A New Approach to Self-Supervised Learning](https://paperswithcode.com/paper/bootstrap-your-own-latent-a-new-approach-to-1)\r
\r
Image credit: [Bootstrap Your Own Latent - A New Approach to Self-Supervised Learning](https://paperswithcode.com/paper/bootstrap-your-own-latent-a-new-approach-to-1)""" ;
    skos:prefLabel "BYOL" .

:BalancedFeaturePyramid a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1904.02701v1> ;
    rdfs:seeAlso <https://github.com/open-mmlab/mmdetection/blob/ec7d83bb4c2b5ba84f198e5eb5ed491b955bce81/mmdet/models/necks/bfp.py#L10> ;
    skos:definition """**Balanced Feature Pyramid** is a feature pyramid module. It differs from approaches like [FPNs](https://paperswithcode.com/method/fpn) that integrate multi-level features using lateral connections. Instead the BFP strengthens the multi-level features using the same deeply integrated balanced semantic features. The pipeline is shown in the Figure to the right. It consists of four steps, rescaling, integrating, refining and strengthening.\r
\r
Features at resolution level $l$ are denoted as $C\\_{l}$. The number of multi-level features is denoted as $L$. The indexes of involved lowest and highest levels are denoted as $l\\_{min}$ and $l\\_{max}$. In the Figure, $C\\_{2}$ has the highest resolution. To integrate multi-level features and preserve their semantic hierarchy at the same time, we first resize the multi-level features {$C\\_{2}, C\\_{3}, C\\_{4}, C\\_{5}$} to an intermediate size, i.e., the same size as $C\\_{4}$, with interpolation and max-pooling respectively. Once the features are rescaled, the balanced semantic features are obtained by simple averaging as:\r
\r
$$ C = \\frac{1}{L}\\sum^{l\\_{max}}\\_{l=l\\_{min}}C\\_{l} $$\r
\r
The obtained features are then rescaled using the same but reverse procedure to strengthen the original features. Each resolution obtains equal information from others in this procedure. Note that this procedure does not contain any parameter. The authors observe improvement with this nonparametric method, proving the effectiveness of the information flow. \r
\r
The balanced semantic features can be further refined to be more discriminative. The authors found both the refinements with convolutions directly and the non-local module work well. But the\r
non-local module works in a more stable way. Therefore, embedded Gaussian non-local attention is utilized as default. The refining step helps us enhance the integrated features and further improve the results.\r
\r
With this method, features from low-level to high-level are aggregated at the same time. The outputs\r
{$P\\_{2}, P\\_{3}, P\\_{4}, P\\_{5}$} are used for object detection following the same pipeline in FPN.""" ;
    skos:prefLabel "Balanced Feature Pyramid" .

:BalancedL1Loss a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1904.02701v1> ;
    rdfs:seeAlso <https://github.com/OceanPang/Libra_R-CNN/blob/5d6096f39b90eeafaf3457f5a39572fe5e991808/mmdet/models/losses/balanced_l1_loss.py#L9> ;
    skos:definition """**Balanced L1 Loss** is a loss function used for the object detection task. Classification and localization problems are solved simultaneously under the guidance of a multi-task loss since\r
[Fast R-CNN](https://paperswithcode.com/method/fast-r-cnn), defined as:\r
\r
$$ L\\_{p,u,t\\_{u},v} = L\\_{cls}\\left(p, u\\right) + \\lambda\\left[u \\geq 1\\right]L\\_{loc}\\left(t^{u}, v\\right) $$\r
\r
$L\\_{cls}$ and $L\\_{loc}$ are objective functions corresponding to recognition and localization respectively. Predictions and targets in $L\\_{cls}$ are denoted as $p$ and $u$. $t\\_{u}$ is the corresponding regression results with class $u$. $v$ is the regression target. $\\lambda$ is used for tuning the loss weight under multi-task learning. We call samples with a loss greater than or equal to 1.0 outliers. The other samples are called inliers.\r
\r
A natural solution for balancing the involved tasks is to tune the loss weights of them. However, owing to the unbounded regression targets, directly raising the weight of localization loss will make the model more sensitive to outliers. These outliers, which can be regarded as hard samples, will produce excessively large gradients that are harmful to the training process. The inliers, which can be regarded as the easy samples, contribute little gradient to the overall gradients compared with the outliers. To be more specific, inliers only contribute 30% gradients average per sample compared with outliers. Considering these issues, the authors introduced the balanced L1 loss, which is denoted as $L\\_{b}$.\r
\r
Balanced L1 loss is derived from the conventional smooth L1 loss, in which an inflection point is set to separate inliers from outliners, and clip the large gradients produced by outliers with a maximum value of 1.0, as shown by the dashed lines in the Figure to the right. The key idea of balanced L1 loss is promoting the crucial regression gradients, i.e. gradients from inliers (accurate samples), to rebalance\r
the involved samples and tasks, thus achieving a more balanced training within classification, overall localization and accurate localization. Localization loss $L\\_{loc}$ uses balanced L1 loss is defined as:\r
\r
$$ L\\_{loc} = \\sum\\_{i\\in{x,y,w,h}}L\\_{b}\\left(t^{u}\\_{i}-v\\_{i}\\right) $$\r
\r
The Figure to the right shows that the balanced L1 loss increases the gradients of inliers under the control of a factor denoted as $\\alpha$. A small $\\alpha$ increases more gradient for inliers, but the gradients of outliers are not influenced. Besides, an overall promotion magnification controlled by γ is also brought in for tuning the upper bound of regression errors, which can help the objective function better balancing involved tasks. The two factors that control different aspects are mutually enhanced to reach a more balanced training.$b$ is used to ensure $L\\_{b}\\left(x = 1\\right)$ has the same value for both formulations in the equation below.\r
\r
By integrating the gradient formulation above, we can get the balanced L1 loss as:\r
\r
$$ L\\_{b}\\left(x\\right) = \\frac{\\alpha}{b}\\left(b|x| + 1\\right)ln\\left(b|x| + 1\\right) - \\alpha|x| \\text{ if } |x| < 1$$\r
\r
$$ L\\_{b}\\left(x\\right) = \\gamma|x| + C \\text{ otherwise } $$\r
\r
in which the parameters $\\gamma$, $\\alpha$, and $b$ are constrained by $\\alpha\\text{ln}\\left(b + 1\\right) = \\gamma$. The default parameters are set as $\\alpha = 0.5$ and $\\gamma = 1.5$""" ;
    skos:prefLabel "Balanced L1 Loss" .

:BarlowTwins a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2103.03230v3> ;
    skos:definition "**Barlow Twins** is a self-supervised learning method that applies redundancy-reduction — a principle first proposed in neuroscience — to self supervised learning. The objective function measures the cross-correlation matrix between the embeddings of two identical networks fed with distorted versions of a batch of samples, and tries to make this matrix close to the identity. This causes the embedding vectors of distorted version of a sample to be similar, while minimizing the redundancy between the components of these vectors. Barlow Twins does not require large batches nor asymmetry between the network twins such as a predictor network, gradient stopping, or a moving average on the weight updates. Intriguingly it benefits from very high-dimensional output vectors." ;
    skos:prefLabel "Barlow Twins" .

:BaseBoosting a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2005.06194v1> ;
    rdfs:seeAlso <https://github.com/a-wozniakowski/scikit-physlearn/blob/03f944c4fecf512ee763ec4552f30c4a3565612d/physlearn/supervised/regression.py#L581> ;
    skos:definition """In the setting of multi-target regression, base boosting permits us to incorporate prior knowledge into the learning mechanism of gradient boosting (or Newton boosting, etc.). Namely, from the vantage of statistics, base boosting is a way of building the following additive expansion in a set of elementary basis functions:\r
\\begin{equation}\r
h_{j}(X ; \\{ \\alpha_{j}, \\theta_{j} \\}) = X_{j} + \\sum_{k=1}^{K_{j}} \\alpha_{j,k} b(X ; \\theta_{j,k}),\r
\\end{equation}\r
where \r
$X$ is an example from the domain $\\mathcal{X},$\r
$\\{\\alpha_{j}, \\theta_{j}\\} = \\{\\alpha_{j,1},\\dots, \\alpha_{j,K_{j}},\\theta_{j,1},\\dots,\\theta_{j,K_{j}}\\}$ collects the expansion coefficients and parameter sets,\r
$X_{j}$ is the image of $X$ under the $j$th coordinate function (a prediction from a user-specified model),\r
$K_{j}$ is the number of basis functions in the linear sum,\r
$b(X; \\theta_{j,k})$ is a real-valued function of the example $X,$ characterized by a parameter set $\\theta_{j,k}.$\r
\r
The aforementioned additive expansion differs from the \r
[standard  additive expansion](https://projecteuclid.org/download/pdf_1/euclid.aos/1013203451):\r
\\begin{equation}\r
h_{j}(X ; \\{ \\alpha_{j}, \\theta_{j}\\}) = \\alpha_{j, 0} + \\sum_{k=1}^{K_{j}} \\alpha_{j,k} b(X ; \\theta_{j,k}),\r
\\end{equation}\r
as it replaces the constant offset value $\\alpha_{j, 0}$ with a prediction from a user-specified model. In essence, this modification permits us to incorporate prior knowledge into the for loop of gradient boosting, as the for loop proceeds to build the linear sum by computing residuals that depend upon predictions from the user-specified model instead of the optimal constant model: $\\mbox{argmin} \\sum_{i=1}^{m_{train}} \\ell_{j}(Y_{j}^{(i)}, c),$ where $m_{train}$ denotes the number of training examples, $\\ell_{j}$ denotes a single-target loss function, and $c \\in \\mathbb{R}$ denotes a real number, e.g, $\\mbox{argmin} \\sum_{i=1}^{m_{train}} (Y_{j}^{(i)} - c)^{2} = \\frac{\\sum_{i=1}^{m_{train}} Y_{j}^{(i)}}{m_{train}}.$""" ;
    skos:prefLabel "Base Boosting" .

:BasicVSR a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2012.02181v2> ;
    skos:definition "**BasicVSR** is a video super-resolution pipeline including optical flow and [residual blocks](https://paperswithcode.com/method/residual-connection). It adopts a typical bidirectional recurrent network. The upsampling module $U$ contains multiple [pixel-shuffle](https://paperswithcode.com/method/pixelshuffle) and convolutions. In the Figure, red and blue colors represent the backward and forward propagations, respectively.  The propagation branches contain only generic components. $S, W$, and $R$ refer to the flow estimation module, spatial warping module, and residual blocks, respectively." ;
    skos:prefLabel "BasicVSR" .

:BatchChannelNormalization a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1911.09738v1> ;
    rdfs:seeAlso <https://github.com/joe-siyuan-qiao/Batch-Channel-Normalization/blob/ad9516d65e0f2c3a0091e532a8bacc4949f4d970/README.md> ;
    skos:definition "**Batch-Channel Normalization**, or **BCN**, uses batch knowledge to prevent channel-normalized models from getting too close to \"elimination singularities\". Elimination singularities correspond to the points on the training trajectory where neurons become consistently deactivated. They cause degenerate manifolds in the loss landscape which will slow down training and harm model performances." ;
    skos:prefLabel "BatchChannel Normalization" .

:BatchFormer a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2203.01522v2> ;
    skos:altLabel "Batch Transformer" ;
    skos:definition "learn to explore the sample relationships via transformer networks" ;
    skos:prefLabel "BatchFormer" .

:BatchNormalization a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1502.03167v3> ;
    rdfs:seeAlso <https://github.com/google/jax/blob/36f91261099b00194922bd93ed1286fe1c199724/jax/experimental/stax.py#L116> ;
    skos:definition """**Batch Normalization** aims to reduce internal covariate shift, and in doing so aims to accelerate the training of deep neural nets. It accomplishes this via a normalization step that fixes the means and variances of layer inputs. Batch Normalization also has a beneficial effect on the gradient flow through the network, by reducing the dependence of gradients on the scale of the parameters or of their initial values. This allows for use of much higher learning rates without the risk of divergence. Furthermore, batch normalization regularizes the model and reduces the need for [Dropout](https://paperswithcode.com/method/dropout).\r
\r
We apply a batch normalization layer as follows for a minibatch $\\mathcal{B}$:\r
\r
$$ \\mu\\_{\\mathcal{B}} = \\frac{1}{m}\\sum^{m}\\_{i=1}x\\_{i} $$\r
\r
$$ \\sigma^{2}\\_{\\mathcal{B}} = \\frac{1}{m}\\sum^{m}\\_{i=1}\\left(x\\_{i}-\\mu\\_{\\mathcal{B}}\\right)^{2} $$\r
\r
$$ \\hat{x}\\_{i} = \\frac{x\\_{i} - \\mu\\_{\\mathcal{B}}}{\\sqrt{\\sigma^{2}\\_{\\mathcal{B}}+\\epsilon}} $$\r
\r
$$ y\\_{i} = \\gamma\\hat{x}\\_{i} + \\beta = \\text{BN}\\_{\\gamma, \\beta}\\left(x\\_{i}\\right) $$\r
\r
Where $\\gamma$ and $\\beta$ are learnable parameters.""" ;
    skos:prefLabel "Batch Normalization" .

:BatchNuclear-normMaximization a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2003.12237v1> ;
    skos:definition "**Batch Nuclear-norm Maximization** is an approach for aiding classification in label insufficient situations. It involves maximizing the nuclear-norm of the batch output matrix. The nuclear-norm of a matrix is an upper bound of the Frobenius-norm of the matrix. Maximizing nuclear-norm ensures large Frobenius-norm of the batch matrix, which leads to increased discriminability. The nuclear-norm of the batch matrix is also a convex approximation of the matrix rank, which refers to the prediction diversity." ;
    skos:prefLabel "Batch Nuclear-norm Maximization" .

:Batchboost a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2001.07627v1> ;
    rdfs:seeAlso <https://github.com/maciejczyzewski/batchboost/blob/93f01d9d9fbe86ca9ef0bf1540dc55e2dfd1ef2a/batchboost.py#L7> ;
    skos:definition "**Batchboost** is a variation on [MixUp](https://paperswithcode.com/method/mixup) that instead of mixing just two images, mixes many images together." ;
    skos:prefLabel "Batchboost" .

:BayesianREX a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2002.09089v4> ;
    skos:altLabel "Bayesian Reward Extrapolation" ;
    skos:definition "**Bayesian Reward Extrapolation** is a Bayesian reward learning algorithm that scales to high-dimensional imitation learning problems by pre-training a low-dimensional feature encoding via self-supervised tasks and then leveraging preferences over demonstrations to perform fast Bayesian inference." ;
    skos:prefLabel "Bayesian REX" .

:Beta-VAE a skos:Concept ;
    dcterms:source <https://openreview.net/forum?id=Sy2fzU9gl> ;
    rdfs:seeAlso <https://github.com/AntixK/PyTorch-VAE/blob/master/models/beta_vae.py> ;
    skos:definition """**Beta-VAE** is a type of variational autoencoder that seeks to discover disentangled latent factors. It modifies [VAEs](https://paperswithcode.com/method/vae) with an adjustable hyperparameter $\\beta$ that balances latent channel capacity and independence constraints with reconstruction accuracy. The idea is to maximize the probability of generating the real data while keeping the distance between the real and estimated distributions small, under a threshold $\\epsilon$. We can use the Kuhn-Tucker conditions to write this as a single equation:\r
\r
$$ \\mathcal{F}\\left(\\theta, \\phi, \\beta; \\mathbf{x}, \\mathbf{z}\\right) = \\mathbb{E}\\_{q\\_{\\phi}\\left(\\mathbf{z}|\\mathbf{x}\\right)}\\left[\\log{p}\\_{\\theta}\\left(\\mathbf{x}\\mid\\mathbf{z}\\right)\\right] - \\beta\\left[D\\_{KL}\\left(\\log{q}\\_{\\theta}\\left(\\mathbf{z}\\mid\\mathbf{x}\\right)||p\\left(\\mathbf{z}\\right)\\right) - \\epsilon\\right]$$\r
\r
where the KKT multiplier $\\beta$ is the regularization coefficient that constrains the capacity of the latent channel $\\mathbf{z}$ and puts implicit independence pressure on the learnt posterior due to the isotropic nature of the Gaussian prior $p\\left(\\mathbf{z}\\right)$.\r
\r
We write this again using the complementary slackness assumption to get the Beta-VAE formulation:\r
\r
$$ \\mathcal{F}\\left(\\theta, \\phi, \\beta; \\mathbf{x}, \\mathbf{z}\\right) \\geq  \\mathcal{L}\\left(\\theta, \\phi, \\beta; \\mathbf{x}, \\mathbf{z}\\right) = \\mathbb{E}\\_{q\\_{\\phi}\\left(\\mathbf{z}|\\mathbf{x}\\right)}\\left[\\log{p}\\_{\\theta}\\left(\\mathbf{x}\\mid\\mathbf{z}\\right)\\right] - \\beta\\{D}\\_{KL}\\left(\\log{q}\\_{\\theta}\\left(\\mathbf{z}\\mid\\mathbf{x}\\right)||p\\left(\\mathbf{z}\\right)\\right)$$""" ;
    skos:prefLabel "Beta-VAE" .

:BezierAlign a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2002.10200v2> ;
    skos:definition """**BezierAlign** is a feature sampling method for arbitrarily-shaped scene text recognition that exploits parameterization nature of a compact Bezier curve bounding box.  Unlike RoIAlign, the shape of sampling grid of BezierAlign is not rectangular. Instead, each column of the arbitrarily-shaped grid is orthogonal to the Bezier curve boundary of the text. The sampling points have equidistant interval in width and height, respectively, which are bilinear interpolated with respect to the coordinates.\r
\r
Formally given an input feature map and Bezier curve control points, we concurrently process all the output pixels of the rectangular output feature map with size $h\\_{\\text {out }} \\times w\\_{\\text {out }}$. Taking pixel $g\\_{i}$ with position $\\left(g\\_{i w}, g\\_{i h}\\right)$ (from output feature map) as an example, we calculate $t$ by:\r
\r
$$\r
t=\\frac{g\\_{i w}}{w\\_{o u t}}\r
$$\r
\r
We then calculate the point of upper Bezier curve boundary $tp$ and lower Bezier curve boundary $bp$. Using $tp$ and $bp$, we can linearly index the sampling point $op$ by:\r
\r
$$\r
op=bp \\cdot \\frac{g\\_{i h}}{h\\_{\\text {out }}}+tp \\cdot\\left(1-\\frac{g\\_{i h}}{h\\_{\\text {out }}}\\right)\r
$$\r
\r
With the position of $op$, we can easily apply bilinear interpolation to calculate the result. Comparisons among previous sampling methods and BezierAlign are shown in the Figure.""" ;
    skos:prefLabel "BezierAlign" .

:Bi-attention a skos:Concept ;
    dcterms:source <http://openaccess.thecvf.com/content_ICCV_2019/html/Fang_Bilinear_Attention_Networks_for_Person_Retrieval_ICCV_2019_paper.html> ;
    skos:altLabel "Bilinear Attention" ;
    skos:definition "Bi-attention employs the attention-in-attention (AiA) mechanism to capture second-order statistical information: the outer point-wise channel attention vectors are computed from the output of the inner channel attention." ;
    skos:prefLabel "Bi-attention" .

:Bi3D a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2005.07274v2> ;
    skos:definition "**Bi3D** is a stereo depth estimation framework that estimates depth via a series of binary classifications. Rather than testing if objects are at a particular depth *D*, as existing stereo methods do, it classifies them as being closer or farther than *D*. It takes the stereo pair and a disparity $d\\_{i}$ and produces a confidence map, which can be thresholded to yield the binary segmentation. To estimate depth on $N + 1$ quantization levels we run this network $N$ times and maximize the probability in Equation 8 (see paper). To estimate continuous depth, whether full or selective, we run the [SegNet](https://paperswithcode.com/method/segnet) block of Bi3DNet for each disparity level and work directly on the confidence volume." ;
    skos:prefLabel "Bi3D" .

:BiDet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2003.03961v1> ;
    skos:definition "**BiDet** is a binarized neural network learning method for efficient object detection. Conventional network binarization methods directly quantize the weights and activations in one-stage or two-stage detectors with constrained representational capacity, so that the information redundancy in the networks causes numerous false positives and degrades the performance significantly. On the contrary, BiDet fully utilizes the representational capacity of the binary neural networks for object detection by redundancy removal, through which the detection precision is enhanced with alleviated false positives. Specifically, the information bottleneck (IB) principle is generalized to object detection, where the amount of information in the high-level feature maps is constrained and the mutual information between the feature maps and object detection is maximized." ;
    skos:prefLabel "BiDet" .

:BiFPN a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1911.09070v7> ;
    rdfs:seeAlso <https://github.com/zylo117/Yet-Another-EfficientDet-Pytorch/blob/c533bc2de65135a6fe1d25ca437765c630943afb/efficientdet/model.py#L55> ;
    skos:definition """A **BiFPN**, or **Weighted Bi-directional Feature Pyramid Network**, is a type of feature pyramid network which allows easy and fast multi-scale feature fusion. It incorporates the multi-level feature fusion idea from [FPN](https://paperswithcode.com/method/fpn), [PANet](https://paperswithcode.com/method/panet) and [NAS-FPN](https://paperswithcode.com/method/nas-fpn) that enables information to flow in both the top-down and bottom-up directions, while using regular and efficient connections. It also utilizes a fast normalized fusion technique. Traditional approaches usually treat all features input to the FPN equally, even those with different resolutions. However, input features at different resolutions often have unequal contributions to the output features. Thus, the BiFPN adds an additional weight for each input feature allowing the network to learn the importance of each. All regular convolutions are also replaced with less expensive depthwise separable convolutions.\r
\r
Comparing with PANet, PANet added an extra bottom-up path for information flow at the expense of more computational cost. Whereas BiFPN optimizes these cross-scale connections by removing nodes with a single input edge, adding an extra edge from the original input to output node if they are on the same level, and treating each bidirectional path as one feature network layer (repeating it several times for more high-level future fusion).""" ;
    skos:prefLabel "BiFPN" .

:BiGAN a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1605.09782v7> ;
    rdfs:seeAlso <https://github.com/jeffdonahue/bigan> ;
    skos:altLabel "Bidirectional GAN" ;
    skos:definition """A **BiGAN**, or **Bidirectional GAN**, is a type of generative adversarial network where the generator  not only maps latent samples to generated data, but also has an inverse mapping from data to the latent representation. The motivation is to make a type of GAN that can learn rich representations for us in applications like unsupervised learning.\r
\r
In addition to the generator $G$ from the standard [GAN](https://paperswithcode.com/method/gan) framework, BiGAN includes an encoder $E$ which maps data $\\mathbf{x}$ to latent representations $\\mathbf{z}$. The BiGAN discriminator $D$ discriminates not only in data space ($\\mathbf{x}$ versus $G\\left(\\mathbf{z}\\right)$), but jointly in data and latent space (tuples $\\left(\\mathbf{x}, E\\left(\\mathbf{x}\\right)\\right)$ versus $\\left(G\\left(z\\right), z\\right)$), where the latent component is either an encoder output $E\\left(\\mathbf{x}\\right)$ or a generator input $\\mathbf{z}$.""" ;
    skos:prefLabel "BiGAN" .

:BiGCN a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2001.06362v1> ;
    skos:altLabel "Bi-Directional Graph Convolutional Network" ;
    skos:definition "" ;
    skos:prefLabel "BiGCN" .

:BiGG a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2006.15502v1> ;
    skos:definition "**BiGG** is an autoregressive model for generative modeling for sparse graphs. It utilizes sparsity to avoid generating the full adjacency matrix, and reduces the graph generation time complexity to $O(((n + m)\\log n)$. Furthermore, during training this autoregressive model can be parallelized with $O(\\log n)$ synchronization stages, which makes it much more efficient than other autoregressive models that require $\\Omega(n)$. The approach is based on three key elements: (1) an $O(\\log n)$ process for generating each edge using a binary tree data structure, inspired by R-MAT; (2) a tree-structured autoregressive model for generating the set of edges associated with each node; and (3) an autoregressive model defined over the sequence of nodes." ;
    skos:prefLabel "BiGG" .

:BiGRU a skos:Concept ;
    skos:altLabel "Bidirectional GRU" ;
    skos:definition """A **Bidirectional GRU**, or **BiGRU**, is a sequence processing model that consists of two [GRUs](https://paperswithcode.com/method/gru). one taking the input in a forward direction, and the other in a backwards direction. It is a bidirectional recurrent neural network with only the input and forget gates.\r
\r
Image Source: *Rana R (2016). Gated Recurrent Unit (GRU) for Emotion Classification from Noisy Speech.*""" ;
    skos:prefLabel "BiGRU" .

:BiLSTM a skos:Concept ;
    skos:altLabel "Bidirectional LSTM" ;
    skos:definition """A **Bidirectional LSTM**, or **biLSTM**, is a sequence processing model that consists of two LSTMs: one taking the input in a forward direction, and the other in a backwards direction. BiLSTMs effectively increase the amount of information available to the network, improving the context available to the algorithm (e.g. knowing what words immediately follow *and* precede a word in a sentence).\r
\r
Image Source: Modelling Radiological Language with Bidirectional Long Short-Term Memory Networks, Cornegruta et al""" ;
    skos:prefLabel "BiLSTM" .

:BiSeNetV2 a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2004.02147v1> ;
    skos:definition "**BiSeNet V2** is a two-pathway architecture for real-time semantic segmentation. One pathway is designed to capture the spatial details with wide channels and shallow layers, called Detail Branch. In contrast, the other pathway is introduced to extract the categorical semantics with narrow channels and deep layers, called Semantic Branch. The Semantic Branch simply requires a large receptive field to capture semantic context, while the detail information can be supplied by the Detail Branch. Therefore, the Semantic Branch can be made very lightweight with fewer channels and a fast-downsampling strategy. Both types of feature representation are merged to construct a stronger and more comprehensive feature representation." ;
    skos:prefLabel "BiSeNet V2" .

:Big-LittleModule a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1807.03848v3> ;
    rdfs:seeAlso <https://github.com/IBM/BigLittleNet/blob/dff465428df4e921fdb73ac5cd4510c20b303d95/models/blresnet.py#L63> ;
    skos:definition "**Big-Little Modules** are blocks for image models that have two branches: each of which represents a separate block from a deep model and a less deep counterpart. They were proposed as part of the [BigLittle-Net](https://paperswithcode.com/method/big-little-net) architecture. The two branches are fused with a linear combination and unit weights. These two branches are known as Big-Branch (more layers and channels at low resolutions) and Little-Branch (fewer layers and channels at high resolution)." ;
    skos:prefLabel "Big-Little Module" .

:Big-LittleNet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1807.03848v3> ;
    rdfs:seeAlso <https://github.com/IBM/BigLittleNet/blob/dff465428df4e921fdb73ac5cd4510c20b303d95/models/blresnet.py#L106> ;
    skos:definition """**Big-Little Net** is a convolutional neural network architecture for learning multi-scale feature representations. This is achieved by using a multi-branch network, which has different computational complexity at different branches with different resolutions. Through frequent merging of features from branches at distinct scales, the model obtains multi-scale features while using less computation.\r
\r
It consists of Big-Little Modules, which have two branches: each of which represents a separate block from a deep model and a less deep counterpart. The two branches are fused with linear combination + unit weights. These two branches are known as Big-Branch (more layers and channels at low resolutions) and Little-Branch (fewer layers and channels at high resolution).""" ;
    skos:prefLabel "Big-Little Net" .

:BigBiGAN a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1907.02544v2> ;
    rdfs:seeAlso <https://github.com/LEGO999/BigBiGAN-TensorFlow2.0/blob/master/model_small.py> ;
    skos:definition "**BigBiGAN** is a type of [BiGAN](https://paperswithcode.com/method/bigan) with a [BigGAN](https://paperswithcode.com/method/biggan) image generator. The authors initially used [ResNet](https://paperswithcode.com/method/resnet) as a baseline for the encoder $\\mathcal{E}$ followed by a 4-layer MLP with skip connections, but they experimented with RevNets and found they outperformed with increased network width, so opted for this type of encoder for the final architecture." ;
    skos:prefLabel "BigBiGAN" .

:BigBird a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2007.14062v2> ;
    skos:definition """**BigBird** is a [Transformer](https://paperswithcode.com/method/transformer) with a sparse attention mechanism that reduces the quadratic dependency of self-attention to linear in the number of tokens. BigBird is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model.  In particular, BigBird consists of three main parts:\r
\r
- A set of $g$ global tokens attending on all parts of the sequence.\r
- All tokens attending to a set of $w$ local neighboring tokens.\r
- All tokens attending to a set of $r$ random tokens.\r
\r
This leads to a high performing attention mechanism scaling to much longer sequence lengths (8x).""" ;
    skos:prefLabel "BigBird" .

:BigGAN a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1809.11096v2> ;
    rdfs:seeAlso <https://github.com/ajbrock/BigGAN-PyTorch/blob/master/BigGAN.py> ;
    skos:definition """**BigGAN** is a type of generative adversarial network that was designed for scaling generation to high-resolution, high-fidelity images. It includes a number of incremental changes and innovations. The baseline and incremental changes are:\r
\r
- Using [SAGAN](https://paperswithcode.com/method/sagan) as a baseline with spectral norm. for G and D, and using [TTUR](https://paperswithcode.com/method/ttur).\r
- Using a Hinge Loss [GAN](https://paperswithcode.com/method/gan) objective\r
- Using class-[conditional batch normalization](https://paperswithcode.com/method/conditional-batch-normalization) to provide class information to G (but with linear projection not MLP.\r
- Using a [projection discriminator](https://paperswithcode.com/method/projection-discriminator) for D to provide class information to D.\r
- Evaluating with EWMA of G's weights, similar to ProGANs.\r
\r
The innovations are:\r
\r
- Increasing batch sizes, which has a big effect on the Inception Score of the model.\r
- Increasing the width in each layer leads to a further Inception Score improvement.\r
- Adding skip connections from the latent variable $z$ to further layers helps performance.\r
- A new variant of [Orthogonal Regularization](https://paperswithcode.com/method/orthogonal-regularization).""" ;
    skos:prefLabel "BigGAN" .

:BigGAN-deep a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1809.11096v2> ;
    rdfs:seeAlso <https://github.com/ajbrock/BigGAN-PyTorch/blob/master/BigGANdeep.py> ;
    skos:definition """**BigGAN-deep** is a deeper version (4x) of [BigGAN](https://paperswithcode.com/method/biggan).  The main difference is a slightly differently designed [residual block](https://paperswithcode.com/method/residual-block). Here the $z$ vector is concatenated with the conditional vector without splitting it into chunks.  It is also based on residual blocks with bottlenecks. BigGAN-deep uses a different strategy than BigGAN aimed at preserving identity throughout the skip connections. In G, where the number of channels needs to be reduced, BigGAN-deep simply retains the first group of channels and drop the rest to produce the required number of channels. In D, where the number of channels should be increased, BigGAN-deep passes the input channels unperturbed, and concatenates them with the remaining channels produced by a 1 × 1 [convolution](https://paperswithcode.com/method/convolution). As far as the\r
network configuration is concerned, the discriminator is an exact reflection of the generator. \r
\r
There are two blocks at each resolution (BigGAN uses one), and as a result BigGAN-deep is four times\r
deeper than BigGAN. Despite their increased depth, the BigGAN-deep models have significantly\r
fewer parameters mainly due to the bottleneck structure of their residual blocks.""" ;
    skos:prefLabel "BigGAN-deep" .

:BilateralGrid a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1707.02880v2> ;
    skos:definition """Bilateral grid is a new data structure that enables fast edge-aware image processing. It enables edge-aware image manipulations such as local tone mapping on high resolution images in real time.\r
\r
Source: [Chen et al.](https://people.csail.mit.edu/sparis/publi/2007/siggraph/Chen_07_Bilateral_Grid.pdf)\r
\r
Image source: [Chen et al.](https://people.csail.mit.edu/sparis/publi/2007/siggraph/Chen_07_Bilateral_Grid.pdf)""" ;
    skos:prefLabel "Bilateral Grid" .

:BilateralGuidedAggregationLayer a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2004.02147v1> ;
    skos:definition "**Bilateral Guided Aggregation Layer** is a feature fusion layer for semantic segmentation that aims to enhance mutual connections and fuse different types of feature representation. It was used in the [BiSeNet V2](https://paperswithcode.com/method/bisenet-v2) architecture. Specifically, within the BiSeNet implementation, the layer was used to employ the contextual information of the Semantic Branch to guide the feature response of Detail Branch. With different scale guidance, different scale feature representations can be captured, which inherently encodes the multi-scale information." ;
    skos:prefLabel "Bilateral Guided Aggregation Layer" .

:BinaryBERT a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2012.15701v2> ;
    skos:definition "**BinaryBERT** is a [BERT](https://paperswithcode.com/method/bert)-variant that applies quantization in the form of weight binarization. Specifically, ternary weight splitting is proposed which initializes BinaryBERT by equivalently splitting from a half-sized ternary network. To obtain BinaryBERT, we first train a half-sized [ternary BERT](https://paperswithcode.com/method/ternarybert) model, and then apply a [ternary weight splitting](https://paperswithcode.com/method/ternary-weight-splitting) operator to obtain the latent full-precision and quantized weights as the initialization of the full-sized BinaryBERT. We then fine-tune BinaryBERT for further refinement." ;
    skos:prefLabel "BinaryBERT" .

:BlendMask a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2001.00309v3> ;
    skos:definition "**BlendMask** is an [instance segmentation framework](https://paperswithcode.com/methods/category/instance-segmentation-models) built on top of the[ FCOS](https://paperswithcode.com/method/fcos) object detector. The bottom module uses either backbone or [FPN](https://paperswithcode.com/method/fpn) features to predict a set of bases. A single [convolution](https://paperswithcode.com/methods/category/convolutions) layer is added on top of the detection towers to produce attention masks along with each bounding box prediction. For each predicted instance, the [blender](https://paperswithcode.com/method/blender) crops the bases with its bounding box and linearly combine them according the learned attention maps. Note that the Bottom Module can take features either from ‘C’, or ‘P’ as the input." ;
    skos:prefLabel "BlendMask" .

:BlendedDiffusion a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2111.14818v2> ;
    rdfs:seeAlso <https://github.com/omriav/blended-diffusion> ;
    skos:definition """Blended Diffusion enables a zero-shot local text-guided image editing of natural images.\r
Given an input image $x$, an input mask $m$ and a target guiding text $t$ - the method enables to change the masked area within the image corresponding the the guiding text s.t. the unmasked area is left unchanged.""" ;
    skos:prefLabel "Blended Diffusion" .

:Blender a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2001.00309v3> ;
    skos:definition """**Blender** is a proposal-based instance mask generation module which incorporates rich instance-level information with accurate dense pixel features. A single [convolution](https://paperswithcode.com/method/convolution) layer is added on top of the detection towers to produce attention masks along with each bounding box prediction. For each predicted instance, the blender crops predicted bases with its bounding box and linearly combines them according the learned attention maps.\r
\r
The inputs of the blender module are bottom-level bases $\\mathbf{B}$, the selected top-level attentions $A$ and bounding box proposals $P$. First [RoIPool](https://paperswithcode.com/method/roi-pooling) of Mask R-CNN to crop bases with each proposal $\\mathbf{p}\\_{d}$ and then resize the region to a fixed size $R \\times R$ feature map $\\mathbf{r}\\_{d}$\r
\r
$$\r
\\mathbf{r}\\_{d}=\\operatorname{RoIPool}_{R \\times R}\\left(\\mathbf{B}, \\mathbf{p}\\_{d}\\right), \\quad \\forall d \\in\\{1 \\ldots D\\}\r
$$\r
\r
More specifically,  asampling ratio 1 is used for [RoIAlign](https://paperswithcode.com/method/roi-align), i.e. one bin for each sampling point. During training, ground truth boxes are used as the proposals. During inference, [FCOS](https://paperswithcode.com/method/fcos) prediction results are used.\r
\r
The attention size $M$ is smaller than $R$. We interpolate $\\mathbf{a}\\_{d}$ from $M \\times M$ to $R \\times R$, into the shapes of $R=\\left\\(\\mathbf{r}\\_{d} \\mid d=1 \\ldots D\\right)$\r
\r
$$\r
\\mathbf{a}\\_{d}^{\\prime}=\\text { interpolate }\\_{M \\times M \\rightarrow R \\times R}\\left(\\mathbf{a}\\_{d}\\right), \\quad \\forall d \\in\\{1 \\ldots D\\}\r
$$\r
\r
Then $\\mathbf{a}\\_{d}^{\\prime}$ is normalized with a softmax function along the $K$ dimension to make it a set of score maps $\\mathbf{s}\\_{d}$.\r
\r
$$\r
\\mathbf{s}\\_{d}=\\operatorname{softmax}\\left(\\mathbf{a}\\_{d}^{\\prime}\\right), \\quad \\forall d \\in\\{1 \\ldots D\\}\r
$$\r
\r
Then we apply element-wise product between each entity $\\mathbf{r}\\_{d}, \\mathbf{s}\\_{d}$ of the regions $R$ and scores $S$, and sum along the $K$ dimension to get our mask logit $\\mathbf{m}\\_{d}:$\r
\r
$$\r
\\mathbf{m}\\_{d}=\\sum\\_{k=1}^{K} \\mathbf{s}\\_{d}^{k} \\circ \\mathbf{r}\\_{d}^{k}, \\quad \\forall d \\in\\{1 \\ldots D\\}\r
$$\r
\r
where $k$ is the index of the basis. The mask blending process with $K=4$ is visualized in the Figure.""" ;
    skos:prefLabel "Blender" .

:BlinkCommunication a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1910.04940v1> ;
    skos:definition "**Blink** is a communication library for inter-GPU parameter exchange that achieves near-optimal link utilization. To handle topology heterogeneity from hardware generations or partial allocations from cluster schedulers, Blink dynamically generates optimal communication primitives for a given topology. Blink probes the set of links available for a given job at runtime and builds a topology with appropriate link capacities. Given the topology, Blink achieves the optimal communication rate by packing spanning trees, that can utilize more links (Lovasz, 1976; Edmonds, 1973) when compared to rings. The authors use a multiplicative-weight update based approximation algorithm to quickly compute the maximal packing and extend the algorithm to further minimize the number of trees generated. Blink’s collectives extend across multiple machines effectively utilizing all available network interfaces." ;
    skos:prefLabel "Blink Communication" .

:BlueRiverControls a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2001.02254v1> ;
    skos:definition "**Blue River Controls** is a tool that allows users to train and test reinforcement learning algorithms on real-world hardware. It features a simple interface based on OpenAI Gym, that works directly on both simulation and hardware." ;
    skos:prefLabel "Blue River Controls" .

:BoomLayer a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1911.11423v2> ;
    rdfs:seeAlso <https://github.com/Smerity/sha-rnn/blob/218d748022dbcf32d50bbbb4d151a9b6de3f8bba/model.py#L341> ;
    skos:definition """A **Boom Layer** is a type of feedforward layer that is closely related to the feedforward layers used in Transformers. The layer takes a vector of the form $v \\in \\mathbb{R}^{H}$ and uses a matrix\r
multiplication with a GeLU activation to produce a vector $u \\in \\mathbb{R}^{N\\times{H}}$. We then break $u$ into $N$ vectors and sum those together, producing $w \\in \\mathbb{R}^{H}$. This minimizes computation and removes an entire matrix of parameters compared to traditional down-projection layers.\r
\r
The Figure to the right shows the Boom Layer used in the context of [SHA-RNN](https://paperswithcode.com/method/sha-rnn) from the original paper.""" ;
    skos:prefLabel "Boom Layer" .

:Boost-GNN a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2101.08543v2> ;
    skos:definition "**Boost-GNN** is an architecture that trains GBDT and GNN jointly to get the best of both worlds: the GBDT model deals with heterogeneous features, while GNN accounts for the graph structure. The model benefits from end-to-end optimization by allowing new trees to fit the gradient updates of GNN." ;
    skos:prefLabel "Boost-GNN" .

:Bort a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2010.10499v2> ;
    skos:definition "**Bort** is a parametric architectural variant of the [BERT](https://paperswithcode.com/method/bert) architecture. It extracts an optimal subset of architectural parameters for the BERT architecture through a [neural architecture search](https://paperswithcode.com/method/neural-architecture-search) approach; in particular, a fully polynomial-time approximation scheme (FPTAS). This optimal subset - “Bort” - is demonstrably smaller, having an effective size of $5.5 \\%$ the original BERT-large architecture, and $16\\%$ of the net size. Bort is also able to be pretrained in $288$ GPU hours, which is $1.2\\%$ less than the time required to pretrain the highest-performing BERT parametric architecture variant, RoBERTa-large ([RoBERTa](https://paperswithcode.com/method/roberta)), and about $33\\%" ;
    skos:prefLabel "Bort" .

:BottleneckResidualBlock a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1512.03385v1> ;
    rdfs:seeAlso <https://github.com/pytorch/vision/blob/1aef87d01eec2c0989458387fa04baebcc86ea7b/torchvision/models/resnet.py#L75> ;
    skos:definition "A **Bottleneck Residual Block** is a variant of the [residual block](https://paperswithcode.com/method/residual-block) that utilises 1x1 convolutions to create a bottleneck. The use of a bottleneck reduces the number of parameters and matrix multiplications. The idea is to make residual blocks as thin as possible to increase depth and have less parameters. They were introduced as part of the [ResNet](https://paperswithcode.com/method/resnet) architecture, and are used as part of deeper ResNets such as ResNet-50 and ResNet-101." ;
    skos:prefLabel "Bottleneck Residual Block" .

:BottleneckTransformer a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2101.11605v2> ;
    skos:definition "The **Bottleneck Transformer (BoTNet) ** is an image classification model that incorporates self-attention for multiple computer vision tasks including image classification, object detection and instance segmentation. By just replacing the spatial convolutions with global self-attention in the final three bottleneck blocks of a [ResNet](https://paperswithcode.com/method/resnet) and no other changes, the approach improves upon baselines significantly on instance segmentation and object detection while also reducing the parameters, with minimal overhead in latency." ;
    skos:prefLabel "Bottleneck Transformer" .

:BottleneckTransformerBlock a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2101.11605v2> ;
    skos:definition "A **Bottleneck Transformer Block** is a block used in [Bottleneck Transformers](https://www.paperswithcode.com/method/bottleneck-transformer) that replaces the spatial 3 × 3 [convolution](https://paperswithcode.com/method/convolution) layer in a [Residual Block](https://paperswithcode.com/method/residual-block) with Multi-Head Self-Attention (MHSA)." ;
    skos:prefLabel "Bottleneck Transformer Block" .

:Bottom-upPathAugmentation a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1803.01534v4> ;
    rdfs:seeAlso <https://github.com/ShuLiu1993/PANet/blob/2644d5ad6ae98c2bf58df45c8792c019b1d7b2b9/lib/modeling/FPN.py#L135> ;
    skos:definition """**Bottom-up Path Augmentation** is a feature extraction technique that seeks to shorten the information path and enhance a feature pyramid with accurate localization signals existing in low-levels. This is based on the fact that high response to edges or instance parts is a strong indicator to accurately localize instances. \r
\r
Each building block takes a higher resolution feature map $N\\_{i}$ and a coarser map $P\\_{i+1}$ through lateral connection and generates the new feature map $N\\_{i+1}$ Each feature map $N\\_{i}$ first goes through a $3 \\times 3$ convolutional layer with stride $2$ to reduce the spatial size. Then each element of feature map $P\\_{i+1}$ and the down-sampled map are added through lateral connection. The fused feature map is then processed by another $3 \\times 3$ convolutional layer to generate $N\\_{i+1}$ for following sub-networks. This is an iterative process and terminates after approaching $P\\_{5}$. In these building blocks, we consistently use channel 256 of feature maps. The feature grid for each proposal is then pooled from new feature maps, i.e., {$N\\_{2}$, $N\\_{3}$, $N\\_{4}$, $N\\_{5}$}.""" ;
    skos:prefLabel "Bottom-up Path Augmentation" .

:BoundaryNet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2108.09433v1> ;
    skos:definition "**BoundaryNet** is a resizing-free approach for layout annotation. The variable-sized user selected region of interest is first processed by an attention-guided skip network. The network optimization is guided via Fast Marching distance maps to obtain a good quality initial boundary estimate and an associated feature representation. These outputs are processed by a Residual Graph [Convolution](https://paperswithcode.com/method/convolution) Network optimized using Hausdorff loss to obtain the final region boundary." ;
    skos:prefLabel "BoundaryNet" .

:Branchattention a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1507.06228v2> ;
    skos:definition "Branch attention can be seen as a dynamic branch selection mechanism: which to pay attention to, used with a multi-branch structure." ;
    skos:prefLabel "Branch attention" .

:Bridge-net a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1807.07281v3> ;
    skos:definition "**Bridge-net** is an audio model block used in the [ClariNet](https://paperswithcode.com/method/clarinet) text-to-speech architecture. Bridge-net maps frame-level hidden representation to sample-level through several [convolution](https://paperswithcode.com/method/convolution) blocks and [transposed convolution](https://paperswithcode.com/method/transposed-convolution) layers interleaved with softsign non-linearities." ;
    skos:prefLabel "Bridge-net" .

:BytePS a skos:Concept ;
    skos:definition "**BytePS** is a distributed training method for deep neural networks. BytePS handles cases with varying number of CPU machines and makes traditional all-reduce and PS as two special cases of its framework. To further accelerate DNN training, BytePS proposes Summation Service and splits a DNN optimizer into two parts: gradient summation and parameter update. It keeps the CPU-friendly part, gradient summation, in CPUs, and moves parameter update, which is more computation heavy, to GPUs." ;
    skos:prefLabel "BytePS" .

:ByteScheduler a skos:Concept ;
    skos:definition "**ByteScheduler** is a generic communication scheduler for distributed DNN training acceleration. It is based on analysis that partitioning and rearranging the tensor transmissions can result in optimal results in theory and good performance in real-world even with scheduling overhead." ;
    skos:prefLabel "ByteScheduler" .

:CABiNet a skos:Concept ;
    skos:altLabel "Context Aggregated Bi-lateral Network for Semantic Segmentation" ;
    skos:definition "With the increasing demand of autonomous systems, pixelwise semantic segmentation for visual scene understanding needs to be not only accurate but also efficient for potential real-time applications. In this paper, we propose Context Aggregation Network, a dual branch convolutional neural network, with significantly lower computational costs as compared to the state-of-the-art, while maintaining a competitive prediction accuracy. Building upon the existing dual branch architectures for high-speed semantic segmentation, we design a high resolution branch for effective spatial detailing and a context branch with light-weight versions of global aggregation and local distribution blocks, potent to capture both long-range and local contextual dependencies required for accurate semantic segmentation, with low computational overheads. We evaluate our method on two semantic segmentation datasets, namely Cityscapes dataset and UAVid dataset. For Cityscapes test set, our model achieves state-of-the-art results with mIOU of 75.9%, at 76 FPS on an NVIDIA RTX 2080Ti and 8 FPS on a Jetson Xavier NX. With regards to UAVid dataset, our proposed network achieves mIOU score of 63.5% with high execution speed (15 FPS)." ;
    skos:prefLabel "CABiNet" .

:CAG a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2007.05405v1> ;
    skos:altLabel "Class activation guide" ;
    skos:definition """Class activation guide is a module which uses weak localization information from the instrument activation maps to guide the verb and target recognition. \r
\r
Image source: [Nwoye et al.](https://arxiv.org/pdf/2007.05405v1.pdf)""" ;
    skos:prefLabel "CAG" .

:CAM a skos:Concept ;
    dcterms:source <http://openaccess.thecvf.com/content_cvpr_2015/html/Oquab_Is_Object_Localization_2015_CVPR_paper.html> ;
    skos:altLabel "Class-activation map" ;
    skos:definition """Class activation maps could be used to interpret the prediction decision made by the convolutional neural network (CNN).\r
\r
Image source: [Learning Deep Features for Discriminative Localization](https://paperswithcode.com/paper/learning-deep-features-for-discriminative)""" ;
    skos:prefLabel "CAM" .

:CAMoE a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2109.04290v3> ;
    skos:definition "**CAMoE** is a multi-stream Corpus Alignment network with single gate Mixture-of-Experts (MoE) for video-text retrieval. The CAMoE employs Mixture-of-Experts (MoE) to extract multi-perspective video representations, including action, entity, scene, etc., then align them with the corresponding part of the text. A [Dual Softmax Loss](https://paperswithcode.com/method/dual-softmax-loss) (DSL) is used to avoid the one-way optimum-match which occurs in previous contrastive methods. Introducing the intrinsic prior of each pair in a batch, DSL serves as a reviser to correct the similarity matrix and achieves the dual optimal match." ;
    skos:prefLabel "CAMoE" .

:CANINE a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2103.06874v4> ;
    skos:definition "**CANINE** is a pre-trained encoder for language understanding that operates directly on character sequences—without explicit tokenization or vocabulary—and a pre-training strategy with soft inductive biases in place of hard token boundaries. To use its finer-grained input effectively and efficiently, Canine combines downsampling, which reduces the input sequence length, with a deep [transformer](https://paperswithcode.com/method/transformer) stack, which encodes context." ;
    skos:prefLabel "CANINE" .

:CARAFE a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1905.02188v3> ;
    skos:definition "**Content-Aware ReAssembly of FEatures (CARAFE)** is an operator for feature upsampling in convolutional neural networks. CARAFE has several appealing properties: (1) Large field of view. Unlike previous works (e.g. bilinear interpolation) that only exploit subpixel neighborhood, CARAFE can aggregate contextual information within a large receptive field. (2) Content-aware handling. Instead of using a fixed kernel for all samples (e.g. deconvolution), CARAFE enables instance-specific content-aware handling, which generates adaptive kernels on-the-fly. (3) Lightweight and fast to compute." ;
    skos:prefLabel "CARAFE" .

:CARLA a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1711.03938v1> ;
    skos:altLabel "CARLA: An Open Urban Driving Simulator" ;
    skos:definition """CARLA is an open-source simulator for autonomous driving research. CARLA has been developed from the ground up to support development, training, and validation of autonomous urban driving systems. In addition to open-source code and protocols, CARLA provides open digital assets (urban layouts, buildings, vehicles) that were created for this purpose and can be used freely. \r
\r
Source: [Dosovitskiy et al.](https://arxiv.org/pdf/1711.03938v1.pdf)\r
\r
Image source: [Dosovitskiy et al.](https://arxiv.org/pdf/1711.03938v1.pdf)""" ;
    skos:prefLabel "CARLA" .

:CBAM a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1807.06521v2> ;
    rdfs:seeAlso <https://github.com/Jongchan/attention-module/blob/c06383c514ab0032d044cc6fcd8c8207ea222ea7/MODELS/cbam.py#L84> ;
    skos:altLabel "Convolutional Block Attention Module" ;
    skos:definition """**Convolutional Block Attention Module (CBAM)** is an attention module for convolutional neural networks. Given an intermediate feature map, the module sequentially infers attention maps along two separate dimensions, channel and spatial, then the attention maps are multiplied to the input feature map for adaptive feature refinement.\r
\r
Given an intermediate feature map $\\mathbf{F} \\in \\mathbb{R}^{C×H×W}$ as input, CBAM sequentially infers a 1D channel attention map $\\mathbf{M}\\_{c} \\in \\mathbb{R}^{C×1×1}$ and a 2D spatial attention map $\\mathbf{M}\\_{s} \\in \\mathbb{R}^{1×H×W}$. The overall attention process can be summarized as:\r
\r
$$ \\mathbf{F}' = \\mathbf{M}\\_{c}\\left(\\mathbf{F}\\right) \\otimes \\mathbf{F} $$\r
\r
$$ \\mathbf{F}'' = \\mathbf{M}\\_{s}\\left(\\mathbf{F'}\\right) \\otimes \\mathbf{F'} $$\r
\r
During multiplication, the attention values are broadcasted (copied) accordingly: channel attention values are broadcasted along the spatial dimension, and vice versa. $\\mathbf{F}''$ is the final refined\r
output.""" ;
    skos:prefLabel "CBAM" .

:CBHG a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1703.10135v2> ;
    rdfs:seeAlso <https://github.com/mozilla/TTS/blob/3cbf9052f78ac40f025cdf598437eabc9bda9298/layers/tacotron.py#L93> ;
    skos:definition """**CBHG** is a building block used in the [Tacotron](https://paperswithcode.com/method/tacotron) text-to-speech model. It consists of a bank of 1-D convolutional filters, followed by highway networks and a bidirectional gated recurrent unit ([BiGRU](https://paperswithcode.com/method/bigru)). \r
\r
The module is used to extract representations from sequences. The input sequence is first\r
convolved with $K$ sets of 1-D convolutional filters, where the $k$-th set contains $C\\_{k}$ filters of width $k$ (i.e. $k = 1, 2, \\dots , K$). These filters explicitly model local and contextual information (akin to modeling unigrams, bigrams, up to K-grams). The [convolution](https://paperswithcode.com/method/convolution) outputs are stacked together and further max pooled along time to increase local invariances. A stride of 1 is used to  preserve the original time resolution. The processed sequence is further passed to a few fixed-width 1-D convolutions, whose outputs are added with the original input sequence via residual connections. [Batch normalization](https://paperswithcode.com/method/batch-normalization) is used for all convolutional layers. The convolution outputs are fed into a multi-layer [highway network](https://paperswithcode.com/method/highway-network) to extract high-level features. Finally, a bidirectional [GRU](https://paperswithcode.com/method/gru) RNN is stacked on top to extract sequential features from both forward and backward context.""" ;
    skos:prefLabel "CBHG" .

:CBNet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1909.03625v1> ;
    skos:altLabel "Composite Backbone Network" ;
    skos:definition """**CBNet** is a backbone architecture that consists of multiple identical backbones (specially called Assistant Backbones and Lead Backbone) and composite connections between neighbor backbones. From left to right, the output of each stage in an Assistant Backbone, namely higher-level\r
features, flows to the parallel stage of the succeeding backbone as part of inputs through composite connections. Finally, the feature maps of the last backbone named Lead\r
Backbone are used for object detection. The features extracted by CBNet for object detection fuse the high-level and low-level features of multiple backbones, hence improve the detection performance.""" ;
    skos:prefLabel "CBNet" .

:CBoWWord2Vec a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1301.3781v3> ;
    skos:altLabel "Continuous Bag-of-Words Word2Vec" ;
    skos:definition """**Continuous Bag-of-Words Word2Vec** is an architecture for creating word embeddings that uses $n$ future words as well as $n$ past words to create a word embedding. The objective function for CBOW is:\r
\r
$$ J\\_\\theta = \\frac{1}{T}\\sum^{T}\\_{t=1}\\log{p}\\left(w\\_{t}\\mid{w}\\_{t-n},\\ldots,w\\_{t-1}, w\\_{t+1},\\ldots,w\\_{t+n}\\right) $$\r
\r
In the CBOW model, the distributed representations of context are used to predict the word in the middle of the window. This contrasts with [Skip-gram Word2Vec](https://paperswithcode.com/method/skip-gram-word2vec) where the distributed representation of the input word is used to predict the context.""" ;
    skos:prefLabel "CBoW Word2Vec" .

:CCAC a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2006.08914v1> ;
    skos:altLabel "Confidence Calibration with an Auxiliary Class)" ;
    skos:definition "**Confidence Calibration with an Auxiliary Class**, or **CCAC**, is a post-hoc confidence calibration method for DNN classifiers on OOD datasets. The key feature of CCAC is an auxiliary class in the calibration model which separates mis-classified samples from correctly classified ones, thus effectively mitigating the target DNN’s being confidently wrong. It also reduces the number of free parameters in CCAC to reduce free parameters and facilitate transfer to a new unseen dataset." ;
    skos:prefLabel "CCAC" .

:CCNet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1811.11721v2> ;
    skos:altLabel "Criss-Cross Network" ;
    skos:definition """**Criss-Cross Network** (**CCNet**) aims to obtain full-image contextual information in an effective and efficient way. Concretely,\r
for each pixel, a novel criss-cross attention module harvests the contextual information of all the pixels on its criss-cross path. By taking a further recurrent operation, each pixel can finally capture the full-image dependencies. **CCNet** is with the following\r
merits: **1)** GPU memory friendly. Compared with the [non-local block](https://paperswithcode.com/method/non-local-block), the proposed recurrent criss-cross attention module requires 11× less GPU memory usage. **2)** High computational efficiency. The recurrent criss-cross attention significantly reduces FLOPs by about 85% of the non-local block. **3)** The state-of-the-art performance.""" ;
    skos:prefLabel "CCNet" .

:CCT a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2104.05704v4> ;
    skos:altLabel "Compact Convolutional Transformers" ;
    skos:definition "**Compact Convolutional Transformers** utilize sequence pooling and replace the patch embedding with a convolutional embedding, allowing for better inductive bias and making positional embeddings optional. CCT achieves better accuracy than ViT-Lite (smaller ViTs) and increases the flexibility of the input parameters." ;
    skos:prefLabel "CCT" .

:CDCC-NET a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2009.10181v5> ;
    skos:definition "CDCC-NET is a multi-task network that analyzes the detected counter region and predicts 9 outputs: eight float numbers referring to the corner positions (x0/w, y0/h, ... , x3/w, y3/h) and an array containing two float numbers regarding the probability of the counter being legible/operational or illegible/faulty." ;
    skos:prefLabel "CDCC-NET" .

:CDEP a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1909.13584v4> ;
    rdfs:seeAlso <https://github.com/laura-rieger/deep-explanation-penalization> ;
    skos:altLabel "Contextual Decomposition Explanation Penalization" ;
    skos:definition """**Contextual Decomposition Explanation Penalization (CDEP)** is a method which leverages existing explanation techniques for neural networks in order to prevent a model from learning\r
unwanted relationships and ultimately improve predictive accuracy. Given particular importance\r
scores, CDEP works by allowing the user to directly penalize importances of certain features, or\r
interactions. This forces the neural network to not only produce the correct prediction, but also the\r
correct explanation for that prediction""" ;
    skos:prefLabel "CDEP" .

:CDIL-CNN a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2201.02143v2> ;
    skos:altLabel "Circular Dilated Convolutional Neural Networks" ;
    skos:definition "" ;
    skos:prefLabel "CDIL-CNN" .

:CELU a skos:Concept ;
    skos:altLabel "Continuously Differentiable Exponential Linear Units" ;
    skos:definition """Exponential Linear Units (ELUs) are a useful rectifier for constructing deep learning architectures, as they may speed up and otherwise improve learning by virtue of not have vanishing gradients and by having mean activations near zero. However, the ELU activation as parametrized in [1] is not continuously differentiable with respect to its input when the shape parameter alpha is not equal to 1. We present an alternative parametrization which is C1 continuous for all values of alpha, making the rectifier easier to reason about and making alpha easier to tune. This alternative parametrization has several other useful properties that the original parametrization of ELU does not: 1) its derivative with respect to x is bounded, 2) it contains both the linear transfer function and ReLU as special cases, and 3) it is scale-similar with respect to alpha.\r
$$\\text{CELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x/\\alpha) - 1))$$""" ;
    skos:prefLabel "CELU" .

:CGMM a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1805.10636v2> ;
    skos:altLabel "Contextual Graph Markov Model" ;
    skos:definition """Contextual Graph Markov Model (CGMM) is an approach combining ideas from generative models and neural networks for the processing of graph data. It founds on a constructive methodology to build a deep architecture comprising layers of probabilistic models that learn to encode the structured information in an incremental fashion. Context is diffused in an efficient and scalable way across the graph vertexes and edges. The resulting graph encoding is used in combination with discriminative models to address structure classification benchmarks.\r
\r
Description and image from: [Contextual Graph Markov Model: A Deep and Generative Approach to Graph Processing](https://arxiv.org/pdf/1805.10636.pdf)""" ;
    skos:prefLabel "CGMM" .

:CGNN a skos:Concept ;
    dcterms:source <https://www.researchgate.net/publication/333667001_Crystal_Graph_Neural_Networks_for_Data_Mining_in_Materials_Science> ;
    skos:altLabel "Crystal Graph Neural Network" ;
    skos:definition "The full architecture of CGNN is presented at [CGNN's official site](https://tony-y.github.io/cgnn/architectures/)." ;
    skos:prefLabel "CGNN" .

:CGRU a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1511.06432v4> ;
    skos:altLabel "Convolutional GRU" ;
    skos:definition """A **Convolutional Gated Recurrent Unit** is a type of [GRU](https://paperswithcode.com/method/gru) that combines GRUs with the [convolution](https://paperswithcode.com/method/convolution) operation. The update rule for input $x\\_{t}$ and the previous output $h\\_{t-1}$ is given by the following:\r
\r
$$ r = \\sigma\\left(W\\_{r} \\star\\_{n}\\left[h\\_{t-1};x\\_{t}\\right] + b\\_{r}\\right) $$\r
\r
$$ u = \\sigma\\left(W\\_{u} \\star\\_{n}\\left[h\\_{t-1};x\\_{t}\\right] + b\\_{u} \\right) $$\r
\r
$$ c = \\rho\\left(W\\_{c} \\star\\_{n}\\left[x\\_{t}; r \\odot h\\_{t-1}\\right] + b\\_{c} \\right) $$\r
\r
$$ h\\_{t} = u \\odot h\\_{t-1} + \\left(1-u\\right) \\odot c $$\r
\r
In these equations $\\sigma$ and $\\rho$ are the elementwise sigmoid and [ReLU](https://paperswithcode.com/method/relu) functions respectively and the $\\star\\_{n}$ represents a convolution with a kernel of size $n \\times n$. Brackets are used to represent a feature concatenation.""" ;
    skos:prefLabel "CGRU" .

:CHM a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2109.05221v1> ;
    skos:altLabel "Convolutional Hough Matching" ;
    skos:definition "**Convolutional Hough Matching**, or **CHM**, is a geometric matching algorithm that distributes similarities of candidate matches over a geometric transformation space and evaluates them in a convolutional manner. It is casted into a trainable neural layer with a  semi-isotropic high-dimensional kernel, which learns non-rigid matching with a small number of interpretable parameters." ;
    skos:prefLabel "CHM" .

:CIDA a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2007.01807v2> ;
    skos:altLabel "Continuously Indexed Domain Adaptation" ;
    skos:definition """**Continuously Indexed Domain Adaptation** combines traditional adversarial adaptation with a novel discriminator that models the encoding-conditioned domain index distribution.\r
\r
Image Source: [Wang et al.](https://arxiv.org/pdf/2007.01807v2.pdf)""" ;
    skos:prefLabel "CIDA" .

:CInCFlow a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2107.01358v1> ;
    skos:altLabel "Characterizable Invertible 3x3 Convolution" ;
    skos:definition "Characterizable Invertible $3\\times3$  Convolution" ;
    skos:prefLabel "CInC Flow" .

:CKConv a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2102.02611v3> ;
    skos:altLabel "Continuous Kernel Convolution" ;
    skos:definition "" ;
    skos:prefLabel "CKConv" .

:CLIP a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2103.00020v1> ;
    skos:altLabel "Contrastive Language-Image Pre-training" ;
    skos:definition """**Contrastive Language-Image Pre-training** (**CLIP**), consisting of a simplified version of ConVIRT trained from scratch, is an efficient method of image representation learning from natural language supervision. , CLIP jointly trains an image encoder and a text encoder to predict the correct pairings of a batch of (image, text) training examples. At test time the learned text encoder synthesizes a zero-shot linear classifier by embedding the names or descriptions of the target dataset’s classes. \r
\r
For pre-training, CLIP is trained to predict which of the $N X N$ possible (image, text) pairings across a batch actually occurred. CLIP learns a multi-modal embedding space by jointly training an image encoder and text encoder to maximize the cosine similarity of the image and text embeddings of the $N$ real pairs in the batch while minimizing the cosine similarity of the embeddings of the $N^2 - N$ incorrect pairings. A symmetric cross entropy loss is optimized over these similarity scores. \r
\r
Image credit: [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/pdf/2103.00020.pdf)""" ;
    skos:prefLabel "CLIP" .

:CLIPort a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2109.12098v1> ;
    skos:definition "CLIPort, a language-conditioned imitation-learning agent that combines the broad semantic understanding (what) of CLIP [1] with the spatial precision (where) of Transporter [2]." ;
    skos:prefLabel "CLIPort" .

:CLRNet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2009.07480v1> ;
    skos:altLabel "Convolutional LSTM based Residual Network" ;
    skos:definition "" ;
    skos:prefLabel "CLRNet" .

:CMCL a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2012.15409v4> ;
    skos:altLabel "Crossmodal Contrastive Learning" ;
    skos:definition "**CMCL**, or **Crossmodal Contrastive Learning**, is a method for unifying visual and textual representations into the same semantic space based on a large-scale corpus of image collections, text corpus and image-text pairs. The CMCL aligns the visual representations and textual representations, and unifies them into the same semantic space based on image-text pairs. As shown in the Figure, to facilitate different levels of semantic alignment between vision and language, a series of text rewriting techniques are utilized to improve the diversity of cross-modal information. Specifically, for an image-text pair, various positive examples and hard negative examples can be obtained by rewriting the original caption at different levels. Moreover, to incorporate more background information from the single-modal data, text and image retrieval are also applied to augment each image-text pair with various related texts and images. The positive pairs, negative pairs, related images and texts are learned jointly by CMCL. In this way, the model can effectively unify different levels of visual and textual representations into the same semantic space, and incorporate more single-modal knowledge to enhance each other." ;
    skos:prefLabel "CMCL" .

:CNNBiLSTM a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1511.08308v5> ;
    skos:altLabel "CNN Bidirectional LSTM" ;
    skos:definition "A **CNN BiLSTM** is a hybrid bidirectional [LSTM](https://paperswithcode.com/method/lstm) and CNN architecture. In the original formulation applied to named entity recognition, it learns both character-level and word-level features. The CNN component is used to induce the character-level features. For each word the model employs a [convolution](https://paperswithcode.com/method/convolution) and a [max pooling](https://paperswithcode.com/method/max-pooling) layer to extract a new feature vector from the per-character feature vectors such as character embeddings and (optionally) character type." ;
    skos:prefLabel "CNN BiLSTM" .

:COCO-FUNIT a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2007.07431v3> ;
    skos:definition """**COCO-FUNIT** is few-shot image translation model which computes the style embedding of the example images conditioned on the input image and a new module called the constant style bias. It builds on top of [FUNIT](https://arxiv.org/abs/1905.01723) by identifying the content loss problem and then addressing it with a novel content-conditioned style encoder architecture.\r
\r
The FUNIT method suffers from the content loss problem—the translation result is not well-aligned with the input image. While a direct theoretical analysis is likely elusive, we conduct an empirical study, aiming at identify the cause of the content loss problem. In analyses, the authors show that the FUNIT style encoder produces very different style codes using different crops -- suggesting the style code contains other information about the style image such as the object pose.\r
\r
To make the style embedding more robust to small variations in the style image, a new style encoder architecture, the Content-Conditioned style encoder (COCO), is introduced. The most distinctive feature of this new encoder is the conditioning in the content image as illustrated in the top-right of the Figure. Unlike the style encoder in FUNIT, COCO takes both content and style image as input. With this content-conditioning scheme, a direct feedback path is created during learning to let the content image influence how the style code is computed. It also helps reduce the direct influence of the style image to the extract style code.""" ;
    skos:prefLabel "COCO-FUNIT" .

:COLA a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2010.10915v1> ;
    skos:definition "**COLA** is a self-supervised pre-training approach for learning a general-purpose representation of audio. It is based on contrastive learning: it learns a representation which assigns high similarity to audio segments extracted from the same recording while assigning lower similarity to segments from different recordings." ;
    skos:prefLabel "COLA" .

:CORAD a skos:Concept ;
    dcterms:source <https://ieeexplore.ieee.org/abstract/document/9005580> ;
    skos:altLabel "CORAD: Correlation-Aware Compression of Massive Time Series using Sparse Dictionary Coding" ;
    skos:definition "" ;
    skos:prefLabel "CORAD" .

:CP-N3 a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1806.07297v1> ;
    skos:altLabel "Canonical Tensor Decomposition with N3 Regularizer" ;
    skos:definition "Canonical Tensor Decomposition, trained with N3 regularizer" ;
    skos:prefLabel "CP-N3" .

:CP-N3-RP a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2110.02834v1> ;
    skos:altLabel "CP with N3 Regularizer and Relation Prediction" ;
    skos:definition "CP with N3 Regularizer and Relation Prediction" ;
    skos:prefLabel "CP-N3-RP" .

:CPCv2 a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1905.09272v3> ;
    skos:definition """**Contrastive Predictive Coding v2 (CPC v2)** is a self-supervised learning approach that builds upon the original [CPC](https://paperswithcode.com/method/contrastive-predictive-coding) with several improvements. These improvements include:\r
\r
- **Model capacity** - The third residual stack of [ResNet](https://paperswithcode.com/method/resnet)-101 (originally containing 23 blocks, 1024-dimensional feature maps, and 256-dimensional bottleneck layers), is converted to use 46 blocks, with 4096-dimensional feature maps and 512-dimensional bottleneck layers: ResNet-161.\r
\r
- **Layer Normalization** - The authors find CPC with [batch normalization](https://paperswithcode.com/method/batch-normalization) harms downstream performance. They hypothesize this is due to batch normalization allowing large models to find a trivial solution to CPC: it introduces a dependency between patches (through the batch statistics) that can be exploited to bypass the constraints on the receptive field. They replace batch normalization with [layer normalization](https://paperswithcode.com/method/layer-normalization).\r
\r
- **Predicting lengths and directions** - patches are predicted with contexts from both directions rather than just spatially underneath.\r
\r
- **Patch-based Augmentation** - Utilising "color dropping" which randomly drops two of the three color channels in each patch, as well as random horizontal flips.\r
\r
\r
Consistent with prior results, this new architecture delivers better performance regardless of""" ;
    skos:prefLabel "CPC v2" .

:CPM-2 a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2106.10715v3> ;
    skos:definition "**CPM-2** is a 11 billion parameters pre-trained language model based on a standard Transformer architecture consisting of a bidirectional encoder and a unidirectional decoder. The model is pre-trained on WuDaoCorpus which contains 2.3TB cleaned Chinese data as well as 300GB cleaned English data. The pre-training process of CPM-2 can be divided into three stages: Chinese pre-training, bilingual pre-training, and MoE pre-training. Multi-stage training with knowledge inheritance can significantly reduce the computation cost." ;
    skos:prefLabel "CPM-2" .

:CPN a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2104.03393v1> ;
    rdfs:seeAlso <https://github.com/FZJ-INM1-BDA/celldetection> ;
    skos:altLabel "Contour Proposal Network" ;
    skos:definition "The Contour Proposal Network (CPN) detects possibly overlapping objects in an image while simultaneously fitting pixel-precise closed object contours. The CPN can incorporate state of the art object detection architectures as backbone networks into a fast single-stage instance segmentation model that can be trained end-to-end." ;
    skos:prefLabel "CPN" .

:CPN3 a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1806.07297v1> ;
    rdfs:seeAlso <https://github.com/facebookresearch/kbc> ;
    skos:altLabel "CP with N3 Regularizer" ;
    skos:definition "CP with N3 Regularizer" ;
    skos:prefLabel "CP N3" .

:CPVT a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2102.10882v3> ;
    skos:altLabel "Conditional Position Encoding Vision Transformer" ;
    skos:definition "**CPVT**, or **Conditional Position Encoding Vision Transformer**, is a type of [vision transformer](https://paperswithcode.com/methods/category/vision-transformer) which utilizes [conditional positional encoding](https://paperswithcode.com/method/conditional-positional-encoding). Other than the new encodings, it follows the same architecture of [ViT](https://paperswithcode.com/method/vision-transformer) and [DeiT](https://paperswithcode.com/method/deit)." ;
    skos:prefLabel "CPVT" .

:CPconv a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2104.01538v3> ;
    skos:altLabel "Center-pivot convolution" ;
    skos:definition "" ;
    skos:prefLabel "CP conv" .

:CR-NET a skos:Concept ;
    dcterms:source <http://openaccess.thecvf.com/content_ECCV_2018/html/Sergio_Silva_License_Plate_Detection_ECCV_2018_paper.html> ;
    skos:definition "CR-NET is a YOLO-based model proposed for license plate character detection and recognition" ;
    skos:prefLabel "CR-NET" .

:CRF a skos:Concept ;
    skos:altLabel "Conditional Random Field" ;
    skos:definition """**Conditional Random Fields** or **CRFs** are a type of probabilistic graph model that take neighboring sample context into account for tasks like classification. Prediction is modeled as a graphical model, which implements dependencies between the predictions. Graph choice depends on the application, for example linear chain CRFs are popular in natural language processing, whereas in image-based tasks, the graph would connect to neighboring locations in an image to enforce that they have similar predictions.\r
\r
Image Credit: [Charles Sutton and Andrew McCallum, An Introduction to Conditional Random Fields](https://homepages.inf.ed.ac.uk/csutton/publications/crftut-fnt.pdf)""" ;
    skos:prefLabel "CRF" .

:CRF-RNN a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1502.03240v3> ;
    skos:definition "**CRF-RNN** is a formulation of a [CRF](https://paperswithcode.com/method/crf) as a Recurrent Neural Network. Specifically it formulates mean-field approximate inference for the Conditional Random Fields with Gaussian pairwise potentials as Recurrent Neural Networks." ;
    skos:prefLabel "CRF-RNN" .

:CRISS a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2006.09526v2> ;
    skos:definition "**CRISS**, or **Cross-lingual Retrievial for Iterative Self-Supervised Training (CRISS)**, is a self-supervised learning method for multilingual sequence generation. CRISS is developed based on the finding that the encoder outputs of multilingual denoising autoencoder can be used as language agnostic representation to retrieve parallel sentence pairs, and training the model on these retrieved sentence pairs can further improve its sentence retrieval and translation capabilities in an iterative manner. Using only unlabeled data from many different languages, CRISS iteratively mines for parallel sentences across languages, trains a new better multilingual model using these mined sentence pairs, mines again for better parallel sentences, and repeats." ;
    skos:prefLabel "CRISS" .

:CRN a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2002.10698v3> ;
    skos:altLabel "Conditional Relation Network" ;
    skos:definition "**Conditional Relation Network**, or **CRN**, is a building block to construct more sophisticated structures for representation and reasoning over video. CRN takes as input an array of tensorial objects and a conditioning feature, and computes an array of encoded output objects. Model building becomes a simple exercise of replication, rearrangement and stacking of these reusable units for diverse modalities and contextual information. This design thus supports high-order relational and multi-step reasoning." ;
    skos:prefLabel "CRN" .

:CReLU a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1603.05201v2> ;
    rdfs:seeAlso <https://gist.github.com/lintangsutawika/f2f3fb422d6d7df28bd74e26940da2e6> ;
    skos:definition """**CReLU**, or **Concatenated Rectified Linear Units**, is a type of activation function which preserves both positive and negative phase information while enforcing non-saturated non-linearity. We compute by concatenating the layer output $h$ as:\r
\r
$$ \\left[\\text{ReLU}\\left(h\\right), \\text{ReLU}\\left(-h\\right)\\right] $$""" ;
    skos:prefLabel "CReLU" .

:CS-GAN a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1905.06723v2> ;
    rdfs:seeAlso <https://github.com/deepmind/deepmind-research/tree/master/cs_gan> ;
    skos:definition "**CS-GAN** is a type of generative adversarial network that uses a form of deep compressed sensing, and [latent optimisation](https://paperswithcode.com/method/latent-optimisation), to improve the quality of generated samples." ;
    skos:prefLabel "CS-GAN" .

:CSGLD a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2010.09800v2> ;
    rdfs:seeAlso <https://github.com/WayneDW/Contour-Stochastic-Gradient-Langevin-Dynamics> ;
    skos:altLabel "Contour Stochastic Gradient Langevin Dynamics" ;
    skos:definition "Simulations of multi-modal distributions can be very costly and often lead to unreliable predictions. To accelerate the computations, we propose to sample from a flattened distribution to accelerate the computations and estimate the importance weights between the original distribution and the flattened distribution to ensure the correctness of the distribution." ;
    skos:prefLabel "CSGLD" .

:CSL a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2003.05597v4> ;
    skos:altLabel "Circular Smooth Label" ;
    skos:definition "**Circular Smooth Label** (CSL) is a classification-based rotation detection technique for arbitrary-oriented object detection. It is used for circularly distributed angle classification and addresses the periodicity of the angle and increases the error tolerance to adjacent angles." ;
    skos:prefLabel "CSL" .

:CSPDarknet53 a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2004.10934v1> ;
    rdfs:seeAlso <https://github.com/Tianxiaomo/pytorch-YOLOv4/blob/be3a20bb4a87988b30dddb018d74ee677d1434e8/tool/darknet2pytorch.py#L134> ;
    skos:definition """**CSPDarknet53** is a convolutional neural network and backbone for object detection that uses [DarkNet-53](https://paperswithcode.com/method/darknet-53). It employs a CSPNet strategy to partition the feature map of the base layer into two parts and then merges them through a cross-stage hierarchy. The use of a split and merge strategy allows for more gradient flow through the network. \r
\r
This CNN is used as the backbone for [YOLOv4](https://paperswithcode.com/method/yolov4).""" ;
    skos:prefLabel "CSPDarknet53" .

:CSPDenseNet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1911.11929v1> ;
    rdfs:seeAlso <https://github.com/WongKinYiu/CrossStagePartialNetworks> ;
    skos:definition "**CSPDenseNet** is a convolutional neural network and object detection backbone where we apply the Cross Stage Partial Network (CSPNet) approach to [DenseNet](https://paperswithcode.com/method/densenet). The CSPNet partitions the feature map of the base layer into two parts and then merges them through a cross-stage hierarchy. The use of a split and merge strategy allows for more gradient flow through the network." ;
    skos:prefLabel "CSPDenseNet" .

:CSPDenseNet-Elastic a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1911.11929v1> ;
    skos:definition "**CSPDenseNet-Elastic** is a convolutional neural network and object detection backbone where we apply the Cross Stage Partial Network (CSPNet) approach to [DenseNet-Elastic](https://paperswithcode.com/method/densenet-elastic). The CSPNet partitions the feature map of the base layer into two parts and then merges them through a cross-stage hierarchy. The use of a split and merge strategy allows for more gradient flow through the network." ;
    skos:prefLabel "CSPDenseNet-Elastic" .

:CSPPeleeNet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1911.11929v1> ;
    rdfs:seeAlso <https://github.com/WongKinYiu/CrossStagePartialNetworks> ;
    skos:definition "**CSPPeleeNet** is a convolutional neural network and object detection backbone  where we apply the Cross Stage Partial Network (CSPNet) approach to [PeleeNet](https://paperswithcode.com/method/peleenet). The CSPNet partitions the feature map of the base layer into two parts and then merges them through a cross-stage hierarchy. The use of a split and merge strategy allows for more gradient flow through the network." ;
    skos:prefLabel "CSPPeleeNet" .

:CSPResNeXt a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1911.11929v1> ;
    rdfs:seeAlso <https://github.com/WongKinYiu/CrossStagePartialNetworks> ;
    skos:definition "**CSPResNeXt** is a convolutional neural network where we apply the Cross Stage Partial Network (CSPNet) approach to [ResNeXt](https://paperswithcode.com/method/resnext). The CSPNet partitions the feature map of the base layer into two parts and then merges them through a cross-stage hierarchy. The use of a split and merge strategy allows for more gradient flow through the network." ;
    skos:prefLabel "CSPResNeXt" .

:CSPResNeXtBlock a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1911.11929v1> ;
    rdfs:seeAlso <https://github.com/Tianxiaomo/pytorch-YOLOv4/blob/99045748bd6fdbb55c7dac48ef82941c641e65c6/models.py#L66> ;
    skos:definition "**CSPResNeXt Block** is an extended [ResNext Block](https://paperswithcode.com/method/resnext-block) where we partition the feature map of the base layer into two parts and then merge them through a cross-stage hierarchy. The use of a split and merge strategy allows for more gradient flow through the network." ;
    skos:prefLabel "CSPResNeXt Block" .

:CT-Layer a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2206.07369v3> ;
    rdfs:seeAlso <https://anonymous.4open.science/r/DiffWireNeurIPS22/CT_layer.py> ;
    skos:altLabel "Commute Times Layer" ;
    skos:definition """**TL;DR: CT-Layer is a GNN Layer which is able to rewire a graph in an inductive an parameter-free way according to the commute times distance (or effective resistance). We address it learning a differentiable way to compute the CT-embedding of the graph.**\r
\r
### Summary\r
\r
**CT-Layer** is able to Learn the *Commute Times distance*  between nodes (i.e. *effective resistance distance*) in a **differentiable** way, instead of the common spectral version, and in a **parameter free** manner, which is not the cased of the heat kernel. This approach allow to solve it as an optimization problem inside a GNN, leading to have a new layer which is able to learn how rewire a given graph in an optimal, and **inductive** way. \r
\r
In addition, **CT-Layer** also is able to learn *Commute Times embeddings*, and then calculate it for any graph in an inductive way. The Commute Times embedding is also related with the *eigenvalues* and *eigenvectors* of the Laplacian of the graph, because CT embedding is just the eigenvectors scaled. Therefore, CT-Layer is also able to learn hot to calculate the spectrum of the Laplacian in a differentiable way. Therefore, this embedding must satisfy orthogonality and normality.\r
\r
Finally, recent connections has been found between commute times distance and **curvature** (which is non-differentiable), establishing equivalences between them. Therefore, **CT-Layer** can also be seen as the differentiable version of the curvature rewiring.\r
\r
**We are going through a quick overview of the layer, but I suggest go to the paper for a detailed explanation. **\r
\r
### Spectral CT- Embedding downsides\r
CT-embedding $\\mathbf{Z}$ is computed spectrally  in the literature (until the proposal of this method) or it is approximated using the heat kernel (very dependent on hyperparameter $t$). This fact does not allow us to propose differentiable methods using that measure:\r
$$\r
\\mathbf{Z}=\\sqrt{vol(G)}\\mathbf{\\Lambda}^\\frac{1}{2}\\mathbf{F}^T \\textrm{ given } \\mathbf{L}=\\mathbf{F}\\mathbf{\\Lambda}\\mathbf{F}^T\r
$$\r
\r
Then, CT-distance  is given by the Euclidean distances between the embeddings $CT_{uv} = ||\\mathbf{z_u}-\\mathbf{z_v}||^2$. The spectral form is: \r
\r
$$\r
\\frac{CT_{uv}}{vol(G)} = \\sum_{i=2}^n \\frac{1}{\\lambda_i} (\\mathbf{f}(u)-\\mathbf{f}(v))^2 \r
$$\r
being $\\mathbf{f}$ the eigenvectors of the graph Laplacian. \r
\r
This embedding and distances gives us desirable properties of the graph, such an understanding of the structure, or an embedding based on the spectrum which minimizes Dirichlet energies. However, **the spectral computation is not differentiable**.\r
\r
### CT-Layer as an optimization problem: Differentiable, learnable and inductive CT-Layer\r
Giving that $\\mathbf{Z}$ minimizes Dirichlet energies s.t. being orthogonal and normalized, we can formulate this problem as constraining neighboring nodes to have a similar embeddings s.t. $\\mathbf{Z}\\mathbf{Z}^T=\\mathbf{I}$.\r
\r
$$\r
\\mathbf{Z} = \\arg\\min_{\\mathbf{Z}^T\\mathbf{Z}=\\mathbf{I}} \\frac{\\sum\\_{u,v} ||\\mathbf{z_u}-\\mathbf{z_v}||^2\\mathbf{A}\\_{uv}}{\\sum\\_{u,v} \\mathbf{Z}^2\\_{uv} d_u}=\\frac{Tr[\\mathbf{Z}^T\\mathbf{L}\\mathbf{Z}]}{Tr[\\mathbf{Z}^T\\mathbf{D}\\mathbf{Z}]}\r
$$\r
\r
With the above elements we have a definition of **CT-Layer**, our rewiring layer: \r
Given the matrix $\\mathbf{X}\\_{n\\times F}$ encoding the features of the nodes after any message passing (MP) layer, $\\mathbf{Z}\\_{n\\times O(n)}=\\tanh(\\textrm{MLP}(\\mathbf{X}))$ learns the association $\\mathbf{X}\\rightarrow \\mathbf{Z}$ while $\\mathbf{Z}$ is optimized according to the loss \r
$$\r
L\\_{CT} = \\frac{Tr[\\mathbf{Z}^T\\mathbf{L}\\mathbf{Z}]}{Tr[\\mathbf{Z}^T\\mathbf{D}\\mathbf{Z}]} + \\left\\|\\frac{\\mathbf{Z}^T\\mathbf{Z}}{\\|\\mathbf{Z}^T\\mathbf{Z}\\|\\_F} - \\mathbf{I}\\_n\\right\\|\\_F\r
$$\r
 This results in the following **resistance diffusion** $\\mathbf{T}^{CT} = \\mathbf{R}(\\mathbf{S})\\odot \\mathbf{A}$ (Hadamard product between the resistance distance and the adjacency) which provides as input to the subsequent MP layer a learnt convolution matrix.\r
\r
As explained before, $\\mathbf{Z}$ is the **commute times embedding matrix** and the pairwise euclidian distance of that learned embeddings are the **commute times distances** or resistance distances. **Therefore, once trained this layer, it will be able to calculate the commute times embedding for a new graph, and rewire that new and unseen graph in a principled way based on the commute times distance.**\r
\r
## Preservation of Structure\r
Does this rewiring preserve the original structure? Let $G' = \\textrm{Sparsify}(G, q)$ be a sampling algorithm of graph $G = (V, E)$, where edges $e \\in E$ are sampled with probability $q\\propto R_e$ (**proportional to the effective resistance, i.e. commute times**).\r
Then, for $n = |V|$ sufficiently large and $1/\\sqrt{n}< \\epsilon\\le 1$, we need O(n\\log n/\\epsilon^2)$ samples to satisfy:\r
\r
$$\r
\\forall \\mathbf{x}\\in\\mathbb{R}^n:\\; (1-\\epsilon)\\mathbf{x}^T\\mathbf{L}\\_G\\mathbf{x}\\le\\mathbf{x}^T\\mathbf{L}\\_{G'}\\mathbf{x}\\le (1+\\epsilon)\\mathbf{x}^T\\mathbf{L}\\_G\\mathbf{x}\r
$$\r
\r
The intuitions behind is that Dirichlet energies in $G'$ are bounded in $(1\\pm \\epsilon)$ of the Dirichlet energies of the original graph $G$.""" ;
    skos:prefLabel "CT-Layer" .

:CT3D a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2108.10723v2> ;
    skos:definition """**CT3D** is a two-stage 3D object detection framework that leverages a high-quality region proposal network and a Channel-wise [Transformer](https://paperswithcode.com/method/transformer) architecture. The proposed CT3D simultaneously performs proposal-aware embedding and channel-wise context aggregation for the point features within each proposal. Specifically, CT3D uses a proposal's keypoints for spatial contextual modelling and learns attention propagation in the encoding module, mapping the proposal to point embeddings. Next, a new channel-wise decoding module enriches the query-key interaction via channel-wise re-weighting to effectively merge multi-level contexts, which contributes to more accurate object predictions. \r
\r
In CT3D, the raw points are first fed into the [RPN](https://paperswithcode.com/method/rpn) for generating 3D proposals. Then the raw points along with the corresponding proposals are processed by the channel-wise Transformer composed of the proposal-to-point encoding module and the channel-wise decoding module. Specifically, the proposal-to-point encoding module is to modulate each point feature with global proposal-aware context information. After that, the encoded point features are transformed into an effective proposal feature representation by the\r
channel-wise decoding module for confidence prediction and box regression.""" ;
    skos:prefLabel "CT3D" .

:CTAB-GAN a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2108.10064v1> ;
    skos:definition "**CTAB-GAN** is a model for conditional tabular data generation. The generator and discriminator utilize the [DCGAN](https://paperswithcode.com/method/dcgan) architecture. An [auxiliary classifier](https://paperswithcode.com/method/auxiliary-classifier) is also used with an MLP architecture." ;
    skos:prefLabel "CTAB-GAN" .

:CTAL a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2109.00181v1> ;
    skos:definition "**CTAL** is a pre-training framework for strong audio-and-language representations with a [Transformer](https://paperswithcode.com/method/transformer), which aims to learn the intra-modality and inter-modalities connections between audio and language through two proxy tasks on a large amount of audio- and-language pairs: masked language modeling and masked cross-modal acoustic modeling. The pre-trained model is a Transformer for Audio and Language, i.e., CTAL, which consists of two modules, a language stream encoding module which adapts word as input element, and a text-referred audio stream encoder module which accepts both frame-level Mel-spectrograms and token-level output embeddings from the language stream" ;
    skos:prefLabel "CTAL" .

:CTCLoss a skos:Concept ;
    rdfs:seeAlso <https://github.com/pytorch/pytorch/blob/8850fd1952c3983793dcac4022fdc8e3913dad96/torch/nn/modules/loss.py#L1247> ;
    skos:altLabel "Connectionist Temporal Classification Loss" ;
    skos:definition "A **Connectionist Temporal Classification Loss**, or **CTC Loss**, is designed for tasks where we need alignment between sequences, but where that alignment is difficult - e.g. aligning each character to its location in an audio file. It calculates a loss between a continuous (unsegmented) time series and a target sequence. It does this by summing over the probability of possible alignments of input to target, producing a loss value which is differentiable with respect to each input node. The alignment of input to target is assumed to be “many-to-one”, which limits the length of the target sequence such that it must be $\\leq$ the input length." ;
    skos:prefLabel "CTC Loss" .

:CTRL a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1909.05858v2> ;
    skos:definition """**CTRL** is conditional [transformer](https://paperswithcode.com/method/transformer) language model, trained\r
to condition on control codes that govern style, content, and task-specific behavior. Control codes were derived from structure that naturally co-occurs with raw\r
text, preserving the advantages of unsupervised learning while providing more\r
explicit control over text generation. These codes also allow CTRL to predict\r
which parts of the training data are most likely given a sequence""" ;
    skos:prefLabel "CTRL" .

:CTracker a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2007.14557v1> ;
    skos:altLabel "Chained-Tracker" ;
    skos:definition """**Chained-Tracker**, or **CTracker**,  is an online model for multiple-object tracking. It chains paired bounding boxes regression results estimated from overlapping nodes, of which each node covers two adjacent frames. The paired regression is made attentive by object-attention (brought by a detection module) and identity-attention (ensured by an ID verification module).\r
\r
The joint attention module guides the paired boxes regression branch to focus on informative spatial regions with two other branches. One is the object classification branch, which predicts the confidence scores for the first box in the detected box pairs, and such scores are used to guide the regression branch to focus on the foreground regions. The other one is the ID verification branch whose prediction facilitates the regression branch to focus on regions corresponding to the same target. Finally, the bounding box pairs are filtered according to the classification confidence. Then, the generated box pairs belonging to the adjacent frame pairs could be associated using simple methods like IoU (Intersection over Union) matching according to their boxes in the common frame. In this way, the tracking process could be achieved by chaining all the adjacent frame pairs (i.e. chain nodes) sequentially.""" ;
    skos:prefLabel "CTracker" .

:CV-MIM a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2012.01405v2> ;
    skos:altLabel "Contrastive Cross-View Mutual Information Maximization" ;
    skos:definition "**CV-MIM**, or **Contrastive Cross-View Mutual Information Maximization**, is a representation learning method to disentangle pose-dependent as well as view-dependent factors from 2D human poses. The method trains a network using cross-view mutual information maximization, which maximizes mutual information of the same pose performed from different viewpoints in a contrastive learning manner. It further utilizes two regularization terms to ensure disentanglement and smoothness of the learned representations." ;
    skos:prefLabel "CV-MIM" .

:CVRL a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2008.03800v4> ;
    skos:altLabel "Contrastive Video Representation Learning" ;
    skos:definition """**Contrastive Video Representation Learning**, or **CVRL**, is a self-supervised contrastive learning framework for learning spatiotemporal visual representations from unlabeled videos. Representations are learned using a contrastive loss, where two clips from the same short video are pulled together in the embedding space, while clips from different videos are pushed away. Data augmentations are designed involving spatial and temporal cues. Concretely, a [temporally consistent spatial augmentation](https://paperswithcode.com/method/temporally-consistent-spatial-augmentation#) method is used to impose strong spatial augmentations on each frame of the video while maintaining the temporal consistency across frames. A sampling-based temporal augmentation method is also used to avoid overly enforcing invariance on clips that are distant in time. \r
\r
End-to-end, from a raw video, we first sample a temporal interval from a monotonically decreasing distribution. The temporal interval represents the number of frames between the start points of two clips, and we sample two clips from a video according to this interval. Afterwards we apply a [temporally consistent spatial augmentation](https://paperswithcode.com/method/temporally-consistent-spatial-augmentation) to each of the clips and feed them into a 3D backbone with an MLP head. The contrastive loss is used to train the network to attract the clips from the same video and repel the clips from different videos in the embedding space.""" ;
    skos:prefLabel "CVRL" .

:CW-ERM a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2210.02174v2> ;
    skos:altLabel "Closed-loop Weighted Empirical Risk Minimization" ;
    skos:definition "A closed-loop evaluation procedure is first used in a simulator to identify training data samples that are important for practical driving performance and then we these samples to help debias the policy network." ;
    skos:prefLabel "CW-ERM" .

:CaiT a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2103.17239v2> ;
    skos:altLabel "Class-Attention in Image Transformers" ;
    skos:definition "**CaiT**, or **Class-Attention in Image Transformers**, is a type of [vision transformer](https://paperswithcode.com/methods/category/vision-transformer) with several design alterations upon the original [ViT](https://paperswithcode.com/method/vision-transformer). First a new layer scaling approach called [LayerScale](https://paperswithcode.com/method/layerscale) is used, adding a learnable diagonal matrix on output of each residual block, initialized close to (but not at) 0, which improves the training dynamics. Secondly, [class-attention layers](https://paperswithcode.com/method/ca) are introduced to the architecture. This creates an architecture where the transformer layers involving [self-attention](https://paperswithcode.com/method/scaled) between patches are explicitly separated from class-attention layers -- that are devoted to extract the content of the processed patches into a single vector so that it can be fed to a linear classifier." ;
    skos:prefLabel "CaiT" .

:CanvasMethod a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2001.04011v2> ;
    skos:definition "**Canvas Method** is a method for inference attacks on object detection models. It draws a predicted bounding box distribution on an empty canvas for an attack model input. The canvas is initially set to an image of 300$\\times$300 pixels in size, where every pixel has a value of zero and the boxes drawn on the canvas have the same center as the predicted boxes and the same intensity as the prediction scores." ;
    skos:prefLabel "Canvas Method" .

:CapsNet a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1710.09829v2> ;
    skos:altLabel "Capsule Network" ;
    skos:definition "**Capsule Network** is a machine learning system that is a type of artificial neural network that can be used to better model hierarchical relationships. The approach is an attempt to more closely mimic biological neural organization." ;
    skos:prefLabel "CapsNet" .

:CapsuleNetwork a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1710.09829v2> ;
    skos:definition """A capsule is an activation vector that basically executes on its inputs some complex internal\r
computations. Length of these activation vectors signifies the\r
probability of availability of a feature. Furthermore, the condition\r
of the recognized element is encoded as the direction in which\r
the vector is pointing. In traditional, CNN uses Max pooling for\r
invariance activities of neurons, which is nothing except a minor\r
change in input and the neurons of output signal will remains\r
same.""" ;
    skos:prefLabel "Capsule Network" .

:CascadeCornerPooling a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1904.08189v3> ;
    rdfs:seeAlso <https://github.com/Duankaiwen/CenterNet> ;
    skos:definition "**Cascade Corner Pooling** is a pooling layer for object detection that builds upon the [corner pooling](https://paperswithcode.com/method/corner-pooling) operation. Corners are often outside the objects, which lacks local appearance features. [CornerNet](https://paperswithcode.com/method/cornernet) uses corner pooling to address this issue, where we find the maximum values on the boundary directions so as to determine corners. However, it makes corners sensitive to the edges. To address this problem, we need to let corners see the visual patterns of objects. Cascade corner pooling first looks along a boundary to find a boundary maximum value, then looks inside along the location of the boundary maximum value to find an internal maximum value, and finally, add the two maximum values together. By doing this, the corners obtain both the the boundary information and the visual patterns of objects." ;
    skos:prefLabel "Cascade Corner Pooling" .

:CascadeMaskR-CNN a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1712.00726v1> ;
    skos:definition """**Cascade Mask R-CNN** extends [Cascade R-CNN](https://paperswithcode.com/method/cascade-r-cnn) to instance segmentation, by adding a\r
mask head to the cascade.\r
\r
In the [Mask R-CNN](https://paperswithcode.com/method/mask-r-cnn), the segmentation branch is inserted in parallel to the detection branch. However, the Cascade [R-CNN](https://paperswithcode.com/method/r-cnn) has multiple detection branches. This raises the questions of 1) where to add the segmentation branch and 2) how many segmentation branches to add. The authors consider three strategies for mask prediction in the Cascade R-CNN. The first two strategies address the first question, adding a single mask prediction head at either the first or last stage of the Cascade R-CNN. Since the instances used to train the segmentation branch are the positives of the detection branch, their number varies in these two strategies. Placing the segmentation head later on the cascade leads to more examples. However, because segmentation is a pixel-wise operation, a large number of highly overlapping instances is not necessarily as helpful as for object detection, which is a patch-based operation. The third strategy addresses the second question, adding a segmentation branch to each\r
cascade stage. This maximizes the diversity of samples used to learn the mask prediction task. \r
\r
At inference time, all three strategies predict the segmentation masks on the patches produced by the final object detection stage, irrespective of the cascade stage on which the segmentation mask is implemented and how many segmentation branches there are.""" ;
    skos:prefLabel "Cascade Mask R-CNN" .

:CascadePSP a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2005.02551v1> ;
    skos:definition "**CascadePSP** is a general segmentation refinement model that refines any given segmentation from low to high resolution. The model takes as input an initial mask that can be an output of any algorithm to provide a rough object location. Then the CascadePSP will output a refined mask. The model is designed in a cascade fashion that generates refined segmentation in a coarse-to-fine manner. Coarse outputs from the early levels predict object structure which will be used as input to the latter levels to refine boundary details." ;
    skos:prefLabel "CascadePSP" .

:CascadeR-CNN a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1712.00726v1> ;
    rdfs:seeAlso <https://github.com/open-mmlab/mmdetection/blob/588536de9905feb7f37c2c977d146a64c74ef28e/mmdet/models/detectors/cascade_rcnn.py#L6> ;
    skos:definition """**Cascade R-CNN** is an object detection architecture that seeks to address problems with degrading performance with increased IoU thresholds (due to overfitting during training and inference-time mismatch between IoUs for which detector is optimal and the inputs). It is a multi-stage extension of the [R-CNN](https://paperswithcode.com/method/r-cnn), where detector stages deeper into the cascade are sequentially more selective against close false positives. The cascade of R-CNN stages are trained sequentially, using the output of one stage to train the next. This is motivated by the observation that the output IoU of a regressor is almost invariably better than the input IoU. \r
\r
Cascade R-CNN does not aim to mine hard negatives. Instead, by adjusting bounding boxes, each stage aims to find a good set of close false positives for training the next stage. When operating in this manner, a sequence of detectors adapted to increasingly higher IoUs can beat the overfitting problem, and thus be effectively trained. At inference, the same cascade procedure is applied. The progressively improved hypotheses are better matched to the increasing detector quality at each stage.""" ;
    skos:prefLabel "Cascade R-CNN" .

:CategoricalModularity a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2106.00877v1> ;
    skos:definition """A novel low-resource intrinsic metric to evaluate word\r
embedding quality based on graph modularity.""" ;
    skos:prefLabel "Categorical Modularity" .

:CausalConvolution a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1609.03499v2> ;
    skos:definition "**Causal convolutions** are a type of [convolution](https://paperswithcode.com/method/convolution) used for temporal data which ensures the model cannot violate the ordering in which we model the data: the prediction $p(x_{t+1} | x_{1}, \\ldots, x_{t})$ emitted by the model at timestep $t$ cannot depend on any of the future timesteps $x_{t+1}, x_{t+2}, \\ldots, x_{T}$. For images, the equivalent of a causal convolution is a [masked convolution](https://paperswithcode.com/method/masked-convolution) which can be implemented by constructing a mask tensor and doing an element-wise multiplication of this mask with the convolution kernel before applying it. For 1-D data such as audio one can more easily implement this by shifting the output of a normal convolution by a few timesteps." ;
    skos:prefLabel "Causal Convolution" .

:CausalInference a skos:Concept ;
    skos:definition "Causal inference is the process of drawing a conclusion about a causal connection based on the conditions of the occurrence of an effect. The main difference between causal inference and inference of association is that the former analyzes the response of the effect variable when the cause is changed." ;
    skos:prefLabel "Causal Inference" .

:CayleyNet a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1705.07664v2> ;
    skos:definition """The core ingredient of **CayleyNet** is a new class of parametric rational complex functions (Cayley polynomials) allowing to efficiently compute spectral filters on graphs that specialize on frequency bands of interest. The model generates rich spectral filters that are localized in space, scales linearly with the size of the input data for sparsely-connected graphs, and can handle different constructions of Laplacian operators.\r
\r
Description adapted from: [CayleyNets: Graph Convolutional Neural Networks with Complex Rational Spectral Filters](https://arxiv.org/pdf/1705.07664.pdf)""" ;
    skos:prefLabel "CayleyNet" .

:CeiT a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2103.11816v2> ;
    skos:altLabel "Convolution-enhanced image Transformer" ;
    skos:definition "**Convolution-enhanced image Transformer** (**CeiT**) combines the advantages of CNNs in extracting low-level features, strengthening locality, and the advantages of Transformers in establishing long-range dependencies. Three modifications are made to the original Transformer: 1) instead of the straightforward tokenization from raw input images, we design an **Image-to-Tokens** (**I2T**) module that extracts patches from generated low-level features; 2) the feed-froward network in each encoder block is replaced with a **Locally-enhanced Feed-Forward** (**LeFF**) layer that promotes the correlation among neighbouring tokens in the spatial dimension; 3) a **Layer-wise Class token Attention** (**LCA**) is attached at the top of the Transformer that utilizes the multi-level representations." ;
    skos:prefLabel "CeiT" .

:CenterMask a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1911.06667v6> ;
    rdfs:seeAlso <https://github.com/youngwanLEE/CenterMask> ;
    skos:definition "**CenterMask** is an anchor-free instance segmentation method that adds a novel [spatial attention-guided mask](https://paperswithcode.com/method/spatial-attention-guided-mask) (SAG-Mask) branch to anchor-free one stage object detector ([FCOS](https://paperswithcode.com/method/fcos)) in the same vein with [Mask R-CNN](https://paperswithcode.com/method/mask-r-cnn). Plugged into the FCOS object detector, the SAG-Mask branch predicts a segmentation mask on each detected box with the spatial attention map that helps to focus on informative pixels and suppress noise." ;
    skos:prefLabel "CenterMask" .

:CenterNet a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1904.08189v3> ;
    rdfs:seeAlso <https://github.com/Duankaiwen/CenterNet/blob/c2fdffd4af42ed270542ebebee7081f27dbb2eb1/models/CenterNet-104.py#L127> ;
    skos:definition "**CenterNet** is a one-stage object detector that detects each object as a triplet, rather than a pair, of keypoints. It utilizes two customized modules named [cascade corner pooling](https://paperswithcode.com/method/cascade-corner-pooling) and [center pooling](https://paperswithcode.com/method/center-pooling), which play the roles of enriching information collected by both top-left and bottom-right corners and providing more recognizable information at the central regions, respectively. The intuition is that, if a predicted bounding box has a high IoU with the ground-truth box, then the probability that the center keypoint in its central region is predicted as the same class is high, and vice versa. Thus, during inference, after a proposal is generated as a pair of corner keypoints, we determine if the proposal is indeed an object by checking if there is a center keypoint of the same class falling within its central region." ;
    skos:prefLabel "CenterNet" .

:CenterPoint a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2006.11275v2> ;
    skos:definition "**CenterPoint** is a two-stage 3D detector that finds centers of objects and their properties using a keypoint detector and regresses to other attributes, including 3D size, 3D orientation and velocity. In a second-stage, it refines these estimates using additional point features on the object. CenterPoint uses a standard Lidar-based backbone network, i.e., VoxelNet or PointPillars, to build a representation of the input point-cloud. CenterPoint predicts the relative offset (velocity) of objects between consecutive frames, which are then linked up greedily -- so in Centerpoint, 3D object tracking simplifies to greedy closest-point matching." ;
    skos:prefLabel "CenterPoint" .

:CenterPooling a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1904.08189v3> ;
    rdfs:seeAlso <https://github.com/Duankaiwen/CenterNet/blob/435b86aa602a4d28768c192884173727d4b45ea2/models/CenterNet-104.py#L106> ;
    skos:definition """**Center Pooling** is a pooling technique for object detection that aims to capture richer and more recognizable visual patterns. The geometric centers of objects do not necessarily convey very recognizable visual patterns (e.g., the human head contains strong visual patterns, but the center keypoint is often in the middle of the human body). \r
\r
The detailed process of center pooling is as follows: the backbone outputs a feature map, and to determine if a pixel in the feature map is a center keypoint, we need to find the maximum value in its both horizontal and vertical directions and add them together. By doing this, center pooling helps the better detection of center keypoints.""" ;
    skos:prefLabel "Center Pooling" .

:CenterTrack a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2004.01177v2> ;
    skos:altLabel "Track objects as points" ;
    skos:definition "Our tracker, CenterTrack, applies a detection model to a pair of images and detections from the prior frame. Given this minimal input, CenterTrack localizes objects and predicts their associations with the previous frame. That's it. CenterTrack is simple, online (no peeking into the future), and real-time." ;
    skos:prefLabel "CenterTrack" .

:CentripetalNet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2003.09119v1> ;
    skos:definition "**CentripetalNet** is a keypoint-based detector which uses centripetal shift to pair corner keypoints from the same instance. CentripetalNet predicts the position and the centripetal shift of the corner points and matches corners whose shifted results are aligned." ;
    skos:prefLabel "CentripetalNet" .

<http://w3id.org/mlso/vocab/ml_algorithm/Channel&Spatialattention> a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1704.06904v1> ;
    skos:definition "Channel & spatial attention combines the advantages of channel attention and spatial attention. It adaptively selects both important objects and regions" ;
    skos:prefLabel "Channel & Spatial attention" .

:Channel-wiseCrossAttention a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2109.04335v3> ;
    skos:definition """**Channel-wise Cross Attention** is a module for semantic segmentation used in the [UCTransNet](https://paperswithcode.com/method/uctransnet) architecture. It is used to fuse features of inconsistent semantics between the Channel [Transformer](https://paperswithcode.com/method/transformer) and [U-Net](https://paperswithcode.com/method/u-net) decoder. It guides the channel and information filtration of the Transformer features and eliminates the ambiguity with the decoder features.\r
\r
Mathematically, we take the $i$-th level Transformer output $\\mathbf{O\\_{i}} \\in \\mathbb{R}^{C×H×W}$ and i-th level decoder feature map $\\mathbf{D\\_{i}} \\in \\mathbb{R}^{C×H×W}$ as the inputs of Channel-wise Cross Attention. Spatial squeeze is performed by a [global average pooling](https://paperswithcode.com/method/global-average-pooling) (GAP) layer, producing vector $\\mathcal{G}\\left(\\mathbf{X}\\right) \\in \\mathbb{R}^{C×1×1}$ with its $k$th channel $\\mathcal{G}\\left(\\mathbf{X}\\right) = \\frac{1}{H×W}\\sum^{H}\\_{i=1}\\sum^{W}\\_{j=1}\\mathbf{X}^{k}\\left(i, j\\right)$. We use this operation to embed the global spatial information and then generate the attention mask:\r
\r
$$ \\mathbf{M}\\_{i} = \\mathbf{L}\\_{1} \\cdot \\mathcal{G}\\left(\\mathbf{O\\_{i}}\\right) + \\mathbf{L}\\_{2} \\cdot \\mathcal{G}\\left(\\mathbf{D}\\_{i}\\right) $$\r
\r
where $\\mathbf{L}\\_{1} \\in \\mathbb{R}^{C×C}$ and $\\mathbf{L}\\_{2} \\in \\mathbb{R}^{C×C}$ and being weights of two Linear layers and the [ReLU](https://paperswithcode.com/method/relu) operator $\\delta\\left(\\cdot\\right)$. This operation in the equation above encodes the channel-wise dependencies. Following [ECA-Net](https://paperswithcode.com/method/eca-net) which empirically showed avoiding dimensionality reduction is important for learning channel attention, the authors use a single [Linear layer](https://paperswithcode.com/method/linear-layer) and sigmoid function to build the channel attention map. The resultant vector is used to recalibrate or excite $\\mathbf{O\\_{i}}$ to $\\mathbf{\\bar{O}\\_{i}} = \\sigma\\left(\\mathbf{M\\_{i}}\\right) \\cdot \\mathbf{O\\_{i}}$, where the activation $\\sigma\\left(\\mathbf{M\\_{i}}\\right)$ indicates the importance of each channel. Finally, the masked $\\mathbf{\\bar{O}}\\_{i}$ is concatenated with the up-sampled features of the $i$-th level decoder.""" ;
    skos:prefLabel "Channel-wise Cross Attention" .

:Channel-wiseCrossFusionTransformer a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2109.04335v3> ;
    skos:definition "**Channel-wise Cross Fusion Transformer** is a module used in the [UCTransNet](https://paperswithcode.com/method/uctransnet) architecture for semantic segmentation. It fuses the multi-scale encoder features with the advantage of the long dependency modeling in the [Transformer](https://paperswithcode.com/method/transformer). The [CCT](https://paperswithcode.com/method/cct) module consists of three steps: multi-scale feature embedding, multi-head [channel-wise cross attention](https://paperswithcode.com/method/channel-wise-cross-attention) and Multi-Layer Perceptron (MLP)." ;
    skos:prefLabel "Channel-wise Cross Fusion Transformer" .

:Channel-wiseSoftAttention a skos:Concept ;
    skos:definition """**Channel-wise Soft Attention** is an attention mechanism in computer vision that assigns "soft" attention weights for each channel $c$. In soft channel-wise attention, the alignment weights are learned and placed "softly" over each channel. This would contrast with hard attention which would only selects one channel to attend to at a time.\r
\r
Image: [Xu et al](http://proceedings.mlr.press/v37/xuc15.pdf)""" ;
    skos:prefLabel "Channel-wise Soft Attention" .

:ChannelAttentionModule a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1807.06521v2> ;
    rdfs:seeAlso <https://github.com/Jongchan/attention-module/blob/5d3a54af0f6688bedca3f179593dff8da63e8274/MODELS/cbam.py#L26> ;
    skos:definition """A **Channel Attention Module** is a module for channel-based attention in convolutional neural networks. We produce a channel attention map by exploiting the inter-channel relationship of features. As each channel of a feature map is considered as a feature detector, channel attention focuses on ‘what’ is meaningful given an input image. To compute the channel attention efficiently, we squeeze the spatial dimension of the input feature map. \r
\r
We first aggregate spatial information of a feature map by using both average-pooling and max-pooling operations, generating two different spatial context descriptors: $\\mathbf{F}^{c}\\_{avg}$ and $\\mathbf{F}^{c}\\_{max}$, which denote average-pooled features and max-pooled features respectively. \r
\r
Both descriptors are then forwarded to a shared network to produce our channel attention map $\\mathbf{M}\\_{c} \\in \\mathbb{R}^{C\\times{1}\\times{1}}$. Here $C$ is the number of channels. The shared network is composed of multi-layer perceptron (MLP) with one hidden layer. To reduce parameter overhead, the hidden activation size is set to $\\mathbb{R}^{C/r×1×1}$, where $r$ is the reduction ratio. After the shared network is applied to each descriptor, we merge the output feature vectors using element-wise summation. In short, the channel attention is computed as:\r
\r
$$  \\mathbf{M\\_{c}}\\left(\\mathbf{F}\\right) = \\sigma\\left(\\text{MLP}\\left(\\text{AvgPool}\\left(\\mathbf{F}\\right)\\right)+\\text{MLP}\\left(\\text{MaxPool}\\left(\\mathbf{F}\\right)\\right)\\right) $$\r
\r
$$  \\mathbf{M\\_{c}}\\left(\\mathbf{F}\\right) = \\sigma\\left(\\mathbf{W\\_{1}}\\left(\\mathbf{W\\_{0}}\\left(\\mathbf{F}^{c}\\_{avg}\\right)\\right) +\\mathbf{W\\_{1}}\\left(\\mathbf{W\\_{0}}\\left(\\mathbf{F}^{c}\\_{max}\\right)\\right)\\right) $$\r
\r
where $\\sigma$ denotes the sigmoid function, $\\mathbf{W}\\_{0} \\in \\mathbb{R}^{C/r\\times{C}}$, and $\\mathbf{W}\\_{1} \\in \\mathbb{R}^{C\\times{C/r}}$. Note that the MLP weights, $\\mathbf{W}\\_{0}$ and $\\mathbf{W}\\_{1}$, are shared for both inputs and the [ReLU](https://paperswithcode.com/method/relu) activation function is followed by $\\mathbf{W}\\_{0}$.\r
\r
Note that the channel attention module with just [average pooling](https://paperswithcode.com/method/average-pooling) is the same as the [Squeeze-and-Excitation Module](https://paperswithcode.com/method/squeeze-and-excitation-block).""" ;
    skos:prefLabel "Channel Attention Module" .

:ChannelShuffle a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1707.01083v2> ;
    rdfs:seeAlso <https://github.com/osmr/imgclsmob/blob/c03fa67de3c9e454e9b6d35fe9cbb6b15c28fda7/pytorch/pytorchcv/models/common.py#L862> ;
    skos:definition """**Channel Shuffle** is an operation to help information flow across feature channels in convolutional neural networks. It was used as part of the [ShuffleNet](https://paperswithcode.com/method/shufflenet) architecture. \r
\r
If we allow a group [convolution](https://paperswithcode.com/method/convolution) to obtain input data from different groups, the input and output channels will be fully related. Specifically, for the feature map generated from the previous group layer, we can first divide the channels in each group into several subgroups, then feed each group in the next layer with different subgroups. \r
\r
The above can be efficiently and elegantly implemented by a channel shuffle operation: suppose a convolutional layer with $g$ groups whose output has $g \\times n$ channels; we first reshape the output channel dimension into $\\left(g, n\\right)$, transposing and then flattening it back as the input of next layer. Channel shuffle is also differentiable, which means it can be embedded into network structures for end-to-end training.""" ;
    skos:prefLabel "Channel Shuffle" .

:ChannelSqueezeandSpatialExcitation a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1808.08127v1> ;
    rdfs:seeAlso <https://github.com/jlcsilva/segmentation_models.pytorch/blob/53c7f956ca557eb2cf386d28faacadf30ce4d0e2/segmentation_models_pytorch/base/modules.py#L117> ;
    skos:altLabel "Channel Squeeze and Spatial Excitation (sSE)" ;
    skos:definition "Inspired on the widely known [spatial squeeze and channel excitation (SE)](https://paperswithcode.com/method/squeeze-and-excitation-block) block, the sSE block performs channel squeeze and spatial excitation, to recalibrate the feature maps spatially and achieve more fine-grained image segmentation." ;
    skos:prefLabel "Channel Squeeze and Spatial Excitation" .

:Channelattention a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1709.01507v4> ;
    skos:altLabel "squeeze-and-excitation networks" ;
    skos:definition """SENet pioneered channel attention. The core of SENet is a squeeze-and-excitation (SE) block which is used to collect global information, capture channel-wise relationships and improve representation ability.\r
SE blocks are divided into two parts, a squeeze module and an excitation module. Global spatial information is collected in the squeeze module by global average pooling. The excitation module captures channel-wise relationships and outputs an attention vector by using fully-connected layers and non-linear layers (ReLU and sigmoid). Then, each channel of the input feature is scaled by multiplying the corresponding element in the attention vector. Overall, a squeeze-and-excitation block $F_\\text{se}$ (with parameter $\\theta$) which takes $X$ as input and outputs $Y$ can be formulated \r
as:\r
\\begin{align}\r
    s = F_\\text{se}(X, \\theta) & = \\sigma (W_{2} \\delta (W_{1}\\text{GAP}(X)))\r
\\end{align}\r
\\begin{align}\r
    Y = sX\r
\\end{align}""" ;
    skos:prefLabel "Channel attention" .

:CharacterBERT a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2010.10392v3> ;
    rdfs:seeAlso <https://github.com/helboukkouri/character-bert#using-characterbert-in-practice> ;
    skos:definition "CharacterBERT is a variant of [BERT](https://paperswithcode.com/method/bert) that **drops the wordpiece system** and **replaces it with a CharacterCNN module** just like the one [ELMo](https://paperswithcode.com/method/elmo) uses to produce its first layer representation. This allows CharacterBERT to represent any input token without splitting it into wordpieces. Moreover, this frees BERT from the burden of a domain-specific wordpiece vocabulary which may not be suited to your domain of interest (e.g. medical domain). Finally, it allows the model to be more robust to noisy inputs." ;
    skos:prefLabel "CharacterBERT" .

:CharacteristicFunctions a skos:Concept ;
    dcterms:source <https://link.springer.com/article/10.1007/s41060-023-00409-5> ;
    skos:altLabel "Characteristic Function Estimation for Discrete Probability Distributions" ;
    skos:definition "" ;
    skos:prefLabel "Characteristic Functions" .

:Charformer a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2106.12672v3> ;
    skos:definition "**Charformer** is a type of [Transformer](https://paperswithcode.com/methods/category/transformers) model that learns a subword tokenization end-to-end as part of the model. Specifically it uses [GBST](https://paperswithcode.com/method/gradient-based-subword-tokenization) that automatically learns latent subword representations from characters in a data-driven fashion. Following GBST, the soft subword sequence is passed through [Transformer](https://paperswithcode.com/method/transformer) layers." ;
    skos:prefLabel "Charformer" .

:CheXNet a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1711.05225v3> ;
    rdfs:seeAlso <https://github.com/arnoweng/CheXNet/blob/3d45ee77e2cfe6ec81bf1b87e2ed4e3fc10af6c1/model.py#L34> ;
    skos:definition "**CheXNet** is a 121-layer [DenseNet](https://paperswithcode.com/method/densenet) trained on ChestX-ray14 for pneumonia detection." ;
    skos:prefLabel "CheXNet" .

:ChebNet a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1606.09375v3> ;
    skos:definition """ChebNet involves a formulation of CNNs in the context of spectral graph theory, which provides the necessary mathematical background and efficient numerical schemes to design fast localized convolutional filters on graphs.\r
\r
Description from: [Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering](https://arxiv.org/pdf/1606.09375.pdf)""" ;
    skos:prefLabel "ChebNet" .

:Child-Tuning a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2109.05687v1> ;
    skos:definition "**Child-Tuning** is a fine-tuning technique that updates a subset of parameters (called child network) of large pretrained models via strategically masking out the gradients of the non-child network during the backward process. It decreases the hypothesis space of the model via a task-specific mask applied to the full gradients, helping to effectively adapt the large-scale pretrained model to various tasks and meanwhile aiming to maintain its original generalization ability." ;
    skos:prefLabel "Child-Tuning" .

:Chimera a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2107.06925v3> ;
    skos:definition """**Chimera** is a pipeline model parallelism scheme which combines bidirectional pipelines for efficiently training large-scale models. The key idea of Chimera is to combine two pipelines in different directions (down and up pipelines). \r
\r
Denote $N$ as the number of micro-batches executed by each worker within a training iteration, and $D$ the number of pipeline stages (depth), and $P$ the number of workers.\r
\r
The Figure shows an example with four pipeline stages (i.e. $D=4$). Here we assume there are $D$ micro-batches executed by each worker within a training iteration, namely $N=D$, which is the minimum to keep all the stages active. \r
\r
In the down pipeline, stage$\\_{0}$∼stage$\\_{3}$ are mapped to $P\\_{0}∼P\\_{3}$ linearly, while in the up pipeline the stages are mapped in a completely opposite order. The $N$ (assuming an even number) micro-batches are equally partitioned among the two pipelines. Each pipeline schedules $N/2$ micro-batches using 1F1B strategy, as shown in the left part of the Figure. Then, by merging these two pipelines together, we obtain the pipeline schedule of Chimera. Given an even number of stages $D$ (which can be easily satisfied in practice), it is guaranteed that there is no conflict (i.e., there is at most one micro-batch occupies the same time slot on each worker) during merging.""" ;
    skos:prefLabel "Chimera" .

:Chinchilla a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2203.15556v1> ;
    skos:definition "Chinchilla is a 70B parameters model trained as a compute-optimal model with 1.4 trillion tokens. Findings suggest that these types of models are trained optimally by equally scaling both model size and training tokens. It uses the same compute budget as Gopher but with 4x more training data. Chinchilla and Gopher are trained for the same number of FLOPs. It is trained using [MassiveText](/dataset/massivetext) using a slightly modified SentencePiece tokenizer. More architectural details in the paper." ;
    skos:prefLabel "Chinchilla" .

:ChinesePre-trainedUnbalancedTransformer a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2109.05729v4> ;
    skos:definition "**CPT**, or **Chinese Pre-trained Unbalanced Transformer**, is a pre-trained unbalanced [Transformer](https://paperswithcode.com/method/transformer) for Chinese natural language understanding (NLU) and natural language generation (NLG) tasks. CPT consists of three parts: a shared encoder, an understanding decoder, and a generation decoder. Two specific decoders with a shared encoder are pre-trained with masked language modeling (MLM) and denoising auto-encoding (DAE) tasks, respectively. With the partially shared architecture and multi-task pre-training, CPT can (1) learn specific knowledge of both NLU or NLG tasks with two decoders and (2) be fine-tuned flexibly that fully exploits the potential of the model. Two specific decoders with a shared encoder are pre-trained with masked language modeling (MLM) and denoising auto-encoding (DAE) tasks, respectively. With the partially shared architecture and multi-task pre-training, CPT can (1) learn specific knowledge of both NLU or NLG tasks with two decoders and (2) be fine-tuned flexibly that fully exploits the potential of the model." ;
    skos:prefLabel "Chinese Pre-trained Unbalanced Transformer" .

:ClariNet a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1807.07281v3> ;
    skos:definition "**ClariNet** is an end-to-end text-to-speech architecture. Unlike previous TTS systems which use text-to-spectogram models with a separate waveform [synthesizer](https://paperswithcode.com/method/synthesizer) (vocoder), ClariNet is a text-to-wave architecture that is fully convolutional and can be trained from scratch. In ClariNet, the [WaveNet](https://paperswithcode.com/method/wavenet) module is conditioned on the hidden states instead of the mel-spectogram. The architecture is otherwise based on [Deep Voice 3](https://paperswithcode.com/method/deep-voice-3)." ;
    skos:prefLabel "ClariNet" .

:Class-MLP a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2105.03404v2> ;
    skos:definition "**Class-MLP** is an alternative to [average pooling](https://paperswithcode.com/method/average-pooling), which is an adaptation of the class-attention token introduced in [CaiT](https://paperswithcode.com/method/cait). In CaiT, this consists of two layers that have the same structure as the [transformer](https://paperswithcode.com/method/transformer), but in which only the class token is updated based on the frozen patch embeddings. In Class-MLP, the same approach is used, but after aggregating the patches with a [linear layer](https://paperswithcode.com/method/linear-layer), we replace the [attention-based interaction](https://paperswithcode.com/method/scaled) between the class and patch embeddings by simple linear layers, still keeping the patch embeddings frozen. This increases the performance, at the expense of adding some parameters and computational cost. This pooling variant is referred to as “class-MLP”, since the purpose of these few layers is to replace average pooling." ;
    skos:prefLabel "Class-MLP" .

:ClassActivationGuidedAttentionMechanism a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2109.03223v2> ;
    skos:altLabel "Class Activation Guided Attention Mechanism (CAGAM)" ;
    skos:definition "CAGAM is a form of spatial attention mechanism that propagates attention from a known to an unknown context features thereby enhancing the unknown context for relevant pattern discovery. Usually the known context feature is a class activation map ([CAM](https://paperswithcode.com/method/cam))." ;
    skos:prefLabel "Class Activation Guided Attention Mechanism" .

:ClassAttention a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2103.17239v2> ;
    skos:definition """A **Class Attention** layer, or **CA Layer**, is an [attention mechanism](https://paperswithcode.com/methods/category/attention-mechanisms-1) for [vision transformers](https://paperswithcode.com/methods/category/vision-transformer) used in [CaiT](https://paperswithcode.com/method/cait) that aims to extract information from a set of processed patches. It is identical to a [self-attention layer](https://paperswithcode.com/method/scaled), except that it relies on the attention between (i) the class embedding $x_{\\text {class }}$ (initialized at CLS in the first CA) and (ii) itself plus the set of frozen patch embeddings $x_{\\text {patches }} .$ \r
\r
Considering a network with $h$ heads and $p$ patches, and denoting by $d$ the embedding size, the multi-head class-attention is parameterized with several projection matrices, $W_{q}, W_{k}, W_{v}, W_{o} \\in \\mathbf{R}^{d \\times d}$, and the corresponding biases $b_{q}, b_{k}, b_{v}, b_{o} \\in \\mathbf{R}^{d} .$ With this notation, the computation of the CA residual block proceeds as follows. We first augment the patch embeddings (in matrix form) as $z=\\left[x_{\\text {class }}, x_{\\text {patches }}\\right]$. We then perform the projections:\r
\r
$$Q=W\\_{q} x\\_{\\text {class }}+b\\_{q}$$\r
\r
$$K=W\\_{k} z+b\\_{k}$$\r
\r
$$V=W\\_{v} z+b\\_{v}$$\r
\r
The class-attention weights are given by\r
\r
$$\r
A=\\operatorname{Softmax}\\left(Q . K^{T} / \\sqrt{d / h}\\right)\r
$$\r
\r
where $Q . K^{T} \\in \\mathbf{R}^{h \\times 1 \\times p}$. This attention is involved in the weighted sum $A \\times V$ to produce the residual output vector\r
\r
$$\r
\\operatorname{out}\\_{\\mathrm{CA}}=W\\_{o} A V+b\\_{o}\r
$$\r
\r
which is in turn added to $x\\_{\\text {class }}$ for subsequent processing.""" ;
    skos:prefLabel "Class Attention" .

:ClassSR a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2103.04039v1> ;
    skos:definition "**ClassSR** is a framework to accelerate super-resolution (SR) networks on large images (2K-8K). ClassSR combines classification and SR in a unified framework. In particular, it first uses a Class-Module to classify the sub-images into different classes according to restoration difficulties, then applies an SR-Module to perform SR for different classes. The Class-Module is a conventional classification network, while the SR-Module is a network container that consists of the to-be-accelerated SR network and its simplified versions." ;
    skos:prefLabel "ClassSR" .

:ClipBERT a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2102.06183v1> ;
    skos:definition """**ClipBERT** is a framework for end-to-end-learning for video-and-language tasks, by employing sparse sampling, where only a single or a few sparsely sampled short clips from a video are used at each training step. Two aspects distinguish ClipBERT from previous work. \r
\r
First, in contrast to densely extracting video features (adopted by most existing methods), CLIPBERT sparsely samples only one single or a few short clips from the full-length videos at each training step. The hypothesis is that visual features from sparse clips already capture key visual and semantic information in the video, as consecutive clips usually contain similar semantics from a continuous scene. Thus, a handful of clips are sufficient for training, instead of using the full video. Then, predictions from multiple densely-sampled clips are aggregated to obtain the final video-level prediction during inference, which is less computational demanding. \r
\r
The second differentiating aspect concerns the initialization of model weights (i.e., transfer through pre-training). The authors use 2D architectures (e.g., [ResNet](https://paperswithcode.com/method/resnet)-50) instead of 3D features as the visual backbone for video encoding, allowing them to harness the power of image-text pretraining for video-text understanding along with the advantages of low memory cost and runtime efficiency.""" ;
    skos:prefLabel "ClipBERT" .

:ClippedDoubleQ-learning a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1802.09477v3> ;
    skos:definition """**Clipped Double Q-learning** is a variant on [Double Q-learning](https://paperswithcode.com/method/double-q-learning) that upper-bounds the less biased Q estimate $Q\\_{\\theta\\_{2}}$ by the biased estimate $Q\\_{\\theta\\_{1}}$. This is equivalent to taking the minimum of the two estimates, resulting in the following target update:\r
\r
$$ y\\_{1} = r + \\gamma\\min\\_{i=1,2}Q\\_{\\theta'\\_{i}}\\left(s', \\pi\\_{\\phi\\_{1}}\\left(s'\\right)\\right) $$\r
\r
The motivation for this extension is that vanilla double [Q-learning](https://paperswithcode.com/method/q-learning) is sometimes ineffective if the target and current networks are too similar, e.g. with a slow-changing policy in an actor-critic framework.""" ;
    skos:prefLabel "Clipped Double Q-learning" .

:Cluster-GCN a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1905.07953v2> ;
    skos:definition """Cluster-GCN is a novel GCN algorithm that is suitable for SGD-based training by exploiting the graph clustering structure. Cluster-GCN works as the following: at each step, it samples a block of nodes that associate with a dense subgraph identified by a graph clustering algorithm, and restricts the neighborhood search within this subgraph. This simple but effective strategy leads to significantly improved memory and computational efficiency while being able to achieve comparable test accuracy with previous algorithms.\r
\r
Description and image from: [Cluster-GCN: An Efficient Algorithm for Training Deep and Large Graph Convolutional Networks](https://arxiv.org/pdf/1905.07953.pdf)""" ;
    skos:prefLabel "Cluster-GCN" .

:ClusterFit a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1912.03330v1> ;
    skos:definition "**ClusterFit** is a self-supervision approach for learning image representations.  Given a dataset, we (a) cluster its features extracted from a pre-trained network using k-means and (b) re-train a new network from scratch on this dataset using cluster assignments as pseudo-labels." ;
    skos:prefLabel "ClusterFit" .

:Co-Correcting a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2109.05159v1> ;
    skos:definition "**Co-Correcting** is a noise-tolerant deep learning framework for medical image classification based on mutual learning and annotation correction. It consists of three modules: the dual-network architecture, the curriculum learning module, and the label correction module." ;
    skos:prefLabel "Co-Correcting" .

:CoBERL a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2107.05431v2> ;
    skos:altLabel "Contrastive BERT" ;
    skos:definition """**Contrastive BERT** is a reinforcement learning agent that combines a new contrastive loss and a hybrid [LSTM](https://paperswithcode.com/method/lstm)-[transformer](https://paperswithcode.com/method/transformer) architecture to tackle the challenge of improving data efficiency for RL. It uses bidirectional masked prediction in combination with a generalization of recent contrastive methods to learn better representations for transformers in RL, without the need of hand engineered data augmentations.\r
\r
For the architecture, a residual network is used to encode observations into embeddings $Y\\_{t}$. $Y_{t}$  is fed through a causally masked [GTrXL transformer](https://www.paperswithcode.com/method/gtrxl), which computes the predicted masked inputs $X\\_{t}$ and passes those together with $Y\\_{t}$ to a learnt gate. The output of the gate is passed through a single [LSTM](https://www.paperswithcode.com/method/lstm) layer to produce the values that we use for computing the RL loss. A contrastive loss is computed using predicted masked inputs $X_{t}$ and $Y_{t}$ as targets. For this, we do not use the causal mask of the Transformer.""" ;
    skos:prefLabel "CoBERL" .

:CoLU a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2112.12078v1> ;
    skos:altLabel "Collapsing Linear Unit" ;
    skos:definition """CoLU is an activation function similar to Swish and Mish in properties. It is defined as:\r
$$f(x)=\\frac{x}{1-x^{-(x+e^x)}}$$\r
It is smooth, continuously differentiable, unbounded above, bounded below, non-saturating, and non-monotonic. Based on experiments done with CoLU with different activation functions, it is observed that CoLU usually performs better than other functions on deeper neural networks.""" ;
    skos:prefLabel "CoLU" .

:CoOp a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2109.01134v6> ;
    skos:altLabel "Context Optimization" ;
    skos:definition "**CoOp**, or **Context Optimization**, is an automated prompt engineering method that avoids manual prompt tuning by modeling context words with continuous vectors that are end-to-end learned from data. The context could be shared among all classes or designed to be class-specific. During training, we simply minimize the prediction error using the cross-entropy loss with respect to the learnable context vectors, while keeping the pre-trained parameters fixed. The gradients can be back-propagated all the way through the text encoder, distilling the rich knowledge encoded in the parameters for learning task-relevant context." ;
    skos:prefLabel "CoOp" .

:CoTPrompting a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2201.11903v6> ;
    skos:altLabel "Chain-of-thought prompting" ;
    skos:definition "Chain-of-thought prompts contain a series of intermediate reasoning steps, and they are shown to significantly improve the ability of large language models to perform certain tasks that involve complex reasoning (e.g., arithmetic, commonsense reasoning, symbolic reasoning, etc.)" ;
    skos:prefLabel "CoT Prompting" .

:CoVA a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2110.12320v1> ;
    rdfs:seeAlso <https://github.com/kevalmorabia97/CoVA-Web-Object-Detection/blob/master/models.py> ;
    skos:altLabel "Context-aware Visual Attention-based (CoVA) webpage object detection pipeline" ;
    skos:definition """Context-Aware Visual Attention-based end-to-end pipeline for Webpage Object Detection (_CoVA_) aims to learn function _f_ to predict labels _y = [$y_1, y_2, ..., y_N$]_ for a webpage containing _N_ elements. The input to CoVA consists of:\r
1. a screenshot of a webpage,\r
2. list of bounding boxes _[x, y, w, h]_ of the web elements, and\r
3. neighborhood information for each element obtained from the DOM tree.\r
\r
This information is processed in four stages:\r
1. the graph representation extraction for the webpage,\r
2. the Representation Network (_RN_),\r
3. the Graph Attention Network (_GAT_), and\r
4. a fully connected (_FC_) layer.\r
\r
The graph representation extraction computes for every web element _i_ its set of _K_ neighboring web elements _$N_i$_. The _RN_ consists of a Convolutional Neural Net (_CNN_) and a positional encoder aimed to learn a visual representation _$v_i$_ for each web element _i &isin; {1, ..., N}_. The _GAT_ combines the visual representation _$v_i$_ of the web element _i_ to be classified and those of its neighbors, i.e., _$v_k$ &forall;k &isin; $N_i$_ to compute the contextual representation _$c_i$_ for web element _i_. Finally, the visual and contextual representations of the web element are concatenated and passed through the _FC_ layer to obtain the classification output.""" ;
    skos:prefLabel "CoVA" .

:CoVR a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2308.14746v1> ;
    skos:altLabel "Composed Video Retrieval" ;
    skos:definition "The composed video retrieval (CoVR) task is a new task, where the goal is to find a video that matches both a query image and a query text. The query image represents a visual concept that the user is interested in, and the query text specifies how the concept should be modified or refined. For example, given an image of a fountain and the text _during show at night_, the CoVR task is to retrieve a video that shows the fountain at night with a show." ;
    skos:prefLabel "CoVR" .

:CoVe a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1708.00107v2> ;
    skos:altLabel "Contextual Word Vectors" ;
    skos:definition """**CoVe**, or **Contextualized Word Vectors**, uses a deep [LSTM](https://paperswithcode.com/method/lstm) encoder from an attentional sequence-to-sequence model trained for machine translation to contextualize word vectors. $\\text{CoVe}$ word embeddings are therefore a function of the entire input sequence. These word embeddings can then be used in downstream tasks by concatenating them with $\\text{GloVe}$ embeddings:\r
\r
$$ v = \\left[\\text{GloVe}\\left(x\\right), \\text{CoVe}\\left(x\\right)\\right]$$\r
\r
and then feeding these in as features for the task-specific models.""" ;
    skos:prefLabel "CoVe" .

:CoaT a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2104.06399v2> ;
    skos:altLabel "Co-Scale Conv-attentional Image Transformer" ;
    skos:definition "**Co-Scale Conv-Attentional Image Transformer** (CoaT) is a [Transformer](https://paperswithcode.com/method/transformer)-based image classifier equipped with co-scale and conv-attentional mechanisms. First, the co-scale mechanism maintains the integrity of Transformers' encoder branches at individual scales, while allowing representations learned at different scales to effectively communicate with each other. Second, the conv-attentional mechanism is designed by realizing a relative position embedding formulation in the factorized attention module with an efficient [convolution](https://paperswithcode.com/method/convolution)-like implementation. CoaT empowers image Transformers with enriched multi-scale and contextual modeling capabilities." ;
    skos:prefLabel "CoaT" .

:CodeBERT a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2002.08155v4> ;
    skos:definition "**CodeBERT** is a bimodal pre-trained model for programming language (PL) and natural language (NL). CodeBERT learns general-purpose representations that support downstream NL-PL applications such as natural language code search, code documentation generation, etc. CodeBERT is developed with a [Transformer](https://paperswithcode.com/method/transformer)-based neural architecture, and is trained with a hybrid objective function that incorporates the pre-training task of replaced token detection, which is to detect plausible alternatives sampled from generators. This enables the utilization of both bimodal data of NL-PL pairs and unimodal data, where the former provides input tokens for model training while the latter helps to learn better generators." ;
    skos:prefLabel "CodeBERT" .

:CodeGen a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2203.13474v5> ;
    skos:definition "**CodeGen** is an autoregressive transformers with next-token prediction language modeling as the learning objective trained on a natural language corpus and programming language data curated from GitHub." ;
    skos:prefLabel "CodeGen" .

:CodeSLAM a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1804.00874v2> ;
    skos:definition "CodeSLAM represents the 3D geometry of a scene using the latent space of a variational autoencoder. The depth thus becomes a function of the RGB image and the unknown code, $D = G_\\theta(I,c)$. During training time, the weights of the network $G_\\theta$ are learnt by training the generator and encoder using a standard autoencoding task. At test time the code $c$ and the pose of the images is found by optimizing the reprojection error over multiple images." ;
    skos:prefLabel "CodeSLAM" .

:CodeT5 a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2109.00859v1> ;
    skos:definition "**CodeT5** is a [Transformer](https://paperswithcode.com/methods/category/transformers)-based model for code understanding and generation based on the [T5 architecture](https://paperswithcode.com/method/t5). It utilizes an identifier-aware pre-training objective that considers the crucial token type information (identifiers) from code. Specifically, the denoising [Seq2Seq](https://paperswithcode.com/method/seq2seq) objective of T5 is extended with two identifier tagging and prediction tasks to enable the model to better leverage the token type information from programming languages, which are the identifiers assigned by developers. To improve the natural language-programming language alignment, a bimodal dual learning objective is used for a bidirectional conversion between natural language and programming language." ;
    skos:prefLabel "CodeT5" .

:CollaborativeDistillation a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2003.08436v2> ;
    skos:definition "**Collaborative Distillation** is a new knowledge distillation method (named Collaborative Distillation) for encoder-decoder based neural style transfer to reduce the number of convolutional filters. The main idea is underpinned by a finding that the encoder-decoder pairs construct an exclusive collaborative relationship, which is regarded as a new kind of knowledge for style transfer models." ;
    skos:prefLabel "Collaborative Distillation" .

:ColorJitter a skos:Concept ;
    skos:altLabel "Color Jitter" ;
    skos:definition """**ColorJitter** is a type of image data augmentation where we randomly change the brightness, contrast and saturation of an image.\r
\r
Image Credit: [Apache MXNet](https://mxnet.apache.org/versions/1.5.0/tutorials/gluon/data_augmentation.html)""" ;
    skos:prefLabel "ColorJitter" .

:Colorization a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1603.08511v5> ;
    skos:definition "**Colorization** is a self-supervision approach that relies on colorization as the pretext task in order to learn image representations." ;
    skos:prefLabel "Colorization" .

:ColorizationTransformer a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2102.04432v2> ;
    skos:definition """**Colorization Transformer** is a probabilistic [colorization](https://paperswithcode.com/method/colorization) model composed only of [axial self-attention blocks](https://paperswithcode.com/method/axial). The main advantages of these blocks are the ability to capture a global receptive field with only two layers and $\\mathcal{O}(D\\sqrt{D})$ instead of $\\text{O}(D^{2})$ complexity. In order to enable colorization of high-resolution grayscale images, the task is decomposed into three simpler sequential subtasks: coarse low resolution autoregressive colorization, parallel color and spatial super-resolution.\r
\r
For coarse low resolution colorization, a conditional variant of [Axial Transformer](https://paperswithcode.com/method/axial) is applied. The authors leverage the semi-parallel sampling mechanism of Axial Transformers. Finally, fast parallel deterministic upsampling models are employed to super-resolve the coarsely colorized image into the final high resolution output.""" ;
    skos:prefLabel "Colorization Transformer" .

:ComiRec a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2005.09347v2> ;
    skos:definition "**ComiRec** is a multi-interest framework for sequential recommendation. The multi-interest module captures multiple interests from user behavior sequences, which can be exploited for retrieving candidate items from the large-scale item pool. These items are then fed into an aggregation module to obtain the overall recommendation. The aggregation module leverages a controllable factor to balance the recommendation accuracy and diversity." ;
    skos:prefLabel "ComiRec" .

:CompactGlobalDescriptor a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1907.09665v10> ;
    rdfs:seeAlso <https://github.com/HolmesShuan/Compact-Global-Descriptor/blob/bb22bca13bd37132abc2db5086868cf65238a99e/classification/models/imagenet/attention_best.py#L9> ;
    skos:definition "A **Compact Global Descriptor** is an image model block for modelling interactions between positions across different dimensions (e.g., channels, frames). This descriptor enables subsequent convolutions to access the informative global features. It is a form of attention." ;
    skos:prefLabel "Compact Global Descriptor" .

:ComplEx-N3 a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1806.07297v1> ;
    skos:altLabel "ComplEx with N3 Regularizer" ;
    skos:definition "ComplEx model trained with a nuclear norm regularizer" ;
    skos:prefLabel "ComplEx-N3" .

:ComplEx-N3-RP a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2110.02834v1> ;
    skos:altLabel "ComplEx with N3 Regularizer and Relation Prediction Objective" ;
    skos:definition "ComplEx model trained with a nuclear norm regularizer; A relation prediction objective is added on top of the commonly used 1vsAll objective." ;
    skos:prefLabel "ComplEx-N3-RP" .

:CompositeFields a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1903.06593v2> ;
    skos:definition "Represent and associate with a composite of primitive fields." ;
    skos:prefLabel "Composite Fields" .

:CompressedMemory a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1911.05507v1> ;
    rdfs:seeAlso <https://github.com/lucidrains/compressive-transformer-pytorch/blob/a73d146ee7bc9e47cc6c525f645a5bbb583537ad/compressive_transformer_pytorch/compressive_transformer_pytorch.py#L252> ;
    skos:definition """**Compressed Memory** is a secondary FIFO memory component proposed as part of the [Compressive Transformer](https://paperswithcode.com/method/compressive-transformer) model. The Compressive [Transformer](https://paperswithcode.com/method/transformer) keeps a fine-grained memory of past activations, which are then compressed into coarser compressed memories. \r
\r
For choices of compression functions $f\\_{c}$ the authors consider (1) max/mean pooling, where the kernel and stride is set to the compression rate $c$; (2) 1D [convolution](https://paperswithcode.com/method/convolution) also with kernel & stride set to $c$; (3) dilated convolutions; (4) *most-used* where the memories are sorted by their average attention (usage) and the most-used are preserved.""" ;
    skos:prefLabel "Compressed Memory" .

:CompressiveTransformer a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1911.05507v1> ;
    rdfs:seeAlso <https://github.com/lucidrains/compressive-transformer-pytorch/blob/a73d146ee7bc9e47cc6c525f645a5bbb583537ad/compressive_transformer_pytorch/compressive_transformer_pytorch.py#L291> ;
    skos:definition """The **Compressive Transformer** is an extension to the [Transformer](https://paperswithcode.com/method/transformer) which maps past hidden activations (memories) to a smaller set of compressed representations (compressed memories). The Compressive Transformer uses the same attention mechanism over its set of memories and compressed memories, learning to query both its short-term granular memory and longer-term coarse memory. It builds on the ideas of [Transformer-XL](https://paperswithcode.com/method/transformer-xl) which maintains a memory of past activations at each layer to preserve a longer history of context. The Transformer-XL discards past activations when they become sufficiently old (controlled by the size of the memory). The key principle of the Compressive Transformer is to compress these old memories, instead of discarding them, and store them in an additional [compressed memory](https://paperswithcode.com/method/compressed-memory).\r
\r
At each time step $t$, we discard the oldest compressed memories (FIFO) and then the oldest $n$ states from ordinary memory are compressed and shifted to the new slot in compressed memory. During training, the compressive memory component is optimized separately from the main language model (separate training loop).""" ;
    skos:prefLabel "Compressive Transformer" .

:ComputationRedistribution a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2105.04714v1> ;
    skos:definition "**Computation Redistribution** is an [neural architecture search](https://paperswithcode.com/task/architecture-search) method for [face detection](https://paperswithcode.com/task/face-detection), which reallocates the computation between the backbone, neck and head of the model based on a predefined search methodology. Directly utilising the backbone of a classification network for scale-specific face detection can be sub-optimal. Therefore, [network structure search](https://paperswithcode.com/method/regnety) is used to reallocate the computation on the backbone, neck and head, under a wide range of flop regimes. The search method is applied to [RetinaNet](https://paperswithcode.com/method/retinanet), with [ResNet](https://paperswithcode.com/method/resnet) as backbone, [Path Aggregation Feature Pyramid Network](https://paperswithcode.com/method/pafpn) (PAFPN)  as the neck and stacked 3 × 3 [convolutional layers](https://paperswithcode.com/method/convolution) for the head. While the general structure is simple, the total number of possible networks in the search space is unwieldy. In the first step, the authors explore the reallocation of the computation within the backbone parts (i.e. stem, C2, C3, C4, and C5), while fixing the neck and head components. Based on the optimised computation distribution on the backbone they find, they further explore the reallocation of the computation across the backbone, neck and head." ;
    skos:prefLabel "Computation Redistribution" .

:ConViT a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2103.10697v2> ;
    rdfs:seeAlso <https://github.com/facebookresearch/convit/blob/main/convit.py> ;
    skos:definition "**ConViT** is a type of [vision transformer](https://paperswithcode.com/method/vision-transformer) that uses a gated positional self-attention module ([GPSA](https://paperswithcode.com/method/gpsa)), a form of positional self-attention which can be equipped with a “soft” convolutional inductive bias. The GPSA layers are initialized to mimic the locality of convolutional layers, then each attention head is given the freedom to escape locality by adjusting a gating parameter regulating the attention paid to position versus content information." ;
    skos:prefLabel "ConViT" .

:ConcatenatedSkipConnection a skos:Concept ;
    rdfs:seeAlso <https://github.com/pytorch/vision/blob/7c077f6a986f05383bcb86b535aedb5a63dd5c4b/torchvision/models/densenet.py#L113> ;
    skos:definition "A **Concatenated Skip Connection** is a type of skip connection that seeks to reuse features by concatenating them to new layers, allowing more information to be retained from previous layers of the network. This contrasts with say, residual connections, where element-wise summation is used instead to incorporate information from previous layers. This type of skip connection is prominently used in DenseNets (and also Inception networks), which the Figure to the right illustrates." ;
    skos:prefLabel "Concatenated Skip Connection" .

:ConcatenationAffinity a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1711.07971v3> ;
    rdfs:seeAlso <https://github.com/tea1528/Non-Local-NN-Pytorch/blob/986937674eb3b85d3d3fbaaa8f384c0a26624121/models/non_local.py#L105> ;
    skos:definition """**Concatenation Affinity** is a type of affinity or self-similarity function between two points $\\mathbb{x\\_{i}}$ and $\\mathbb{x\\_{j}}$ that uses a concatenation function:\r
\r
$$ f\\left(\\mathbb{x\\_{i}}, \\mathbb{x\\_{j}}\\right) = \\text{ReLU}\\left(\\mathbb{w}^{T}\\_{f}\\left[\\theta\\left(\\mathbb{x}\\_{i}\\right), \\phi\\left(\\mathbb{x}\\_{j}\\right)\\right]\\right)$$\r
\r
Here $\\left[·, ·\\right]$ denotes concatenation and $\\mathbb{w}\\_{f}$ is a weight vector that projects the concatenated vector to a scalar.""" ;
    skos:prefLabel "Concatenation Affinity" .

:ConcreteDropout a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1705.07832v1> ;
    skos:definition "Please enter a description about the method here" ;
    skos:prefLabel "Concrete Dropout" .

<http://w3id.org/mlso/vocab/ml_algorithm/ConcurrentSpatialandChannelSqueeze&Excitation> a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1808.08127v1> ;
    rdfs:seeAlso <https://github.com/qubvel/segmentation_models.pytorch/blob/a6e1123983548be55d4d1320e0a2f5fd9174d4ac/segmentation_models_pytorch/base/modules.py#L50> ;
    skos:altLabel "Concurrent Spatial and Channel Squeeze & Excitation (scSE)" ;
    skos:definition "Combines the channel attention of the widely known [spatial squeeze and channel excitation (SE)](https://paperswithcode.com/method/squeeze-and-excitation-block) block and the spatial attention of the [channel squeeze and spatial excitation (sSE)](https://paperswithcode.com/method/channel-squeeze-and-spatial-excitation#) block to build a spatial and channel attention mechanism for image segmentation tasks." ;
    skos:prefLabel "Concurrent Spatial and Channel Squeeze & Excitation" .

:CondConv a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1904.04971v3> ;
    rdfs:seeAlso <https://github.com/tensorflow/tpu/blob/bb03c18d7c2501b4df070c9936a46a9dcd6ad1cb/models/official/efficientnet/condconv/condconv_layers.py#L63> ;
    skos:definition "**CondConv**, or **Conditionally Parameterized Convolutions**, are a type of [convolution](https://paperswithcode.com/method/convolution) which learn specialized convolutional kernels for each example. In particular, we parameterize the convolutional kernels in a CondConv layer as a linear combination of $n$ experts $(\\alpha_1 W_1 + \\ldots + \\alpha_n W_n) * x$, where $\\alpha_1, \\ldots, \\alpha_n$ are functions of the input learned through gradient descent. To efficiently increase the capacity of a CondConv layer, developers can increase the number of experts. This can be more computationally efficient than increasing the size of the convolutional kernel itself, because the convolutional kernel is applied at many different positions within the input, while the experts are combined only once per input." ;
    skos:prefLabel "CondConv" .

:CondInst a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2003.05664v4> ;
    rdfs:seeAlso <https://github.com/aim-uofa/AdelaiDet/blob/master/configs/CondInst/README.md> ;
    skos:altLabel "Conditional Convolutions for Instance Segmentation" ;
    skos:definition "CondInst is a simple yet effective instance segmentation framework. It eliminates ROI cropping and feature alignment with the instance-aware mask heads. As a result, CondInst can solve instance segmentation with fully convolutional networks. CondInst is able to produce high-resolution instance masks without longer computational time. Extensive experiments show that CondInst can achieve even better performance and inference speed than [Mask R-CNN](https://paperswithcode.com/method/mask-r-cnn). It can be a strong alternative to previous ROI-based instance segmentation methods. Code is at https://github.com/aim-uofa/AdelaiDet." ;
    skos:prefLabel "CondInst" .

:ConditionalBatchNormalization a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1707.00683v3> ;
    rdfs:seeAlso <https://github.com/ap229997/Conditional-Batch-Norm/blob/6e237ed5794246e1bbbe95bbda9acf81d0cdeace/model/cbn.py#L9> ;
    skos:definition """**Conditional Batch Normalization (CBN)** is a class-conditional variant of [batch normalization](https://paperswithcode.com/method/batch-normalization). The key idea is to predict the $\\gamma$ and $\\beta$ of the batch normalization from an embedding - e.g. a language embedding in VQA. CBN enables the linguistic embedding to manipulate entire feature maps by scaling them up or down, negating them, or shutting them off. CBN has also been used in [GANs](https://paperswithcode.com/methods/category/generative-adversarial-networks) to allow class information to affect the batch normalization parameters.\r
\r
Consider a single convolutional layer with batch normalization module $\\text{BN}\\left(F\\_{i,c,h,w}|\\gamma\\_{c}, \\beta\\_{c}\\right)$ for which pretrained scalars $\\gamma\\_{c}$ and $\\beta\\_{c}$ are available. We would like to directly predict these affine scaling parameters from, e.g., a language embedding $\\mathbf{e\\_{q}}$. When starting the training procedure, these parameters must be close to the pretrained values to recover the original [ResNet](https://paperswithcode.com/method/resnet) model as a poor initialization could significantly deteriorate performance. Unfortunately, it is difficult to initialize a network to output the pretrained $\\gamma$ and $\\beta$. For these reasons, the authors propose to predict a change $\\delta\\beta\\_{c}$ and $\\delta\\gamma\\_{c}$ on the frozen original scalars, for which it is straightforward to initialize a neural network to produce an output with zero-mean and small variance.\r
\r
The authors use a one-hidden-layer MLP to predict these deltas from a question embedding $\\mathbf{e\\_{q}}$ for all feature maps within the layer:\r
\r
$$\\Delta\\beta = \\text{MLP}\\left(\\mathbf{e\\_{q}}\\right)$$\r
\r
$$\\Delta\\gamma = \\text{MLP}\\left(\\mathbf{e\\_{q}}\\right)$$\r
\r
So, given a feature map with $C$ channels, these MLPs output a vector of size $C$. We then add these predictions to the $\\beta$ and $\\gamma$ parameters:\r
\r
$$ \\hat{\\beta}\\_{c} = \\beta\\_{c} + \\Delta\\beta\\_{c} $$\r
\r
$$ \\hat{\\gamma}\\_{c} = \\gamma\\_{c} + \\Delta\\gamma\\_{c} $$\r
\r
Finally, these updated $\\hat{β}$ and $\\hat{\\gamma}$ are used as parameters for the batch normalization: $\\text{BN}\\left(F\\_{i,c,h,w}|\\hat{\\gamma\\_{c}}, \\hat{\\beta\\_{c}}\\right)$. The authors freeze all ResNet parameters, including $\\gamma$ and $\\beta$, during training. A ResNet consists of\r
four stages of computation, each subdivided in several residual blocks. In each block, the authors apply CBN to the three convolutional layers.""" ;
    skos:prefLabel "Conditional Batch Normalization" .

:ConditionalDBlock a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1909.11646v2> ;
    rdfs:seeAlso <https://github.com/yanggeng1995/GAN-TTS/blob/75d70dec97ba11dbe5ee3e2e9ebfbdd10bd52389/models/discriminator.py#L67> ;
    skos:definition "**Conditional DBlock** is a residual based block used in the discriminator of the [GAN-TTS](https://paperswithcode.com/method/gan-tts) architecture. They are similar to the [GBlocks](https://paperswithcode.com/method/gblock) used in the generator, but without [batch normalization](https://paperswithcode.com/method/batch-normalization). Unlike the [DBlock](https://paperswithcode.com/method/dblock), the Conditional DBlock adds the embedding of the linguistic features after the first [convolution](https://paperswithcode.com/method/convolution)." ;
    skos:prefLabel "Conditional DBlock" .

:ConditionalInstanceNormalization a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1610.07629v5> ;
    rdfs:seeAlso <https://github.com/kewellcjj/pytorch-multiple-style-transfer/blob/8a78000360cc36d3bbabdf838b91b6d12f88ae82/transformer_net.py#L46> ;
    skos:definition """**Conditional Instance Normalization** is a normalization technique where all convolutional weights of a style transfer network are shared across many styles.  The goal of the procedure is transform\r
a layer’s activations $x$ into a normalized activation $z$ specific to painting style $s$. Building off\r
[instance normalization](https://paperswithcode.com/method/instance-normalization), we augment the $\\gamma$ and $\\beta$ parameters so that they’re $N \\times C$ matrices, where $N$ is the number of styles being modeled and $C$ is the number of output feature maps. Conditioning on a style is achieved as follows:\r
\r
$$ z = \\gamma\\_{s}\\left(\\frac{x - \\mu}{\\sigma}\\right) + \\beta\\_{s}$$\r
\r
where $\\mu$ and $\\sigma$ are $x$’s mean and standard deviation taken across spatial axes and $\\gamma\\_{s}$ and $\\beta\\_{s}$ are obtained by selecting the row corresponding to $s$ in the $\\gamma$ and $\\beta$ matrices. One added benefit of this approach is that one can stylize a single image into $N$ painting styles with a single feed forward pass of the network with a batch size of $N$.""" ;
    skos:prefLabel "Conditional Instance Normalization" .

:ConditionalPositionalEncoding a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2102.10882v3> ;
    skos:definition """**Conditional Positional Encoding**, or **CPE**, is a type of positional encoding for [vision transformers](https://paperswithcode.com/methods/category/vision-transformer). Unlike previous fixed or learnable positional encodings, which are predefined and independent of input tokens, CPE is dynamically generated and conditioned on the local neighborhood of the input tokens. As a result, CPE aims to generalize to the input sequences that are longer than what the model has ever seen during training. CPE can also keep the desired translation-invariance in the image classification task. CPE can be implemented with a [Position\r
Encoding Generator](https://paperswithcode.com/method/positional-encoding-generator) (PEG) and incorporated into the current [Transformer framework](https://paperswithcode.com/methods/category/transformers).""" ;
    skos:prefLabel "Conditional Positional Encoding" .

:Conffusion a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2211.09795v1> ;
    skos:altLabel "Confidence Intervals for Diffusion Models" ;
    skos:definition "Given a corrupted input image, Con\\textit{ffusion}, repurposes a pretrained diffusion model to generate lower and upper bounds around each reconstructed pixel. The true pixel value is guaranteed to fall within these bounds with probability $p$." ;
    skos:prefLabel "Conffusion" .

:Content-ConditionedStyleEncoder a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2007.07431v3> ;
    skos:definition """The **Content-Conditioned Style Encoder**, or **COCO**, is a style encoder used for image-to-image translation in the [COCO-FUNIT](https://paperswithcode.com/method/coco-funit#) architecture.  Unlike the style encoder in [FUNIT](https://arxiv.org/abs/1905.01723), COCO takes both content and style image as input. With this content conditioning scheme, we create a direct feedback path during learning to let the content image influence how the style code is computed. It also helps reduce the direct influence of the style image to the extract style code.\r
\r
The bottom part of the Figure details architecture. First, the content image is fed into an encoder $E\\_{S, C}$ to compute a spatial feature map. This content feature map is then mean-pooled and mapped to a vector $\\zeta\\_{c} .$ Similarly, the style image is fed into encoder $E\\_{S, S}$ to compute a spatial feature map. The style feature map is then mean-pooled and concatenated with an input-independent bias vector: the constant style bias (CSB). Note that while the regular bias in deep networks is added to the activations, in CSB, the bias is concatenated with the activations. The CSB provides a fixed input to the style encoder, which helps compute a style code that is less sensitive to the variations in the style image.\r
\r
The concatenation of the style vector and the CSB is mapped to a vector $\\zeta\\_{s}$ via a fully connected layer. We then perform an element-wise product operation to $\\zeta\\_{c}$ and $\\zeta\\_{s}$, which is the final style code. The style code is then mapped to produce the [AdaIN](https://paperswithcode.com/method/adaptive-instance-normalization) parameters for generating the translation. Through this element-wise product operation, the resulting style code is heavily influenced by the content image. One way to look at this mechanism is that it produces a customized style code for the input content image.\r
\r
The COCO is used as a drop-in replacement for the style encoder in FUNIT. Let $\\phi$ denote the COCO mapping. The translation output is then computed via\r
\r
$$\r
z\\_{c}=E\\_{c}\\left(x_{c}\\right), z_{s}=\\phi\\left(E\\_{s, s}\\left(x_{s}\\right), E\\_{s, c}\\left(x\\_{c}\\right)\\right), \\overline{\\mathbf{x}}=F\\left(z\\_{c}, z\\_{s}\\right)\r
$$\r
\r
The style code extracted by the COCO is more robust to variations in the style image. Note that we set $E\\_{S, C} \\equiv E\\_{C}$ to keep the number of parameters in our model similar to that in FUNIT.""" ;
    skos:prefLabel "Content-Conditioned Style Encoder" .

:Content-basedAttention a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1410.5401v2> ;
    rdfs:seeAlso <https://github.com/loudinthecloud/pytorch-ntm/blob/3c64937cb075e737528ff7ecf02b2c1c6d8347a1/ntm/memory.py#L82> ;
    skos:definition """**Content-based attention** is an attention mechanism based on cosine similarity:\r
\r
$$f_{att}\\left(\\textbf{h}_{i}, \\textbf{s}\\_{j}\\right) = \\cos\\left[\\textbf{h}\\_{i};\\textbf{s}\\_{j}\\right] $$\r
\r
It was utilised in [Neural Turing Machines](https://paperswithcode.com/method/neural-turing-machine) as part of the Addressing Mechanism.\r
\r
We produce a normalized attention weighting by taking a [softmax](https://paperswithcode.com/method/softmax) over these attention alignment scores.""" ;
    skos:prefLabel "Content-based Attention" .

:ContextEnhancementModule a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1903.11752v3> ;
    rdfs:seeAlso <https://github.com/ouyanghuiyu/Thundernet_Pytorch/blob/ab66b733a39c9d1c60b5373f84f861d9627d8c20/lib/model/faster_rcnn/modules.py#L21> ;
    skos:definition """**Context Enhancement Module (CEM)** is a feature extraction module used in object detection (specifically, [ThunderNet](https://paperswithcode.com/method/thundernet)) which aims to  to enlarge the receptive field. The key idea of CEM is to aggregate multi-scale local context information and global context information to generate more discriminative features. In CEM, the feature maps from three scales are merged: $C\\_{4}$, $C\\_{5}$ and $C\\_{glb}$. $C\\_{glb}$ is the global context feature vector by applying a [global average pooling](https://paperswithcode.com/method/global-average-pooling) on $C\\_{5}$. We then apply a 1 × 1 [convolution](https://paperswithcode.com/method/convolution) on each feature map to squeeze the number of channels to $\\alpha \\times p \\times p = 245$.\r
\r
Afterwards, $C\\_{5}$ is upsampled by 2× and $C\\_{glb}$ is broadcast so that the spatial dimensions of the three feature maps are\r
equal. At last, the three generated feature maps are aggregated. By leveraging both local and global context, CEM effectively enlarges the receptive field and refines the representation ability of the thin feature map. Compared with prior [FPN](https://paperswithcode.com/method/fpn) structures, CEM involves only two 1×1 convolutions and a fc layer.""" ;
    skos:prefLabel "Context Enhancement Module" .

:ContextualResidualAggregation a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2005.09704v1> ;
    skos:definition "**Contextual Residual Aggregation**, or **CRA**, is a module for image inpainting. It can produce high-frequency residuals for missing contents by weighted aggregating residuals from contextual patches, thus only requiring a low-resolution prediction from the network. Specifically, it involves a neural network to predict a low-resolution inpainted result and up-sample it to yield a large blurry image. Then we produce the high-frequency residuals for in-hole patches by aggregating weighted high-frequency residuals from contextual patches. Finally, we add the aggregated residuals to the large blurry image to obtain a sharp result." ;
    skos:prefLabel "Contextual Residual Aggregation" .

:ContextualizedTopicModels a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2004.07737v2> ;
    rdfs:seeAlso <https://github.com/MilaNLProc/contextualized-topic-models> ;
    skos:definition """Contextualized Topic Models are based on the Neural-ProdLDA variational autoencoding approach by Srivastava and Sutton (2017). \r
\r
This approach trains an encoding neural network to map pre-trained contextualized word embeddings (e.g., [BERT](https://paperswithcode.com/method/bert)) to latent representations. Those latent representations are sampled variationally from a Gaussian distribution $N(\\mu, \\sigma^2)$ and passed to a decoder network that has to reconstruct the document bag-of-word representation.""" ;
    skos:prefLabel "Contextualized Topic Models" .

:ContractiveAutoencoder a skos:Concept ;
    skos:definition "A **Contractive Autoencoder** is an autoencoder that adds a penalty term to the classical reconstruction cost function. This penalty term corresponds to the Frobenius norm of the Jacobian matrix of the encoder activations with respect to the input. This penalty term results in a localized space contraction which in turn yields robust features on the activation layer. The penalty helps to carve a representation that better captures the local directions of variation dictated by the data, corresponding to a lower-dimensional non-linear manifold, while being more invariant to the vast majority of directions orthogonal to the manifold." ;
    skos:prefLabel "Contractive Autoencoder" .

:ContrastiveLearning a skos:Concept ;
    skos:altLabel "None" ;
    skos:definition "" ;
    skos:prefLabel "Contrastive Learning" .

:ContrastiveMultiviewCoding a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1906.05849v5> ;
    rdfs:seeAlso <https://github.com/HobbitLong/CMC> ;
    skos:definition "**Contrastive Multiview Coding (CMC)** is a self-supervised learning approach, based on [CPC](https://paperswithcode.com/method/contrastive-predictive-coding), that  learns representations that capture information shared between multiple sensory views. The core idea is to set an anchor view and the sample positive and negative data points from the other view and maximise agreement between positive pairs in learning from two views. Contrastive learning is used to build the embedding." ;
    skos:prefLabel "Contrastive Multiview Coding" .

:ContrastivePredictiveCoding a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1807.03748v2> ;
    rdfs:seeAlso <https://github.com/jefflai108/Contrastive-Predictive-Coding-PyTorch> ;
    skos:definition """**Contrastive Predictive Coding (CPC)** learns self-supervised representations by predicting the future in latent space by using powerful autoregressive models. The model uses a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful\r
to predict future samples.\r
\r
First, a non-linear encoder $g\\_{enc}$ maps the input sequence of observations $x\\_{t}$ to a sequence of latent representations $z\\_{t} = g\\_{enc}\\left(x\\_{t}\\right)$, potentially with a lower temporal resolution. Next, an autoregressive model $g\\_{ar}$ summarizes all $z\\leq{t}$ in the latent space and produces a context latent representation $c\\_{t} = g\\_{ar}\\left(z\\leq{t}\\right)$.\r
\r
A density ratio is modelled which preserves the mutual information between $x\\_{t+k}$ and $c\\_{t}$ as follows:\r
\r
$$ f\\_{k}\\left(x\\_{t+k}, c\\_{t}\\right) \\propto \\frac{p\\left(x\\_{t+k}|c\\_{t}\\right)}{p\\left(x\\_{t+k}\\right)} $$\r
\r
where $\\propto$ stands for ’proportional to’ (i.e. up to a multiplicative constant). Note that the density ratio $f$ can be unnormalized (does not have to integrate to 1). The authors use a simple log-bilinear model:\r
\r
$$ f\\_{k}\\left(x\\_{t+k}, c\\_{t}\\right) = \\exp\\left(z^{T}\\_{t+k}W\\_{k}c\\_{t}\\right) $$\r
\r
Any type of autoencoder and autoregressive can be used. An example the authors opt for is strided convolutional layers with residual blocks and GRUs.\r
\r
The autoencoder and autoregressive models are trained to minimize an [InfoNCE](https://paperswithcode.com/method/infonce) loss (see components).""" ;
    skos:prefLabel "Contrastive Predictive Coding" .

:ControlVAE a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2011.01754v1> ;
    skos:definition "**ControlVAE** is a [variational autoencoder](https://paperswithcode.com/method/vae) (VAE) framework that combines the automatic control theory with the basic VAE to stabilize the KL-divergence of VAE models to a specified value. It leverages a non-linear PI controller, a variant of the proportional-integral-derivative (PID) control, to dynamically tune the weight of the KL-divergence term in the evidence lower bound (ELBO) using the output KL-divergence as feedback. This allows for control of the KL-divergence to a desired value (set point), which is effective in avoiding posterior collapse and learning disentangled representations." ;
    skos:prefLabel "ControlVAE" .

:ConvBERT a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2008.02496v3> ;
    skos:definition "**ConvBERT** is a modification on the [BERT](https://paperswithcode.com/method/bert) architecture which uses a [span-based dynamic convolution](https://paperswithcode.com/method/span-based-dynamic-convolution) to replace self-attention heads to directly model local dependencies. Specifically a new [mixed attention module](https://paperswithcode.com/method/mixed-attention-block) replaces the [self-attention modules](https://paperswithcode.com/method/scaled) in BERT, which leverages the advantages of [convolution](https://paperswithcode.com/method/convolution) to better capture local dependency. Additionally, a new span-based dynamic convolution operation is used to utilize multiple input tokens to dynamically generate the convolution kernel. Lastly, ConvBERT also incorporates some new model designs including the bottleneck attention and grouped linear operator for the feed-forward module (reducing the number of parameters)." ;
    skos:prefLabel "ConvBERT" .

:ConvLSTM a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1506.04214v2> ;
    skos:definition """**ConvLSTM** is a type of recurrent neural network for spatio-temporal prediction that has convolutional structures in both the input-to-state and state-to-state transitions. The ConvLSTM determines the future state of a certain cell in the grid by the inputs and past states of its local neighbors. This can easily be achieved by using a [convolution](https://paperswithcode.com/method/convolution) operator in the state-to-state and input-to-state transitions (see Figure). The key equations of ConvLSTM are shown  below, where $∗$ denotes the convolution operator and $\\odot$ the Hadamard product:\r
\r
$$ i\\_{t} = \\sigma\\left(W\\_{xi} ∗ X\\_{t} + W\\_{hi} ∗ H\\_{t−1} + W\\_{ci} \\odot \\mathcal{C}\\_{t−1} + b\\_{i}\\right) $$\r
\r
$$ f\\_{t} = \\sigma\\left(W\\_{xf} ∗ X\\_{t} + W\\_{hf} ∗ H\\_{t−1} + W\\_{cf} \\odot \\mathcal{C}\\_{t−1} + b\\_{f}\\right) $$\r
\r
$$ \\mathcal{C}\\_{t} = f\\_{t} \\odot \\mathcal{C}\\_{t−1} + i\\_{t} \\odot \\text{tanh}\\left(W\\_{xc} ∗ X\\_{t} + W\\_{hc} ∗ \\mathcal{H}\\_{t−1} + b\\_{c}\\right) $$\r
\r
$$ o\\_{t} = \\sigma\\left(W\\_{xo} ∗ X\\_{t} + W\\_{ho} ∗ \\mathcal{H}\\_{t−1} + W\\_{co} \\odot \\mathcal{C}\\_{t} + b\\_{o}\\right) $$\r
\r
$$ \\mathcal{H}\\_{t} = o\\_{t} \\odot \\text{tanh}\\left(C\\_{t}\\right) $$\r
\r
If we view the states as the hidden representations of moving objects, a ConvLSTM with a larger transitional kernel should be able to capture faster motions while one with a smaller kernel can capture slower motions. \r
\r
To ensure that the states have the same number of rows and same number of columns as the inputs, padding is needed before applying the convolution operation. Here, padding of the hidden states on the boundary points can be viewed as using the state of the outside world for calculation. Usually, before the first input comes, we initialize all the states of the [LSTM](https://paperswithcode.com/method/lstm) to zero which corresponds to "total ignorance" of the future.""" ;
    skos:prefLabel "ConvLSTM" .

:ConvMLP a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2109.04454v2> ;
    skos:definition "**ConvMLP** is a hierarchical convolutional MLP for visual recognition, which consists of a stage-wise, co-design of [convolution](https://paperswithcode.com/method/convolution) layers, and MLPs. The Conv Stage consists of $C$ convolutional blocks with $1\\times 1$ and $3\\times 3$ kernel sizes. It is repeated $M$ times before a down convolution is utilized to express a level $L$. The MLP-Conv Stage consists of Channelwise MLPs, with skip layers, and a [depthwise convolution](https://paperswithcode.com/method/depthwise-convolution). This is repeated $M$ times before a down convolution is utilized to express a level $\\mathcal{L}$." ;
    skos:prefLabel "ConvMLP" .

:ConvNeXt a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2201.03545v2> ;
    skos:definition "" ;
    skos:prefLabel "ConvNeXt" .

:ConvTasNet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1809.07454v3> ;
    rdfs:seeAlso <https://github.com/asteroid-team/asteroid/blob/master/asteroid/models/conv_tasnet.py> ;
    skos:altLabel "Convolutional time-domain audio separation network" ;
    skos:definition "Combines learned time-frequency representation with a masker architecture based on 1D [dilated convolution](https://paperswithcode.com/method/dilated-convolution)." ;
    skos:prefLabel "ConvTasNet" .

:Convolution a skos:Concept ;
    skos:definition """A **convolution** is a type of matrix operation, consisting of a kernel, a small matrix of weights, that slides over input data performing element-wise multiplication with the part of the input it is on, then summing the results into an output.\r
\r
Intuitively, a convolution allows for weight sharing - reducing the number of effective parameters - and image translation (allowing for the same feature to be detected in different parts of the input space).\r
\r
Image Source: [https://arxiv.org/pdf/1603.07285.pdf](https://arxiv.org/pdf/1603.07285.pdf)""" ;
    skos:prefLabel "Convolution" .

:CoordConv a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1807.03247v2> ;
    rdfs:seeAlso <https://github.com/uber-research/CoordConv/blob/27fab8b86efac87c262c7c596a0c384b83c9d806/CoordConv.py#L87> ;
    skos:definition """A **CoordConv** layer is a simple extension to the standard convolutional layer. It has the same functional signature as a convolutional layer, but accomplishes the mapping by first concatenating extra channels to the incoming representation. These channels contain hard-coded coordinates, the most basic version of which is one channel for the $i$ coordinate and one for the $j$ coordinate.\r
\r
The CoordConv layer keeps the properties of few parameters and efficient computation from convolutions, but allows the network to learn to keep or to discard translation invariance as is needed for the task being learned. This is useful for coordinate transform based tasks where regular convolutions can fail.""" ;
    skos:prefLabel "CoordConv" .

:Coordinateattention a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2103.02907v1> ;
    skos:definition """Hou et al. proposed coordinate attention,\r
a novel attention mechanism which\r
embeds positional information into channel attention,\r
so that the network can focus on large important regions \r
at little computational cost.\r
\r
The coordinate attention mechanism has two consecutive steps, coordinate information embedding and coordinate attention generation. First, two spatial extents of pooling kernels encode each channel horizontally  and  vertically. In the second step, a shared $1\\times 1$ convolutional transformation function is applied to the concatenated outputs of the two pooling layers. Then coordinate attention splits the resulting tensor into two separate tensors to yield attention vectors with the same number of channels for horizontal and vertical coordinates of the  input $X$ along. This can be written as \r
\\begin{align}\r
    z^h &= \\text{GAP}^h(X) \r
\\end{align}\r
\\begin{align}\r
    z^w &= \\text{GAP}^w(X)\r
\\end{align}\r
\\begin{align}\r
    f &= \\delta(\\text{BN}(\\text{Conv}_1^{1\\times 1}([z^h;z^w])))\r
\\end{align}\r
\\begin{align}\r
    f^h, f^w &= \\text{Split}(f)\r
\\end{align}\r
\\begin{align}\r
    s^h &= \\sigma(\\text{Conv}_h^{1\\times 1}(f^h))\r
\\end{align}\r
\\begin{align}\r
    s^w &= \\sigma(\\text{Conv}_w^{1\\times 1}(f^w))\r
\\end{align}\r
\\begin{align}\r
    Y &= X s^h  s^w\r
\\end{align}\r
where $\\text{GAP}^h$ and $\\text{GAP}^w$ denote pooling functions for vertical and horizontal coordinates, and $s^h \\in \\mathbb{R}^{C\\times 1\\times W}$ and $s^w \\in \\mathbb{R}^{C\\times H\\times 1}$ represent corresponding attention weights. \r
\r
Using coordinate attention, the network can accurately obtain the position of a targeted object.\r
This approach has a larger receptive field than BAM and CBAM.\r
Like an SE block, it also models cross-channel relationships, effectively enhancing the expressive power of the learned features.\r
Due to its lightweight design and flexibility, \r
it can be easily used in classical building blocks of mobile networks.""" ;
    skos:prefLabel "Coordinate attention" .

:Copy-Paste a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2012.07177v2> ;
    skos:altLabel "simple Copy-Paste" ;
    skos:definition "" ;
    skos:prefLabel "Copy-Paste" .

:Coresets a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1708.00489v4> ;
    skos:definition "" ;
    skos:prefLabel "Coresets" .

:CornerNet a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1808.01244v2> ;
    rdfs:seeAlso <https://github.com/princeton-vl/CornerNet/blob/e5c39a31a8abef5841976c8eab18da86d6ee5f9a/models/CornerNet.py#L67> ;
    skos:definition "**CornerNet** is an object detection model that detects an object bounding box as a pair of keypoints, the top-left corner and the bottom-right corner, using a single [convolution](https://paperswithcode.com/method/convolution) neural network. By detecting objects as paired keypoints, we eliminate the need for designing a set of anchor boxes commonly used in prior single-stage detectors. It also utilises [corner pooling](https://paperswithcode.com/method/corner-pooling), a new type of pooling layer than helps the network better localize corners." ;
    skos:prefLabel "CornerNet" .

:CornerNet-Saccade a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1904.08900v2> ;
    rdfs:seeAlso <https://github.com/princeton-vl/CornerNet-Lite> ;
    skos:definition "**CornerNet-Saccade** is an extension of [CornerNet](https://paperswithcode.com/method/cornernet) with an attention mechanism similar to saccades in human vision. It starts with a downsized full image and generates an attention map, which is then zoomed in on and processed further by the model. This differs from the original CornerNet in that it is applied fully convolutionally across multiple scales." ;
    skos:prefLabel "CornerNet-Saccade" .

:CornerNet-Squeeze a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1904.08900v2> ;
    rdfs:seeAlso <https://github.com/princeton-vl/CornerNet-Lite> ;
    skos:definition "**CornerNet-Squeeze** is an object detector that extends [CornerNet](https://paperswithcode.com/method/cornernet) with a new compact hourglass architecture that makes use of fire modules with depthwise separable convolutions." ;
    skos:prefLabel "CornerNet-Squeeze" .

:CornerNet-SqueezeHourglass a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1904.08900v2> ;
    rdfs:seeAlso <https://github.com/princeton-vl/CornerNet-Lite/blob/6a54505d830a9d6afe26e99f0864b5d06d0bbbaf/core/models/CornerNet_Squeeze.py#L53> ;
    skos:definition "**CornerNet-Squeeze Hourglass** is a convolutional neural network and object detection backbone used in the [CornerNet-Squeeze](https://paperswithcode.com/method/cornernet-squeeze) object detector. It uses a modified [hourglass module](https://paperswithcode.com/method/hourglass-module) that makes use of a [fire module](https://paperswithcode.com/method/fire-module): containing 1x1 convolutions and depthwise convolutions." ;
    skos:prefLabel "CornerNet-Squeeze Hourglass" .

:CornerNet-SqueezeHourglassModule a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1904.08900v2> ;
    rdfs:seeAlso <https://github.com/princeton-vl/CornerNet-Lite/blob/6a54505d830a9d6afe26e99f0864b5d06d0bbbaf/core/models/CornerNet_Squeeze.py#L10> ;
    skos:definition "**CornerNet-Squeeze Hourglass Module** is an image model block used in [CornerNet](https://paperswithcode.com/method/cornernet)-Lite that is based on an [hourglass module](https://paperswithcode.com/method/hourglass-module), but uses modified fire modules instead of residual blocks. Other than replacing the residual blocks, further modifications include: reducing the maximum feature map resolution of the hourglass modules by adding one more downsampling layer before the hourglass modules, removing one downsampling layer in each hourglass module, replacing the 3 × 3 filters with 1 x 1 filters in the prediction modules of CornerNet, and finally replacing the nearest neighbor upsampling in the hourglass network with transpose [convolution](https://paperswithcode.com/method/convolution) with a 4 × 4 kernel." ;
    skos:prefLabel "CornerNet-Squeeze Hourglass Module" .

:CornerPooling a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1808.01244v2> ;
    rdfs:seeAlso <https://github.com/princeton-vl/CornerNet/blob/0017821eb0e127918dfd29a1a5976558babb582d/models/CornerNet.py#L7> ;
    skos:definition "**Corner Pooling** is a pooling technique for object detection that seeks to better localize corners by encoding explicit prior knowledge. Suppose we want to determine if a pixel at location $\\left(i, j\\right)$ is a top-left corner. Let $f\\_{t}$ and $f\\_{l}$ be the feature maps that are the inputs to the top-left corner pooling layer, and let $f\\_{t\\_{ij}}$ and $f\\_{l\\_{ij}}$ be the vectors at location $\\left(i, j\\right)$ in $f\\_{t}$ and $f\\_{l}$ respectively. With $H \\times W$ feature maps, the corner pooling layer first max-pools all feature vectors between $\\left(i, j\\right)$ and $\\left(i, H\\right)$ in $f\\_{t}$ to a feature vector $t\\_{ij}$ , and max-pools all feature vectors between $\\left(i, j\\right)$ and $\\left(W, j\\right)$ in $f\\_{l}$ to a feature vector $l\\_{ij}$. Finally, it adds $t\\_{ij}$ and $l\\_{ij}$ together." ;
    skos:prefLabel "Corner Pooling" .

:CosLU a skos:Concept ;
    dcterms:source <https://doi.org/10.20944/preprints202301.0463.v1> ;
    skos:altLabel "Cosine Linear Unit" ;
    skos:definition """The **Cosine Linear Unit**, or **CosLU**, is a type of activation function that has trainable parameters and uses the cosine function.\r
\r
$$CosLU(x) = (x + \\alpha \\cos(\\beta x))\\sigma(x)$$""" ;
    skos:prefLabel "CosLU" .

:CosineAnnealing a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1608.03983v5> ;
    skos:definition """**Cosine Annealing** is a type of learning rate schedule that has the effect of starting with a large learning rate that is relatively rapidly decreased to a minimum value before being increased rapidly again. The resetting of the learning rate acts like a simulated restart of the learning process and the re-use of good weights as the starting point of the restart is referred to as a "warm restart" in contrast to a "cold restart" where a new set of small random numbers may be used as a starting point.\r
\r
$$\\eta\\_{t} = \\eta\\_{min}^{i} + \\frac{1}{2}\\left(\\eta\\_{max}^{i}-\\eta\\_{min}^{i}\\right)\\left(1+\\cos\\left(\\frac{T\\_{cur}}{T\\_{i}}\\pi\\right)\\right)\r
$$\r
\r
Where where $\\eta\\_{min}^{i}$ and $ \\eta\\_{max}^{i}$ are ranges for the learning rate, and $T\\_{cur}$ account for how many epochs have been performed since the last restart.\r
\r
Text Source: [Jason Brownlee](https://machinelearningmastery.com/snapshot-ensemble-deep-learning-neural-network/)\r
\r
Image Source: [Gao Huang](https://www.researchgate.net/figure/Training-loss-of-100-layer-DenseNet-on-CIFAR10-using-standard-learning-rate-blue-and-M_fig2_315765130)""" ;
    skos:prefLabel "Cosine Annealing" .

:CosineNormalization a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1702.05870v5> ;
    rdfs:seeAlso <https://github.com/iwyoo/tf_conv_cosnorm/blob/e76d987110e4af42ad75154bfbf994c5cc935794/conv_cosnorm.py#L13> ;
    skos:definition """Multi-layer neural networks traditionally use  dot products between the output vector of previous layer and the incoming weight vector as the input to activation function. The result of dot product is unbounded. To bound dot product and decrease the variance, **Cosine Normalization** uses cosine similarity or centered cosine similarity (Pearson Correlation Coefficient) instead of dot products in neural networks. \r
\r
Using cosine normalization, the output of a hidden unit is computed by:\r
\r
$$o = f(net_{norm})= f(\\cos \\theta) = f(\\frac{\\vec{w} \\cdot \\vec{x}} {\\left|\\vec{w}\\right|  \\left|\\vec{x}\\right|})$$\r
\r
where $net_{norm}$ is the normalized pre-activation,  $\\vec{w}$ is the incoming weight vector and $\\vec{x}$ is the input vector, ($\\cdot$) indicates dot product, $f$ is nonlinear activation function. Cosine normalization bounds the pre-activation between -1 and 1.""" ;
    skos:prefLabel "Cosine Normalization" .

:CosinePowerAnnealing a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1903.09900v1> ;
    rdfs:seeAlso <https://raw.githubusercontent.com/ahundt/sharpDARTS/master/cnn/cosine_power_annealing.py> ;
    skos:definition "Interpolation between [exponential decay](https://paperswithcode.com/method/exponential-decay) and [cosine annealing](https://paperswithcode.com/method/cosine-annealing)." ;
    skos:prefLabel "Cosine Power Annealing" .

:Counterfactuals a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1711.00399v3> ;
    skos:altLabel "Counterfactuals Explanations" ;
    skos:definition "" ;
    skos:prefLabel "Counterfactuals" .

:Cross-AttentionModule a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2103.14899v2> ;
    rdfs:seeAlso <https://github.com/lucidrains/vit-pytorch/blob/22da26fa4b7d98c2a936b48c1b1dcf83c404dfb8/vit_pytorch/cross_vit.py#L115> ;
    skos:definition "The **Cross-Attention** module is an attention module used in [CrossViT](https://paperswithcode.com/method/crossvit) for fusion of multi-scale features. The CLS token of the large branch (circle) serves as a query token to interact with the patch tokens from the small branch through attention. $f\\left(·\\right)$ and $g\\left(·\\right)$ are projections to align dimensions. The small branch follows the same procedure but swaps CLS and patch tokens from another branch." ;
    skos:prefLabel "Cross-Attention Module" .

:Cross-CovarianceAttention a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2106.09681v2> ;
    skos:definition """**Cross-Covariance Attention**, or **XCA**, is an [attention mechanism](https://paperswithcode.com/methods/category/attention-mechanisms-1) which operates along the feature dimension instead of the token dimension as in [conventional transformers](https://paperswithcode.com/methods/category/transformers).\r
\r
Using the definitions of queries, keys and values from conventional attention, the cross-covariance attention function is defined as:\r
\r
$$\r
\\text { XC-Attention }(Q, K, V)=V \\mathcal{A}_{\\mathrm{XC}}(K, Q), \\quad \\mathcal{A}\\_{\\mathrm{XC}}(K, Q)=\\operatorname{Softmax}\\left(\\hat{K}^{\\top} \\hat{Q} / \\tau\\right)\r
$$\r
\r
where each output token embedding is a convex combination of the $d\\_{v}$ features of its corresponding token embedding in $V$. The attention weights $\\mathcal{A}$ are computed based on the cross-covariance matrix.""" ;
    skos:prefLabel "Cross-Covariance Attention" .

:Cross-ScaleNon-LocalAttention a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2006.01424v1> ;
    skos:definition "**Cross-Scale Non-Local Attention**, or **CS-NL**,  is a non-local attention module for image super-resolution deep networks. It learns to mine long-range dependencies between LR features to larger-scale HR patches within the same feature map. Specifically, suppose we are conducting an s-scale super-resolution with the module, given a feature map $X$ of spatial size $(W, H)$, we first bilinearly downsample it to $Y$ with scale $s$, and match the $p\\times p$ patches in $X$ with the downsampled $p \\times p$ candidates in $Y$ to obtain the [softmax](https://paperswithcode.com/method/softmax) matching score. Finally, we conduct deconvolution.on the score by weighted adding the patches of size $\\left(sp, sp\\right)$ extracted from $X$. The obtained $Z$ of size $(sW, sH)$ will be $s$ times super-resolved than $X$." ;
    skos:prefLabel "Cross-Scale Non-Local Attention" .

:Cross-ViewTraining a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1809.08370v1> ;
    skos:definition """**Cross View Training**, or **CVT**, is a semi-supervised algorithm for training distributed word representations that makes use of unlabelled and labelled examples. \r
\r
CVT adds $k$ auxiliary prediction modules to the model, a Bi-[LSTM](https://paperswithcode.com/method/lstm) encoder, which are used when learning on unlabeled examples. A prediction module is usually a small neural network (e.g., a hidden layer followed by a [softmax](https://paperswithcode.com/method/softmax) layer). Each one takes as input an intermediate representation $h^j(x_i)$ produced by the model (e.g., the outputs of one of the LSTMs in a Bi-LSTM model). It outputs a distribution over labels $p\\_{j}^{\\theta}\\left(y\\mid{x\\_{i}}\\right)$.\r
\r
Each $h^j$ is chosen such that it only uses a part of the input $x_i$; the particular choice can depend on the task and model architecture. The auxiliary prediction modules are only used during training; the test-time prediction come from the primary prediction module that produces $p_\\theta$.""" ;
    skos:prefLabel "Cross-View Training" .

:Cross-encoderReranking a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2102.06328v2> ;
    skos:definition "Cross-encoder Reranking" ;
    skos:prefLabel "Cross-encoder Reranking" .

:Cross-resolutionfeatures a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2004.12186v2> ;
    skos:definition "" ;
    skos:prefLabel "Cross-resolution features" .

:CrossTransformers a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2007.11498v5> ;
    skos:definition "CrossTransformers is a Transformer-based neural network architecture which can take a small number of labeled images and an unlabeled query, find coarse spatial correspondence between the query and the labeled images, and then infer class membership by computing distances between spatially-corresponding features." ;
    skos:prefLabel "CrossTransformers" .

:CrossViT a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2103.14899v2> ;
    rdfs:seeAlso <https://github.com/lucidrains/vit-pytorch/blob/22da26fa4b7d98c2a936b48c1b1dcf83c404dfb8/vit_pytorch/cross_vit.py#L205> ;
    skos:definition """**CrossViT** is a type of [vision transformer](https://paperswithcode.com/method/vision-transformer) that uses a dual-branch architecture to extract multi-scale feature representations for image classification. The architecture combines image patches (i.e. tokens in a [transformer](https://paperswithcode.com/method/transformer)) of different sizes to produce stronger visual features for image classification. It processes small and large patch tokens with two separate branches of different computational complexities and these tokens are fused together multiple times to complement each other.\r
\r
Fusion is achieved by an efficient [cross-attention module](https://paperswithcode.com/method/cross-attention-module), in which each transformer branch creates a non-patch token as an agent to exchange information with the other branch by attention. This allows for linear-time generation of the attention map in fusion instead of quadratic time otherwise.""" ;
    skos:prefLabel "CrossViT" .

:Crossbow a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1901.02244v1> ;
    skos:definition "**Crossbow** is a single-server multi-GPU system for training deep learning models that enables users to freely choose their preferred batch size—however small—while scaling to multiple GPUs. Crossbow uses many parallel model replicas and avoids reduced statistical efficiency through a new synchronous training method. [SMA](https://paperswithcode.com/method/slime-mould-algorithm-sma), a synchronous variant of model averaging, is used in which replicas independently explore the solution space with gradient descent, but adjust their search synchronously based on the trajectory of a globally-consistent average model." ;
    skos:prefLabel "Crossbow" .

:CuBERT a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2001.00059v3> ;
    skos:definition "**CuBERT**, or **Code Understanding BERT**, is a [BERT](https://paperswithcode.com/method/bert) based model for code understanding. In order to achieve this, the authors curate a massive corpus of Python programs collected from GitHub. GitHub projects are known to contain a large amount of duplicate code. To avoid biasing the model to such duplicated code, authors perform deduplication using the method of [Allamanis (2018)](https://arxiv.org/abs/1812.06469). The resulting corpus has 7.4 million files with a total of 9.3 billion tokens (16 million unique)." ;
    skos:prefLabel "CuBERT" .

:CubeRE a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2211.10018v1> ;
    skos:definition "Our model known as CubeRE first encodes each input sentence using a language model encoder to obtain the contextualized sequence representation. We then capture the interaction between each possible head and tail entity as a pair representation for predicting the entity-relation label scores. To reduce the computational cost, each sentence is pruned to retain only words that have higher entity scores. Finally, we capture the interaction between each possible relation triplet and qualifier to predict the qualifier label scores and decode the outputs." ;
    skos:prefLabel "CubeRE" .

:CurricularFace a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2004.00288v1> ;
    skos:definition "**CurricularFace**, or **Adaptive Curriculum Learning**, is a method for face recognition that embeds the idea of curriculum learning into the loss function to achieve a new training scheme. This training scheme mainly addresses easy samples in the early training stage and hard ones in the later stage. Specifically, CurricularFace adaptively adjusts the relative importance of easy and hard samples during different training stages." ;
    skos:prefLabel "CurricularFace" .

:CurvVAE a skos:Concept ;
    dcterms:source <https://ras.papercept.net/conferences/conferences/IROS22/program/IROS22_ContentListWeb_4.html> ;
    skos:altLabel "Curvature Regularized Variational Auto-Encoder" ;
    skos:definition "" ;
    skos:prefLabel "CurvVAE" .

:CutBlur a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2004.00448v2> ;
    skos:definition "**CutBlur** is a data augmentation method that is specifically designed for the low-level vision tasks. It cuts a low-resolution patch and pastes it to the corresponding high-resolution image region and vice versa. The key intuition of Cutblur is to enable a model to learn not only \"how\" but also \"where\" to super-resolve an image. By doing so, the model can understand \"how much\" instead of blindly learning to apply super-resolution to every given pixel." ;
    skos:prefLabel "CutBlur" .

:CutMix a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1905.04899v2> ;
    skos:definition "**CutMix** is an image data augmentation strategy. Instead of simply removing pixels as in [Cutout](https://paperswithcode.com/method/cutout), we replace the removed regions with a patch from another image. The ground truth labels are also mixed proportionally to the number of pixels of combined images. The added patches further enhance localization ability by requiring the model to identify the object from a partial view." ;
    skos:prefLabel "CutMix" .

:Cutout a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1708.04552v2> ;
    skos:definition """**Cutout** is an image augmentation and regularization technique that randomly masks out square regions of input during training. and can be used to improve the robustness and overall performance of convolutional neural networks. The main motivation for cutout comes from the problem of object occlusion, which is commonly encountered in many computer vision tasks, such as object recognition,\r
tracking, or human pose estimation. By generating new images which simulate occluded examples, we not only better prepare the model for encounters with occlusions in the real world, but the model also learns to take more of the image context into consideration when making decisions""" ;
    skos:prefLabel "Cutout" .

:CvT a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2103.15808v1> ;
    skos:altLabel "Convolutional Vision Transformer" ;
    skos:definition """The **Convolutional vision Transformer (CvT)** is an architecture which incorporates convolutions into the [Transformer](https://paperswithcode.com/method/transformer). The CvT design introduces convolutions to two core sections of the ViT architecture.\r
\r
First, the Transformers are partitioned into multiple stages that form a hierarchical structure of Transformers. The beginning of each stage consists of a convolutional token embedding that performs an overlapping [convolution](https://paperswithcode.com/method/convolution) operation with stride on a 2D-reshaped token map (i.e., reshaping flattened token sequences back to the spatial grid), followed by [layer normalization](https://paperswithcode.com/method/layer-normalization). This allows the model to not only capture local information, but also progressively decrease the sequence length while simultaneously increasing the dimension of token features across stages, achieving spatial downsampling while concurrently increasing the number of feature maps, as is performed in CNNs. \r
\r
Second, the linear projection prior to every self-attention block in the Transformer module is replaced with a proposed convolutional projection, which employs a s × s depth-wise separable convolution operation on an 2D-reshaped token map. This allows the model to further capture local spatial context and reduce semantic ambiguity in the attention mechanism. It also permits management of computational complexity, as the stride of convolution can be used to subsample the key and value matrices to improve efficiency by 4× or more, with minimal degradation of performance.""" ;
    skos:prefLabel "CvT" .

:Cycle-CenterNet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2109.02199v1> ;
    skos:definition "**Cycle-CenterNet** is a table structure parsing approach built on [CenterNet](https://paperswithcode.com/method/centernet) that uses a cycle-pairing module to simultaneously detect and group tabular cells into structured tables. It also utilizes a pairing loss which enables the grouping of discrete cells into the structured tables." ;
    skos:prefLabel "Cycle-CenterNet" .

:CycleConsistencyLoss a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1703.10593v7> ;
    rdfs:seeAlso <https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/f5834b3ed339ec268f40cf56928234eed8dfeb92/models/cycle_gan_model.py#L172> ;
    skos:definition """**Cycle Consistency Loss** is a type of loss used for generative adversarial networks that performs unpaired image-to-image translation. It was introduced with the [CycleGAN](https://paperswithcode.com/method/cyclegan) architecture. For two domains $X$ and $Y$, we want to learn a mapping $G : X \\rightarrow Y$ and $F: Y \\rightarrow X$. We want to enforce the intuition that these mappings should be reverses of each other and that both mappings should be bijections. Cycle Consistency Loss encourages $F\\left(G\\left(x\\right)\\right) \\approx x$ and $G\\left(F\\left(y\\right)\\right) \\approx y$.  It reduces the space of possible mapping functions by enforcing forward and backwards consistency:\r
\r
$$ \\mathcal{L}\\_{cyc}\\left(G, F\\right) = \\mathbb{E}\\_{x \\sim p\\_{data}\\left(x\\right)}\\left[||F\\left(G\\left(x\\right)\\right) - x||\\_{1}\\right] + \\mathbb{E}\\_{y \\sim p\\_{data}\\left(y\\right)}\\left[||G\\left(F\\left(y\\right)\\right) - y||\\_{1}\\right] $$""" ;
    skos:prefLabel "Cycle Consistency Loss" .

:CycleGAN a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1703.10593v7> ;
    rdfs:seeAlso <https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/9e6fff7b7d5215a38be3cac074ca7087041bea0d/models/cycle_gan_model.py#L8> ;
    skos:definition """**CycleGAN**, or **Cycle-Consistent GAN**, is a type of generative adversarial network for unpaired image-to-image translation. For two domains $X$ and $Y$, CycleGAN learns a mapping $G : X \\rightarrow Y$ and $F: Y \\rightarrow X$. The novelty lies in trying to enforce the intuition that these mappings should be reverses of each other and that both mappings should be bijections. This is achieved through a [cycle consistency loss](https://paperswithcode.com/method/cycle-consistency-loss) that encourages $F\\left(G\\left(x\\right)\\right) \\approx x$ and $G\\left(F\\left(y\\right)\\right) \\approx y$. Combining this loss with the adversarial losses on $X$ and $Y$ yields the full objective for unpaired image-to-image translation.\r
\r
For the mapping $G : X \\rightarrow Y$ and its discriminator $D\\_{Y}$ we have the objective:\r
\r
$$ \\mathcal{L}\\_{GAN}\\left(G, D\\_{Y}, X, Y\\right) =\\mathbb{E}\\_{y \\sim p\\_{data}\\left(y\\right)}\\left[\\log D\\_{Y}\\left(y\\right)\\right] + \\mathbb{E}\\_{x \\sim p\\_{data}\\left(x\\right)}\\left[log(1 − D\\_{Y}\\left(G\\left(x\\right)\\right)\\right] $$\r
\r
where $G$ tries to generate images $G\\left(x\\right)$ that look similar to images from domain $Y$, while $D\\_{Y}$ tries to discriminate between translated samples $G\\left(x\\right)$ and real samples $y$. A similar loss is postulated for the mapping $F: Y \\rightarrow X$ and its discriminator $D\\_{X}$.\r
\r
The Cycle Consistency Loss reduces the space of possible mapping functions by enforcing forward and backwards consistency:\r
\r
$$ \\mathcal{L}\\_{cyc}\\left(G, F\\right) = \\mathbb{E}\\_{x \\sim p\\_{data}\\left(x\\right)}\\left[||F\\left(G\\left(x\\right)\\right) - x||\\_{1}\\right] + \\mathbb{E}\\_{y \\sim p\\_{data}\\left(y\\right)}\\left[||G\\left(F\\left(y\\right)\\right) - y||\\_{1}\\right] $$\r
\r
The full objective is:\r
\r
$$ \\mathcal{L}\\_{GAN}\\left(G, F, D\\_{X}, D\\_{Y}\\right) = \\mathcal{L}\\_{GAN}\\left(G, D\\_{Y}, X, Y\\right) + \\mathcal{L}\\_{GAN}\\left(F, D\\_{X}, X, Y\\right) + \\lambda\\mathcal{L}\\_{cyc}\\left(G, F\\right) $$\r
\r
Where we aim to solve:\r
\r
$$ G^{\\*}, F^{\\*} = \\arg \\min\\_{G, F} \\max\\_{D\\_{X}, D\\_{Y}} \\mathcal{L}\\_{GAN}\\left(G, F, D\\_{X}, D\\_{Y}\\right) $$\r
\r
For the original architecture the authors use:\r
\r
-  two stride-2 convolutions, several residual blocks, and two fractionally strided convolutions with stride $\\frac{1}{2}$.\r
- [instance normalization](https://paperswithcode.com/method/instance-normalization)\r
- PatchGANs for the discriminator\r
- Least Square Loss for the [GAN](https://paperswithcode.com/method/gan) objectives.""" ;
    skos:prefLabel "CycleGAN" .

:CyclicalLearningRatePolicy a skos:Concept ;
    skos:definition """A **Cyclical Learning Rate Policy** combines a linear learning rate decay with warm restarts.\r
\r
Image: [ESPNetv2](https://paperswithcode.com/method/espnetv2)""" ;
    skos:prefLabel "Cyclical Learning Rate Policy" .

:D4PG a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1804.08617v1> ;
    skos:altLabel "Distributed Distributional DDPG" ;
    skos:definition "**D4PG**, or **Distributed Distributional DDPG**, is a policy gradient algorithm that extends upon the [DDPG](https://paperswithcode.com/method/ddpg). The improvements include a distributional updates to the DDPG algorithm, combined with the use of multiple distributed workers all writing into the same replay table. The biggest performance gain of other simpler changes was the use of $N$-step returns. The authors found that the use of [prioritized experience replay](https://paperswithcode.com/method/prioritized-experience-replay) was less crucial to the overall D4PG algorithm especially on harder problems." ;
    skos:prefLabel "D4PG" .

:DABMD a skos:Concept ;
    skos:altLabel "Distributed Any-Batch Mirror Descent" ;
    skos:definition "**Distributed Any-Batch Mirror Descent** (DABMD) is based on distributed Mirror Descent but uses a fixed per-round computing time to limit the waiting by fast nodes to receive information updates from slow nodes. DABMD is characterized by varying minibatch sizes across nodes. It is applicable to a broader range of problems compared with existing distributed online optimization methods such as those based on dual averaging, and it accommodates time-varying network topology." ;
    skos:prefLabel "DABMD" .

:DAC a skos:Concept ;
    dcterms:source <http://ecai2020.eu/papers/1237_paper.pdf> ;
    skos:altLabel "Dynamic Algorithm Configuration" ;
    skos:definition """Dynamic algorithm configuration (DAC) is capable of generalizing over prior optimization approaches, as well as handling optimization of hyperparameters that need to be adjusted over multiple time-steps.\r
\r
Image Source: [Biedenkapp et al.](http://ecai2020.eu/papers/1237_paper.pdf)""" ;
    skos:prefLabel "DAC" .

:DAEL a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2003.07325v3> ;
    skos:altLabel "Domain Adaptive Ensemble Learning" ;
    skos:definition "**Domain Adaptive Ensemble Learning**, or **DAEL**, is an architecture for domain adaptation. The model is composed of a CNN feature extractor shared across domains and multiple classifier heads each trained to specialize in a particular source domain. Each such classifier is an expert to its own domain and a non-expert to others. DAEL aims to learn these experts collaboratively so that when forming an ensemble, they can leverage complementary information from each other to be more effective for an unseen target domain. To this end, each source domain is used in turn as a pseudo-target-domain with its own expert providing supervisory signal to the ensemble of non-experts learned from the other sources. For unlabeled target data under the UDA setting where real expert does not exist, DAEL uses pseudo-label to supervise the ensemble learning." ;
    skos:prefLabel "DAEL" .

:DAFNe a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2109.06148v4> ;
    skos:definition "**DAFNe** is a dense one-stage anchor-free deep model for oriented object detection. It is a deep neural network that performs predictions on a dense grid over the input image, being architecturally simpler in design, as well as easier to optimize than its two-stage counterparts. Furthermore, it reduces the prediction complexity by refraining from employing bounding box anchors. This enables a tighter fit to oriented objects, leading to a better separation of bounding boxes especially in case of dense object distributions. Moreover, it introduces an orientation-aware generalization of the center-ness function to arbitrary quadrilaterals that takes into account the object's orientation and that, accordingly, accurately down-weights low-quality predictions" ;
    skos:prefLabel "DAFNe" .

:DAGNN a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2101.07965v3> ;
    skos:altLabel "Directed Acyclic Graph Neural Network" ;
    skos:definition "A GNN for dags, which injects their topological order as an inductive bias via asynchronous message passing." ;
    skos:prefLabel "DAGNN" .

:DALL·E2 a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2204.06125v1> ;
    skos:definition "**DALL·E 2** is a generative text-to-image model made up of two main components: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding." ;
    skos:prefLabel "DALL·E 2" .

:DAMO-YOLO a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2211.15444v4> ;
    skos:definition "" ;
    skos:prefLabel "DAMO-YOLO" .

:DANCE a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2002.07953v3> ;
    skos:altLabel "Domain Adaptative Neighborhood Clustering via Entropy Optimization" ;
    skos:definition "**Domain Adaptive Neighborhood Clustering via Entropy Optimization (DANCE)** is a self-supervised clustering method that harnesses the cluster structure of the target domain using self-supervision. This is done with a neighborhood clustering technique that self-supervises feature learning in the target. At the same time, useful source features and class boundaries are preserved and adapted with a partial domain alignment loss that the authors refer to as entropy separation loss. This loss allows the model to either match each target example with the source, or reject it as unknown." ;
    skos:prefLabel "DANCE" .

:DANet a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1809.02983v4> ;
    skos:altLabel "Dual Attention Network" ;
    skos:definition """In the field of scene segmentation,\r
encoder-decoder structures cannot make use of the global relationships \r
between objects, whereas RNN-based structures \r
heavily rely on the output of the long-term memorization.\r
To address the above problems, \r
Fu et al. proposed a novel framework, \r
 the dual attention network (DANet), \r
for natural scene image segmentation. \r
Unlike CBAM and BAM, it adopts a self-attention mechanism \r
instead of simply stacking convolutions to compute the spatial attention map,\r
which enables the network to capture global information directly. \r
\r
DANet uses in parallel a position attention module and a channel attention module to capture feature dependencies in spatial and channel domains. Given the input feature map $X$, convolution layers are applied first in the position attention module to obtain new feature maps. Then the position attention module selectively aggregates the features at each position using a weighted sum of features at all positions, where the weights are determined by feature similarity between corresponding pairs of positions. The channel attention module has a similar form except for dimensional reduction to model cross-channel relations. Finally the outputs from the two branches are fused to obtain final feature representations. For simplicity, we reshape the feature map $X$ to $C\\times (H \\times W)$ whereupon the overall process can be written as \r
\\begin{align}\r
    Q,\\quad K,\\quad V &= W_qX,\\quad W_kX,\\quad W_vX\r
\\end{align}\r
\\begin{align}\r
    Y^\\text{pos} &=  X+ V\\text{Softmax}(Q^TK)\r
\\end{align}\r
\\begin{align}\r
    Y^\\text{chn} &=  X+ \\text{Softmax}(XX^T)X \r
\\end{align}\r
\\begin{align}\r
    Y &= Y^\\text{pos} + Y^\\text{chn}\r
\\end{align}\r
where $W_q$, $W_k$, $W_v \\in \\mathbb{R}^{C\\times C}$ are used to generate new feature maps.   \r
\r
The position attention module enables\r
DANet to capture long-range contextual information\r
and adaptively integrate similar features at any scale\r
from a global viewpoint,\r
while the channel attention module is responsible for \r
enhancing useful channels \r
as well as suppressing noise. \r
Taking spatial and channel \r
relationships into consideration explicitly\r
improves the feature representation for scene segmentation.\r
However, it is computationally costly, especially for large input feature maps.""" ;
    skos:prefLabel "DANet" .

:DAPO a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2009.04984v2> ;
    skos:altLabel "Dialogue-Adaptive Pre-training Objective" ;
    skos:definition "**Dialogue-Adaptive Pre-training Objective (DAPO)** is a pre-training objective for dialogue adaptation, which is designed to measure qualities of dialogues from multiple important aspects, like Readability, Consistency and Fluency which have already been focused on by general LM pre-training objectives, and those also significant for assessing dialogues but ignored by general LM pre-training objectives, like Diversity and Specificity." ;
    skos:prefLabel "DAPO" .

:DARTS a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1806.09055v2> ;
    skos:altLabel "Differentiable Architecture Search" ;
    skos:definition "**Differentiable Architecture Search** (**DART**) is a method for efficient architecture search. The search space is made continuous so that the architecture can be optimized with respect to its validation set performance through gradient descent." ;
    skos:prefLabel "DARTS" .

:DARTSMax-W a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1903.09900v1> ;
    rdfs:seeAlso <https://raw.githubusercontent.com/ahundt/sharpDARTS/master/cnn/model_search.py> ;
    skos:altLabel "Differentiable Architecture Search Max-W" ;
    skos:definition """Like [DARTS](https://paperswithcode.com/method/darts), except subtract the max weight gradients.\r
\r
Max-W Weighting:\r
\\begin{equation}\r
output_i = (1 - max(w) + w_i) * op_i(input_i)\r
\\label{eqn:max_w}\r
\\end{equation}""" ;
    skos:prefLabel "DARTS Max-W" .

:DASPP a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1912.06683v1> ;
    skos:altLabel "Deeper Atrous Spatial Pyramid Pooling" ;
    skos:definition "DASPP is a deeper version of the [ASPP](https://paperswithcode.com/method/aspp) module (the latter from [DeepLabv3](https://paperswithcode.com/method/deeplabv3)) that adds standard 3 × 3 [convolution](https://paperswithcode.com/method/convolution) after 3 × 3 dilated convolutions to refine the features and also fusing the input and the output of the DASPP module via short [residual connection](https://paperswithcode.com/method/residual-connection). Also, the number of convolution filters of ASPP is reduced from 255 to 96 to gain computational performance." ;
    skos:prefLabel "DASPP" .

:DAU-ConvNet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1902.07474v2> ;
    rdfs:seeAlso <https://github.com/skokec/DAU-ConvNet> ;
    skos:altLabel "Displaced Aggregation Units" ;
    skos:definition """**Displaced Aggregation Unit** replaces classic [convolution](https://paperswithcode.com/method/convolution) layer in ConvNets with learnable positions of units.  This introduces explicit structure of hierarchical compositions and results in several benefits:\r
\r
* fully adjustable and **learnable receptive fields** through spatially-adjustable filter units\r
* **reduced parameters** for spatial coverage\r
efficient inference\r
* **decupling** of the parameters from the receptive field sizes\r
\r
More information can be found [here.](https://www.vicos.si/Research/DeepCompositionalNet)""" ;
    skos:prefLabel "DAU-ConvNet" .

:DBGAN a skos:Concept ;
    rdfs:seeAlso <https://github.com/SsGood/DBGAN> ;
    skos:altLabel "Distribution-induced Bidirectional Generative Adversarial Network for Graph Representation Learning" ;
    skos:definition """DBGAN is a method for graph representation learning. Instead of the widely used normal distribution assumption, the prior distribution of latent representation in DBGAN is estimated in a structure-aware way, which implicitly bridges the graph and feature spaces by prototype learning.\r
\r
Source: [Distribution-induced Bidirectional Generative Adversarial Network for Graph Representation Learning](https://arxiv.org/abs/1912.01899)""" ;
    skos:prefLabel "DBGAN" .

:DBlock a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1909.11646v2> ;
    rdfs:seeAlso <https://github.com/yanggeng1995/GAN-TTS/blob/4675fa108c4c52f190d27a32a8d9e9ce1c68d7a1/models/discriminator.py#L101> ;
    skos:definition "**DBlock** is a residual based block used in the discriminator of the [GAN-TTS](https://paperswithcode.com/method/gan-tts) architecture. They are similar to the [GBlocks](https://paperswithcode.com/method/gblock) used in the generator, but without batch normalisation." ;
    skos:prefLabel "DBlock" .

:DCGAN a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1511.06434v2> ;
    rdfs:seeAlso <https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/dcgan/dcgan.py> ;
    skos:altLabel "Deep Convolutional GAN" ;
    skos:definition """**DCGAN**, or **Deep Convolutional GAN**, is a generative adversarial network architecture. It uses a couple of guidelines, in particular:\r
\r
- Replacing any pooling layers with strided convolutions (discriminator) and fractional-strided convolutions (generator).\r
- Using batchnorm in both the generator and the discriminator.\r
- Removing fully connected hidden layers for deeper architectures.\r
- Using [ReLU](https://paperswithcode.com/method/relu) activation in generator for all layers except for the output, which uses tanh.\r
- Using LeakyReLU activation in the discriminator for all layer.""" ;
    skos:prefLabel "DCGAN" .

:DCLS a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2112.03740v4> ;
    rdfs:seeAlso <https://github.com/K-H-Ismail/Dilated-Convolution-with-Learnable-Spacings-PyTorch/blob/main/README.md> ;
    skos:altLabel "Dilated convolution with learnable spacings" ;
    skos:definition """Dilated convolution with learnable spacings (DCLS) is a type of convolution that allows the spacings between the non-zero elements of the kernel to be learned during training. This makes it possible to increase the receptive field of the convolution without increasing the number of parameters, which can improve the performance of the network on tasks that require long-range dependencies.\r
\r
A dilated convolution is a type of convolution that allows the kernel to be skipped over some of the input features. This is done by inserting zeros between the non-zero elements of the kernel. The effect of this is to increase the receptive field of the convolution without increasing the number of parameters.\r
\r
DCLS takes this idea one step further by allowing the spacings between the non-zero elements of the kernel to be learned during training. This means that the network can learn to skip over different input features depending on the task at hand. This can be particularly helpful for tasks that require long-range dependencies, such as image segmentation and object detection.\r
\r
DCLS has been shown to be effective for a variety of tasks, including image classification, object detection, and semantic segmentation. It is a promising new technique that has the potential to improve the performance of convolutional neural networks on a variety of tasks.""" ;
    skos:prefLabel "DCLS" .

:DCN-V2 a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2008.13535v2> ;
    skos:definition "**DCN-V2** is an architecture for learning-to-rank that improves upon the original [DCN](http://paperswithcode.com/method/dcn) model. It first learns explicit feature interactions of the inputs (typically the embedding layer) through cross layers, and then combines with a deep network to learn complementary implicit interactions. The core of DCN-V2 is the cross layers, which inherit the simple structure of the cross network from DCN, however it is significantly more expressive at learning explicit and bounded-degree cross features." ;
    skos:prefLabel "DCN-V2" .

:DCNN a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1511.02136v6> ;
    skos:altLabel "Diffusion-Convolutional Neural Networks" ;
    skos:definition """Diffusion-convolutional neural networks (DCNN) is a model for graph-structured data. Through the introduction of a diffusion-convolution operation, diffusion-based representations can be learned from graph structured data and used as an effective basis for node classification.\r
\r
Description and image from: [Diffusion-Convolutional Neural Networks](https://arxiv.org/pdf/1511.02136.pdf)""" ;
    skos:prefLabel "DCNN" .

:DD-PPO a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1911.00357v2> ;
    skos:altLabel "Decentralized Distributed Proximal Policy Optimization" ;
    skos:definition """**Decentralized Distributed Proximal Policy Optimization (DD-PPO)** is a method for distributed reinforcement learning in resource-intensive simulated environments. DD-PPO is distributed (uses multiple machines), decentralized (lacks a centralized server), and synchronous (no computation is ever `stale'), making it conceptually simple and easy to implement. \r
\r
Proximal Policy Optimization, or [PPO](https://paperswithcode.com/method/ppo), is a policy gradient method for reinforcement learning. The motivation was to have an algorithm with the data efficiency and reliable performance of [TRPO](https://paperswithcode.com/method/trpo), while using only first-order optimization. \r
\r
Let $r\\_{t}\\left(\\theta\\right)$ denote the probability ratio $r\\_{t}\\left(\\theta\\right) = \\frac{\\pi\\_{\\theta}\\left(a\\_{t}\\mid{s\\_{t}}\\right)}{\\pi\\_{\\theta\\_{old}}\\left(a\\_{t}\\mid{s\\_{t}}\\right)}$, so $r\\left(\\theta\\_{old}\\right) = 1$. TRPO maximizes a “surrogate” objective:\r
\r
$$ L^{v}\\left({\\theta}\\right) = \\hat{\\mathbb{E}}\\_{t}\\left[\\frac{\\pi\\_{\\theta}\\left(a\\_{t}\\mid{s\\_{t}}\\right)}{\\pi\\_{\\theta\\_{old}}\\left(a\\_{t}\\mid{s\\_{t}}\\right)})\\hat{A}\\_{t}\\right] = \\hat{\\mathbb{E}}\\_{t}\\left[r\\_{t}\\left(\\theta\\right)\\hat{A}\\_{t}\\right] $$\r
\r
As a general abstraction, DD-PPO implements the following:\r
at step $k$, worker $n$ has a copy of the parameters, $\\theta^k_n$, calculates the gradient, $\\delta \\theta^k_n$, and updates $\\theta$ via \r
\r
$$ \\theta^{k+1}\\_n =  \\text{ParamUpdate}\\Big(\\theta^{k}\\_n, \\text{AllReduce}\\big(\\delta \\theta^k\\_1, \\ldots, \\delta \\theta^k\\_N\\big)\\Big) = \\text{ParamUpdate}\\Big(\\theta^{k}\\_n, \\frac{1}{N}  \\sum_{i=1}^{N} { \\delta \\theta^k_i}   \\Big) $$\r
\r
where $\\text{ParamUpdate}$ is any first-order optimization technique (e.g. gradient descent) and $\\text{AllReduce}$ performs a reduction (e.g. mean) over all copies of a variable and returns the result to all workers.\r
Distributed DataParallel scales very well (near-linear scaling up to 32,000 GPUs), and is reasonably simple to implement (all workers synchronously running identical code).""" ;
    skos:prefLabel "DD-PPO" .

:DDPG a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1509.02971v6> ;
    skos:altLabel "Deep Deterministic Policy Gradient" ;
    skos:definition "**DDPG**, or **Deep Deterministic Policy Gradient**, is an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. It combines the actor-critic approach with insights from [DQNs](https://paperswithcode.com/method/dqn): in particular, the insights that 1) the network is trained off-policy with samples from a replay buffer to minimize correlations between samples, and 2) the network is trained with a target Q network to give consistent targets during temporal difference backups. DDPG makes use of the same ideas along with [batch normalization](https://paperswithcode.com/method/batch-normalization)." ;
    skos:prefLabel "DDPG" .

:DDParser a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2009.00901v2> ;
    skos:altLabel "Baidu Dependency Parser" ;
    skos:definition """**DDParser**, or **Baidu Dependency Parser**, is a Chinese dependency parser trained on a large-scale manually labeled dataset called Baidu Chinese Treebank (DuCTB).\r
\r
For inputs, for the $i$ th word, its input vector $e_{i}$ is the concatenation of the word embedding and character-level representation:\r
\r
$$\r
e\\_{i}=e\\_{i}^{w o r d} \\oplus C h a r L S T M\\left(w\\_{i}\\right)\r
$$\r
\r
Where $\\operatorname{CharLSTM}\\left(w_{i}\\right)$ is the output vectors after feeding the character sequence into a [BiLSTM](https://paperswithcode.com/method/bilstm) layer. The experimental results on DuCTB dataset show that replacing POS tag embeddings with $\\operatorname{CharLSTM}\\left(w_{i}\\right)$ leads to the improvement.\r
\r
For the BiLSTM encoder, three BiLSTM layers are employed over the input vectors for context encoding. Denote $r\\_{i}$ the output vector of the top-layer BiLSTM for $w\\_{i}$\r
\r
The dependency parser of [Dozat and Manning](https://arxiv.org/abs/1611.01734) is used. Dimension-reducing MLPs are applied to each recurrent output vector $r\\_{i}$ before applying the biaffine transformation. Applying smaller MLPs to the recurrent output states before the biaffine classifier has the advantage of stripping away information not relevant to the current decision. Then biaffine attention is used both in the dependency arc classifier and relation classifier. The computations of all symbols in the Figure are shown below:\r
\r
$$\r
h_{i}^{d-a r c}=M L P^{d-a r c}\\left(r_{i}\\right)\r
$$\r
$$\r
h_{i}^{h-a r c}=M L P^{h-a r c}\\left(r_{i}\\right) \\\\\r
$$\r
$$\r
h_{i}^{d-r e l}=M L P^{d-r e l}\\left(r_{i}\\right) \\\\\r
$$\r
$$\r
h_{i}^{h-r e l}=M L P^{h-r e l}\\left(r_{i}\\right) \\\\\r
$$\r
$$\r
S^{a r c}=\\left(H^{d-a r c} \\oplus I\\right) U^{a r c} H^{h-a r c} \\\\\r
$$\r
$$\r
S^{r e l}=\\left(H^{d-r e l} \\oplus I\\right) U^{r e l}\\left(\\left(H^{h-r e l}\\right)^{T} \\oplus I\\right)^{T}\r
$$\r
\r
For the decoder, the first-order Eisner algorithm is used to ensure that the output is a projection tree. Based on the dependency tree built by biaffine parser, we get a word sequence through the in-order traversal of the tree. The output is a projection tree only if the word sequence is in order.""" ;
    skos:prefLabel "DDParser" .

:DDQL a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1509.06461v3> ;
    skos:altLabel "Double Deep Q-Learning" ;
    skos:definition "" ;
    skos:prefLabel "DDQL" .

:DDSP a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2001.04643v1> ;
    skos:altLabel "Differentiable Digital Signal Processing" ;
    skos:definition "" ;
    skos:prefLabel "DDSP" .

:DE-GAN a skos:Concept ;
    skos:altLabel "DE-GAN: A Conditional Generative Adversarial Network for Document Enhancement" ;
    skos:definition """Documents often exhibit various forms of degradation, which make it hard to be read and substantially deteriorate the\r
performance of an OCR system. In this paper, we propose an effective end-to-end framework named Document Enhancement\r
Generative Adversarial Networks (DE-GAN) that uses the conditional GANs (cGANs) to restore severely degraded document images.\r
To the best of our knowledge, this practice has not been studied within the context of generative adversarial deep networks. We\r
demonstrate that, in different tasks (document clean up, binarization, deblurring and watermark removal), DE-GAN can produce an\r
enhanced version of the degraded document with a high quality. In addition, our approach provides consistent improvements compared to state-of-the-art methods over the widely used DIBCO 2013, DIBCO 2017 and H-DIBCO 2018 datasets, proving its ability to restore a degraded document image to its ideal condition. The obtained results on a wide variety of degradation reveal the flexibility of the proposed model to be exploited in other document enhancement problems.""" ;
    skos:prefLabel "DE-GAN" .

:DECA a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2012.04012v2> ;
    skos:altLabel "Detailed Expression Capture and Animation" ;
    skos:definition "**Detailed Expression Capture and Animation**, or **DECA**, is a model for 3D face reconstruction that is trained to robustly produce a UV displacement map from a low-dimensional latent representation that consists of person-specific detail parameters and generic expression parameters, while a regressor is trained to predict detail, shape, albedo, expression, pose and illumination parameters from a single image. A detail-consistency loss is used to disentangle person-specific details and expression-dependent wrinkles. This disentanglement allows us to synthesize realistic person-specific wrinkles by controlling expression parameters while keeping person-specific details unchanged." ;
    skos:prefLabel "DECA" .

:DELG a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2001.05027v4> ;
    skos:definition """**DELG** is a convolutional neural network for image retrieval that combines generalized mean pooling for global features and attentive selection for local features. The entire network can be learned end-to-end by carefully balancing the gradient flow between two heads – requiring only image-level labels. This allows for efficient inference by extracting an image’s global feature, detected keypoints and local descriptors within a single model.\r
\r
The model is enabled by leveraging hierarchical image representations that arise in [CNNs](https://paperswithcode.com/methods/category/convolutional-neural-networks), which are coupled to [generalized mean pooling](https://paperswithcode.com/method/generalized-mean-pooling) and attentive local feature detection. Secondly, a convolutional autoencoder module is adopted that can successfully learn low-dimensional local descriptors. This can be readily integrated into the unified model, and avoids the need of post-processing learning steps, such as [PCA](https://paperswithcode.com/method/pca), that are commonly used. Finally, a procedure is used that enables end-to-end training of the proposed model using only image-level supervision. This requires carefully controlling the gradient flow between the global and local network heads during backpropagation, to avoid disrupting the desired representations.""" ;
    skos:prefLabel "DELG" .

:DELU a skos:Concept ;
    dcterms:source <https://doi.org/10.20944/preprints202301.0463.v1> ;
    skos:definition """The **DELU** is a type of activation function that has trainable parameters, uses the complex linear and exponential functions in the positive dimension and uses the **[SiLU](https://paperswithcode.com/method/silu)** in the negative dimension.\r
\r
$$DELU(x) = SiLU(x), x \\leqslant 0$$\r
$$DELU(x) = (n + 0.5)x + |e^{-x} - 1|, x > 0$$""" ;
    skos:prefLabel "DELU" .

:DEQ a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1909.01377v2> ;
    skos:altLabel "Deep Equilibrium Models" ;
    skos:definition "A new kind of implicit models, where the output of the network is defined as the solution to an \"infinite-level\" fixed point equation. Thanks to this we can compute the gradient of the output without activations and therefore with a significantly reduced memory footprint." ;
    skos:prefLabel "DEQ" .

:DEXTR a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1711.09081v2> ;
    skos:altLabel "Deep Extreme Cut" ;
    skos:definition """**DEXTR**, or **Deep Extreme Cut**, obtains an object segmentation from its four extreme points: the left-most, right-most, top, and bottom pixels. The annotated extreme points are given as a guiding signal to the input of the network. To this end, we create a [heatmap](https://paperswithcode.com/method/heatmap) with activations in the regions of extreme points. We center a 2D Gaussian around each of the points, in order to create a single heatmap. The heatmap is concatenated with the RGB channels of the input image, to form a 4-channel input for the CNN. In order to focus on the object of interest, the input is cropped by the bounding box, formed from the extreme point annotations. To include context on the resulting\r
crop, we relax the tight bounding box by several pixels. After the pre-processing step that comes exclusively from the extreme clicks, the input consists of an RGB crop including an object, plus its extreme points. \r
\r
[ResNet](https://paperswithcode.com/method/resnet)-101 is chosen as backbone of the architecture. We remove the fully connected layers as well as the [max pooling](https://paperswithcode.com/method/max-pooling) layers in the last two stages to preserve acceptable output resolution for dense prediction, and we introduce atrous convolutions in the last two stages to maintain the same receptive field. After the last ResNet-101 stage, we introduce a pyramid scene parsing module to aggregate global context to the final feature map. The output of the CNN is a probability map representing whether a pixel belongs to the object that we want to segment or not. The CNN is trained to minimize the standard cross entropy loss, which takes into account that different classes occur with different frequency in a dataset.""" ;
    skos:prefLabel "DEXTR" .

:DExTra a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2008.00623v2> ;
    rdfs:seeAlso <https://github.com/sacmehta/delight/blob/1854c04735a5ac1f5aca55003ed82d18c1a34e6e/fairseq/delight_modules/dextra_unit.py#L16> ;
    skos:definition """**DExTra**, or **Deep and Light-weight Expand-reduce Transformation**, is a light-weight expand-reduce transformation that enables learning wider representations efficiently.\r
\r
DExTra maps a $d\\_{m}$ dimensional input vector into a high dimensional space (expansion) and then\r
reduces it down to a $d\\_{o}$ dimensional output vector (reduction) using $N$ layers of group transformations. During these expansion and reduction phases, DExTra uses group linear transformations because they learn local representations by deriving the output from a specific part of the input and are more efficient than linear transformations. To learn global representations, DExTra shares information between different groups in the group linear transformation using feature shuffling \r
\r
Formally, the DExTra transformation is controlled by five configuration parameters: (1) depth $N$, (2)\r
width multiplier $m\\_{w}$, (3) input dimension $d\\_{m}$, (4) output dimension $d\\_{o}$, and (5) maximum groups $g\\_{max}$ in a group linear transformation. In the expansion phase, DExTra projects the $d\\_{m}$-dimensional input to a high-dimensional space, $d\\_{max} = m\\_{w}d\\_{m}$, linearly using $\\text{ceil}\\left(\\frac{N}{2}\\right)$ layers. In the reduction phase, DExTra projects the $d\\_{max}$-dimensional vector to a $d\\_{o}$-dimensional space using the remaining $N -\\text{ceil}\\left(\\frac{N}{2}\\right)$ layers. Mathematically, we define the output $Y$ at each layer $l$ as:\r
\r
$$ \\mathbf{Y}\\_{l} = \\mathcal{F}\\left(\\mathbf{X}, \\mathbf{W}^{l}, \\mathbf{b}^{l}, g^{l}\\right) \\text{ if } l=1 $$\r
$$ \\mathbf{Y}\\_{l} = \\mathcal{F}\\left(\\mathcal{H}\\left(\\mathbf{X}, \\mathbf{Y}^{l-1}\\right), \\mathbf{W}^{l}, \\mathbf{b}^{l}, g^{l}\\right) \\text{ Otherwise } $$\r
\r
where the number of groups at each layer $l$ are computed as:\r
\r
$$ g^{l} = \\text{min}\\left(2^{l-1}, g\\_{max}\\right), 1 \\leq l \\leq \\text{ceil}\\left(N/2\\right) $$\r
$$ g^{N-l}, \\text{Otherwise}$$\r
\r
In the above equations, $\\mathcal{F}$ is a group linear transformation function. The function $\\mathcal{F}$ takes the input $\\left(\\mathbf{X} \\text{ or } \\mathcal{H}\\left(\\mathbf{X}, \\mathbf{Y}^{l-1}\\right) \\right)$, splits it into $g^{l}$ groups, and then applies a linear transformation with learnable parameters $\\mathbf{W}^{l}$ and bias $\\mathbf{b}^{l}$ to each group independently. The outputs of each group are then concatenated to produce the final output $\\mathbf{Y}^{l}$. The function $\\mathcal{H}$ first shuffles the output of each group in $\\mathbf{Y}^{l−1}$ and then combines it with the input $\\mathbf{X}$ using an input mixer connection.\r
\r
In the authors' experiments, they use $g\\_{max} = \\text{ceil}\\left(\\frac{d\\_{m}}{32}\\right)$ so that each group has at least 32 input elements. Note that (i) group linear transformations reduce to linear transformations when $g^{l} = 1$, and (ii) DExTra is equivalent to a multi-layer perceptron when $g\\_{max} = 1$.""" ;
    skos:prefLabel "DExTra" .

:DFA a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1609.01596v5> ;
    skos:altLabel "Direct Feedback Alignment" ;
    skos:definition "" ;
    skos:prefLabel "DFA" .

:DFDNet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2008.00418v1> ;
    skos:definition "**DFDNet**, or **DFDNet**, is a deep face dictionary network for face restoration to guide the restoration process of degraded observations. Given a LQ image $I\\_{d}$, the DFDNet selects the dictionary features that have the most similar structure with the input. Specially, we re-norm the whole dictionaries via component AdaIN (termed as CAdaIN) based on the input component to eliminate the distribution or style diversity. The selected dictionary features are then utilized to guide the restoration process via dictionary feature transformation." ;
    skos:prefLabel "DFDNet" .

:DG-Net a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1904.07223v3> ;
    skos:altLabel "Discriminative and Generative Network" ;
    skos:definition "" ;
    skos:prefLabel "DG-Net" .

:DGCNN a skos:Concept ;
    dcterms:source <https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewPaper/17146> ;
    skos:altLabel "Deep Graph Convolutional Neural Network" ;
    skos:definition """DGCNN involves neural networks that read the graphs directly and learn a classification function. There are two main challenges: 1) how to extract useful features characterizing the rich information encoded in a graph for classification purpose, and 2) how to sequentially read a graph in a meaningful and consistent order. To address the first challenge, we design a localized graph convolution model and show its connection with two graph kernels. To address the second challenge, we design a novel SortPooling layer which sorts graph vertices in a consistent order so that traditional neural networks can be trained on the graphs.\r
\r
Description and image from: [An End-to-End Deep Learning Architecture for Graph Classification](https://muhanzhang.github.io/papers/AAAI_2018_DGCNN.pdf)""" ;
    skos:prefLabel "DGCNN" .

:DGI a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1809.10341v2> ;
    skos:altLabel "Deep Graph Infomax" ;
    skos:definition """Deep Graph Infomax (DGI), a general approach for learning node representations within graph-structured data in an unsupervised manner. DGI relies on maximizing mutual information between patch representations and corresponding high-level summaries of graphs—both derived using established graph convolutional network architectures. The learnt patch representations summarize subgraphs centered around nodes of interest, and can thus be reused for downstream node-wise learning tasks. In contrast to most prior approaches to unsupervised learning with GCNs, DGI does not rely on random walk objectives, and is readily applicable to both transductive and inductive learning setups.\r
\r
Description and image from: [DEEP GRAPH INFOMAX](https://arxiv.org/pdf/1809.10341.pdf)""" ;
    skos:prefLabel "DGI" .

:DGRF a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2111.13460v2> ;
    skos:altLabel "Difference of Gaussian Random Forest" ;
    skos:definition "" ;
    skos:prefLabel "DGRF" .

:DIME a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2108.10673v1> ;
    skos:altLabel "Distance to Modelled Embedding" ;
    skos:definition "**DIME**, or **Distance to Modelled Embedding**, is a method for detecting out-of-distribution examples during prediction time. Given a trained neural network, the training data drawn from some high-dimensional distribution in data space $X$ is transformed into the model’s intermediate feature vector space $\\mathbb{R}^{p}$. The training set embedding is linearly approximated as a hyperplane. When we then receive new observations it is difficult to assess if observations are out-of-distribution directly in data space, so we transform them into the same intermediate feature space. Finally, the Distance-to-Modelled-Embedding (DIME) can be used to assess whether new observations fit into the expected embedding covariance structure." ;
    skos:prefLabel "DIME" .

:DINO a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2104.14294v2> ;
    rdfs:seeAlso <https://github.com/facebookresearch/dino/blob/main/main_dino.py> ;
    skos:altLabel "self-DIstillation with NO labels" ;
    skos:definition """**DINO** (self-distillation with no labels) is a self-supervised learning method that directly predicts the output of a teacher network - built with a momentum encoder - using a standard cross-entropy loss. \r
\r
In the example to the right, DINO is illustrated in the case of one single pair of views $\\left(x\\_{1}, x\\_{2}\\right)$ for simplicity.\r
The model passes two different random transformations of an input image to the student and teacher networks. Both networks have the same architecture but other parameters.\r
The output of the teacher network is centered with a mean computed over the batch. Each network outputs a $K$ dimensional feature normalized with a temperature [softmax](https://paperswithcode.com/method/softmax) over the feature dimension.\r
Their similarity is then measured with a cross-entropy loss.\r
A stop-gradient (sg) operator is applied to the teacher to propagate gradients only through the student.\r
The teacher parameters are updated with the student parameters' exponential moving average (ema).""" ;
    skos:prefLabel "DINO" .

:DIoU-NMS a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1911.08287v1> ;
    skos:definition """**DIoU-NMS** is a type of non-maximum suppression where we use Distance IoU rather than regular DIoU, in which the overlap area and the distance between two central points of bounding boxes are simultaneously considered when suppressing redundant boxes.\r
\r
In original NMS, the IoU metric is used to suppress the redundant detection boxes, where the overlap area is the unique factor, often yielding false suppression for the cases with occlusion. With DIoU-NMS, we not only consider the overlap area but also central point distance between two boxes.""" ;
    skos:prefLabel "DIoU-NMS" .

:DLA a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1707.06484v3> ;
    skos:altLabel "Deep Layer Aggregation" ;
    skos:definition """**DLA**, or **Deep Layer Aggregation**,  iteratively and hierarchically merges the feature hierarchy across layers in neural networks to make networks with better accuracy and fewer parameters. \r
\r
In iterative deep aggregation (IDA), aggregation begins at the shallowest, smallest scale and then iteratively merges deeper,\r
larger scales. In this way shallow features are refined as\r
they are propagated through different stages of aggregation.\r
\r
In hierarchical deep aggregation (HDA), blocks and stages\r
in a tree are merged to preserve and combine feature channels. With\r
HDA shallower and deeper layers are combined to learn\r
richer combinations that span more of the feature hierarchy.\r
While IDA effectively combines stages, it is insufficient\r
for fusing the many blocks of a network, as it is still only\r
sequential.""" ;
    skos:prefLabel "DLA" .

:DMA a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2004.03212v4> ;
    skos:altLabel "Dual Multimodal Attention" ;
    skos:definition "In image inpainting task, the mechanism extracts complementary features from the word embedding in two paths by reciprocal attention, which is done by comparing the descriptive text and complementary image areas through reciprocal attention." ;
    skos:prefLabel "DMA" .

:DMAGE a skos:Concept ;
    skos:altLabel "Unsupervised Deep Manifold Attributed Graph Embedding" ;
    skos:definition "Unsupervised attributed graph representation learning is challenging since both structural and feature information are required to be represented in the latent space. Existing methods concentrate on learning latent representation via reconstruction tasks, but cannot directly optimize representation and are prone to oversmoothing, thus limiting the applications on downstream tasks. To alleviate these issues, we propose a novel graph embedding framework named Deep Manifold Attributed Graph Embedding (DMAGE). A node-to-node geodesic similarity is proposed to compute the inter-node similarity between the data space and the latent space and then use Bergman divergence as loss function to minimize the difference between them. We then design a new network structure with fewer aggregation to alleviate the oversmoothing problem and incorporate graph structure augmentation to improve the representation's stability. Our proposed DMAGE surpasses state-of-the-art methods by a significant margin on three downstream tasks: unsupervised visualization, node clustering, and link prediction across four popular datasets." ;
    skos:prefLabel "DMAGE" .

:DMVFN a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2303.09875v2> ;
    skos:altLabel "A Dynamic Multi-Scale Voxel Flow Network" ;
    skos:definition "" ;
    skos:prefLabel "DMVFN" .

:DNAS a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1812.03443v3> ;
    skos:altLabel "Differentiable Neural Architecture Search" ;
    skos:definition """**DNAS**, or **Differentiable Neural Architecture Search**, uses gradient-based methods to optimize ConvNet architectures, avoiding enumerating and training individual architectures separately as in previous methods. DNAS allows us to explore a layer-wise search space where we can choose a different block for each layer of the network. DNAS represents the search space by a super net whose operators execute stochastically. It relaxes the problem of finding the optimal architecture to find a distribution that yields the optimal architecture. By using the [Gumbel Softmax](https://paperswithcode.com/method/gumbel-softmax) technique, it is possible to directly train the architecture distribution using gradient-based optimization such as [SGD](https://paperswithcode.com/method/sgd).\r
\r
The loss used to train the stochastic super net consists of both the cross-entropy loss that leads to better accuracy and the latency loss that penalizes the network's latency on a target device. To estimate the latency of an architecture, the latency of each operator in the search space is measured and a lookup table model is used to compute the overall latency by adding up the latency of each operator. Using this model allows for estimation of the latency of architectures in an enormous search space. More importantly, it makes the latency differentiable with respect to layer-wise block choices.""" ;
    skos:prefLabel "DNAS" .

:DNN2LR a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2008.09775v5> ;
    skos:definition "**DNN2LR** is an automatic feature crossing method to find feature interactions in a deep neural network, and use them as cross features in logistic regression. In general, DNN2LR consists of two steps: (1) generating a compact and accurate candidate set of cross feature fields; (2) searching in the candidate set for the final cross feature fields." ;
    skos:prefLabel "DNN2LR" .

:DOLG a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2108.02927v2> ;
    skos:altLabel "Deep Orthogonal Fusion of Local and Global Features" ;
    skos:definition """Image Retrieval is a fundamental task of obtaining images similar to the query one from a database. A common image retrieval practice is to firstly retrieve candidate images via similarity search using global image features and then re-rank the candidates by leveraging their\r
local features. Previous learning-based studies mainly focus on either global or local image representation learning\r
to tackle the retrieval task. In this paper, we abandon the\r
two-stage paradigm and seek to design an effective singlestage solution by integrating local and global information\r
inside images into compact image representations. Specifically, we propose a Deep Orthogonal Local and Global\r
(DOLG) information fusion framework for end-to-end image retrieval. It attentively extracts representative local information with multi-atrous convolutions and self-attention\r
at first. Components orthogonal to the global image representation are then extracted from the local information.\r
At last, the orthogonal components are concatenated with\r
the global representation as a complementary, and then aggregation is performed to generate the final representation.\r
The whole framework is end-to-end differentiable and can\r
be trained with image-level labels. Extensive experimental\r
results validate the effectiveness of our solution and show\r
that our model achieves state-of-the-art image retrieval performances on Revisited Oxford and Paris datasets.""" ;
    skos:prefLabel "DOLG" .

:DPG a skos:Concept ;
    skos:altLabel "Deterministic Policy Gradient" ;
    skos:definition "**Deterministic Policy Gradient**, or **DPG**, is a policy gradient method for reinforcement learning. Instead of the policy function $\\pi\\left(.\\mid{s}\\right)$ being modeled as a probability distribution, DPG considers and calculates gradients for a deterministic policy $a = \\mu\\_{theta}\\left(s\\right)$." ;
    skos:prefLabel "DPG" .

:DPN a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1707.01629v2> ;
    rdfs:seeAlso <https://github.com/osmr/imgclsmob/blob/c03fa67de3c9e454e9b6d35fe9cbb6b15c28fda7/pytorch/pytorchcv/models/dpn.py#L322> ;
    skos:altLabel "Dual Path Network" ;
    skos:definition """A **Dual Path Network (DPN)** is a convolutional neural network which presents a new topology of connection paths internally. The intuition is that [ResNets](https://paperswithcode.com/method/resnet) enables feature re-usage while [DenseNet](https://paperswithcode.com/method/densenet) enables new feature exploration, and both are important for learning good representations. To enjoy the benefits from both path topologies, Dual Path Networks share common features while maintaining the flexibility to explore new features through dual path architectures. \r
\r
We formulate such a dual path architecture as follows:\r
\r
$$x^{k} = \\sum\\limits\\_{t=1}^{k-1} f\\_t^{k}(h^t) \\text{,}  $$\r
\r
$$\r
y^{k} = \\sum\\limits\\_{t=1}^{k-1} v\\_t(h^t) = y^{k-1} + \\phi^{k-1}(y^{k-1}) \\text{,} \\\\\\\\\r
$$\r
\r
$$\r
r^{k} = x^{k} + y^{k} \\text{,} \\\\\\\\\r
$$\r
\r
$$\r
h^k = g^k \\left( r^{k} \\right) \\text{,}\r
$$\r
\r
where $x^{k}$ and $y^{k}$ denote the extracted information at $k$-th step from individual path, $v_t(\\cdot)$ is a feature learning function as $f_t^k(\\cdot)$. The first equation refers to the densely connected path that enables exploring new features. The second equation refers to the residual path that enables common features re-usage. The third equation defines the dual path that integrates them and feeds them to the last transformation function in the last equation.""" ;
    skos:prefLabel "DPN" .

:DPNBlock a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1707.01629v2> ;
    rdfs:seeAlso <https://github.com/rwightman/pytorch-dpn-pretrained/blob/2923586d8f4ab3fdc05370cc409a620a3dbd1083/dpn.py#L205> ;
    skos:definition """A **Dual Path Network** block is an image model block used in convolutional neural network. The idea of this module is to enable sharing of common features while maintaining the flexibility to explore new features through dual path architectures. In this sense it combines the benefits of [ResNets](https://paperswithcode.com/method/resnet) and [DenseNets](https://paperswithcode.com/method/densenet). It was proposed as part of the [DPN](https://paperswithcode.com/method/dpn) CNN architecture.\r
\r
We formulate such a dual path architecture as follows:\r
\r
$$x^{k} = \\sum\\limits\\_{t=1}^{k-1} f\\_t^{k}(h^t) \\text{,}  $$\r
\r
$$\r
y^{k} = \\sum\\limits\\_{t=1}^{k-1} v\\_t(h^t) = y^{k-1} + \\phi^{k-1}(y^{k-1}) \\text{,} \\\\\\\\\r
$$\r
\r
$$\r
r^{k} = x^{k} + y^{k} \\text{,} \\\\\\\\\r
$$\r
\r
$$\r
h^k = g^k \\left( r^{k} \\right) \\text{,}\r
$$\r
\r
where $x^{k}$ and $y^{k}$ denote the extracted information at $k$-th step from individual path, $v_t(\\cdot)$ is a feature learning function as $f_t^k(\\cdot)$. The first equation refers to the densely connected path that enables exploring new features. The second equation refers to the residual path that enables common features re-usage. The third equation defines the dual path that integrates them and feeds them to the last transformation function in the last equation.""" ;
    skos:prefLabel "DPN Block" .

:DPT a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2103.13413v1> ;
    rdfs:seeAlso <https://github.com/intel-isl/DPT/blob/f43ef9e08d70a752195028a51be5e1aff227b913/dpt/models.py#L26> ;
    skos:altLabel "Dense Prediction Transformer" ;
    skos:definition """**Dense Prediction Transformers** (DPT) are a type of [vision transformer](https://paperswithcode.com/method/vision-transformer) for dense prediction tasks.\r
\r
The input image is transformed into tokens (orange) either by extracting non-overlapping patches followed by a linear projection of their flattened representation (DPT-Base and DPT-Large) or by applying a [ResNet](https://paperswithcode.com/method/resnet)-50 feature extractor (DPT-Hybrid). The image embedding is augmented with a positional embedding and a patch-independent readout token (red) is added. The tokens are passed through multiple [transformer](https://paperswithcode.com/method/transformer) stages. The tokens are reassembled from different stages into an image-like representation at multiple resolutions (green). Fusion modules (purple) progressively fuse and upsample the representations to generate a fine-grained prediction.""" ;
    skos:prefLabel "DPT" .

:DQN a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1312.5602v1> ;
    skos:altLabel "Deep Q-Network" ;
    skos:definition """A **DQN**, or Deep Q-Network, approximates a state-value function in a [Q-Learning](https://paperswithcode.com/method/q-learning) framework with a neural network. In the Atari Games case, they take in several frames of the game as an input and output state values for each action as an output. \r
\r
It is usually used in conjunction with [Experience Replay](https://paperswithcode.com/method/experience-replay), for storing the episode steps in memory for off-policy learning, where samples are drawn from the replay memory at random. Additionally, the Q-Network is usually optimized towards a frozen target network that is periodically updated with the latest weights every $k$ steps (where $k$ is a hyperparameter). The latter makes training more stable by preventing short-term oscillations from a moving target. The former tackles autocorrelation that would occur from on-line learning, and having a replay memory makes the problem more like a supervised learning problem.\r
\r
Image Source: [here](https://www.researchgate.net/publication/319643003_Autonomous_Quadrotor_Landing_using_Deep_Reinforcement_Learning)""" ;
    skos:prefLabel "DQN" .

:DROID-SLAM a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2108.10869v2> ;
    skos:definition "**DROID-SLAM** is a deep learning based SLAM system. It consists of recurrent iterative updates of camera pose and pixelwise depth through a Dense Bundle Adjustment layer. This layer leverages geometric constraints, improves accuracy and robustness, and enables a monocular system to handle stereo or RGB-D input without retraining. It builds a dense 3D map of the environment while simultaneously localizing the camera within the map." ;
    skos:prefLabel "DROID-SLAM" .

:DRPNN a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1705.07556v2> ;
    skos:altLabel "Deep Residual Pansharpening Neural Network" ;
    skos:definition "In the field of fusing multi-spectral and panchromatic images (Pan-sharpening), the impressive effectiveness of deep neural networks has been recently employed to overcome the drawbacks of traditional linear models and boost the fusing accuracy. However, to the best of our knowledge, existing research works are mainly based on simple and flat networks with relatively shallow architecture, which severely limited their performances. In this paper, the concept of residual learning has been introduced to form a very deep convolutional neural network to make a full use of the high non-linearity of deep learning models. By both quantitative and visual assessments on a large number of high quality multi-spectral images from various sources, it has been supported that our proposed model is superior to all mainstream algorithms included in the comparison, and achieved the highest spatial-spectral unified accuracy." ;
    skos:prefLabel "DRPNN" .

:DSAMloss a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2011.06228v3> ;
    skos:altLabel "Distance Shrinking with Angular Marginalizing Loss" ;
    skos:definition "" ;
    skos:prefLabel "DSAM loss" .

:DSGN a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2001.03398v3> ;
    skos:altLabel "Deep Stereo Geometry Network" ;
    skos:definition """**Deep Stereo Geometry Network** is a 3D object detection pipeline that relies on space transformation from 2D features to an effective 3D structure, called 3D geometric volume (3DGV). The whole neural network consists of four components. (a) A 2D image\r
feature extractor for capture of both pixel- and high-level feature. (b) Constructing the plane-sweep volume and 3D geometric volume. (c) Depth Estimation on the plane-sweep volume. (d) 3D object detection on 3D geometric volume.""" ;
    skos:prefLabel "DSGN" .

:DSPT a skos:Concept ;
    dcterms:source <https://doi.org/10.20535/SRIT.2308-8893.2022.1.07> ;
    skos:altLabel "double-stage parameter tuning" ;
    skos:definition "Parameter tuning method for neural network models with adaptive activation functions." ;
    skos:prefLabel "DSPT" .

:DSelect-k a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2106.03760v3> ;
    skos:definition """**DSelect-k** is a continuously differentiable and sparse gate for Mixture-of-experts (MoE), based on a novel binary encoding formulation. Given a user-specified parameter $k$, the gate selects at most $k$ out of the $n$ experts. The gate can be trained using first-order methods, such as stochastic gradient descent, and offers explicit control over the number of experts to select. This explicit control over sparsity leads to a cardinality-constrained optimization problem, which is computationally challenging. To circumvent this challenge, the authors use a unconstrained reformulation that is equivalent to the original problem. The reformulated problem uses a binary encoding scheme to implicitly enforce the cardinality constraint. By carefully smoothing the binary encoding variables, the reformulated problem can be effectively optimized using first-order methods such as [SGD](https://paperswithcode.com/method/sgd).\r
\r
The motivation for this method is that  existing sparse gates, such as Top-k, are not smooth. The lack of smoothness can lead to convergence and statistical performance issues when training with gradient-based methods.""" ;
    skos:prefLabel "DSelect-k" .

:DTW a skos:Concept ;
    rdfs:seeAlso <https://dynamictimewarping.github.io/> ;
    skos:altLabel "Dynamic Time Warping" ;
    skos:definition """Dynamic Time Warping (DTW) [1] is one of well-known distance measures between a pairwise of time series. The main idea of DTW is to compute the distance from the matching of similar elements between time series. It uses the dynamic programming technique to find the optimal temporal matching between elements of two time series.\r
\r
For instance, similarities in walking could be detected using DTW, even if one person was walking faster than the other, or if there were accelerations and decelerations during the course of an observation. DTW has been applied to temporal sequences of video, audio, and graphics data — indeed, any data that can be turned into a linear sequence can be analyzed with DTW. A well known application has been automatic speech recognition, to cope with different speaking speeds. Other applications include speaker recognition and online signature recognition. It can also be used in partial shape matching application.\r
\r
In general, DTW is a method that calculates an optimal match between two given sequences (e.g. time series) with certain restriction and rules:\r
\r
1. Every index from the first sequence must be matched with one or more indices from the other sequence, and vice versa\r
2. The first index from the first sequence must be matched with the first index from the other sequence (but it does not have to be its only match)\r
3. The last index from the first sequence must be matched with the last index from the other sequence (but it does not have to be its only match)\r
4. The mapping of the indices from the first sequence to indices from the other sequence must be monotonically increasing, and vice versa, i.e. if j>i  are indices from the first sequence, then there must not be two indices l>k in the other sequence, such that index i is matched with index l and index j is matched with index k, and vice versa.\r
\r
[1] Sakoe, Hiroaki, and Seibi Chiba. "Dynamic programming algorithm optimization for spoken word recognition." IEEE transactions on acoustics, speech, and signal processing 26, no. 1 (1978): 43-49.""" ;
    skos:prefLabel "DTW" .

:DU-GAN a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2108.10772v2> ;
    skos:definition "**DU-GAN** is a [generative adversarial network](https://www.paperswithcode.com/methods/category/generative-adversarial-networks) for LDCT denoising in medical imaging. The generator produces denoised LDCT images, and two independent branches with [U-Net](https://paperswithcode.com/method/u-net) based discriminators perform at the image and gradient domains. The U-Net based discriminator provides both global structure and local per-pixel feedback to the generator. Furthermore, the image discriminator encourages the generator to produce photo-realistic CT images while the gradient discriminator is utilized for better edge and alleviating streak artifacts caused by photon starvation." ;
    skos:prefLabel "DU-GAN" .

:DV3AttentionBlock a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1710.07654v3> ;
    rdfs:seeAlso <https://github.com/r9y9/deepvoice3_pytorch/blob/0b89c10cb473e032641c05cb8b105358c26c02b5/deepvoice3_pytorch/deepvoice3.py#L108> ;
    skos:definition "**DV3 Attention Block** is an attention-based module used in the [Deep Voice 3](https://paperswithcode.com/method/deep-voice-3) architecture. It uses a [dot-product attention](https://paperswithcode.com/method/dot-product-attention) mechanism. A query vector (the hidden states of the decoder) and the per-timestep key vectors from the encoder are used to compute attention weights. This then outputs a context vector computed as the weighted average of the value vectors." ;
    skos:prefLabel "DV3 Attention Block" .

:DV3ConvolutionBlock a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1710.07654v3> ;
    rdfs:seeAlso <https://github.com/r9y9/deepvoice3_pytorch/blob/897f31e57eb6ec2f0cafa8dc62968e60f6a96407/deepvoice3_pytorch/modules.py#L112> ;
    skos:definition "**DV3 Convolution Block** is a convolutional block used for the [Deep Voice 3](https://paperswithcode.com/method/deep-voice-3) text-to-speech architecture. It consists of a 1-D [convolution](https://paperswithcode.com/method/convolution) with a gated linear unit and a [residual connection](https://paperswithcode.com/method/residual-connection). In the Figure, $c$ denotes the dimensionality of the input. The convolution output of size $2 \\cdot c$ is split into equal-sized portions: the gate vector and the input vector. A scaling factor $\\sqrt{0.5}$ is used to ensure that we preserve the input variance early in training. The gated linear unit provides a linear path for the gradient flow, which alleviates the vanishing gradient issue for stacked convolution blocks while retaining non-linearity. To introduce speaker-dependent control, a speaker-dependent embedding is added as a bias to the convolution filter output, after a softsign function. The authors use the softsign nonlinearity because it limits the range of the output while also avoiding the saturation problem that exponential based nonlinearities sometimes exhibit. Convolution filter weights are initialized with zero-mean and unit-variance activations throughout the entire network." ;
    skos:prefLabel "DV3 Convolution Block" .

:DVD-GAN a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1907.06571v2> ;
    skos:definition """**DVD-GAN** is a generative adversarial network for video generation built upon the [BigGAN](https://paperswithcode.com/method/biggan) architecture.\r
\r
DVD-GAN uses two discriminators: a Spatial Discriminator $\\mathcal{D}\\_{S}$ and a\r
Temporal Discriminator $\\mathcal{D}\\_{T}$. $\\mathcal{D}\\_{S}$ critiques single frame content and structure by randomly sampling $k$ full-resolution frames and judging them individually.  The temporal discriminator $\\mathcal{D}\\_{T}$ must provide $G$ with the learning signal to generate movement (not evaluated by $\\mathcal{D}\\_{S}$).\r
\r
The input to $G$ consists of a Gaussian latent noise $z \\sim N\\left(0, I\\right)$ and a learned linear embedding $e\\left(y\\right)$ of the desired class $y$. Both inputs are 120-dimensional vectors. $G$ starts by computing an affine transformation of $\\left[z; e\\left(y\\right)\\right]$ to a $\\left[4, 4, ch\\_{0}\\right]$-shaped tensor. $\\left[z; e\\left(y\\right)\\right]$ is used as the input to all class-[conditional Batch Normalization](https://paperswithcode.com/method/conditional-batch-normalization) layers\r
throughout $G$. This is then treated as the input (at each frame we would like to generate) to a Convolutional [GRU](https://paperswithcode.com/method/gru).\r
\r
This RNN is unrolled once per frame. The output of this RNN is processed by two residual blocks. The time dimension is combined with the batch dimension here, so each frame proceeds through the blocks independently. The output of these blocks has width and height dimensions which\r
are doubled (we skip upsampling in the first block). This is repeated a number of times, with the\r
output of one RNN + residual group fed as the input to the next group, until the output tensors have\r
the desired spatial dimensions. \r
\r
The spatial discriminator $\\mathcal{D}\\_{S}$ functions almost identically to BigGAN’s discriminator. A score is calculated for each of the uniformly sampled $k$ frames (default $k = 8$) and the $\\mathcal{D}\\_{S}$ output is the sum over per-frame scores. The temporal discriminator $\\mathcal{D}\\_{T}$ has a similar architecture, but pre-processes the real or generated video with a $2 \\times 2$ average-pooling downsampling function $\\phi$. Furthermore, the first two residual blocks of $\\mathcal{D}\\_{T}$ are 3-D, where every [convolution](https://paperswithcode.com/method/convolution) is replaced with a 3-D convolution with a kernel size of $3 \\times 3 \\times 3$. The rest of the architecture follows BigGAN.""" ;
    skos:prefLabel "DVD-GAN" .

:DVD-GANDBlock a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1907.06571v2> ;
    skos:definition "**DVD-GAN DBlock** is a residual block for the discriminator used in the [DVD-GAN](https://paperswithcode.com/method/dvd-gan) architecture for video generation. Unlike regular [residual blocks](https://paperswithcode.com/method/residual-block), [3D convolutions](https://paperswithcode.com/method/3d-convolution) are employed due to the application to multiple frames in a video." ;
    skos:prefLabel "DVD-GAN DBlock" .

:DVD-GANGBlock a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1907.06571v2> ;
    skos:definition "**DVD-GAN GBlock** is a [residual block](https://paperswithcode.com/method/residual-block) for the generator used in the [DVD-GAN](https://paperswithcode.com/method/dvd-gan) architecture for video generation." ;
    skos:prefLabel "DVD-GAN GBlock" .

:Darknet-19 a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1612.08242v1> ;
    rdfs:seeAlso <https://github.com/longcw/yolo2-pytorch/blob/17056ca69f097a07884135d9031c53d4ef217a6a/darknet.py#L140> ;
    skos:definition "**Darknet-19** is a convolutional neural network that is used as the backbone of [YOLOv2](https://paperswithcode.com/method/yolov2).  Similar to the [VGG](https://paperswithcode.com/method/vgg) models it mostly uses $3 \\times 3$ filters and doubles the number of channels after every pooling step. Following the work on Network in Network (NIN) it uses [global average pooling](https://paperswithcode.com/method/global-average-pooling) to make predictions as well as $1 \\times 1$ filters to compress the feature representation between $3 \\times 3$ convolutions. [Batch Normalization](https://paperswithcode.com/method/batch-normalization) is used to stabilize training, speed up convergence, and regularize the model batch." ;
    skos:prefLabel "Darknet-19" .

:Darknet-53 a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1804.02767v1> ;
    rdfs:seeAlso <https://github.com/ultralytics/yolov3/blob/06138062869c41d3df130e07c5aa92fa5a01dad5/models.py#L225> ;
    skos:definition "**Darknet-53** is a convolutional neural network that acts as a backbone for the [YOLOv3](https://paperswithcode.com/method/yolov3) object detection approach. The improvements upon its predecessor [Darknet-19](https://paperswithcode.com/method/darknet-19) include the use of residual connections, as well as more layers." ;
    skos:prefLabel "Darknet-53" .

:DeBERTa a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2006.03654v6> ;
    skos:definition "**DeBERTa** is a [Transformer](https://paperswithcode.com/methods/category/transformers)-based neural language model that aims to improve the [BERT](https://paperswithcode.com/method/bert) and [RoBERTa](https://paperswithcode.com/method/roberta) models with two techniques: a [disentangled attention mechanism](https://paperswithcode.com/method/disentangled-attention-mechanism) and an enhanced mask decoder. The disentangled attention mechanism is where each word is represented unchanged using two vectors that encode its content and position, respectively, and the attention weights among words are computed using disentangle matrices on their contents and relative positions. The enhanced mask decoder is used to replace the output [softmax](https://paperswithcode.com/method/softmax) layer to predict the masked tokens for model pre-training.  In addition, a new virtual adversarial training method is used for fine-tuning to improve model’s generalization on downstream tasks." ;
    skos:prefLabel "DeBERTa" .

:DeCLUTR a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2006.03659v4> ;
    skos:definition "**DeCLUTR** is an approach for learning universal sentence embeddings that utilizes a self-supervised objective that does not require labelled training data. The objective learns universal sentence embeddings by training an encoder to minimize the distance between the embeddings of textual segments randomly sampled from nearby in the same document." ;
    skos:prefLabel "DeCLUTR" .

:DeLighT a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2008.00623v2> ;
    rdfs:seeAlso <https://github.com/sacmehta/delight/blob/1197d8ffbcff5c3bfc3b6a040a2ae8af811278c4/fairseq/models/delight_transformer.py> ;
    skos:definition "**DeLiGHT** is a [transformer](https://paperswithcode.com/method/transformer) architecture that delivers parameter efficiency improvements by (1) within each Transformer block using [DExTra](https://paperswithcode.com/method/dextra), a deep and light-weight transformation, allowing for the use of [single-headed attention](https://paperswithcode.com/method/single-headed-attention) and bottleneck FFN layers and (2) across blocks using block-wise scaling, that allows for shallower and narrower [DeLighT blocks](https://paperswithcode.com/method/delight-block) near the input and wider and deeper DeLighT blocks near the output." ;
    skos:prefLabel "DeLighT" .

:DeLighTBlock a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2008.00623v2> ;
    rdfs:seeAlso <https://github.com/sacmehta/delight/blob/1197d8ffbcff5c3bfc3b6a040a2ae8af811278c4/fairseq/modules/delight_transformer_layer.py#L19> ;
    skos:definition "A **DeLighT Block** is a block used in the [DeLighT](https://paperswithcode.com/method/delight) [transformer](https://paperswithcode.com/method/transformer) architecture. It uses a [DExTra](https://paperswithcode.com/method/dextra) transformation to reduce the dimensionality of the vectors entered into the attention layer, where a [single-headed attention](https://paperswithcode.com/method/single-headed-attention) module is used.  Since the DeLighT block learns wider representations of the input across different layers using DExTra, it enables the authors to replace [multi-head attention](https://paperswithcode.com/method/multi-head-attention) with single-head attention. This is then followed by a light-weight FFN which, rather than expanding the dimension (as in normal Transformers which widen to a dimension 4x the size), imposes a bottleneck and squeezes the dimensions. Again, the reason for this is that the DExTra transformation has already incorporated wider representations so we can squeeze instead at this layer." ;
    skos:prefLabel "DeLighT Block" .

:DeactivableSkipConnection a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2003.09691v2> ;
    skos:definition """A **Deactivable Skip Connection** is a type of skip connection which, instead of concatenating the encoder features\r
(red) and decoder features (blue), as with [standard skip connections](https://paperswithcode.com/methods/category/skip-connections), it instead fuses the encoder features with part of the decoder features (light blue), to be able to deactivate this operation when needed.""" ;
    skos:prefLabel "Deactivable Skip Connection" .

:DecorrelatedBatchNormalization a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1804.08450v1> ;
    rdfs:seeAlso <https://github.com/bhneo/decorrelated_bn/blob/ded9279387d62209c7fd4d1d21918431c0d6e96d/common/normalization.py#L16> ;
    skos:definition """**Decorrelated Batch Normalization (DBN)** \r
is a normalization technique which not just centers and scales activations but whitens them. ZCA whitening instead of [PCA](https://paperswithcode.com/method/pca) whitening is employed since PCA whitening causes a problem called *stochastic axis swapping*, which is detrimental to learning.""" ;
    skos:prefLabel "Decorrelated Batch Normalization" .

:DeeBERT a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2004.12993v1> ;
    skos:definition "**DeeBERT** is a method for accelerating [BERT](https://paperswithcode.com/method/bert) inference. It inserts extra classification layers (which are referred to as off-ramps) between each [transformer](https://paperswithcode.com/method/transformer) layer of BERT. All transformer layers and off-ramps are jointly fine-tuned on a given downstream dataset. At inference time, after a sample goes through a transformer layer, it is passed to the following off-ramp. If the off-ramp is confident of the prediction, the result is returned; otherwise, the sample is sent to the next transformer layer." ;
    skos:prefLabel "DeeBERT" .

:Deep-CAPTCHA a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2006.08296v2> ;
    skos:definition "" ;
    skos:prefLabel "Deep-CAPTCHA" .

:Deep-MAC a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2104.00613v2> ;
    skos:definition """**Deep-MAC**, or **Deep Mask-heads Above CenterNet**, is a type of anchor-free instance segmentation model based on [CenterNet](https://paperswithcode.com/method/centernet).  The motivation for this new architecture is that boxes are much cheaper to annotate than masks, so the authors address the “partially supervised” instance segmentation problem, where all classes have bounding box annotations but only a subset of classes have mask annotations. \r
\r
For predicting bounding boxes, CenterNet outputs 3 tensors: (1) a class-specific [heatmap](https://paperswithcode.com/method/heatmap) which indicates the probability of the center of a bounding box being present at each location, (2) a class-agnostic 2-channel tensor indicating the height and width of the bounding box at each center pixel, and (3) since the output feature map is typically smaller than the image (stride 4 or 8), CenterNet also predicts an x and y direction offset to recover this discretization error at each center pixel.\r
\r
For Deep-MAC, in parallel to the box-related prediction heads, we add a fourth pixel embedding branch $P$. For each bounding box\r
$b$, we crop a region $P\\_{b}$ from $P$ corresponding to $b$ via [ROIAlign](https://paperswithcode.com/method/roi-align) which results in a 32 × 32 tensor. We then feed each $P\\_{b}$ to a mask-head. The final prediction at the end is a class-agnostic 32 × 32 tensor which we pass through a sigmoid to get per-pixel probabilities. We train this mask-head via a per-pixel cross-entropy loss averaged over all pixels and instances. During post-processing, the predicted mask is re-aligned according to the predicted box and resized to the resolution of the image. \r
\r
In addition to this 32 × 32 cropped feature map, we add two inputs for improved stability of some mask-heads: (1) Instance embedding: an additional head is added to the backbone that predicts a per-pixel embedding. For each bounding box $b$ we extract its embedding from the center pixel. This embedding is tiled to a size of 32 × 32 and concatenated to the pixel embedding crop. This helps condition the mask-head on a particular instance and disambiguate it from others. (2) Coordinate Embedding: Inspired by [CoordConv](https://paperswithcode.com/method/coordconv), the authors add a 32 × 32 × 2 tensor holding normalized $\\left(x, y\\right)$ coordinates relative to the bounding box $b$.""" ;
    skos:prefLabel "Deep-MAC" .

:DeepBeliefNetwork a skos:Concept ;
    skos:definition """A **Deep Belief Network (DBN)** is a multi-layer generative graphical model. DBNs have bi-directional connections ([RBM](https://paperswithcode.com/method/restricted-boltzmann-machine)-type connections) on the top layer while the bottom layers only have top-down connections. They are trained using layerwise pre-training. Pre-training occurs by training the network component by component bottom up: treating the first two layers as an RBM and training, then treating the second layer and third layer as another RBM and training for those parameters.\r
\r
Source: [Origins of Deep Learning](https://arxiv.org/pdf/1702.07800.pdf)\r
\r
Image Source: [Wikipedia](https://en.wikipedia.org/wiki/Deep_belief_network)""" ;
    skos:prefLabel "Deep Belief Network" .

:DeepBoltzmannMachine a skos:Concept ;
    skos:definition """A **Deep Boltzmann Machine (DBM)** is a three-layer generative model. It is similar to a [Deep Belief Network](https://paperswithcode.com/method/deep-belief-network), but instead allows bidirectional connections in the bottom layers. Its energy function is  as an extension of the energy function of the RBM:\r
\r
$$ E\\left(v, h\\right) = -\\sum^{i}\\_{i}v\\_{i}b\\_{i} - \\sum^{N}\\_{n=1}\\sum_{k}h\\_{n,k}b\\_{n,k}-\\sum\\_{i, k}v\\_{i}w\\_{ik}h\\_{k} - \\sum^{N-1}\\_{n=1}\\sum\\_{k,l}h\\_{n,k}w\\_{n, k, l}h\\_{n+1, l}$$\r
\r
for a DBM with $N$ hidden layers.\r
\r
Source: [On the Origin of Deep Learning](https://arxiv.org/pdf/1702.07800.pdf)""" ;
    skos:prefLabel "Deep Boltzmann Machine" .

:DeepCluster a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1807.05520v2> ;
    skos:definition """**DeepCluster** is a self-supervision approach for learning image representations.  DeepCluster iteratively groups the features with a standard clustering algorithm, k-means, and uses the subsequent assignments as supervision to update\r
the weights of the network""" ;
    skos:prefLabel "DeepCluster" .

:DeepDrug a skos:Concept ;
    dcterms:source <https://www.researchgate.net/publication/346825980_DeepDrug_A_general_graph-based_deep_learning_framework_for_drug_relation_prediction> ;
    skos:definition "**DeepDrug** is a deep learning framework to overcome these shortcomings by using graph convolutional networks to learn the graphical representations of drugs and proteins such as molecular fingerprints and residual structures in order to boost the prediction accuracy." ;
    skos:prefLabel "DeepDrug" .

:DeepEnsembles a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1612.01474v3> ;
    skos:definition "" ;
    skos:prefLabel "Deep Ensembles" .

:DeepIR a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2108.07973v2> ;
    skos:definition "**DeepIR**, or **Deep InfraRed image processing**, is a thermal image processing framework for recovering high quality images from a very small set of images captured with camera motion. Enhancement is achieved by noting that camera motion, which is usually a hinderance, can be exploited to our advantage to separate a sequence of images into the scene-dependent radiant flux, and a slowly changing scene-independent non-uniformity. DeepIR combines the physics of microbolometer sensors, with powerful regularization capabilities by neural network-based representations. DeepIR relies on the key observation that jittering a camera, while unwanted in visible domain, is highly desirable in the thermal domain as it allows an accurate separation of the sensor-specific non-uniformities from the scene’s radiant flux." ;
    skos:prefLabel "DeepIR" .

:DeepLSTMReader a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1506.03340v3> ;
    skos:definition """The **Deep LSTM Reader** is a neural network for reading comprehension. We feed documents one word at a time into a Deep [LSTM](https://paperswithcode.com/method/lstm) encoder, after a delimiter we then also feed the query into the encoder. The model therefore processes each document query pair as a single long sequence. Given the embedded document and query the network predicts which token in the document answers the query.\r
\r
The model consists of a Deep LSTM cell with skip connections from each input $x\\left(t\\right)$ to every hidden layer, and from every hidden layer to the output $y\\left(t\\right)$:\r
\r
$$x'\\left(t, k\\right) = x\\left(t\\right)||y'\\left(t, k - 1\\right) \\text{,  } y\\left(t\\right) = y'\\left(t, 1\\right)|| \\dots ||y'\\left(t, K\\right) $$\r
\r
$$ i\\left(t, k\\right) =  \\left(W\\_{kxi}x'\\left(t, k\\right) + W\\_{khi}h(t - 1, k) + W\\_{kci}c\\left(t - 1, k\\right) + b\\_{ki}\\right) $$\r
\r
$$ f\\left(t, k\\right) =  \\left(W\\_{kxf}x\\left(t\\right) + W\\_{khf}h\\left(t - 1, k\\right) + W\\_{kcf}c\\left(t - 1, k\\right) + b\\_{kf}\\right) $$\r
\r
$$ c\\left(t, k\\right) = f\\left(t, k\\right)c\\left(t - 1, k\\right) + i\\left(t, k\\right)\\text{tanh}\\left(W\\_{kxc}x'\\left(t, k\\right) + W\\_{khc}h\\left(t -  1, k\\right) + b\\_{kc}\\right) $$\r
\r
$$ o\\left(t, k\\right) =  \\left(W\\_{kxo}x'\\left(t, k\\right) + W\\_{kho}h\\left(t - 1, k\\right) + W\\_{kco}c\\left(t, k\\right) + b\\_{ko}\\right) $$\r
\r
$$ h\\left(t, k\\right) = o\\left(t, k\\right)\\text{tanh}\\left(c\\left(t, k\\right)\\right) $$\r
\r
$$ y'\\left(t, k\\right) = W\\_{kyh}\\left(t, k\\right) + b\\_{ky} $$\r
\r
where || indicates vector concatenation, $h\\left(t, k\\right)$ is the hidden state for layer $k$ at time $t$, and $i$, $f$, $o$ are the input, forget, and output gates respectively. Thus our Deep LSTM Reader is defined by $g^{\\text{LSTM}}\\left(d, q\\right) = y\\left(|d|+|q|\\right)$ with input $x\\left(t\\right)$ the concatenation of $d$ and $q$ separated by the delimiter |||.""" ;
    skos:prefLabel "Deep LSTM Reader" .

:DeepLab a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1412.7062v4> ;
    rdfs:seeAlso <https://github.com/tensorflow/models/tree/master/research/deeplab> ;
    skos:definition "**DeepLab** is a semantic segmentation architecture. First, the input image goes through the network with the use of dilated convolutions. Then the output from the network is bilinearly interpolated and goes through the fully connected [CRF](https://paperswithcode.com/method/crf) to fine tune the result we obtain the final predictions." ;
    skos:prefLabel "DeepLab" .

:DeepLabv2 a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1606.00915v2> ;
    rdfs:seeAlso <https://github.com/tensorflow/models/tree/master/research/deeplab> ;
    skos:definition "**DeepLabv2** is an architecture for semantic segmentation that build on [DeepLab](https://paperswithcode.com/method/deeplab) with an atrous [spatial pyramid pooling](https://paperswithcode.com/method/spatial-pyramid-pooling) scheme. Here we have parallel dilated convolutions with different rates applied in the input feature map, which are then fused together. As objects of the same class can have different sizes in the image, [ASPP](https://paperswithcode.com/method/aspp) helps to account for different object sizes." ;
    skos:prefLabel "DeepLabv2" .

:DeepLabv3 a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1706.05587v3> ;
    rdfs:seeAlso <https://github.com/tensorflow/models/tree/master/research/deeplab> ;
    skos:definition """**DeepLabv3** is a semantic segmentation architecture that improves upon [DeepLabv2](https://paperswithcode.com/method/deeplabv2) with several modifications. To handle the problem of segmenting objects at multiple scales, modules are designed which employ atrous [convolution](https://paperswithcode.com/method/convolution) in cascade or in parallel to capture multi-scale context by adopting multiple atrous rates. Furthermore, the Atrous [Spatial Pyramid Pooling](https://paperswithcode.com/method/spatial-pyramid-pooling) module from DeepLabv2 augmented with image-level features encoding global context and further boost performance. \r
\r
The changes to the ASSP module are that the authors apply [global average pooling](https://paperswithcode.com/method/global-average-pooling) on the last feature map of the model, feed the resulting image-level features to a 1 × 1 convolution with 256 filters (and [batch normalization](https://paperswithcode.com/method/batch-normalization)), and then bilinearly upsample the feature to the desired spatial dimension. In the\r
end, the improved [ASPP](https://paperswithcode.com/method/aspp) consists of (a) one 1×1 convolution and three 3 × 3 convolutions with rates = (6, 12, 18) when output stride = 16 (all with 256 filters and batch normalization), and (b) the image-level features.\r
\r
Another interesting difference is that DenseCRF post-processing from DeepLabv2 is no longer needed.""" ;
    skos:prefLabel "DeepLabv3" .

:DeepMask a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1612.01057v4> ;
    skos:definition """**DeepMask** is an object proposal algorithm based on a convolutional neural network. Given an input image patch, DeepMask generates a class-agnostic mask and an associated score which estimates the likelihood of the patch fully containing a centered object (without any notion of an object category). The core of the model is a ConvNet which jointly predicts the mask and the object score. A large part of the network is shared between those two tasks: only the last few network\r
layers are specialized for separately outputting a mask and score prediction.""" ;
    skos:prefLabel "DeepMask" .

:DeepSIM a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2109.06151v3> ;
    skos:definition "**DeepSIM** is a generative model for conditional image manipulation based on a single image. The network learns to map between a primitive representation of the image to the image itself. At manipulation time, the generator allows for making complex image changes by modifying the primitive input representation and mapping it through the network. The choice of a primitive representations has an impact on the ease and expressiveness of the manipulations and can be automatic (e.g. edges), manual, or hybrid such as edges on top of segmentations." ;
    skos:prefLabel "DeepSIM" .

:DeepViT a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2103.11886v4> ;
    rdfs:seeAlso <https://github.com/zhoudaquan/dvit_repo/blob/f652676687384074a891e967c26e0a5daf5663a2/models/deep_vision_transformer.py#L55> ;
    skos:definition "**DeepViT** is a type of [vision transformer](https://paperswithcode.com/method/vision-transformer) that replaces the self-attention layer within the [transformer](https://paperswithcode.com/method/transformer) block with a [Re-attention module](https://paperswithcode.com/method/re-attention-module) to address the issue of attention collapse and enables training deeper ViTs." ;
    skos:prefLabel "DeepViT" .

:DeepVoice3 a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1710.07654v3> ;
    skos:definition """**Deep Voice 3 (DV3)** is a fully-convolutional attention-based neural text-to-speech system. The Deep Voice 3 architecture consists of three components:\r
\r
- Encoder: A fully-convolutional encoder, which converts textual features to an internal\r
learned representation.\r
\r
- Decoder: A fully-convolutional causal decoder, which decodes the learned representation\r
with a multi-hop convolutional attention mechanism into a low-dimensional audio representation (mel-scale spectrograms) in an autoregressive manner.\r
\r
- Converter: A fully-convolutional post-processing network, which predicts final vocoder\r
parameters (depending on the vocoder choice) from the decoder hidden states. Unlike the\r
decoder, the converter is non-causal and can thus depend on future context information.\r
\r
The overall objective function to be optimized is a linear combination of the losses from the decoder and the converter. The authors separate decoder and converter and apply multi-task training, because it makes attention learning easier in practice. To be specific, the loss for mel-spectrogram prediction guides training of the attention mechanism, because the attention is trained with the gradients from mel-spectrogram prediction besides vocoder parameter prediction.""" ;
    skos:prefLabel "Deep Voice 3" .

:DeepWalk a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1403.6652v2> ;
    skos:definition """**DeepWalk** learns embeddings (social representations) of a graph's vertices, by modeling a stream of short random walks. Social representations are latent features of the vertices that capture neighborhood similarity and community membership. These latent representations encode social relations in a continuous vector space with a relatively small number of dimensions. It generalizes neural language models to process a special language composed of a set of randomly-generated walks. \r
\r
The goal is to learn a latent representation, not only a probability distribution of node co-occurrences, and so as to introduce a mapping function $\\Phi \\colon v \\in V \\mapsto \\mathbb{R}^{|V|\\times d}$.\r
This mapping $\\Phi$ represents the latent social representation associated with each vertex $v$ in the graph. In practice, $\\Phi$ is represented by a $|V| \\times d$ matrix of free parameters.""" ;
    skos:prefLabel "DeepWalk" .

:Deflation a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2006.16228v2> ;
    skos:definition "**Deflation** is a video-to-image operation to transform a video network into a network that can ingest a single image. In the two types of video networks considered in the original paper, this deflation corresponds to the following operations: for [3D convolutional based networks](https://paperswithcode.com/method/3d-convolution), summing the 3D spatio-temporal filters over the temporal dimension to obtain 2D filters; for TSM networks,, turning off the channel shifting which results in a standard [residual architecture](https://paperswithcode.com/method/resnet) (ResNet50) for images." ;
    skos:prefLabel "Deflation" .

:DeformableAttentionModule a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2010.04159v4> ;
    skos:definition """**Deformable Attention Module** is an attention module used in the [Deformable DETR](https://paperswithcode.com/method/deformable-detr) architecture, which seeks to overcome one issue base [Transformer attention](https://paperswithcode.com/method/scaled) in that it looks over all possible spatial locations. Inspired by [deformable convolution](https://paperswithcode.com/method/deformable-convolution), the deformable attention module only attends to a small set of key sampling points around a reference point, regardless of the spatial size of the feature maps. By assigning only a small fixed number of keys for each query, the issues of convergence and feature spatial resolution can be mitigated.\r
\r
Given an input feature map $x \\in \\mathbb{R}^{C \\times H \\times W}$, let $q$ index a query element with content feature $\\mathbf{z}\\_{q}$ and a 2-d reference point $\\mathbf{p}\\_{q}$, the deformable attention feature is calculated by:\r
\r
$$ \\text{DeformAttn}\\left(\\mathbf{z}\\_{q}, \\mathbf{p}\\_{q}, \\mathbf{x}\\right)=\\sum\\_{m=1}^{M} \\mathbf{W}\\_{m}\\left[\\sum\\_{k=1}^{K} A\\_{m q k} \\cdot \\mathbf{W}\\_{m}^{\\prime} \\mathbf{x}\\left(\\mathbf{p}\\_{q}+\\Delta \\mathbf{p}\\_{m q k}\\right)\\right]\r
$$\r
\r
where $m$ indexes the attention head, $k$ indexes the sampled keys, and $K$ is the total sampled key number $(K \\ll H W) . \\Delta p_{m q k}$ and $A_{m q k}$ denote the sampling offset and attention weight of the $k^{\\text {th }}$ sampling point in the $m^{\\text {th }}$ attention head, respectively. The scalar attention weight $A_{m q k}$ lies in the range $[0,1]$, normalized by $\\sum_{k=1}^{K} A_{m q k}=1 . \\Delta \\mathbf{p}_{m q k} \\in \\mathbb{R}^{2}$ are of 2-d real numbers with unconstrained range. As $p\\_{q}+\\Delta p\\_{m q k}$ is fractional, bilinear interpolation is applied as in Dai et al. (2017) in computing $\\mathbf{x}\\left(\\mathbf{p}\\_{q}+\\Delta \\mathbf{p}\\_{m q k}\\right)$. Both $\\Delta \\mathbf{p}\\_{m q k}$ and $A\\_{m q k}$ are obtained via linear projection over the query feature $z\\_{q} .$ In implementation, the query feature $z\\_{q}$ is fed to a linear projection operator of $3 M K$ channels, where the first $2 M K$ channels encode the sampling offsets $\\Delta p\\_{m q k}$, and the remaining $M K$ channels are fed to a softmax operator to obtain the attention weights $A\\_{m q k}$.""" ;
    skos:prefLabel "Deformable Attention Module" .

:DeformableConvNets a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1703.06211v3> ;
    skos:altLabel "Deformable Convolutional Networks" ;
    skos:definition """Deformable ConvNets do not learn an affine transformation. They divide convolution into two steps, firstly sampling features on a regular grid $ \\mathcal{R} $ from the input feature map, then aggregating sampled features by weighted summation using a convolution kernel. The process can be written as:\r
\\begin{align}\r
    Y(p_{0}) &= \\sum_{p_i \\in \\mathcal{R}} w(p_{i}) X(p_{0} + p_{i})\r
\\end{align}\r
\\begin{align}\r
    \\mathcal{R}  &= \\{(-1,-1), (-1, 0), \\dots, (1, 1)\\}\r
\\end{align}\r
The deformable convolution augments the sampling process by introducing a group of learnable offsets $\\Delta p_{i}$ which can be generated by a lightweight CNN. Using the offsets $\\Delta p_{i}$, the deformable convolution can be formulated as:\r
\\begin{align}\r
    Y(p_{0}) &= \\sum_{p_i \\in \\mathcal{R}} w(p_{i}) X(p_{0} + p_{i} + \\Delta p_{i}). \r
\\end{align}\r
Through the above method, adaptive sampling is achieved.\r
However, $\\Delta p_{i}$ is a floating point value\r
unsuited to grid sampling. \r
To address this problem, bilinear interpolation is used. Deformable RoI pooling is also used, which greatly improves object detection. \r
\r
Deformable ConvNets adaptively select the important regions and enlarge the valid receptive field of convolutional neural networks; this is important in object detection and semantic segmentation tasks.""" ;
    skos:prefLabel "Deformable ConvNets" .

:DeformableConvolution a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1703.06211v3> ;
    rdfs:seeAlso <https://github.com/chengdazhi/Deformable-Convolution-V2-PyTorch/blob/2f57c5db49161bd6c899670a5e4fba50e6b8fd26/modules/deform_conv.py#L10> ;
    skos:definition "**Deformable convolutions** add 2D offsets to the regular grid sampling locations in the standard [convolution](https://paperswithcode.com/method/convolution). It enables free form deformation of the sampling grid. The offsets are learned from the preceding feature maps, via additional convolutional layers. Thus, the deformation is conditioned on the input features in a local, dense, and adaptive manner." ;
    skos:prefLabel "Deformable Convolution" .

:DeformableDETR a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2010.04159v4> ;
    skos:definition """**Deformable DETR** is an object detection method that aims mitigates the slow convergence and high complexity issues of [DETR](https://www.paperswithcode.com/method/detr). It combines the best of the sparse spatial sampling of [deformable convolution](https://paperswithcode.com/method/deformable-convolution), and the relation modeling capability of [Transformers](https://paperswithcode.com/methods/category/transformers). Specifically, it introduces a \r
 deformable attention module, which attends to a small set of sampling locations as a pre-filter for prominent key elements out of all the feature map pixels. The module can be naturally extended to aggregating multi-scale features, without the help of [FPN](https://paperswithcode.com/method/fpn).""" ;
    skos:prefLabel "Deformable DETR" .

:DeformableKernel a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1910.02940v2> ;
    rdfs:seeAlso <https://github.com/hangg7/deformable-kernels/blob/886c35389ab9fb96b36d2ab89c56186381acff32/deformable_kernels/modules/deform_kernel.py#L228> ;
    skos:definition """A **Deformable Kernels** is a type of convolutional operator for deformation modeling. DKs learn free-form offsets on kernel coordinates to deform the original kernel space towards specific data modality, rather than recomposing data. This can directly adapt the effective receptive field (ERF) while leaving the receptive field untouched. They can be used as a drop-in replacement of rigid kernels. \r
\r
As shown in the Figure, for each input patch, a local DK first generates a group of kernel offsets $\\{\\Delta \\mathcal{k}\\}$ from input feature patch using the light-weight generator $\\mathcal{G}$ (a 3$\\times$3 [convolution](https://paperswithcode.com/method/convolution) of rigid kernel). Given the original kernel weights $\\mathcal{W}$ and the offset group $\\{\\Delta \\mathcal{k}\\}$, DK samples a new set of kernel $\\mathcal{W}'$ using a bilinear sampler $\\mathcal{B}$. Finally, DK convolves the input feature map and the sampled kernels to complete the whole computation.""" ;
    skos:prefLabel "Deformable Kernel" .

:DeformablePosition-SensitiveRoIPooling a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1703.06211v3> ;
    skos:definition "**Deformable Position-Sensitive RoI Pooling** is similar to PS RoI Pooling but it adds an offset to each bin position in the regular bin partition. Offset learning follows the “fully convolutional” spirit. In the top branch, a convolutional layer generates the full spatial resolution offset fields. For each RoI (also for each class), PS RoI pooling is applied on such fields to obtain normalized offsets, which are then transformed to the real offsets, in the same way as in deformable RoI pooling." ;
    skos:prefLabel "Deformable Position-Sensitive RoI Pooling" .

:DeformableRoIPooling a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1703.06211v3> ;
    skos:definition "**Deformable RoI Pooling** adds an offset to each bin position in the regular bin partition of the RoI Pooling. Similarly, the offsets are learned from the preceding feature maps and the RoIs, enabling adaptive part localization for objects with different shapes." ;
    skos:prefLabel "Deformable RoI Pooling" .

:DeiT a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2012.12877v2> ;
    rdfs:seeAlso <https://github.com/facebookresearch/deit/blob/2aefd8fc8634d099c1495ce9dba2b6c6a921d611/models.py#L16> ;
    skos:altLabel "Data-efficient Image Transformer" ;
    skos:definition "A **Data-Efficient Image Transformer** is a type of [Vision Transformer](https://paperswithcode.com/method/vision-transformer) for image classification tasks. The model is trained using a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention." ;
    skos:prefLabel "DeiT" .

:DeltaConv a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2111.08799v5> ;
    skos:definition "Anisotropic convolution is a central building block of CNNs but challenging to transfer to surfaces. DeltaConv learns combinations and compositions of operators from vector calculus, which are a natural fit for curved surfaces. The result is a simple and robust anisotropic convolution operator for point clouds with state-of-the-art results." ;
    skos:prefLabel "DeltaConv" .

:Demon a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1910.04952v4> ;
    rdfs:seeAlso <https://github.com/JRC1995/DemonRangerOptimizer/blob/5a3e6e352ab766f96cd8d20eabd5b71843c595fe/optimizers.py#L205> ;
    skos:definition """**Decaying Momentum**, or **Demon**, is a stochastic optimizer motivated by decaying the total contribution of a gradient to all future updates. By decaying the momentum parameter, the total contribution of a gradient to all future updates is decayed. A particular gradient term $g\\_{t}$ contributes a total of  $\\eta\\sum\\_{i}\\beta^{i}$ of its "energy" to all future gradient updates, and this results in the geometric sum, $\\sum^{\\infty}\\_{i=1}\\beta^{i} = \\beta\\sum^{\\infty}\\_{i=0}\\beta^{i} = \\frac{\\beta}{\\left(1-\\beta\\right)}$. Decaying this sum results in the Demon algorithm. Letting $\\beta\\_{init}$ be the initial $\\beta$; then at the current step $t$ with total $T$ steps, the decay routine is given by solving the below for $\\beta\\_{t}$:\r
\r
$$ \\frac{\\beta\\_{t}}{\\left(1-\\beta\\_{t}\\right)} =  \\left(1-t/T\\right)\\beta\\_{init}/\\left(1-\\beta\\_{init}\\right)$$\r
\r
Where $\\left(1-t/T\\right)$ refers to the proportion of iterations remaining. Note that Demon typically requires no hyperparameter tuning as it is usually decayed to $0$ or a small negative value at time \r
$T$. Improved performance is observed by delaying the decaying. Demon can be applied to any gradient descent algorithm with a momentum parameter.""" ;
    skos:prefLabel "Demon" .

:DemonADAM a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1910.04952v4> ;
    rdfs:seeAlso <https://github.com/JRC1995/DemonRangerOptimizer/blob/5a3e6e352ab766f96cd8d20eabd5b71843c595fe/optimizers.py#L205> ;
    skos:definition """**Demon Adam** is a stochastic optimizer where the [Demon](https://paperswithcode.com/method/demon) momentum rule is applied to the [Adam](https://paperswithcode.com/method/adam) optimizer.\r
\r
$$ \\beta\\_{t} = \\beta\\_{init}\\cdot\\frac{\\left(1-\\frac{t}{T}\\right)}{\\left(1-\\beta\\_{init}\\right) + \\beta\\_{init}\\left(1-\\frac{t}{T}\\right)} $$\r
\r
$$ m\\_{t, i} = g\\_{t, i} + \\beta\\_{t}m\\_{t-1, i} $$\r
\r
$$ v\\_{t+1} = \\beta\\_{2}v\\_{t}  + \\left(1-\\beta\\_{2}\\right)g^{2}\\_{t} $$\r
\r
$$ \\theta_{t} = \\theta_{t-1} - \\eta\\frac{\\hat{m}\\_{t}}{\\sqrt{\\hat{v}\\_{t}} + \\epsilon}  $$""" ;
    skos:prefLabel "Demon ADAM" .

:DemonCM a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1910.04952v4> ;
    rdfs:seeAlso <https://github.com/JRC1995/DemonRangerOptimizer/blob/5a3e6e352ab766f96cd8d20eabd5b71843c595fe/optimizers.py#L205> ;
    skos:definition """**Demon CM**, or **SGD with Momentum and Demon**,  is the [Demon](https://paperswithcode.com/method/demon) momentum rule applied to [SGD with momentum](https://paperswithcode.com/method/sgd-with-momentum).\r
\r
$$ \\beta\\_{t} = \\beta\\_{init}\\cdot\\frac{\\left(1-\\frac{t}{T}\\right)}{\\left(1-\\beta\\_{init}\\right) + \\beta\\_{init}\\left(1-\\frac{t}{T}\\right)} $$\r
\r
$$ \\theta\\_{t+1} = \\theta\\_{t} - \\eta{g}\\_{t} + \\beta\\_{t}v\\_{t} $$\r
\r
$$ v\\_{t+1} = \\beta\\_{t}{v\\_{t}} - \\eta{g\\_{t}} $$""" ;
    skos:prefLabel "Demon CM" .

:DenoisedSmoothing a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2003.01908v2> ;
    skos:definition "**Denoised Smoothing** is a method for obtaining a provably robust classifier from a fixed pretrained one, without any additional training or fine-tuning of the latter. The basic idea is to prepend a custom-trained denoiser before the pretrained classifier, and then apply randomized smoothing. Randomized smoothing is a certified defense that converts any given classifier $f$ into a new smoothed classifier $g$ that is characterized by a non-linear Lipschitz property. When queried at a point $x$, the smoothed classifier $g$ outputs the class that is most likely to be returned by $f$ under isotropic Gaussian perturbations of its inputs. Unfortunately, randomized smoothing requires that the underlying classifier $f$ is robust to relatively large random Gaussian perturbations of the input, which is not the case for off-the-shelf pretrained models. By applying our custom-trained denoiser to the classifier $f$, we can effectively make $f$ robust to such Gaussian perturbations, thereby making it “suitable” for randomized smoothing." ;
    skos:prefLabel "Denoised Smoothing" .

:DenoisingAutoencoder a skos:Concept ;
    skos:definition """A **Denoising Autoencoder** is a modification on the [autoencoder](https://paperswithcode.com/method/autoencoder) to prevent the network learning the identity function. Specifically, if the autoencoder is too big, then it can just learn the data, so the output equals the input, and does not perform any useful representation learning or dimensionality reduction. Denoising autoencoders solve this problem by corrupting the input data on purpose, adding noise or masking some of the input values.\r
\r
Image Credit: [Kumar et al](https://www.semanticscholar.org/paper/Static-hand-gesture-recognition-using-stacked-Kumar-Nandi/5191ddf3f0841c89ba9ee592a2f6c33e4a40d4bf)""" ;
    skos:prefLabel "Denoising Autoencoder" .

:DenoisingScoreMatching a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1907.05600v3> ;
    skos:definition "Training a denoiser on signals gives you a powerful prior over this signal that you can then use to sample examples of this signal." ;
    skos:prefLabel "Denoising Score Matching" .

:DenseBlock a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1608.06993v5> ;
    rdfs:seeAlso <https://github.com/pytorch/vision/blob/1aef87d01eec2c0989458387fa04baebcc86ea7b/torchvision/models/densenet.py#L93> ;
    skos:definition "A **Dense Block** is a module used in convolutional neural networks that connects *all layers* (with matching feature-map sizes) directly with each other. It was originally proposed as part of the [DenseNet](https://paperswithcode.com/method/densenet) architecture. To preserve the feed-forward nature, each layer obtains additional inputs from all preceding layers and passes on its own feature-maps to all subsequent layers. In contrast to [ResNets](https://paperswithcode.com/method/resnet), we never combine features through summation before they are passed into a layer; instead, we combine features by concatenating them. Hence, the $\\ell^{th}$ layer has $\\ell$ inputs, consisting of the feature-maps of all preceding convolutional blocks. Its own feature-maps are passed on to all $L-\\ell$ subsequent layers. This introduces $\\frac{L(L+1)}{2}$  connections in an $L$-layer network, instead of just $L$, as in traditional architectures: \"dense connectivity\"." ;
    skos:prefLabel "Dense Block" .

:DenseConnections a skos:Concept ;
    skos:definition """**Dense Connections**, or **Fully Connected Connections**, are a type of layer in a deep neural network that use a linear operation where every input is connected to every output by a weight. This means there are $n\\_{\\text{inputs}}*n\\_{\\text{outputs}}$ parameters, which can lead to a lot of parameters for a sizeable network.\r
\r
$$h\\_{l} = g\\left(\\textbf{W}^{T}h\\_{l-1}\\right)$$\r
\r
where $g$ is an activation function.\r
\r
Image Source: Deep Learning by Goodfellow, Bengio and Courville""" ;
    skos:prefLabel "Dense Connections" .

:DenseContrastiveLearning a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2011.09157v2> ;
    skos:definition "**Dense Contrastive Learning** is a self-supervised learning method for dense prediction tasks. It implements self-supervised learning by optimizing a pairwise contrastive (dis)similarity loss at the pixel level between two views of input images. Contrasting with regular contrastive loss, the contrastive loss is computed between the single feature vectors outputted by the global projection head, at the level of global feature, while the dense contrastive loss is computed between the dense feature vectors outputted by the dense projection head, at the level of local feature." ;
    skos:prefLabel "Dense Contrastive Learning" .

:DenseNAS a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1906.09607v3> ;
    rdfs:seeAlso <https://github.com/JaminFong/DenseNAS> ;
    skos:definition "**DenseNAS** is a [neural architecture search](https://paperswithcode.com/method/neural-architecture-search) method that utilises a densely connected search space. The search space is represented as a dense super network, which is built upon designed routing blocks. In the super network, routing blocks are densely connected and we search for the best path between them to derive the final architecture. A chained cost estimation algorithm is used to approximate the model cost during the search." ;
    skos:prefLabel "DenseNAS" .

:DenseNAS-A a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1906.09607v3> ;
    rdfs:seeAlso <https://github.com/JaminFong/DenseNAS/blob/e479206f9d73808d4167c669bb1d045a73b6f470/models/search_space_mbv2.py#L7> ;
    skos:definition "**DenseNAS-A** is a mobile convolutional neural network discovered through the [DenseNAS](https://paperswithcode.com/method/densenas) [neural architecture search](https://paperswithcode.com/method/neural-architecture-search) method. The basic building block is MBConvs, or inverted bottleneck residuals, from the MobileNet architectures." ;
    skos:prefLabel "DenseNAS-A" .

:DenseNAS-B a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1906.09607v3> ;
    rdfs:seeAlso <https://github.com/JaminFong/DenseNAS/blob/e479206f9d73808d4167c669bb1d045a73b6f470/models/search_space_mbv2.py#L7> ;
    skos:definition "**DenseNAS-B** is a mobile convolutional neural network discovered through the [DenseNAS](https://paperswithcode.com/method/densenas) [neural architecture search](https://paperswithcode.com/method/neural-architecture-search) method. The basic building block is MBConvs, or inverted bottleneck residuals, from the [MobileNet](https://paperswithcode.com/method/mobilenetv2) architectures." ;
    skos:prefLabel "DenseNAS-B" .

:DenseNAS-C a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1906.09607v3> ;
    rdfs:seeAlso <https://github.com/JaminFong/DenseNAS/blob/e479206f9d73808d4167c669bb1d045a73b6f470/models/search_space_mbv2.py#L7> ;
    skos:definition "**DenseNAS-C** is a mobile convolutional neural network discovered through the [DenseNAS](https://paperswithcode.com/method/densenas) [neural architecture search](https://paperswithcode.com/method/neural-architecture-search) method. The basic building block is MBConvs, or inverted bottleneck residuals, from the [MobileNet](https://paperswithcode.com/method/mobilenetv2) architectures." ;
    skos:prefLabel "DenseNAS-C" .

:DenseNet a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1608.06993v5> ;
    rdfs:seeAlso <https://github.com/pytorch/vision/blob/6db1569c89094cf23f3bc41f79275c45e9fcb3f3/torchvision/models/densenet.py#L126> ;
    skos:definition "A **DenseNet** is a type of convolutional neural network that utilises [dense connections](https://paperswithcode.com/method/dense-connections) between layers, through [Dense Blocks](http://www.paperswithcode.com/method/dense-block), where we connect *all layers* (with matching feature-map sizes) directly with each other. To preserve the feed-forward nature, each layer obtains additional inputs from all preceding layers and passes on its own feature-maps to all subsequent layers." ;
    skos:prefLabel "DenseNet" .

:DenseNet-Elastic a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1812.05262v2> ;
    rdfs:seeAlso <https://github.com/allenai/elastic/blob/57345c600c63fbde163c41929d6d6dd894d408ce/models/densenet.py#L115> ;
    skos:definition "**DenseNet-Elastic** is a convolutional neural network that is a modification of a [DenseNet](https://paperswithcode.com/method/densenet) with elastic blocks (extra upsampling and downsampling)." ;
    skos:prefLabel "DenseNet-Elastic" .

:DenseSynthesizedAttention a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2005.00743v3> ;
    skos:definition """**Dense Synthesized Attention**, introduced with the [Synthesizer](https://paperswithcode.com/method/synthesizer) architecture, is a type of synthetic attention mechanism that replaces the notion of [query-key-values](https://paperswithcode.com/method/scaled) in the self-attention module and directly synthesizes the alignment matrix instead. Dense attention is conditioned on each input token. The method accepts an input $X \\in \\mathbb{R}^{l\\text{ x }d}$ and produces an output of $Y \\in \\mathbb{R}^{l\\text{ x }d}$. Here $l$ refers to the sequence length and $d$ refers to the dimensionality of the model. We first adopt $F\\left(.\\right)$, a parameterized function, for projecting input $X\\_{i}$ from $d$ dimensions to $l$ dimensions.\r
\r
$$B\\_{i} = F\\left(X\\_{i}\\right)$$\r
\r
where $F\\left(.\\right)$ is a parameterized function that maps $\\mathbb{R}^{d}$ to $\\mathbb{R}^{l}$ and $i$ is the $i$-th token of $X$. Intuitively, this can be interpreted as learning a token-wise projection to the sequence length $l$. Essentially, with this model, each token predicts weights for each token in the input sequence. In practice, a simple two layered feed-forward layer with [ReLU](https://paperswithcode.com/method/relu) activations for $F\\left(.\\right)$ is adopted:\r
\r
$$ F\\left(X\\right) = W\\left(\\sigma\\_{R}\\left(W(X) + b\\right)\\right) + b$$\r
\r
where $\\sigma\\_{R}$ is the ReLU activation function. Hence, $B$ is now of $\\mathbb{R}^{l\\text{ x }d}$. Given $B$, we now compute:\r
\r
$$ Y = \\text{Softmax}\\left(B\\right)G\\left(X\\right) $$\r
\r
where $G\\left(.\\right)$ is another parameterized function of $X$ that is analogous to $V$ (value) in the standard [Transformer](https://paperswithcode.com/method/transformer) model. This approach eliminates the [dot product](https://paperswithcode.com/method/scaled) altogether by replacing $QK^{T}$ in standard Transformers with the synthesizing function $F\\left(.\\right)$.""" ;
    skos:prefLabel "Dense Synthesized Attention" .

:Depth-wisePlaneSweeping a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2204.03039v3> ;
    skos:definition "" ;
    skos:prefLabel "Depth-wise Plane Sweeping" .

:DepthwiseConvolution a skos:Concept ;
    skos:definition """**Depthwise Convolution** is a type of convolution where we apply a single convolutional filter for each input channel. In the regular 2D [convolution](https://paperswithcode.com/method/convolution) performed over multiple input channels, the filter is as deep as the input and lets us freely mix channels to generate each element in the output. In contrast, depthwise convolutions keep each channel separate. To summarize the steps, we:\r
\r
1. Split the input and filter into channels.\r
2. We convolve each input with the respective filter.\r
3. We stack the convolved outputs together.\r
\r
Image Credit: [Chi-Feng Wang](https://towardsdatascience.com/a-basic-introduction-to-separable-convolutions-b99ec3102728)""" ;
    skos:prefLabel "Depthwise Convolution" .

:DepthwiseDilatedSeparableConvolution a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1811.11431v3> ;
    rdfs:seeAlso <https://github.com/dr-costas/dnd-sed/blob/2732762ffc1d64ebdcec4d8a07ab9a7dc2cde925/models/_modules/depthwise_separable_conv_block.py#L17> ;
    skos:definition "A **Depthwise Dilated Separable Convolution** is a type of [convolution](https://paperswithcode.com/method/convolution) that combines [depthwise separability](https://paperswithcode.com/method/depthwise-separable-convolution) with the use of [dilated convolutions](https://paperswithcode.com/method/dilated-convolution)." ;
    skos:prefLabel "Depthwise Dilated Separable Convolution" .

:DepthwiseFireModule a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1904.08900v2> ;
    skos:definition "A **Depthwise Fire Module** is a modification of a [Fire Module](https://paperswithcode.com/method/fire-module) with depthwise separable convolutions to improve the inference time performance. It is used in the [CornerNet](https://paperswithcode.com/method/cornernet)-Lite architecture for object detection." ;
    skos:prefLabel "Depthwise Fire Module" .

:DepthwiseSeparableConvolution a skos:Concept ;
    dcterms:source <http://openaccess.thecvf.com/content_cvpr_2017/html/Chollet_Xception_Deep_Learning_CVPR_2017_paper.html> ;
    rdfs:seeAlso <https://github.com/kwotsin/TensorFlow-Xception/blob/c42ad8cab40733f9150711be3537243278612b22/xception.py#L67> ;
    skos:definition """While [standard convolution](https://paperswithcode.com/method/convolution) performs the channelwise and spatial-wise computation in one step, **Depthwise Separable Convolution**  splits the computation into two steps: [depthwise convolution](https://paperswithcode.com/method/depthwise-convolution) applies a single convolutional filter per each input channel and [pointwise convolution](https://paperswithcode.com/method/pointwise-convolution) is used to create a linear combination of the output of the depthwise convolution. The comparison of standard convolution and depthwise separable convolution is shown to the right.\r
\r
Credit: [Depthwise Convolution Is All You Need for Learning Multiple Visual Domains](https://paperswithcode.com/paper/depthwise-convolution-is-all-you-need-for)""" ;
    skos:prefLabel "Depthwise Separable Convolution" .

:DetNAS a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1903.10979v4> ;
    rdfs:seeAlso <https://github.com/megvii-model/DetNAS/tree/master/distributed_arch_search> ;
    skos:definition "**DetNAS** is a [neural architecture search](https://paperswithcode.com/method/neural-architecture-search) algorithm for the design of better backbones for object detection. It is based on the technique of one-shot supernet, which contains all possible networks in the search space. The supernet is trained under the typical detector training schedule: ImageNet pre-training and detection fine-tuning. Then, the architecture search is performed on the trained supernet, using the detection task as the guidance. DetNAS uses evolutionary search as opposed to RL-based methods or gradient-based methods." ;
    skos:prefLabel "DetNAS" .

:DetNASNet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1903.10979v4> ;
    rdfs:seeAlso <https://github.com/megvii-model/DetNAS/blob/94623fd3c65934d1fba976c5c5d9d7f6b855ea90/Supernet-ImageNet/detnasnet.py#L5> ;
    skos:definition "**DetNASNet** is a convolutional neural network designed to be an object detection backbone and discovered through [DetNAS](https://paperswithcode.com/method/detnas) architecture search. It uses [ShuffleNet V2](https://paperswithcode.com/method/shufflenet-v2) blocks as its basic building block." ;
    skos:prefLabel "DetNASNet" .

:DetNet a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1804.06215v2> ;
    rdfs:seeAlso <https://github.com/tsing-cv/DetNet/blob/7987321606b3f44968e6f64ac4bd51ed01d8d981/DetNet.py#L163> ;
    skos:definition "**DetNet** is a backbone convolutional neural network for object detection. Different from traditional pre-trained models for ImageNet classification, DetNet maintains the spatial resolution of the features even though extra stages are included. DetNet attempts to stay efficient by employing a low complexity dilated bottleneck structure." ;
    skos:prefLabel "DetNet" .

:Detr a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2005.12872v3> ;
    skos:altLabel "Detection Transformer" ;
    skos:definition """**Detr**, or **Detection Transformer**, is a set-based object detector using a [Transformer](https://paperswithcode.com/method/transformer) on top of a convolutional backbone. It uses a conventional CNN backbone to learn a 2D representation of an input image. The model flattens it and supplements it with a positional encoding before passing it into a transformer encoder. A transformer decoder then takes as input a small fixed number of learned positional embeddings, which we call object queries, and additionally attends to the encoder output. We pass each output embedding of the decoder to a shared feed forward network (FFN) that predicts either a detection (class\r
and bounding box) or a “no object” class.""" ;
    skos:prefLabel "Detr" .

:DiCENet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1906.03516v3> ;
    rdfs:seeAlso <https://github.com/sacmehta/EdgeNets/blob/c4c515700df32bb0417de3dd1eb39cfe23bf9148/model/classification/dicenet.py#L44> ;
    skos:definition "**DiCENet** is a convolutional neural network architecture that utilizes dimensional convolutions (and dimension-wise fusion). The dimension-wise convolutions apply light-weight convolutional filtering across each dimension of the input tensor while dimension-wise fusion efficiently combines these dimension-wise representations; allowing the [DiCE Unit](https://paperswithcode.com/method/dice-unit) in the network to efficiently encode spatial and channel-wise information contained in the input tensor." ;
    skos:prefLabel "DiCENet" .

:DiCEUnit a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1906.03516v3> ;
    rdfs:seeAlso <https://github.com/sacmehta/EdgeNets/blob/c4c515700df32bb0417de3dd1eb39cfe23bf9148/model/classification/dicenet.py#L14> ;
    skos:definition """A **DiCE Unit** is an image model block that is built using dimension-wise convolutions and dimension-wise fusion.  The dimension-wise convolutions apply light-weight convolutional filtering across each dimension of the input tensor while dimension-wise fusion efficiently combines these dimension-wise representations; allowing the DiCE unit to efficiently encode spatial and channel-wise information contained in the input tensor. \r
\r
Standard convolutions encode spatial and channel-wise information simultaneously, but they are computationally expensive. To improve the efficiency of standard convolutions, separable [convolution](https://paperswithcode.com/method/convolution) are introduced, where spatial and channelwise information are encoded separately using depth-wise and point-wise convolutions, respectively. Though this factorization is effective, it puts a significant computational load on point-wise convolutions and makes them a computational bottleneck.\r
\r
DiCE Units utilize a dimension-wise convolution to encode depth-wise, width-wise, and height-wise information independently. The dimension-wise convolutions encode local information from different dimensions of the input tensor, but do not capture global information. One approach is a [pointwise convolution](https://paperswithcode.com/method/pointwise-convolution), but it is computationally expensive, so instead dimension-wise fusion factorizes the point-wise convolution in two steps: (1) local fusion and (2) global fusion.""" ;
    skos:prefLabel "DiCE Unit" .

:DiceLoss a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1707.03237v3> ;
    skos:definition """\\begin{equation}\r
DiceLoss\\left( y, \\overline{p} \\right) = 1 - \\dfrac{\\left(  2y\\overline{p} + 1 \\right)} {\\left( y+\\overline{p } + 1 \\right)}\r
\\end{equation}""" ;
    skos:prefLabel "Dice Loss" .

:DiffAugment a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2006.10738v4> ;
    rdfs:seeAlso <https://github.com/mit-han-lab/data-efficient-gans/blob/ed7e725ae83c7bb4d2b0eace558ba1609d098e66/DiffAugment_pytorch.py#L9> ;
    skos:definition """**Differentiable Augmentation (DiffAugment)** is a set of differentiable image transformations used to augment data during [GAN](https://paperswithcode.com/method/gan) training. The transformations are applied to the real and generated images. It enables the gradients to be propagated through the augmentation back to the generator, regularizes\r
the discriminator without manipulating the target distribution, and maintains the balance of training\r
dynamics. Three choices of transformation are preferred by the authors in their experiments: Translation, [CutOut](https://paperswithcode.com/method/cutout), and Color.""" ;
    skos:prefLabel "DiffAugment" .

:DiffPool a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1806.08804v4> ;
    skos:definition """DiffPool is a differentiable graph pooling module that can generate hierarchical representations of graphs and can be combined with various graph neural network architectures in an end-to-end fashion. DiffPool learns a differentiable soft cluster assignment for nodes at each layer of a deep GNN, mapping nodes to a set of clusters, which then form the coarsened input for the next GNN layer.\r
\r
Description and image from: [Hierarchical Graph Representation Learning with Differentiable Pooling](https://arxiv.org/pdf/1806.08804.pdf)""" ;
    skos:prefLabel "DiffPool" .

:DifferNet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2008.12577v1> ;
    skos:definition "" ;
    skos:prefLabel "DifferNet" .

:DifferentiableHyperparameterSearch a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1903.09900v1> ;
    skos:definition "Differentiable simultaneous optimization of hyperparameters and neural network architecture. Also a [Neural Architecture Search](https://paperswithcode.com/method/neural-architecture-search) (NAS) method." ;
    skos:prefLabel "Differentiable Hyperparameter Search" .

:DifferentiableNAS a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1806.09055v2> ;
    skos:altLabel "Differentiable Neural Architecture Search" ;
    skos:definition "" ;
    skos:prefLabel "Differentiable NAS" .

:DifferentialDiffusion a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2306.00950v1> ;
    skos:definition "**Differential Diffusion** is an enhancement of image-to-image diffusion models that adds the ability to control the amount of change applied to each image fragment via a change map." ;
    skos:prefLabel "Differential Diffusion" .

:Differentialattentionforvisualquestionanswering a skos:Concept ;
    skos:definition "In this paper we aim to answer questions based on images when provided with a dataset of question-answer pairs for a number of images during training. A number of methods have focused on solving this problem by using image based attention. This is done by focusing on a specific part of the image while answering the question. Humans also do so when solving this problem. However, the regions that the previous systems focus on are not correlated with the regions that humans focus on. The accuracy is limited due to this drawback. In this paper, we propose to solve this problem by using an exemplar based method. We obtain one or more supporting and opposing exemplars to obtain a differential attention region. This differential attention is closer to human attention than other image based attention methods. It also helps in obtaining improved accuracy when answering questions. The method is evaluated on challenging benchmark datasets. We perform better than other image based attention methods and are competitive with other state of the art methods that focus on both image and questions." ;
    skos:prefLabel "Differential attention for visual question answering" .

:Diffusion a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2006.11239v2> ;
    skos:definition """Diffusion models generate samples by gradually\r
removing noise from a signal, and their training objective can be expressed as a reweighted variational lower-bound (https://arxiv.org/abs/2006.11239).""" ;
    skos:prefLabel "Diffusion" .

:DilatedBottleneckBlock a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1804.06215v2> ;
    rdfs:seeAlso <https://github.com/tsing-cv/DetNet/blob/7987321606b3f44968e6f64ac4bd51ed01d8d981/DetNet.py#L131> ;
    skos:definition "**Dilated Bottleneck Block** is an image model block used in the [DetNet](https://paperswithcode.com/method/detnet) convolutional neural network architecture. It employs a bottleneck structure with dilated convolutions to efficiently enlarge the receptive field." ;
    skos:prefLabel "Dilated Bottleneck Block" .

:DilatedBottleneckwithProjectionBlock a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1804.06215v2> ;
    rdfs:seeAlso <https://github.com/tsing-cv/DetNet/blob/7987321606b3f44968e6f64ac4bd51ed01d8d981/DetNet.py#L131> ;
    skos:definition "**Dilated Bottleneck with Projection Block** is an image model block used in the [DetNet](https://paperswithcode.com/method/detnet) convolutional neural network architecture. It employs a bottleneck structure with dilated convolutions to efficiently enlarge the receptive field. It uses a [1x1 convolution](https://paperswithcode.com/method/1x1-convolution) to ensure the spatial size stays fixed." ;
    skos:prefLabel "Dilated Bottleneck with Projection Block" .

:DilatedCausalConvolution a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1609.03499v2> ;
    skos:definition "A **Dilated Causal Convolution** is a [causal convolution](https://paperswithcode.com/method/causal-convolution) where the filter is applied over an area larger than its length by skipping input values with a certain step. A dilated causal [convolution](https://paperswithcode.com/method/convolution) effectively allows the network to have very large receptive fields with just a few layers." ;
    skos:prefLabel "Dilated Causal Convolution" .

:DilatedConvolution a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1511.07122v3> ;
    rdfs:seeAlso <https://github.com/pytorch/pytorch/blob/ecb88c5d11895a68e5f20917d27a0debbc0f0697/torch/nn/modules/conv.py#L260> ;
    skos:definition """**Dilated Convolutions** are a type of [convolution](https://paperswithcode.com/method/convolution) that “inflate” the kernel by inserting holes between the kernel elements. An additional parameter $l$ (dilation rate) indicates how much the kernel is widened. There are usually $l-1$ spaces inserted between kernel elements. \r
\r
Note that concept has existed in past literature under different names, for instance the *algorithme a trous*,  an algorithm for wavelet decomposition (Holschneider et al., 1987; Shensa, 1992).""" ;
    skos:prefLabel "Dilated Convolution" .

:DilatedSlidingWindowAttention a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2004.05150v2> ;
    rdfs:seeAlso <https://github.com/allenai/longformer/blob/74523f7f2897b3a5ffcdb537f67a03f83aa1affb/longformer/sliding_chunks.py#L40> ;
    skos:definition """**Dilated Sliding Window Attention** is an attention pattern for attention-based models. It was proposed as part of the [Longformer](https://paperswithcode.com/method/longformer) architecture. It is motivated by the fact that non-sparse attention in the original [Transformer](https://paperswithcode.com/method/transformer) formulation has a [self-attention component](https://paperswithcode.com/method/scaled) with $O\\left(n^{2}\\right)$ time and memory complexity where $n$ is the input sequence length and thus, is not efficient to scale to long inputs. \r
\r
Compared to a [Sliding Window Attention](https://paperswithcode.com/method/sliding-window-attention) pattern, we can further increase the receptive field without increasing computation by making the sliding window "dilated". This is analogous to [dilated CNNs](https://paperswithcode.com/method/dilated-convolution) where the window has gaps of size dilation $d$. Assuming a fixed $d$ and $w$ for all layers, the receptive field is $l × d × w$, which can reach tens of thousands of tokens even for small values of $d$.""" ;
    skos:prefLabel "Dilated Sliding Window Attention" .

:DimConv a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1906.03516v3> ;
    rdfs:seeAlso <https://github.com/sacmehta/EdgeNets/blob/c4c515700df32bb0417de3dd1eb39cfe23bf9148/nn_layers/dice.py#L62> ;
    skos:altLabel "Dimension-wise Convolution" ;
    skos:definition """A **Dimension-wise Convolution**, or **DimConv**, is a type of [convolution](https://paperswithcode.com/method/convolution) that can encode depth-wise, width-wise, and height-wise information independently. To achieve this, DimConv extends depthwise convolutions to all dimensions of the input tensor $X \\in \\mathbb{R}^{D\\times{H}\\times{W}}$, where $W$, $H$, and $D$ corresponds to width, height, and depth of $X$. DimConv has three branches, one branch per dimension. These branches apply $D$ depth-wise convolutional kernels $k\\_{D} \\in \\mathbb{R}^{1\\times{n}\\times{n}}$ along depth, $W$ width-wise convolutional kernels $k\\_{W} \\in \\mathbb{R}^{n\\times{1}\\times{1}}$ along width, and $H$ height-wise convolutional kernels $k\\_{H} \\in \\mathbb{R}^{n\\times{1}\\times{n}}$ kernels along height\r
to produce outputs $Y\\_{D}$, $Y\\_{W}$, and $Y\\_{H} \\in \\mathbb{R}^{D\\times{H}\\times{W}}$ that\r
encode information from all dimensions of the input tensor. The outputs of these independent branches are concatenated along the depth dimension, such that the first spatial plane of $Y\\_{D}$, $Y\\_{W}$, and $Y\\_{H}$ are put together and so on, to produce the output $Y\\_{Dim} = ${$Y\\_{D}$, $Y\\_{W}$, $Y\\_{H}$} $\\in \\mathbb{R}^{3D\\times{H}\\times{W}}$.""" ;
    skos:prefLabel "DimConv" .

:DimFuse a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1906.03516v3> ;
    skos:altLabel "Dimension-wise Fusion" ;
    skos:definition "**Dimension-wise Fusion** is an image model block that attempts to capture global information by combining features globally. It is an alternative to point-wise [convolution](https://paperswithcode.com/method/convolution). A point-wise convolutional layer applies $D$ point-wise kernels $\\mathbf{k}\\_p \\in \\mathbb{R}^{3D \\times 1 \\times 1}$ and performs $3D^2HW$ operations to combine dimension-wise representations of $\\mathbf{Y_{Dim}} \\in \\mathbb{R}^{3D \\times H \\times W}$ and produce an output $\\mathbf{Y} \\in \\mathbb{R}^{D \\times H \\times W}$. This is computationally expensive. Dimension-wise fusion is an alternative that can allow us to combine representations of $\\mathbf{Y\\_{Dim}}$ efficiently. As illustrated in the Figure to the right, it factorizes the point-wise convolution in two steps: (1) local fusion and (2) global fusion." ;
    skos:prefLabel "DimFuse" .

:DirectionalSparseFiltering a skos:Concept ;
    dcterms:source <https://ieeexplore.ieee.org/document/7952589> ;
    skos:altLabel "Directional Sparse FIltering" ;
    skos:definition "" ;
    skos:prefLabel "Directional Sparse Filtering" .

:DiscreteCosineTransform a skos:Concept ;
    skos:definition """**Discrete Cosine Transform (DCT)** is an orthogonal transformation method that decomposes an\r
image to its spatial frequency spectrum. It expresses a finite sequence of data points in terms of a sum of cosine functions oscillating at different frequencies. It is used a lot in compression tasks, e..g image compression where for example high-frequency components can be discarded. It is a type of Fourier-related Transform, similar to discrete fourier transforms (DFTs), but only using real numbers.\r
\r
Image Credit: [Wikipedia](https://en.wikipedia.org/wiki/Discrete_cosine_transform#/media/File:Example_dft_dct.svg)""" ;
    skos:prefLabel "Discrete Cosine Transform" .

:DiscriminativeAdversarialSearch a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2002.10375v2> ;
    skos:definition "**Discriminative Adversarial Search**, or **DAS**, is a sequence decoding approach which aims to alleviate the effects of exposure bias and to optimize on the data distribution itself rather than for external metrics. Inspired by generative adversarial networks (GANs), wherein a discriminator is used to improve the generator, DAS differs from GANs in that the generator parameters are not updated at training time and the discriminator is only used to drive sequence generation at inference time." ;
    skos:prefLabel "Discriminative Adversarial Search" .

:DiscriminativeFine-Tuning a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1801.06146v5> ;
    rdfs:seeAlso <https://github.com/fastai/fastai/blob/43001e17ba469308e9688dfe99a891018bcf7ad4/courses/dl2/imdb_scripts/finetune_lm.py#L132> ;
    skos:definition """**Discriminative Fine-Tuning** is a fine-tuning strategy that is used for [ULMFiT](https://paperswithcode.com/method/ulmfit) type models. Instead of using the same learning rate for all layers of the model, discriminative fine-tuning allows us to tune each layer with different learning rates. For context, the regular stochastic gradient descent ([SGD](https://paperswithcode.com/method/sgd)) update of a model’s parameters $\\theta$ at time step $t$ looks like the following (Ruder, 2016):\r
\r
$$ \\theta\\_{t} = \\theta\\_{t-1} − \\eta\\cdot\\nabla\\_{\\theta}J\\left(\\theta\\right)$$\r
\r
where $\\eta$ is the learning rate and $\\nabla\\_{\\theta}J\\left(\\theta\\right)$ is the gradient with regard to the model’s objective function. For discriminative fine-tuning, we split the parameters $\\theta$ into {$\\theta\\_{1}, \\ldots, \\theta\\_{L}$} where $\\theta\\_{l}$ contains the parameters of the model at the $l$-th layer and $L$ is the number of layers of the model. Similarly, we obtain {$\\eta\\_{1}, \\ldots, \\eta\\_{L}$} where $\\theta\\_{l}$ where $\\eta\\_{l}$ is the learning rate of the $l$-th layer. The SGD update with discriminative finetuning is then:\r
\r
$$ \\theta\\_{t}^{l} = \\theta\\_{t-1}^{l} - \\eta^{l}\\cdot\\nabla\\_{\\theta^{l}}J\\left(\\theta\\right) $$\r
\r
The authors find that empirically it worked well to first choose the learning rate $\\eta^{L}$ of the last layer by fine-tuning only the last layer and using $\\eta^{l-1}=\\eta^{l}/2.6$ as the learning rate for lower layers.""" ;
    skos:prefLabel "Discriminative Fine-Tuning" .

:DiscriminativeRegularization a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1602.03220v4> ;
    rdfs:seeAlso <https://github.com/vdumoulin/discgen/blob/419687c218ac349586d12531764547560c8fe434/experiments/train_celeba_vae.py#L271> ;
    skos:definition "**Discriminative Regularization** is a regularization technique for [variational autoencoders](https://paperswithcode.com/methods/category/likelihood-based-generative-models) that uses representations from discriminative classifiers to augment the [VAE](https://paperswithcode.com/method/vae) objective function (the lower bound) corresponding to a generative model. Specifically, it encourages the model’s reconstructions to be close to the data example in a representation space defined by the hidden layers of highly-discriminative, neural network based classifiers." ;
    skos:prefLabel "Discriminative Regularization" .

:DisentangledAttentionMechanism a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2006.03654v6> ;
    skos:definition "**Disentangled Attention Mechanism** is an attention mechanism used in the [DeBERTa](https://paperswithcode.com/method/deberta) architecture. Unlike [BERT](https://paperswithcode.com/method/bert) where each word in the input layer is represented using a vector which is the sum of its word (content) embedding and position embedding, each word in DeBERTa is represented using two vectors that encode its content and position, respectively, and the attention weights among words are computed using disentangled matrices based on their contents and relative positions, respectively. This is motivated by the observation that the attention weight of a word pair depends on not only their contents but their relative positions. For example, the dependency between the words “deep” and “learning” is much stronger when they occur next to each other than when they occur in different sentences." ;
    skos:prefLabel "Disentangled Attention Mechanism" .

:DisentangledAttributionCurves a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1905.07631v1> ;
    rdfs:seeAlso <https://github.com/csinva/disentangled-attribution-curves> ;
    skos:definition """**Disentangled Attribution Curves (DAC)** provide interpretations of tree ensemble methods in the form of (multivariate) feature importance curves. For a given variable, or group of variables, [DAC](https://paperswithcode.com/method/dac) plots the importance of a variable(s) as their value changes.\r
\r
The Figure to the right shows an example. The tree depicts a decision tree which performs binary classification using two features (representing the XOR function). In this problem, knowing the value of one of the features without knowledge of the other feature yields no information - the classifier still has a 50% chance of predicting either class. As a result, DAC produces curves which assign 0 importance to either feature on its own. Knowing both features yields perfect information about the classifier, and thus the DAC curve for both features together correctly shows that the interaction of the features produces the model’s predictions.""" ;
    skos:prefLabel "Disentangled Attribution Curves" .

:DispR-CNN a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2004.03572v1> ;
    skos:definition "**Disp R-CNN** is a 3D object detection system for stereo images. It utilizes an instance disparity estimation network (iDispNet) that predicts disparity only for pixels on objects of interest and learns a category-specific shape prior for more accurate disparity estimation. To address the challenge from scarcity of disparity annotation in training, a statistical shape model is used to generate dense disparity pseudo-ground-truth without the need of LiDAR point clouds." ;
    skos:prefLabel "Disp R-CNN" .

:DistDGL a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2010.05337v3> ;
    skos:definition "**DistDGL** is a system for training GNNs in a mini-batch fashion on a cluster of machines. It is is based on the Deep Graph Library (DGL), a popular GNN development framework. DistDGL distributes the graph and its associated data (initial features and embeddings) across the machines and uses this distribution to derive a computational decomposition by following an owner-compute rule. DistDGL follows a synchronous training approach and allows ego-networks forming the mini-batches to include non-local nodes. To minimize the overheads associated with distributed computations, DistDGL uses a high-quality and light-weight mincut graph partitioning algorithm along with multiple balancing constraints. This allows it to reduce communication overheads and statically balance the computations. It further reduces the communication by replicating halo nodes and by using sparse embedding updates. The combination of these design choices allows DistDGL to train high-quality models while achieving high parallel efficiency and memory scalability" ;
    skos:prefLabel "DistDGL" .

:DistanceNet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2001.04362v3> ;
    skos:definition "**DistanceNet** is a learning algorithm for multi-source domain adaptation that uses various distance measures, or a mixture of these distance measures, as an additional loss function to be minimized jointly with the task's loss function, so as to achieve better unsupervised domain adaptation." ;
    skos:prefLabel "DistanceNet" .

:DistilBERT a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1910.01108v4> ;
    skos:definition "**DistilBERT**  is a small, fast, cheap and light [Transformer](https://paperswithcode.com/method/transformer) model based on the [BERT](https://paperswithcode.com/method/bert) architecture. Knowledge distillation is performed during the pre-training phase to reduce the size of a BERT model by 40%. To leverage the inductive biases learned by larger models during pre-training, the authors introduce a triple loss combining language modeling, distillation and cosine-distance losses." ;
    skos:prefLabel "DistilBERT" .

:DistributedShampoo a skos:Concept ;
    dcterms:source <https://openreview.net/forum?id=Sc8cY4Jpi3s> ;
    rdfs:seeAlso <https://github.com/google-research/google-research/tree/master/scalable_shampoo> ;
    skos:definition """A scalable second order optimization algorithm for deep learning.\r
\r
Optimization in machine learning, both theoretical and applied, is presently dominated by first-order gradient methods such as stochastic gradient descent. Second-order optimization methods, that involve second derivatives and/or second order statistics of the data, are far less prevalent despite strong theoretical properties, due to their prohibitive computation, memory and communication costs. In an attempt to bridge this gap between theoretical and practical optimization, we present a scalable implementation of a second-order preconditioned method (concretely, a variant of full-matrix Adagrad), that along with several critical algorithmic and numerical improvements, provides significant convergence and wall-clock time improvements compared to conventional first-order methods on state-of-the-art deep models. Our novel design effectively utilizes the prevalent heterogeneous hardware architecture for training deep models, consisting of a multicore CPU coupled with multiple accelerator units. We demonstrate superior performance compared to state-of-the-art on very large learning tasks such as machine translation with Transformers, language modeling with BERT, click-through rate prediction on Criteo, and image classification on ImageNet with ResNet-50.""" ;
    skos:prefLabel "Distributed Shampoo" .

:DistributionalGeneralization a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2009.08092v2> ;
    skos:definition "**Distributional Generalization** is a type of generalization that roughly states that outputs of a classifier at train and test time are close as distributions, as opposed to close in just their average error. This behavior is not captured by classical generalization, which would only consider the average error and not the distribution of errors over the input domain." ;
    skos:prefLabel "Distributional Generalization" .

:Dorylus a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2105.11118v2> ;
    skos:definition """**Dorylus** is a distributed system for training graph neural networks which uses cheap CPU servers and Lambda threads. It scales to\r
large billion-edge graphs with low-cost cloud resources.""" ;
    skos:prefLabel "Dorylus" .

:Dot-ProductAttention a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1508.04025v5> ;
    rdfs:seeAlso <https://github.com/heykeetae/Self-Attention-GAN/blob/8714a54ba5027d680190791ba3a6bb08f9c9a129/sagan_models.py#L32> ;
    skos:definition """**Dot-Product Attention** is an attention mechanism where the alignment score function is calculated as: \r
\r
$$f_{att}\\left(\\textbf{h}_{i}, \\textbf{s}\\_{j}\\right) = h\\_{i}^{T}s\\_{j}$$\r
\r
It is equivalent to [multiplicative attention](https://paperswithcode.com/method/multiplicative-attention) (without a trainable weight matrix, assuming this is instead an identity matrix). Here $\\textbf{h}$ refers to the hidden states for the encoder, and $\\textbf{s}$ is the hidden states for the decoder. The function above is thus a type of alignment score function. \r
\r
Within a neural network, once we have the alignment scores, we calculate the final scores/weights using a [softmax](https://paperswithcode.com/method/softmax) function of these alignment scores (ensuring it sums to 1).""" ;
    skos:prefLabel "Dot-Product Attention" .

:DouZero a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2106.06135v1> ;
    skos:definition "**DouZero** is an AI system for the card game DouDizhu that enhances traditional Monte-Carlo methods with deep neural networks, action encoding, and parallel actors. The [Q-network](https://paperswithcode.com/method/dqn) of DouZero consists of an [LSTM](https://paperswithcode.com/method/lstm) to encode historical actions and six layers of [MLP](https://paperswithcode.com/method/feedforward-network) with hidden dimension of 512. The network predicts a value for a given state-action pair based on the concatenated representation of action and state." ;
    skos:prefLabel "DouZero" .

:DoubleDQN a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1509.06461v3> ;
    skos:definition """A **Double Deep Q-Network**, or **Double DQN** utilises [Double Q-learning](https://paperswithcode.com/method/double-q-learning) to reduce overestimation by decomposing the max operation in the target into action selection and action evaluation. We evaluate the greedy policy according to the online network, but we use the target network to estimate its value.  The update is the same as for [DQN](https://paperswithcode.com/method/dqn), but replacing the target $Y^{DQN}\\_{t}$ with:\r
\r
$$ Y^{DoubleDQN}\\_{t} = R\\_{t+1}+\\gamma{Q}\\left(S\\_{t+1}, \\arg\\max\\_{a}Q\\left(S\\_{t+1}, a; \\theta\\_{t}\\right);\\theta\\_{t}^{-}\\right) $$\r
\r
Compared to the original formulation of Double [Q-Learning](https://paperswithcode.com/method/q-learning), in Double DQN the weights of the second network $\\theta^{'}\\_{t}$ are replaced with the weights of the target network $\\theta\\_{t}^{-}$ for the evaluation of the current greedy policy.""" ;
    skos:prefLabel "Double DQN" .

:DoubleQ-learning a skos:Concept ;
    dcterms:source <http://papers.nips.cc/paper/3964-double-q-learning> ;
    skos:definition """**Double Q-learning** is an off-policy reinforcement learning algorithm that utilises double estimation to counteract overestimation problems with traditional Q-learning. \r
\r
The max operator in standard [Q-learning](https://paperswithcode.com/method/q-learning) and [DQN](https://paperswithcode.com/method/dqn) uses the same values both to select and to evaluate an action. This makes it more likely to select overestimated values, resulting in overoptimistic value estimates. To prevent this, we can decouple the selection from the evaluation, which is the idea behind Double Q-learning:\r
\r
$$ Y^{Q}\\_{t} = R\\_{t+1} + \\gamma{Q}\\left(S\\_{t+1}, \\arg\\max\\_{a}Q\\left(S\\_{t+1}, a; \\mathbb{\\theta}\\_{t}\\right);\\mathbb{\\theta}\\_{t}\\right) $$\r
\r
The Double Q-learning error can then be written as:\r
\r
$$ Y^{DoubleQ}\\_{t} = R\\_{t+1} + \\gamma{Q}\\left(S\\_{t+1}, \\arg\\max\\_{a}Q\\left(S\\_{t+1}, a; \\mathbb{\\theta}\\_{t}\\right);\\mathbb{\\theta}^{'}\\_{t}\\right) $$\r
\r
Here the selection of the action in the $\\arg\\max$ is still due to the online weights $\\theta\\_{t}$. But we use a second set of weights $\\mathbb{\\theta}^{'}\\_{t}$ to fairly evaluate the value of this policy.\r
\r
Source: [Deep Reinforcement Learning with Double Q-learning](https://paperswithcode.com/paper/deep-reinforcement-learning-with-double-q)""" ;
    skos:prefLabel "Double Q-learning" .

:DraftingNetwork a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2104.05376v2> ;
    skos:definition """**Drafting Network** is a style transfer module designed to transfer global style patterns in low-resolution, since global patterns can be transferred easier in low resolution due to larger receptive field and less local details. To achieve single style transfer, earlier work trained an encoder-decoder module, where only the content image is used as input. To better combine the style feature and the content feature, the Drafting Network adopts the [AdaIN module](https://paperswithcode.com/method/adaptive-instance-normalization).\r
\r
The architecture of Drafting Network is shown in the Figure, which includes an encoder, several AdaIN modules and a decoder. (1) The encoder is a pre-trained [VGG](https://paperswithcode.com/method/vgg)-19 network, which is fixed during training. Given $\\bar{x}\\_{c}$ and $\\bar{x}\\_{s}$, the VGG encoder extracts features in multiple granularity at 2_1, 3_1 and 4_1 layers. (2) Then, we apply feature modulation between the content and style feature using AdaIN modules after 2_1, 3_1 and 4_1 layers, respectively. (3) Finally, in each granularity of decoder, the corresponding feature from the AdaIN module is merged via a [skip-connection](https://paperswithcode.com/methods/category/skip-connections). Here, skip-connections after AdaIN modules in both low and high levels are leveraged to help to reserve content structure, especially for low-resolution image.""" ;
    skos:prefLabel "Drafting Network" .

:Dreamix a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2302.01329v1> ;
    skos:altLabel "Dreamix: video diffusion models are general video editors" ;
    skos:definition "" ;
    skos:prefLabel "Dreamix" .

:DropAttack a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2108.12805v1> ;
    skos:definition "**DropAttack** is an adversarial training method that adds intentionally worst-case adversarial perturbations to both the input and hidden layers in different dimensions and minimizes the adversarial risks generated by each layer." ;
    skos:prefLabel "DropAttack" .

:DropBlock a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1810.12890v1> ;
    rdfs:seeAlso <https://github.com/miguelvr/dropblock/blob/7fb8fbfcb197a4bb57dc9193bcd6f375ff683f85/dropblock/dropblock.py#L6> ;
    skos:definition "**DropBlock** is a structured form of [dropout](https://paperswithcode.com/method/dropout) directed at regularizing convolutional networks. In DropBlock, units in a contiguous region of a feature map are dropped together.  As DropBlock discards features in a correlated area, the networks must look elsewhere for evidence to fit the data." ;
    skos:prefLabel "DropBlock" .

:DropPath a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1605.07648v4> ;
    rdfs:seeAlso <https://github.com/osmr/imgclsmob/blob/aa6c6186d2e296bf0687f2c0ab6454f33f5ef4f3/pytorch/pytorchcv/models/fractalnet_cifar.py#L146> ;
    skos:definition """Just as [dropout](https://paperswithcode.com/method/dropout) prevents co-adaptation of activations, **DropPath** prevents co-adaptation of parallel paths in networks such as [FractalNets](https://paperswithcode.com/method/fractalnet) by randomly dropping operands of the join layers. This\r
discourages the network from using one input path as an anchor and another as a corrective term (a\r
configuration that, if not prevented, is prone to overfitting). Two sampling strategies are:\r
\r
- **Local**: a join drops each input with fixed probability, but we make sure at least one survives.\r
- **Global**: a single path is selected for the entire network. We restrict this path to be a single\r
column, thereby promoting individual columns as independently strong predictors.""" ;
    skos:prefLabel "DropPath" .

:DropPathway a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2001.08740v2> ;
    skos:definition """**DropPathway** randomly drops an audio pathway during training as a regularization technique for audiovisual recognition models.  Specifically, at each training iteration, we drop the Audio pathway altogether with probability $P\\_{d}$. This way, we slow down the learning of the Audio pathway and make its learning dynamics more compatible with its visual counterpart. When dropping the audio pathway, we sum zero tensors with the visual pathways.\r
\r
Note that DropPathway is different from simply setting different learning rates for the audio/visual pathways in that it 1) ensures the audio pathway has fewer parameter updates, 2) hinders the visual pathway to 'shortcut' training by memorizing audio information, and 3) provides extra regularization as different audio clips are dropped in each epoch.""" ;
    skos:prefLabel "DropPathway" .

:DualCL a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2201.08702v1> ;
    skos:altLabel "Dual Contrastive Learning" ;
    skos:definition "Contrastive learning has achieved remarkable success in representation learning via self-supervision in unsupervised settings. However, effectively adapting contrastive learning to supervised learning tasks remains as a challenge in practice. In this work, we introduce a dual contrastive learning (DualCL) framework that simultaneously learns the features of input samples and the parameters of classifiers in the same space. Specifically, DualCL regards the parameters of the classifiers as augmented samples associating to different labels and then exploits the contrastive learning between the input samples and the augmented samples. Empirical studies on five benchmark text classification datasets and their low-resource version demonstrate the improvement in classification accuracy and confirm the capability of learning discriminative representations of DualCL." ;
    skos:prefLabel "DualCL" .

:DualGCN a skos:Concept ;
    skos:altLabel "Dual Graph Convolutional Networks" ;
    skos:definition """A dual graph convolutional neural network jointly considers the two essential assumptions of semi-supervised learning: (1) local consistency and (2) global consistency. Accordingly, two convolutional neural networks are devised to embed the local-consistency-based and global-consistency-based knowledge, respectively.\r
\r
Description  and image from: [Dual Graph Convolutional Networks for Graph-Based Semi-Supervised Classification](https://persagen.com/files/misc/zhuang2018dual.pdf)""" ;
    skos:prefLabel "DualGCN" .

:DualSoftmaxLoss a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2109.04290v3> ;
    skos:definition """**Dual Softmax Loss** is a loss function based on symmetric cross-entropy loss used in the [CAMoE](https://paperswithcode.com/method/camoe) video-text retrieval model. Every text and video are calculated the\r
similarity with other videos or texts, which should be maximum in terms of the ground truth pair. For DSL, a prior is introduced to revise the similarity score. Multiplying the prior with the original similarity matrix imposes an efficient constraint and can help to filter those single side match pairs. As a result, DSL highlights the one with both great Text-to-Video and Video-to-Text probability, conducting a more convincing result.""" ;
    skos:prefLabel "Dual Softmax Loss" .

:DuelingNetwork a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1511.06581v3> ;
    skos:definition """A **Dueling Network** is a type of Q-Network that has two streams to separately estimate (scalar) state-value and the advantages for each action. Both streams share a common convolutional feature learning module. The two streams are combined via a special aggregating layer to produce an\r
estimate of the state-action value function Q as shown in the figure to the right.\r
\r
The last module uses the following mapping:\r
\r
$$ Q\\left(s, a, \\theta, \\alpha, \\beta\\right) =V\\left(s, \\theta, \\beta\\right) + \\left(A\\left(s, a, \\theta, \\alpha\\right) - \\frac{1}{|\\mathcal{A}|}\\sum\\_{a'}A\\left(s, a'; \\theta, \\alpha\\right)\\right) $$\r
\r
This formulation is chosen for identifiability so that the advantage function has zero advantage for the chosen action, but instead of a maximum we use an average operator to increase the stability of the optimization.""" ;
    skos:prefLabel "Dueling Network" .

:DutchEligibilityTrace a skos:Concept ;
    skos:definition """A **Dutch Eligibility Trace** is a type of [eligibility trace](https://paperswithcode.com/method/eligibility-trace) where the trace increments grow less quickly than the accumulative eligibility trace (helping avoid large variance updates). For the memory vector $\\textbf{e}\\_{t} \\in \\mathbb{R}^{b} \\geq \\textbf{0}$:\r
\r
$$\\mathbf{e\\_{0}} = \\textbf{0}$$\r
\r
$$\\textbf{e}\\_{t} = \\gamma\\lambda\\textbf{e}\\_{t-1} + \\left(1-\\alpha\\gamma\\lambda\\textbf{e}\\_{t-1}^{T}\\phi\\_{t}\\right)\\phi\\_{t}$$""" ;
    skos:prefLabel "Dutch Eligibility Trace" .

:DyGED a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2110.12148v2> ;
    skos:altLabel "Dynamic Graph Event Detection" ;
    skos:definition "" ;
    skos:prefLabel "DyGED" .

:DynaBERT a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2004.04037v2> ;
    skos:definition """**DynaBERT** is a [BERT](https://paperswithcode.com/method/bert)-variant which can flexibly adjust the size and latency by selecting adaptive width and depth. The training process of DynaBERT includes first training a width-adaptive BERT and then allowing both adaptive width and depth, by distilling knowledge from the full-sized model to small sub-networks. Network rewiring is also used to keep the more important attention heads and neurons shared by more sub-networks. \r
\r
A two-stage procedure is used to train DynaBERT. First, using knowledge distillation (dashed lines) to transfer the knowledge from a fixed teacher model to student sub-networks with adaptive width in DynaBERTW. Then, using knowledge distillation (dashed lines) to transfer the knowledge from a trained DynaBERTW to student sub-networks with adaptive width and depth in DynaBERT.""" ;
    skos:prefLabel "DynaBERT" .

:DynamicConv a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1901.10430v2> ;
    rdfs:seeAlso <https://github.com/pytorch/fairseq/blob/28876638114948711fd4bd4e350fdd6809013f1e/fairseq/modules/dynamic_convolution.py#L16> ;
    skos:altLabel "Dynamic Convolution" ;
    skos:definition """**DynamicConv** is a type of [convolution](https://paperswithcode.com/method/convolution) for sequential modelling where it has kernels that vary over time as a learned function of the individual time steps. It builds upon [LightConv](https://paperswithcode.com/method/lightconv) and takes the same form but uses a time-step dependent kernel:\r
\r
$$ \\text{DynamicConv}\\left(X, i, c\\right) = \\text{LightConv}\\left(X, f\\left(X\\_{i}\\right)\\_{h,:}, i, c\\right) $$""" ;
    skos:prefLabel "DynamicConv" .

:DynamicConvolution a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1912.03458v2> ;
    skos:definition """The extremely low computational cost of lightweight CNNs constrains the depth and width of the networks, further decreasing their representational power. To address the above problem, Chen et al. proposed dynamic convolution, a novel operator design that increases  representational power with negligible additional computational cost and does not change the width or depth of the network in parallel with CondConv.\r
\r
Dynamic convolution uses $K$ parallel convolution kernels of the same  size and input/output dimensions instead of one kernel per layer. Like SE blocks, it adopts a squeeze-and-excitation mechanism to generate the attention weights for the different convolution kernels. These kernels are then aggregated dynamically by weighted summation and applied to the input feature map $X$:\r
\\begin{align}\r
    s & = \\text{softmax} (W_{2} \\delta (W_{1}\\text{GAP}(X)))\r
\\end{align}\r
\\begin{align}\r
    \\text{DyConv} &= \\sum_{i=1}^{K} s_k \\text{Conv}_k \r
\\end{align}\r
\\begin{align}\r
    Y &= \\text{DyConv}(X)\r
\\end{align}\r
Here the convolutions are combined by summation of weights and biases of convolutional kernels. \r
\r
Compared to applying convolution to the feature map, the computational cost of squeeze-and-excitation and weighted summation is extremely low. Dynamic convolution thus provides an efficient operation to improve  representational power and can be easily used as a replacement for any convolution.""" ;
    skos:prefLabel "Dynamic Convolution" .

:DynamicKeypointHead a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2105.14185v1> ;
    skos:definition """**Dynamic Keypoint Head** is an output head for pose estimation that are conditioned on each instance (person), and can encode the instance concept in the dynamically-generated weights of their filters. They are used in the [FCPose](https://paperswithcode.com/method/fcpose) architecture.\r
\r
The Figure shows the core idea. $F$ denotes a level of feature maps. "Rel. Coord." means the relative coordinates, denoting the relative offsets from the locations of $F$ to the location where the filters are generated. Refer to the text for details. $f\\_{\\theta\\_{i}}$ is the dynamically-generated keypoint head for the $i$-th person instance. Note that each person instance has its own keypoint head.""" ;
    skos:prefLabel "Dynamic Keypoint Head" .

:DynamicMemoryNetwork a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1506.07285v5> ;
    rdfs:seeAlso <https://github.com/rgsachin/DMTN> ;
    skos:definition """A **Dynamic Memory Network** is a neural network architecture which processes input sequences and questions, forms episodic memories, and generates relevant answers. Questions trigger an iterative attention process which allows the model to condition its attention on the inputs and the result of previous iterations. These results are then reasoned over in a hierarchical recurrent sequence model to generate answers. \r
\r
The DMN consists of a number of modules:\r
\r
- Input Module: The input module encodes raw text inputs from the task into distributed vector representations. The input takes forms like a sentence, a long story, a movie review and so on.\r
- Question Module: The question module encodes the question of the task into a distributed\r
vector representation. For question answering, the question may be a sentence such as "Where did the author first fly?". The representation is fed into the episodic memory module, and forms the basis, or initial state, upon which the episodic memory module iterates.\r
- Episodic Memory Module: Given a collection of input representations, the episodic memory module chooses which parts of the inputs to focus on through the attention mechanism. It then produces a ”memory” vector representation taking into account the question as well as the previous memory. Each iteration provides the module with newly relevant information about the input. In other words,\r
the module has the ability to retrieve new information, in the form of input representations, which were thought to be irrelevant in previous iterations.\r
- Answer Module: The answer module generates an answer from the final memory vector of the memory module.""" ;
    skos:prefLabel "Dynamic Memory Network" .

:DynamicR-CNN a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2004.06002v2> ;
    rdfs:seeAlso <https://github.com/hkzhang95/DynamicRCNN> ;
    skos:definition """**Dynamic R-CNN** is an object detection method that adjusts the label assignment criteria (IoU threshold) and the shape of regression loss function (parameters of Smooth L1 Loss) automatically based on the statistics of proposals during training. The motivation is that in previous two-stage object detectors, there is an inconsistency problem between the fixed network settings and the dynamic training procedure. For example, the fixed label assignment strategy and regression loss function cannot fit the distribution change of proposals and thus are harmful to training high quality detectors.\r
\r
It consists of two components: Dynamic Label Assignment and Dynamic Smooth L1 Loss, which are designed for the classification and regression branches, respectively. \r
\r
For Dynamic Label Assignment, we want our model to be discriminative for high IoU proposals, so we gradually adjust the IoU threshold for positive/negative samples based on the proposals distribution in the training procedure. Specifically, we set the threshold as the IoU of the proposal at a certain percentage since it can reflect the quality of the overall distribution. \r
\r
For Dynamic Smooth L1 Loss, we want to change the shape of the regression loss function to adaptively fit the distribution change of error and ensure the contribution of high quality samples to training. This is achieved by adjusting the $\\beta$ in Smooth L1 Loss based on the error distribution of the regression loss function, in which $\\beta$ actually controls the magnitude of the gradient of small errors.""" ;
    skos:prefLabel "Dynamic R-CNN" .

:DynamicSmoothL1Loss a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2004.06002v2> ;
    rdfs:seeAlso <https://github.com/hkzhang95/DynamicRCNN/blob/5b6cbe552231c7dbf2cdbd139369c313f16d2a72/dynamic_rcnn/det_opr/loss.py#L10> ;
    skos:definition """**Dynamic SmoothL1 Loss (DSL)** is a loss function in object detection where we change the shape of loss function to gradually focus on high quality samples:\r
\r
$$\\text{DSL}\\left(x, \\beta\\_{now}\\right) = 0.5|{x}|^{2}/\\beta\\_{now}, \\text{ if } |x| < \\beta\\_{now}\\text{,} $$ \r
$$\\text{DSL}\\left(x, \\beta\\_{now}\\right) = |{x}| - 0.5\\beta\\_{now}\\text{, otherwise} $$ \r
\r
DSL will change the value of $\\beta\\_{now}$ according to the statistics of regression errors which can reflect the localization accuracy. It was introduced as part of the [Dynamic R-CNN](https://paperswithcode.com/method/dynamic-r-cnn) model.""" ;
    skos:prefLabel "Dynamic SmoothL1 Loss" .

:E-Branchformer a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2210.00077v2> ;
    skos:definition "E-BRANCHFORMER: BRANCHFORMER WITH ENHANCED MERGING FOR SPEECH RECOGNITION" ;
    skos:prefLabel "E-Branchformer" .

:E-MBConv a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2004.12186v2> ;
    skos:definition "" ;
    skos:prefLabel "E-MBConv" .

:E-swish a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1801.07145v1> ;
    skos:definition "" ;
    skos:prefLabel "E-swish" .

:E2EAdaptiveDistTraining a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2112.02752v1> ;
    skos:altLabel "End-to-end Adaptive Distributed Training" ;
    skos:definition """Distributed training has become a pervasive and effective approach for training a large neural network\r
(NN) model with processing massive data. However, it is very challenging to satisfy requirements\r
from various NN models, diverse computing resources, and their dynamic changes during a training\r
job. In this study, we design our distributed training framework in a systematic end-to-end view to\r
provide the built-in adaptive ability for different scenarios, especially for industrial applications and\r
production environments, by fully considering resource allocation, model partition, task placement,\r
and distributed execution. Based on the unified distributed graph and the unified cluster object,\r
our adaptive framework is equipped with a global cost model and a global planner, which can\r
enable arbitrary parallelism, resource-aware placement, multi-mode execution, fault-tolerant, and\r
elastic distributed training. The experiments demonstrate that our framework can satisfy various\r
requirements from the diversity of applications and the heterogeneity of resources with highly\r
competitive performance.""" ;
    skos:prefLabel "E2EAdaptiveDistTraining" .

:EBM a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1602.03264v3> ;
    skos:altLabel "energy-based model" ;
    skos:definition "" ;
    skos:prefLabel "EBM" .

:ECA-Net a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1910.03151v4> ;
    rdfs:seeAlso <https://github.com/BangguWu/ECANet/blob/cf8a4c8b3d49b27c12e98ceb930d0f7db6c2460a/models/eca_resnet.py#L87> ;
    skos:definition "An **ECA-Net** is a type of convolutional neural network that utilises an [Efficient Channel Attention](https://paperswithcode.com/method/efficient-channel-attention) module." ;
    skos:prefLabel "ECA-Net" .

:ECANet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1910.03151v4> ;
    skos:altLabel "efficient channel attention" ;
    skos:definition """An ECA block has similar formulation to an SE block including a squeeze module for aggregating global spatial information and an efficient excitation module for modeling cross-channel interaction. Instead of indirect correspondence, an ECA block only considers direct interaction between each channel and its k-nearest neighbors to control model complexity. Overall, the formulation of an ECA block is:\r
\\begin{align}\r
    s = F_\\text{eca}(X, \\theta) & = \\sigma (\\text{Conv1D}(\\text{GAP}(X))) \r
\\end{align}\r
\\begin{align}\r
    Y & = s  X\r
\\end{align}\r
where $\\text{Conv1D}(\\cdot)$ denotes 1D convolution with a kernel of shape $k$ across the channel domain, to model local cross-channel interaction. The parameter $k$ decides the coverage of interaction, and in ECA the kernel size $k$ is adaptively determined from the channel dimensionality $C$ instead of by manual tuning, using cross-validation:\r
\\begin{equation}\r
    k = \\psi(C) = \\left | \\frac{\\log_2(C)}{\\gamma}+\\frac{b}{\\gamma}\\right |_\\text{odd}\r
\\end{equation}\r
\r
where $\\gamma$ and $b$ are hyperparameters. $|x|_\\text{odd}$ indicates the nearest odd function of $x$. \r
\r
Compared to SENet, ECANet has an \r
improved excitation module, and provides an efficient and effective block which can readily be \r
 incorporated into various\r
CNNs.""" ;
    skos:prefLabel "ECANet" .

:ED-GNN a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2104.01488v1> ;
    skos:altLabel "Medical Entity Disambiguation using Graph Neural Networks" ;
    skos:definition "" ;
    skos:prefLabel "ED-GNN" .

:EDLPS a skos:Concept ;
    skos:altLabel "Encoder-Decoder model with local and pairwise loss along with shared encoder and discriminator network (EDLPS)" ;
    skos:definition """In this paper, we propose a method for obtaining sentence-level embeddings. While the problem of obtaining word-level embeddings is very well studied, we propose a novel method for obtaining sentence-level embeddings. This is obtained by a simple method in the context of solving the paraphrase generation task. If we use a sequential encoder-decoder model for generating paraphrase, we would like the generated paraphrase to be semantically close to the original sentence. One way to ensure this is by adding constraints for true paraphrase embeddings to be close and unrelated paraphrase candidate sentence embeddings to be far. This is ensured by using a sequential pair-wise discriminator that shares weights with the encoder. This discriminator is trained with a suitable loss function. Our loss function penalizes paraphrase sentence embedding distances from being too large. This loss is used in combination with a sequential encoder-decoder network. We also validate our method by evaluating the obtained embeddings for a sentiment analysis task. The proposed method results in semantic embeddings and provide competitive results on the paraphrase generation and sentiment analysis task on standard dataset. These results are also shown to be statistically significant.\r
\r
\r
\r
\r
Github Link:https://github.com/dev-chauhan/PQG-pytorch.\r
\r
2\r
The PQG dataset is available on this link: https://www.quora.com/q/quoradata/First-Quora-Dataset-Release-Question-Pairs.\r
\r
3\r
website: https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs.\r
\r
4\r
we report same baseline results as mentioned in [10]\r
\r
5\r
website: www.kaggle.com/c/sentiment-analysis-on-movie-reviews.\r
\r
6\r
Code: https://github.com/dev-chauhan/PQG-pytorch.""" ;
    skos:prefLabel "EDLPS" .

:EEND a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2003.02966v1> ;
    skos:altLabel "End-to-End Neural Diarization" ;
    skos:definition "**End-to-End Neural Diarization** is a neural network for speaker diarization in which a neural network directly outputs speaker diarization results given a multi-speaker recording. To realize such an end-to-end model, the speaker diarization problem is formulated as a multi-label classification problem and a permutation-free objective function is introduced to directly minimize diarization errors. The EEND method can explicitly handle speaker overlaps during training and inference. Just by feeding multi-speaker recordings with corresponding speaker segment labels, the model can be adapted to real conversations." ;
    skos:prefLabel "EEND" .

:EESP a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1811.11431v3> ;
    rdfs:seeAlso <https://github.com/osmr/imgclsmob/blob/e8d8e31ad8250142170cbc10d5e7c6a583bd9585/pytorch/pytorchcv/models/espnetv2.py#L94> ;
    skos:altLabel "Extremely Efficient Spatial Pyramid of Depth-wise Dilated Separable Convolutions" ;
    skos:definition """An **EESP Unit**, or  Extremely Efficient Spatial Pyramid of Depth-wise Dilated Separable Convolutions, is an image model block designed for edge devices. It was proposed as part of the [ESPNetv2](https://paperswithcode.com/method/espnetv2) CNN architecture. \r
\r
This building block is based on a reduce-split-transform-merge strategy. The EESP unit first projects the high-dimensional input feature maps into low-dimensional space using groupwise pointwise convolutions and then learns the representations in parallel using depthwise dilated separable convolutions with different dilation rates. Different dilation rates in each branch allow the EESP unit to learn the representations from a large effective receptive field. To remove the gridding artifacts caused by dilated convolutions, the EESP fuses the feature maps using [hierarchical feature fusion](https://paperswithcode.com/method/hierarchical-feature-fusion) (HFF).""" ;
    skos:prefLabel "EESP" .

:EGT a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2108.03348v3> ;
    skos:altLabel "Edge-augmented Graph Transformer" ;
    skos:definition "Transformer neural networks have achieved state-of-the-art results for unstructured data such as text and images but their adoption for graph-structured data has been limited. This is partly due to the difficulty of incorporating complex structural information in the basic transformer framework. We propose a simple yet powerful extension to the transformer - residual edge channels. The resultant framework, which we call Edge-augmented Graph Transformer (EGT), can directly accept, process and output structural information as well as node information. It allows us to use global self-attention, the key element of transformers, directly for graphs and comes with the benefit of long-range interaction among nodes. Moreover, the edge channels allow the structural information to evolve from layer to layer, and prediction tasks on edges/links can be performed directly from the output embeddings of these channels. In addition, we introduce a generalized positional encoding scheme for graphs based on Singular Value Decomposition which can improve the performance of EGT. Our framework, which relies on global node feature aggregation, achieves better performance compared to Convolutional/Message-Passing Graph Neural Networks, which rely on local feature aggregation within a neighborhood. We verify the performance of EGT in a supervised learning setting on a wide range of experiments on benchmark datasets. Our findings indicate that convolutional aggregation is not an essential inductive bias for graphs and global self-attention can serve as a flexible and adaptive alternative." ;
    skos:prefLabel "EGT" .

:ELECTRA a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2003.10555v1> ;
    skos:definition "**ELECTRA** is a [transformer](https://paperswithcode.com/method/transformer) with a new pre-training approach which trains two transformer models: the generator and the discriminator. The generator replaces tokens in the sequence - trained as a masked language model - and the discriminator (the ELECTRA contribution) attempts to identify which tokens are replaced by the generator in the sequence. This pre-training task is called replaced token detection, and is a replacement for masking the input." ;
    skos:prefLabel "ELECTRA" .

:ELMo a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1802.05365v2> ;
    skos:definition """**Embeddings from Language Models**, or **ELMo**, is a type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus.\r
\r
A biLM combines both a forward and backward LM.  ELMo jointly maximizes the log likelihood of the forward and backward directions. To add ELMo to a supervised model, we freeze the weights of the biLM and then concatenate the ELMo vector $\\textbf{ELMO}^{task}_k$ with $\\textbf{x}_k$ and pass the ELMO enhanced representation $[\\textbf{x}_k; \\textbf{ELMO}^{task}_k]$ into the task RNN. Here $\\textbf{x}_k$ is a context-independent token representation for each token position. \r
\r
Image Source: [here](https://medium.com/@duyanhnguyen_38925/create-a-strong-text-classification-with-the-help-from-elmo-e90809ba29da)""" ;
    skos:prefLabel "ELMo" .

:ELR a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2007.00151v2> ;
    skos:altLabel "Early Learning Regularization" ;
    skos:definition "" ;
    skos:prefLabel "ELR" .

:ELU a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1511.07289v5> ;
    rdfs:seeAlso <https://github.com/pytorch/pytorch/blob/96aaa311c0251d24decb9dc5da4957b7c590af6f/torch/nn/modules/activation.py#L422> ;
    skos:altLabel "Exponential Linear Unit" ;
    skos:definition """The **Exponential Linear Unit** (ELU) is an activation function for neural networks. In contrast to [ReLUs](https://paperswithcode.com/method/relu), ELUs have negative values which allows them to push mean unit activations closer to zero like [batch normalization](https://paperswithcode.com/method/batch-normalization) but with lower computational complexity. Mean shifts toward zero speed up learning by bringing the normal gradient closer to the unit natural gradient because of a reduced bias shift effect. While [LReLUs](https://paperswithcode.com/method/leaky-relu) and [PReLUs](https://paperswithcode.com/method/prelu) have negative values, too, they do not ensure a noise-robust deactivation state. ELUs saturate to a negative value with smaller inputs and thereby decrease the forward propagated variation and information.\r
\r
The exponential linear unit (ELU) with $0 < \\alpha$ is:\r
\r
$$f\\left(x\\right) = x \\text{ if } x > 0$$\r
$$\\alpha\\left(\\exp\\left(x\\right) − 1\\right) \\text{ if } x \\leq 0$$""" ;
    skos:prefLabel "ELU" .

:ELiSH a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1808.00783v1> ;
    rdfs:seeAlso <https://github.com/digantamisra98/Echo/blob/ffac8cab7938d8a2ea1eee9a947602d1f577c9f3/echoAI/Activation/Torch/elish.py#L20> ;
    skos:altLabel "Exponential Linear Squashing Activation" ;
    skos:definition """The **Exponential Linear Squashing Activation Function**, or **ELiSH**, is an activation function used for neural networks. It shares common properties with [Swish](https://paperswithcode.com/method/swish), being made up of an [ELU](https://paperswithcode.com/method/elu) and a [Sigmoid](https://paperswithcode.com/method/sigmoid-activation):\r
\r
$$f\\left(x\\right) = \\frac{x}{1+e^{-x}} \\text{ if } x \\geq 0 $$\r
$$f\\left(x\\right) = \\frac{e^{x} - 1}{1+e^{-x}} \\text{ if } x < 0 $$\r
\r
The Sigmoid part of **ELiSH** improves information flow, while the linear parts solve issues of vanishing gradients.""" ;
    skos:prefLabel "ELiSH" .

:EMEA a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2109.04877v1> ;
    skos:altLabel "Entropy Minimized Ensemble of Adapters" ;
    skos:definition "**Entropy Minimized Ensemble of Adapters**, or **EMEA**, is a method that optimizes the ensemble weights of the pretrained language adapters for each test sentence by minimizing the entropy of its predictions. The intuition behind the method is that a good [adapter](https://paperswithcode.com/method/adapter) weight $\\alpha$ for a test input $x$ should make the model more confident in its prediction for $x$, that is, it should lead to lower model entropy over the input" ;
    skos:prefLabel "EMEA" .

:EMF a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2101.06968v2> ;
    skos:altLabel "Enhanced-Multimodal Fuzzy Framework" ;
    skos:definition "BCI MI framework to classifiy brain signals using a multimodal decission making phase, with an addtional differentiation of the signal." ;
    skos:prefLabel "EMF" .

:EMQAP a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2109.05897v2> ;
    skos:definition "**EMQAP**, or **E-Manual Question Answering Pipeline**, is an approach for answering questions pertaining to electronics devices. Built upon the pretrained [RoBERTa](https://paperswithcode.com/method/roberta), it harbors a supervised multi-task learning framework which efficiently performs the dual tasks of identifying the section in the E-manual where the answer can be found and the exact answer span within that section." ;
    skos:prefLabel "EMQAP" .

:ENIGMA a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2102.10242v3> ;
    skos:definition "**ENIGMA** is an evaluation framework for dialog systems based on Pearson and Spearman's rank correlations between the estimated rewards and the true rewards.  ENIGMA only requires a handful of pre-collected experience data, and therefore does not involve human interaction with the target policy during the evaluation, making automatic evaluations feasible. More importantly, ENIGMA is model-free and agnostic to the behavior policies for collecting the experience data (see details in Section 2), which significantly alleviates the technical difficulties of modeling complex dialogue environments and human behaviors." ;
    skos:prefLabel "ENIGMA" .

:ENet a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1606.02147v1> ;
    rdfs:seeAlso <https://github.com/iArunava/ENet-Real-Time-Semantic-Segmentation/blob/8a09a709f46544b229b37654c57ce5ae512fd398/models/ENet.py#L24> ;
    skos:definition """**ENet** is a semantic segmentation architecture which utilises a compact encoder-decoder architecture. Some design choices include:\r
\r
1. Using the [SegNet](https://paperswithcode.com/method/segnet) approach to downsampling y saving indices of elements chosen in max\r
pooling layers, and using them to produce sparse upsampled maps in the decoder.\r
2.  Early downsampling to optimize the early stages of the network and reduce the cost of processing large input frames. The first two blocks of ENet heavily reduce the input size, and use only a small set of feature maps. \r
3. Using PReLUs as an activation function\r
4. Using dilated convolutions \r
5. Using Spatial [Dropout](https://paperswithcode.com/method/dropout)""" ;
    skos:prefLabel "ENet" .

:ENetBottleneck a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1606.02147v1> ;
    rdfs:seeAlso <https://github.com/yassouali/pytorch_segmentation/blob/473503a22c99e78d1f938c0099f7d91f85555917/models/enet.py#L22> ;
    skos:definition """**ENet Bottleneck** is an image model block used in the [ENet](https://paperswithcode.com/method/enet) semantic segmentation architecture. Each block consists of three convolutional layers: a 1 × 1 projection that reduces the dimensionality, a main convolutional layer, and a 1 × 1 expansion. We place [Batch Normalization](https://paperswithcode.com/method/batch-normalization) and [PReLU](https://paperswithcode.com/method/prelu) between all convolutions. If the bottleneck is downsampling, a [max pooling](https://paperswithcode.com/method/max-pooling) layer is added to the main branch.\r
Also, the first 1 × 1 projection is replaced with a 2 × 2 [convolution](https://paperswithcode.com/method/convolution) with stride 2 in both dimensions. We zero pad the activations, to match the number of feature maps.""" ;
    skos:prefLabel "ENet Bottleneck" .

:ENetDilatedBottleneck a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1606.02147v1> ;
    rdfs:seeAlso <https://github.com/yassouali/pytorch_segmentation/blob/473503a22c99e78d1f938c0099f7d91f85555917/models/enet.py#L22> ;
    skos:definition "**ENet Dilated Bottleneck** is an image model block used in the [ENet](https://paperswithcode.com/method/enet) semantic segmentation architecture. It is the same as a regular [ENet Bottleneck](https://paperswithcode.com/method/enet-bottleneck) but employs dilated convolutions instead." ;
    skos:prefLabel "ENet Dilated Bottleneck" .

:ENetInitialBlock a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1606.02147v1> ;
    rdfs:seeAlso <https://github.com/yassouali/pytorch_segmentation/blob/473503a22c99e78d1f938c0099f7d91f85555917/models/enet.py#L8> ;
    skos:definition "The **ENet Initial Block** is an image model block used in the [ENet](https://paperswithcode.com/method/enet) semantic segmentation architecture. [Max Pooling](https://paperswithcode.com/method/max-pooling) is performed with non-overlapping 2 × 2 windows, and the [convolution](https://paperswithcode.com/method/convolution) has 13 filters, which sums up to 16 feature maps after concatenation. This is heavily inspired by Inception Modules." ;
    skos:prefLabel "ENet Initial Block" .

:ERNIE a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1904.09223v1> ;
    skos:definition "ERNIE is a transformer-based model consisting of two stacked modules: 1) textual encoder and 2) knowledgeable encoder, which is responsible to integrate extra token-oriented knowledge information into textual information. This layer consists of stacked aggregators, designed for encoding both tokens and entities as well as fusing their heterogeneous features. To integrate this layer of enhancing representations via knowledge, a special pre-training task is adopted for ERNIE - it involves randomly masking token-entity alignments and training the model to predict all corresponding entities based on aligned tokens (aka denoising entity auto-encoder)." ;
    skos:prefLabel "ERNIE" .

:ERNIE-GEN a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2001.11314v3> ;
    skos:definition "**ERNIE-GEN** is a multi-flow sequence to sequence pre-training and fine-tuning framework which bridges the discrepancy between training and inference with an infilling generation mechanism and a noise-aware generation method. To make generation closer to human writing patterns, this framework introduces a span-by-span generation flow that trains the model to predict semantically-complete spans consecutively rather than predicting word by word. Unlike existing pre-training methods, ERNIE-GEN incorporates multi-granularity target sampling to construct pre-training data, which enhances the correlation between encoder and decoder." ;
    skos:prefLabel "ERNIE-GEN" .

:ERU a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1811.11431v3> ;
    skos:altLabel "Efficient Recurrent Unit" ;
    skos:definition "An **Efficient Recurrent Unit (ERU)** extends [LSTM](https://paperswithcode.com/method/mrnn)-based language models by replacing linear transforms for processing the input vector with the [EESP](https://paperswithcode.com/method/eesp) unit inside the [LSTM](https://paperswithcode.com/method/lstm) cell." ;
    skos:prefLabel "ERU" .

:ESACL a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2108.11992v1> ;
    skos:altLabel "Enhanced Seq2Seq Autoencoder via Contrastive Learning" ;
    skos:definition "**ESACL**, or **Enhanced Seq2Seq Autoencoder via Contrastive Learning**, is a denoising sequence-to-sequence (seq2seq) autoencoder via contrastive learning for abstractive text summarization. The model adopts a standard [Transformer](https://paperswithcode.com/method/transformer)-based architecture with a multilayer bi-directional encoder and an autoregressive decoder. To enhance its denoising ability, self-supervised contrastive learning is incorporated along with various sentence-level document augmentation." ;
    skos:prefLabel "ESACL" .

:ESIM a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1609.06038v3> ;
    skos:altLabel "Enhanced Sequential Inference Model" ;
    skos:definition "**Enhanced Sequential Inference Model** or **ESIM** is a sequential NLI model proposed in [Enhanced LSTM for Natural Language Inference](https://www.aclweb.org/anthology/P17-1152) paper." ;
    skos:prefLabel "ESIM" .

:ESP a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1803.06815v3> ;
    rdfs:seeAlso <https://github.com/sacmehta/ESPNet/blob/afe71c38edaee3514ca44e0adcafdf36109bf437/train/Model.py#L166> ;
    skos:altLabel "Efficient Spatial Pyramid" ;
    skos:definition "An **Efficient Spatial Pyramid (ESP)** is an image model block based on a factorization principle that decomposes a standard [convolution](https://paperswithcode.com/method/convolution) into two steps: (1) point-wise convolutions and (2) spatial pyramid of dilated convolutions. The point-wise convolutions help in reducing the computation, while the spatial pyramid of dilated convolutions re-samples the feature maps to learn the representations from large effective receptive field. This allows for increased efficiency compared to another image blocks like [ResNeXt](https://paperswithcode.com/method/resnext) blocks and Inception modules." ;
    skos:prefLabel "ESP" .

:ESPNet a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1803.06815v3> ;
    rdfs:seeAlso <https://github.com/sacmehta/ESPNet/blob/afe71c38edaee3514ca44e0adcafdf36109bf437/train/Model.py#L310> ;
    skos:definition "**ESPNet** is a convolutional neural network for semantic segmentation of high resolution images under resource constraints. ESPNet is based on a convolutional module, efficient spatial pyramid ([ESP](https://paperswithcode.com/method/esp)), which is efficient in terms of computation, memory, and power." ;
    skos:prefLabel "ESPNet" .

:ESPNetv2 a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1811.11431v3> ;
    rdfs:seeAlso <https://github.com/osmr/imgclsmob/blob/e8d8e31ad8250142170cbc10d5e7c6a583bd9585/pytorch/pytorchcv/models/espnetv2.py#L270> ;
    skos:definition "**ESPNetv2** is a convolutional neural network that utilises group point-wise and depth-wise dilated separable convolutions to learn representations from a large effective receptive field with fewer FLOPs and parameters." ;
    skos:prefLabel "ESPNetv2" .

:ETC a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2004.08483v5> ;
    skos:altLabel "Extended Transformer Construction" ;
    skos:definition "**Extended Transformer Construction**, or **ETC**, is an extension of the [Transformer](https://paperswithcode.com/method/transformer) architecture with a new attention mechanism that extends the original in two main ways: (1) it allows scaling up the input length from 512 to several thousands; and (2) it can ingesting structured inputs instead of just linear sequences. The key ideas that enable ETC to achieve these are a new [global-local attention mechanism](https://paperswithcode.com/method/global-local-attention), coupled with [relative position encodings](https://paperswithcode.com/method/relative-position-encodings). ETC also allows lifting weights from existing [BERT](https://paperswithcode.com/method/bert) models, saving computational resources while training." ;
    skos:prefLabel "ETC" .

:EVM a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1506.06112v4> ;
    skos:altLabel "Extreme Value Machine" ;
    skos:definition "" ;
    skos:prefLabel "EVM" .

:EWC a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1612.00796v2> ;
    skos:altLabel "Elastic Weight Consolidation" ;
    skos:definition "The methon to overcome catastrophic forgetting in neural network while continual learning" ;
    skos:prefLabel "EWC" .

:EarlyDropout a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2303.01500v2> ;
    rdfs:seeAlso <https://gist.github.com/ericup/e437162a1ddb33276d2c3601b2044379> ;
    skos:definition "Introduced by Hinton et al. in 2012, dropout has stood the test of time as a regularizer for preventing overfitting in neural networks. In this study, we demonstrate that dropout can also mitigate underfitting when used at the start of training. During the early phase, we find dropout reduces the directional variance of gradients across mini-batches and helps align the mini-batch gradients with the entire dataset's gradient. This helps counteract the stochasticity of SGD and limit the influence of individual batches on model training. Our findings lead us to a solution for improving performance in underfitting models - early dropout: dropout is applied only during the initial phases of training, and turned off afterwards. Models equipped with early dropout achieve lower final training loss compared to their counterparts without dropout. Additionally, we explore a symmetric technique for regularizing overfitting models - late dropout, where dropout is not used in the early iterations and is only activated later in training. Experiments on ImageNet and various vision tasks demonstrate that our methods consistently improve generalization accuracy. Our results encourage more research on understanding regularization in deep learning and our methods can be useful tools for future neural network training, especially in the era of large data. Code is available at https://github.com/facebookresearch/dropout ." ;
    skos:prefLabel "Early Dropout" .

:EarlyStopping a skos:Concept ;
    skos:definition """**Early Stopping** is a regularization technique for deep neural networks that stops training when parameter updates no longer begin to yield improves on a validation set. In essence, we store and update the current best parameters during training, and when parameter updates no longer yield an improvement (after a set number of iterations) we stop training and use the last best parameters. It works as a regularizer by restricting the optimization procedure to a smaller volume of parameter space.\r
\r
Image Source: [Ramazan Gençay](https://www.researchgate.net/figure/Early-stopping-based-on-cross-validation_fig1_3302948)""" ;
    skos:prefLabel "Early Stopping" .

:Earlyexiting a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1709.01686v1> ;
    skos:altLabel "Early exiting using confidence measures" ;
    skos:definition "Exit whenever the model is confident enough allowing early exiting from hidden layers" ;
    skos:prefLabel "Early exiting" .

:EdgeBoxes a skos:Concept ;
    skos:definition """**EdgeBoxes** is an approach for generating object bounding box proposals directly from edges. Similar to segments, edges provide a simplified but informative representation of an image. In fact, line drawings of an image can accurately convey the high-level information contained in an image\r
using only a small fraction of the information. \r
\r
The main insight behind the method is the observation: the number of contours wholly enclosed by a bounding box is indicative of the likelihood of the box containing an object. We say a contour is wholly enclosed by a box if all edge pixels belonging to the contour lie within the interior of the box. Edges tend to correspond to object boundaries, and as such boxes that tightly enclose a set of edges are likely to contain an object. However, some edges that lie within an object’s bounding box may not be part of the contained object. Specifically, edge pixels that belong to contours straddling the box’s boundaries are likely to correspond to objects or structures that lie outside the box.\r
\r
Source: [Zitnick and Dollar](https://pdollar.github.io/files/papers/ZitnickDollarECCV14edgeBoxes.pdf)""" ;
    skos:prefLabel "EdgeBoxes" .

:EdgeFlow a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2109.09406v2> ;
    skos:definition """**EdgeFlow** is an interactive segmentation architecture that fully utilizes interactive information of user clicks with edge-guided flow. Edge guidance is the idea that interactive segmentation improves segmentation masks progressively with user clicks. Based on user clicks, an edge mask scheme is used, which takes the object edges estimated from the previous iteration as prior information, instead of direct mask estimation (if the previous mask is used as input, poor segmentation results could result).\r
\r
The architecture consists of a coarse-to-fine network including CoarseNet and FineNet. For CoarseNet, [HRNet](https://paperswithcode.com/method/hrnet)-18+OCR is utilized as the base segmentation model and the edge-guided flow is appended to deal with interactive information. For FineNet, three [atrous convolution](https://paperswithcode.com/method/dilated-convolution) blocks are utilized to refine the coarse masks.""" ;
    skos:prefLabel "EdgeFlow" .

:EffectiveSqueeze-and-ExcitationBlock a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1911.06667v6> ;
    rdfs:seeAlso <https://github.com/youngwanLEE/CenterMask/blob/72147e8aae673fcaf4103ee90a6a6b73863e7fa1/maskrcnn_benchmark/modeling/backbone/vovnet.py#L108> ;
    skos:definition "**Effective Squeeze-and-Excitation Block** is an image model block based on squeeze-and-excitation, the difference being that one less FC layer is used. The authors note the SE module has a limitation: channel information loss due to dimension reduction. For avoiding high model complexity burden, two FC layers of the SE module need to reduce channel dimension. Specifically, while the first FC layer reduces input feature channels $C$ to $C/r$ using reduction ratio $r$, the second FC layer expands the reduced channels to original channel size $C$. As a result, this channel dimension reduction causes channel information loss. Therefore, effective SE (eSE) uses only one FC layer with $C$ channels instead of two FCs without channel dimension reduction, which maintains channel information." ;
    skos:prefLabel "Effective Squeeze-and-Excitation Block" .

:EfficientChannelAttention a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1910.03151v4> ;
    rdfs:seeAlso <https://github.com/BangguWu/ECANet/blob/cf8a4c8b3d49b27c12e98ceb930d0f7db6c2460a/models/eca_module.py#L5> ;
    skos:definition """**Efficient Channel Attention** is an architectural unit based on [squeeze-and-excitation](https://paperswithcode.com/method/squeeze-and-excitation-block) blocks that reduces model complexity without dimensionality reduction. It was proposed as part of the [ECA-Net](https://paperswithcode.com/method/eca-net) CNN architecture. \r
\r
After channel-wise [global average pooling](https://paperswithcode.com/method/global-average-pooling) without dimensionality reduction, the ECA captures local cross-channel interaction by considering every channel and its $k$ neighbors. The ECA can be efficiently implemented by fast $1D$ [convolution](https://paperswithcode.com/method/convolution) of size $k$, where kernel size $k$ represents the coverage of local cross-channel interaction, i.e., how many neighbors participate in attention prediction of one channel.""" ;
    skos:prefLabel "Efficient Channel Attention" .

:EfficientDet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1911.09070v7> ;
    rdfs:seeAlso <https://github.com/zylo117/Yet-Another-EfficientDet-Pytorch> ;
    skos:definition "**EfficientDet** is a type of object detection model, which utilizes several optimization and backbone tweaks, such as the use of a [BiFPN](https://paperswithcode.com/method/bifpn), and a compound scaling method that uniformly scales the resolution,depth and width for all backbones, feature networks and box/class prediction networks at the same time." ;
    skos:prefLabel "EfficientDet" .

:EfficientNet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1905.11946v5> ;
    rdfs:seeAlso <https://github.com/lukemelas/EfficientNet-PyTorch/blob/2eb7a7d264344ddf15d0a06ee99b0dca524c6a07/efficientnet_pytorch/model.py#L143> ;
    skos:definition """**EfficientNet** is a convolutional neural network architecture and scaling method that uniformly scales all dimensions of depth/width/resolution using a *compound coefficient*. Unlike conventional practice that arbitrary scales  these factors, the EfficientNet scaling method uniformly scales network width, depth, and resolution with a set of fixed scaling coefficients. For example, if we want to use $2^N$ times more computational resources, then we can simply increase the network depth by $\\alpha ^ N$,  width by $\\beta ^ N$, and image size by $\\gamma ^ N$, where $\\alpha, \\beta, \\gamma$ are constant coefficients determined by a small grid search on the original small model. EfficientNet uses a compound coefficient $\\phi$ to uniformly scales network width, depth, and resolution in a  principled way.\r
\r
The compound scaling method is justified by the intuition that if the input image is bigger, then the network needs more layers to increase the receptive field and more channels to capture more fine-grained patterns on the bigger image.\r
\r
The base EfficientNet-B0 network is based on the inverted bottleneck residual blocks of [MobileNetV2](https://paperswithcode.com/method/mobilenetv2), in addition to squeeze-and-excitation blocks.\r
\r
 EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flowers (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters.""" ;
    skos:prefLabel "EfficientNet" .

:EfficientNetV2 a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2104.00298v3> ;
    skos:definition """**EfficientNetV2** is a type convolutional neural network that has faster training speed and better parameter efficiency than [previous models](https://paperswithcode.com/method/efficientnet). To develop these models, the authors use a combination of training-aware [neural architecture search](https://paperswithcode.com/method/neural-architecture-search) and scaling, to jointly optimize training speed. The models were searched from the search space enriched with new ops such as [Fused-MBConv](https://ai.googleblog.com/2019/08/efficientnet-edgetpu-creating.html).\r
\r
Architecturally the main differences are:\r
\r
- EfficientNetV2 extensively uses both [MBConv](https://paperswithcode.com/method/inverted-residual-block)  and the newly added fused-MBConv in the early layers.\r
- EfficientNetV2 prefers smaller expansion ratio for [MBConv](https://paperswithcode.com/method/inverted-residual-block) since smaller expansion ratios tend to have less memory access overhead.\r
- EfficientNetV2 prefers smaller 3x3 kernel sizes, but it adds more layers to compensate the reduced receptive field resulted from the smaller kernel size. \r
- EfficientNetV2 completely removes the last stride-1 stage in the original EfficientNet, wperhaps due to its large parameter size and memory access overhead.""" ;
    skos:prefLabel "EfficientNetV2" .

<http://w3id.org/mlso/vocab/ml_algorithm/EfficientUNet++> a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2106.11447v1> ;
    rdfs:seeAlso <https://github.com/jlcsilva/segmentation_models.pytorch/blob/53c7f956ca557eb2cf386d28faacadf30ce4d0e2/segmentation_models_pytorch/efficientunetplusplus/decoder.py#L85> ;
    skos:definition """Decoder architecture inspired on the [UNet++](https://paperswithcode.com/method/unet) structure and the [EfficientNet](https://paperswithcode.com/method/efficientnet) building blocks. Keeping the UNet++ structure, the EfficientUNet++ achieves higher performance and significantly lower computational complexity through two simple modifications:\r
\r
* Replaces the 3x3 convolutions of the UNet++ with residual bottleneck blocks with depthwise convolutions\r
* Applies channel and spatial attention to the bottleneck feature maps using [concurrent spatial and channel squeeze & excitation (scSE)](https://paperswithcode.com/method/scse) blocks""" ;
    skos:prefLabel "EfficientUNet++" .

:ElasticDenseBlock a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1812.05262v2> ;
    rdfs:seeAlso <https://github.com/allenai/elastic/blob/57345c600c63fbde163c41929d6d6dd894d408ce/models/densenet.py#L9> ;
    skos:definition "**Elastic Dense Block** is a skip connection block that modifies the [Dense Block](https://paperswithcode.com/method/dense-block) with downsamplings and upsamplings in parallel branches at each layer to let the network learn from a data scaling policy in which inputs are processed at different resolutions in each layer. It is called \"elastic\" because each layer in the network is flexible in terms of choosing the best scale by a soft policy." ;
    skos:prefLabel "Elastic Dense Block" .

:ElasticFace a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2109.09416v4> ;
    skos:altLabel "Elastic Margin Loss for Deep Face Recognition" ;
    skos:definition "" ;
    skos:prefLabel "ElasticFace" .

:ElasticResNeXtBlock a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1812.05262v2> ;
    rdfs:seeAlso <https://github.com/allenai/elastic/blob/57345c600c63fbde163c41929d6d6dd894d408ce/models/resnext.py#L93> ;
    skos:definition "An **Elastic ResNeXt Block** is a modification of the [ResNeXt Block](https://paperswithcode.com/method/resnext-block) that adds downsamplings and upsamplings in parallel branches at each layer. It is called \"elastic\" because each layer in the network is flexible in terms of choosing the best scale by a soft policy." ;
    skos:prefLabel "Elastic ResNeXt Block" .

:Electric a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2012.08561v1> ;
    skos:definition """**Electric** is an energy-based cloze model for representation learning over text. Like BERT, it is a conditional generative model of tokens given their contexts. However, Electric does not use masking or output a full distribution over tokens that could occur in a context. Instead, it assigns a scalar energy score to each input token indicating how likely it is given its context.\r
\r
Specifically, like BERT, Electric also models $p\\_{\\text {data }}\\left(x\\_{t} \\mid \\mathbf{x}\\_{\\backslash t}\\right)$, but does not use masking or a softmax layer. Electric first maps the unmasked input $\\mathbf{x}=\\left[x\\_{1}, \\ldots, x\\_{n}\\right]$ into contextualized vector representations $\\mathbf{h}(\\mathbf{x})=\\left[\\mathbf{h}\\_{1}, \\ldots, \\mathbf{h}\\_{n}\\right]$ using a transformer network. The model assigns a given position $t$ an energy score\r
\r
$$\r
E(\\mathbf{x})\\_{t}=\\mathbf{w}^{T} \\mathbf{h}(\\mathbf{x})\\_{t}\r
$$\r
\r
using a learned weight vector $w$. The energy function defines a distribution over the possible tokens at position $t$ as\r
\r
$$\r
p\\_{\\theta}\\left(x\\_{t} \\mid \\mathbf{x}_{\\backslash t}\\right)=\\exp \\left(-E(\\mathbf{x})\\_{t}\\right) / Z\\left(\\mathbf{x}\\_{\\backslash t}\\right) \r
$$\r
\r
$$\r
=\\frac{\\exp \\left(-E(\\mathbf{x})\\_{t}\\right)}{\\sum\\_{x^{\\prime} \\in \\mathcal{V}} \\exp \\left(-E\\left(\\operatorname{REPLACE}\\left(\\mathbf{x}, t, x^{\\prime}\\right)\\right)\\_{t}\\right)}\r
$$\r
\r
where $\\text{REPLACE}\\left(\\mathbf{x}, t, x^{\\prime}\\right)$ denotes replacing the token at position $t$ with $x^{\\prime}$ and $\\mathcal{V}$ is the vocabulary, in practice usually word pieces. Unlike with BERT, which produces the probabilities for all possible tokens $x^{\\prime}$ using a softmax layer, a candidate $x^{\\prime}$ is passed in as input to the transformer. As a result, computing $p_{\\theta}$ is prohibitively expensive because the partition function $Z\\_{\\theta}\\left(\\mathbf{x}\\_{\\backslash t}\\right)$ requires running the transformer $|\\mathcal{V}|$ times; unlike most EBMs, the intractability of $Z\\_{\\theta}(\\mathbf{x} \\backslash t)$ is more due to the expensive scoring function rather than having a large sample space.""" ;
    skos:prefLabel "Electric" .

:EligibilityTrace a skos:Concept ;
    skos:definition """An **Eligibility Trace** is a memory vector $\\textbf{z}\\_{t} \\in \\mathbb{R}^{d}$ that parallels the long-term weight vector $\\textbf{w}\\_{t} \\in \\mathbb{R}^{d}$. The idea is that when a component of $\\textbf{w}\\_{t}$ participates in producing an estimated value, the corresponding component of $\\textbf{z}\\_{t}$ is bumped up and then begins to fade away. Learning will then occur in that component of $\\textbf{w}\\_{t}$ if a nonzero TD error occurs before the trade falls back to zero. The trace-decay parameter $\\lambda \\in \\left[0, 1\\right]$ determines the rate at which the trace falls.\r
\r
Intuitively, they tackle the credit assignment problem by capturing both a frequency heuristic - states that are visited more often deserve more credit - and a recency heuristic - states that are visited more recently deserve more credit.\r
\r
$$E\\_{0}\\left(s\\right) = 0 $$\r
$$E\\_{t}\\left(s\\right) = \\gamma\\lambda{E}\\_{t-1}\\left(s\\right) + \\textbf{1}\\left(S\\_{t} = s\\right) $$\r
\r
Source: Sutton and Barto, Reinforcement Learning, 2nd Edition""" ;
    skos:prefLabel "Eligibility Trace" .

:EmbeddedDotProductAffinity a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1711.07971v3> ;
    rdfs:seeAlso <https://github.com/tea1528/Non-Local-NN-Pytorch/blob/986937674eb3b85d3d3fbaaa8f384c0a26624121/models/non_local.py#L99> ;
    skos:definition """**Embedded Dot Product Affinity** is a type of affinity or self-similarity function between two points $\\mathbb{x\\_{i}}$ and $\\mathbb{x\\_{j}}$ that uses a dot product function in an embedding space:\r
\r
$$ f\\left(\\mathbb{x\\_{i}}, \\mathbb{x\\_{j}}\\right) = \\theta\\left(\\mathbb{x\\_{i}}\\right)^{T}\\phi\\left(\\mathbb{x\\_{j}}\\right) $$\r
\r
Here $\\theta\\left(x\\_{i}\\right) = W\\_{θ}x\\_{i}$ and $\\phi\\left(x\\_{j}\\right) = W\\_{φ}x\\_{j}$ are two embeddings.\r
\r
The main difference between the dot product and [embedded Gaussian affinity](https://paperswithcode.com/method/embedded-gaussian-affinity) functions is the presence of [softmax](https://paperswithcode.com/method/softmax), which plays the role of an activation function.""" ;
    skos:prefLabel "Embedded Dot Product Affinity" .

:EmbeddedGaussianAffinity a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1711.07971v3> ;
    rdfs:seeAlso <https://github.com/tea1528/Non-Local-NN-Pytorch/blob/986937674eb3b85d3d3fbaaa8f384c0a26624121/models/non_local.py#L99> ;
    skos:definition """**Embedded Gaussian Affinity** is a type of affinity or self-similarity function between two points $\\mathbf{x\\_{i}}$ and $\\mathbf{x\\_{j}}$ that uses a Gaussian function in an embedding space:\r
\r
$$ f\\left(\\mathbf{x\\_{i}}, \\mathbf{x\\_{j}}\\right) = e^{\\theta\\left(\\mathbf{x\\_{i}}\\right)^{T}\\phi\\left(\\mathbf{x\\_{j}}\\right)} $$\r
\r
Here $\\theta\\left(x\\_{i}\\right) = W\\_{θ}x\\_{i}$ and $\\phi\\left(x\\_{j}\\right) = W\\_{φ}x\\_{j}$ are two embeddings.\r
\r
Note that the self-attention module used in the original [Transformer](https://paperswithcode.com/method/transformer) model is a special case of non-local operations in the embedded Gaussian version. This can be seen from the fact that for a given $i$, $\\frac{1}{\\mathcal{C}\\left(\\mathbf{x}\\right)}\\sum\\_{\\forall{j}}f\\left(\\mathbf{x}\\_{i}, \\mathbf{x}\\_{j}\\right)g\\left(\\mathbf{x}\\_{j}\\right)$ becomes the [softmax](https://paperswithcode.com/method/softmax) computation along the dimension $j$. So we have $\\mathbf{y} = \\text{softmax}\\left(\\mathbf{x}^{T}W^{T}\\_{\\theta}W\\_{\\phi}\\mathbf{x}\\right)g\\left(\\mathbf{x}\\right)$, which is the self-attention form in the Transformer model. This shows how we can relate this recent self-attention model to the classic computer vision method of non-local means.""" ;
    skos:prefLabel "Embedded Gaussian Affinity" .

:EmbeddingDropout a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1512.05287v5> ;
    rdfs:seeAlso <https://github.com/salesforce/awd-lstm-lm/blob/32fcb42562aeb5c7e6c9dec3f2a3baaaf68a5cb5/embed_regularize.py#L5> ;
    skos:definition """**Embedding Dropout** is equivalent to performing [dropout](https://paperswithcode.com/method/dropout) on the embedding matrix at a word level, where the dropout is broadcast across all the word vector’s embedding. The remaining non-dropped-out word embeddings are scaled by $\\frac{1}{1-p\\_{e}}$ where $p\\_{e}$ is the probability of embedding dropout. As the dropout occurs on the embedding matrix that is used for a full forward and backward pass, this means that all occurrences of a specific word will disappear within that pass, equivalent to performing [variational dropout](https://paperswithcode.com/method/variational-dropout) on the connection between the one-hot embedding and the embedding lookup.\r
\r
Source: Merity et al, Regularizing and Optimizing [LSTM](https://paperswithcode.com/method/lstm) Language Models""" ;
    skos:prefLabel "Embedding Dropout" .

:EmbraceNet a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1904.09078v1> ;
    skos:altLabel "EmbraceNet: A robust deep learning architecture for multimodal classification" ;
    skos:definition "" ;
    skos:prefLabel "EmbraceNet" .

:EncAttAgg a skos:Concept ;
    rdfs:seeAlso <https://github.com/nefujiangping/EncAttAgg> ;
    skos:altLabel "Encoder-Attender-Aggregator" ;
    skos:definition "EncAttAgg introduced two attenders to tackle two problems: 1) We introduce a mutual attender layer to efficiently obtain the entity-pair-specific mention representations. 2) We introduce an integration attender to weight mention pairs of a target entity pair." ;
    skos:prefLabel "EncAttAgg" .

:End-To-EndMemoryNetwork a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1503.08895v5> ;
    rdfs:seeAlso <https://github.com/carpedm20/MemN2N-tensorflow/blob/f8f4da4ca1b3a4afa764748a6907e79ab2be1ffd/model.py#L8> ;
    skos:definition """An **End-to-End Memory Network** is a neural network with a recurrent attention model over a possibly large external memory. The architecture is a form of [Memory Network](https://paperswithcode.com/method/memory-network), but unlike the model in that work, it is trained end-to-end, and hence requires significantly less supervision during training. It can also be seen as an extension of RNNsearch to the case where multiple computational steps (hops) are performed per output symbol.\r
\r
The model takes a discrete set of inputs $x\\_{1}, \\dots, x\\_{n}$ that are to be stored in the memory, a query $q$, and outputs an answer $a$. Each of the $x\\_{i}$, $q$, and $a$ contains symbols coming from a dictionary with $V$ words. The model writes all $x$ to the memory up to a fixed buffer size, and then finds a continuous representation for the $x$ and $q$. The continuous representation is then processed via multiple hops to output $a$.""" ;
    skos:prefLabel "End-To-End Memory Network" .

:EnergyBasedProcess a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2003.07521v2> ;
    skos:definition "**Energy Based Processes** extend energy based models to exchangeable data while allowing neural network parameterizations of the energy function. They extend the previously separate stochastic process and latent variable model perspectives in a common framework. The result is a generalization of [Gaussian processes](https://paperswithcode.com/method/gaussian-process) and Student-t processes that exploits EBMs for greater flexibility." ;
    skos:prefLabel "Energy Based Process" .

:EnhancedFusionFramework a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2101.06968v2> ;
    skos:definition """The **Enhanced Fusion Framework** proposes three different ideas to improve the existing MI-based BCI frameworks.\r
\r
Image source: [Fumanal-Idocin et al.](https://arxiv.org/pdf/2101.06968v1.pdf)""" ;
    skos:prefLabel "Enhanced Fusion Framework" .

:EnsembleClustering a skos:Concept ;
    dcterms:source <https://ieeexplore.ieee.org/document/9338349> ;
    skos:definition """Ensemble clustering, also called consensus clustering, has\r
been attracting much attention in recent years, aiming to combine multiple base clustering algorithms into a better and more consensus clustering. Due to its good performance, ensemble clustering plays a vital role in many research areas, such as community detection and bioinformatics.""" ;
    skos:prefLabel "Ensemble Clustering" .

:EntropyRegularization a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1602.01783v2> ;
    rdfs:seeAlso <https://github.com/ikostrikov/pytorch-a3c/blob/48d95844755e2c3e2c7e48bbd1a7141f7212b63f/train.py#L100> ;
    skos:definition """**Entropy Regularization** is a type of regularization used in [reinforcement learning](https://paperswithcode.com/methods/area/reinforcement-learning). For on-policy policy gradient based methods like [A3C](https://paperswithcode.com/method/a3c), the same mutual  reinforcement behaviour leads to a highly-peaked $\\pi\\left(a\\mid{s}\\right)$ towards a few actions or action sequences, since it is easier for the actor and critic to overoptimise to a small portion of the environment. To reduce this problem, entropy regularization adds an entropy term to the loss to promote action diversity:\r
\r
$$H(X) = -\\sum\\pi\\left(x\\right)\\log\\left(\\pi\\left(x\\right)\\right) $$\r
\r
Image Credit: Wikipedia""" ;
    skos:prefLabel "Entropy Regularization" .

:EpsilonGreedyExploration a skos:Concept ;
    skos:definition """**$\\epsilon$-Greedy Exploration** is an exploration strategy in reinforcement learning that takes an exploratory action with probability $\\epsilon$ and a greedy action with probability $1-\\epsilon$. It tackles the exploration-exploitation tradeoff with reinforcement learning algorithms: the desire to explore the state space with the desire to seek an optimal policy. Despite its simplicity, it is still commonly used as an behaviour policy $\\pi$ in several state-of-the-art reinforcement learning models.\r
\r
Image Credit: [Robin van Embden](https://cran.r-project.org/web/packages/contextual/vignettes/sutton_barto.html)""" ;
    skos:prefLabel "Epsilon Greedy Exploration" .

:EsViT a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2106.09785v2> ;
    skos:definition "**EsViT** proposes two techniques for developing efficient self-supervised vision transformers for visual representation leaning: a multi-stage architecture with sparse self-attention and a new pre-training task of region matching. The multi-stage architecture reduces modeling complexity but with a cost of losing the ability to capture fine-grained correspondences between image regions. The new pretraining task allows the model to capture fine-grained region dependencies and as a result significantly improves the quality of the learned vision representations." ;
    skos:prefLabel "EsViT" .

:EstimationStatistics a skos:Concept ;
    skos:definition "Estimation statistics is a data analysis framework that uses a combination of effect sizes, confidence intervals, precision planning, and meta-analysis to plan experiments, analyze data and interpret results. It is distinct from null hypothesis significance testing (NHST), which is considered to be less informative. The primary aim of estimation methods is to report an effect size (a point estimate) along with its confidence interval, the latter of which is related to the precision of the estimate. The confidence interval summarizes a range of likely values of the underlying population effect. Proponents of estimation see reporting a P value as an unhelpful distraction from the important business of reporting an effect size with its confidence intervals, and believe that estimation should replace significance testing for data analysis." ;
    skos:prefLabel "Estimation Statistics" .

:EuclideanNormRegularization a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1905.06723v2> ;
    skos:definition """**Euclidean Norm Regularization** is a regularization step used in [generative adversarial networks](https://paperswithcode.com/methods/category/generative-adversarial-networks), and is typically added to both the generator and discriminator losses:\r
\r
$$ R\\_{z} = w\\_{r} \\cdot ||\\Delta{z}||^{2}\\_{2} $$\r
\r
where the scalar weight $w\\_{r}$ is a parameter.\r
\r
Image: [LOGAN](https://paperswithcode.com/method/logan)""" ;
    skos:prefLabel "Euclidean Norm Regularization" .

:EvoNorms a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2004.02967v5> ;
    skos:definition "**EvoNorms** are a set of normalization-activation layers that go beyond existing design patterns. Normalization and activation are unified into a single computation graph, its structure is evolved starting from low-level primitives. EvoNorms consist of two series: B series and S series. The B series are batch-dependent and were discovered by our method without any constraint. The S series work on individual samples, and were discovered by rejecting any batch-dependent operations." ;
    skos:prefLabel "EvoNorms" .

:ExactFusionModel a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1911.11929v1> ;
    skos:definition "**Exact Fusion Model (EFM)** is a method for aggregating a feature pyramid. The EFM is based on [YOLOv3](https://paperswithcode.com/method/yolov3), which assigns exactly one bounding-box prior to each ground truth object. Each ground truth bounding box corresponds to one anchor box that surpasses the threshold IoU. If the size of an anchor box is equivalent to the field-of-view of the grid cell, then for the grid cells of the $s$-th scale, the corresponding bounding box will be lower bounded by the $(s − 1)$th scale and upper bounded by the (s + 1)th scale. Therefore, the EFM assembles features from the three scales." ;
    skos:prefLabel "Exact Fusion Model" .

:ExpectedSarsa a skos:Concept ;
    skos:definition """**Expected Sarsa** is like [Q-learning](https://paperswithcode.com/method/q-learning) but instead of taking the maximum over next state-action pairs, we use the expected value, taking into account how likely each action is under the current policy.\r
\r
$$Q\\left(S\\_{t}, A\\_{t}\\right) \\leftarrow Q\\left(S\\_{t}, A\\_{t}\\right) + \\alpha\\left[R_{t+1} + \\gamma\\sum\\_{a}\\pi\\left(a\\mid{S\\_{t+1}}\\right)Q\\left(S\\_{t+1}, a\\right) - Q\\left(S\\_{t}, A\\_{t}\\right)\\right] $$\r
\r
Except for this change to the update rule, the algorithm otherwise follows the scheme of Q-learning. It is more computationally expensive than [Sarsa](https://paperswithcode.com/method/sarsa) but it eliminates the variance due to the random selection of $A\\_{t+1}$.\r
\r
Source: Sutton and Barto, Reinforcement Learning, 2nd Edition""" ;
    skos:prefLabel "Expected Sarsa" .

:ExperienceReplay a skos:Concept ;
    skos:definition """**Experience Replay** is a replay memory technique used in reinforcement learning where we store the agent’s experiences at each time-step, $e\\_{t} = \\left(s\\_{t}, a\\_{t}, r\\_{t}, s\\_{t+1}\\right)$ in a data-set $D = e\\_{1}, \\cdots, e\\_{N}$ , pooled over many episodes into a replay memory. We then usually sample the memory randomly for a minibatch of experience, and use this to learn off-policy, as with Deep Q-Networks. This tackles the problem of autocorrelation leading to unstable training, by making the problem more like a supervised learning problem.\r
\r
Image Credit: [Hands-On Reinforcement Learning with Python, Sudharsan Ravichandiran](https://subscription.packtpub.com/book/big_data_and_business_intelligence/9781788836524)""" ;
    skos:prefLabel "Experience Replay" .

:ExplanationvsAttention a skos:Concept ;
    skos:altLabel "Explanation vs Attention: A Two-Player Game to Obtain Attention for VQA" ;
    skos:definition "In this paper, we aim to obtain improved attention for a visual question answering (VQA) task. It is challenging to provide supervision for attention. An observation we make is that visual explanations as obtained through class activation mappings (specifically Grad-[CAM](https://paperswithcode.com/method/cam)) that are meant to explain the performance of various networks could form a means of supervision. However, as the distributions of attention maps and that of Grad-CAMs differ, it would not be suitable to directly use these as a form of supervision. Rather, we propose the use of a discriminator that aims to distinguish samples of visual explanation and attention maps. The use of adversarial training of the attention regions as a two-player game between attention and explanation serves to bring the distributions of attention maps and visual explanations closer. Significantly, we observe that providing such a means of supervision also results in attention maps that are more closely related to human attention resulting in a substantial improvement over baseline stacked attention network (SAN) models. It also results in a good improvement in rank correlation metric on the VQA task. This method can also be combined with recent MCB based methods and results in consistent improvement. We also provide comparisons with other means for learning distributions such as based on Correlation Alignment (Coral), Maximum Mean Discrepancy (MMD) and Mean Square Error (MSE) losses and observe that the adversarial loss outperforms the other forms of learning the attention maps. Visualization of the results also confirms our hypothesis that attention maps improve using this form of supervision." ;
    skos:prefLabel "Explanation vs Attention" .

:ExponentialDecay a skos:Concept ;
    skos:definition """**Exponential Decay** is a learning rate schedule where we decay the learning rate with more iterations using an exponential function:\r
\r
$$ \\text{lr} = \\text{lr}\\_{0}\\exp\\left(-kt\\right) $$\r
\r
Image Credit: [Suki Lau](https://towardsdatascience.com/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1)""" ;
    skos:prefLabel "Exponential Decay" .

:ExtremeNet a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1901.08043v3> ;
    rdfs:seeAlso <https://github.com/xingyizhou/ExtremeNet> ;
    skos:definition """**ExtremeNet** is a a bottom-up object detection framework that detects four extreme points (top-most, left-most, bottom-most, right-most) of an object. It uses a keypoint estimation framework to find extreme points, by predicting four multi-peak heatmaps for each object category. In addition, it uses one [heatmap](https://paperswithcode.com/method/heatmap) per category predicting the object center, as the average of two bounding box edges in both the x and y dimension. We group extreme points into objects with a purely geometry-based approach. We group four extreme points, one from each map, if and only if their\r
geometric center is predicted in the center heatmap with a score higher than a pre-defined threshold, We enumerate all $O\\left(n^{4}\\right)$ combinations of extreme point prediction, and select the valid ones.""" ;
    skos:prefLabel "ExtremeNet" .

:F2DNet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2203.02331v2> ;
    skos:altLabel "Fast Focal Detection Network" ;
    skos:definition """F2DNet, a novel two-stage object detection architecture which eliminates redundancy of classical two-stage detectors by replacing the region proposal network with focal detection network and\r
bounding box head with fast suppression head.""" ;
    skos:prefLabel "F2DNet" .

:FA a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1411.0247v1> ;
    skos:altLabel "Feedback Alignment" ;
    skos:definition "" ;
    skos:prefLabel "FA" .

:FASFA a skos:Concept ;
    skos:altLabel "FASFA: A Novel Next-Generation Backpropagation Optimizer" ;
    skos:definition "This paper introduces the fast adaptive stochastic function accelerator (FASFA) for gradient-based optimization of stochastic objective functions. It works based on Nesterov-enhanced first and second momentum estimates. The method is simple and effective during implementation because it has intuitive/familiar hyperparameterization. The training dynamics can be progressive or conservative depending on the decay rate sum. It works well with a low learning rate and mini batch size. Experiments and statistics showed convincing evidence that FASFA could be an ideal candidate for optimizing stochastic objective functions, particularly those generated by multilayer perceptrons with convolution and dropout layers. In addition, the convergence properties and regret bound provide results aligning with the online convex optimization framework. In a first of its kind, FASFA addresses the growing need for diverse optimizers by providing next-generation training dynamics for artificial intelligence algorithms. Future experiments could modify FASFA based on the infinity norm." ;
    skos:prefLabel "FASFA" .

<http://w3id.org/mlso/vocab/ml_algorithm/FAVOR+> a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2009.14794v4> ;
    skos:altLabel "Fast Attention Via Positive Orthogonal Random Features" ;
    skos:definition """**FAVOR+**, or **Fast Attention Via Positive Orthogonal Random Features**, is an efficient attention mechanism used in the [Performer](https://paperswithcode.com/method/performer) architecture which leverages approaches such as kernel methods and random features approximation for approximating [softmax](https://paperswithcode.com/method/softmax) and Gaussian kernels. \r
\r
FAVOR+ works for attention blocks using matrices $\\mathbf{A} \\in \\mathbb{R}^{L×L}$ of the form $\\mathbf{A}(i, j) = K(\\mathbf{q}\\_{i}^{T}, \\mathbf{k}\\_{j}^{T})$, with $\\mathbf{q}\\_{i}/\\mathbf{k}\\_{j}$ standing for the $i^{th}/j^{th}$ query/key row-vector in $\\mathbf{Q}/\\mathbf{K}$ and kernel $K : \\mathbb{R}^{d } × \\mathbb{R}^{d} \\rightarrow \\mathbb{R}\\_{+}$ defined for the (usually randomized) mapping: $\\phi : \\mathbb{R}^{d } → \\mathbb{R}^{r}\\_{+}$ (for some $r > 0$) as:\r
\r
$$K(\\mathbf{x}, \\mathbf{y}) = E[\\phi(\\mathbf{x})^{T}\\phi(\\mathbf{y})] $$\r
\r
We call $\\phi(\\mathbf{u})$ a random feature map for $\\mathbf{u} \\in \\mathbb{R}^{d}$ . For $\\mathbf{Q}^{'}, \\mathbf{K}^{'} \\in \\mathbb{R}^{L \\times r}$ with rows given as $\\phi(\\mathbf{q}\\_{i}^{T})^{T}$ and $\\phi(\\mathbf{k}\\_{i}^{T})^{T}$  respectively, this leads directly to the efficient attention mechanism of the form:\r
\r
$$ \\hat{Att\\_{\\leftrightarrow}}\\left(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}\\right) = \\hat{\\mathbf{D}}^{-1}(\\mathbf{Q^{'}}((\\mathbf{K^{'}})^{T}\\mathbf{V}))$$\r
\r
where\r
\r
$$\\mathbf{\\hat{D}} = \\text{diag}(\\mathbf{Q^{'}}((\\mathbf{K^{'}})\\mathbf{1}\\_{L})) $$\r
\r
The above scheme constitutes the [FA](https://paperswithcode.com/method/dfa)-part of the FAVOR+ mechanism. The other parts are achieved by:\r
\r
- The R part :  The softmax kernel is approximated though trigonometric functions, in the form of a regularized softmax-kernel SMREG, that employs positive random features (PRFs).\r
- The OR+ part : To reduce the variance of the estimator, so we can use a smaller number of random features, different samples are entangled to be exactly orthogonal using the Gram-Schmidt orthogonalization procedure.\r
\r
The details are quite technical, so it is recommended you read the paper for further information on these steps.""" ;
    skos:prefLabel "FAVOR+" .

:FBNet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1812.03443v3> ;
    rdfs:seeAlso <https://github.com/AnnaAraslanova/FBNet/blob/ad683c69fa3029cfea5d819613bf8d4691fd2b91/fbnet_building_blocks/fbnet_builder.py#L699> ;
    skos:definition "**FBNet** is a type of convolutional neural architectures discovered through [DNAS](https://paperswithcode.com/method/dnas) [neural architecture search](https://paperswithcode.com/method/neural-architecture-search). It utilises a basic type of image model block inspired by [MobileNetv2](https://paperswithcode.com/method/mobilenetv2) that utilises depthwise convolutions and an inverted residual structure (see components)." ;
    skos:prefLabel "FBNet" .

:FBNetBlock a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1812.03443v3> ;
    rdfs:seeAlso <https://github.com/AnnaAraslanova/FBNet/blob/ad683c69fa3029cfea5d819613bf8d4691fd2b91/fbnet_building_blocks/fbnet_builder.py#L451> ;
    skos:definition "**FBNet Block** is an image model block used in the [FBNet](https://paperswithcode.com/method/fbnet) architectures discovered through [DNAS](https://paperswithcode.com/method/dnas) [neural architecture search](https://paperswithcode.com/method/neural-architecture-search). The basic building blocks employed are [depthwise convolutions](https://paperswithcode.com/method/depthwise-convolution) and a [residual connection](https://paperswithcode.com/method/residual-connection)." ;
    skos:prefLabel "FBNet Block" .

:FCN a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1605.06211v1> ;
    rdfs:seeAlso <https://github.com/Jackey9797/FCN> ;
    skos:altLabel "Fully Convolutional Network" ;
    skos:definition """**Fully Convolutional Networks**, or **FCNs**, are an architecture used mainly for semantic segmentation. They employ solely locally connected layers, such as [convolution](https://paperswithcode.com/method/convolution), pooling and upsampling. Avoiding the use of dense layers means less parameters (making the networks faster to train). It also means an FCN can work for variable image sizes given all connections are local.\r
\r
The network consists of a downsampling path, used to extract and interpret the context, and an upsampling path, which allows for localization. \r
\r
FCNs also employ skip connections to recover the fine-grained spatial information lost in the downsampling path.""" ;
    skos:prefLabel "FCN" .

:FCOS a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1904.01355v5> ;
    rdfs:seeAlso <https://github.com/tianzhi0549/FCOS/blob/9a015285a44463020e020094126c6768267d44b9/fcos/fcos.py#L79> ;
    skos:definition "**FCOS** is an anchor-box free, proposal free, single-stage object detection model. By eliminating the predefined set of anchor boxes, FCOS avoids computation related to anchor boxes such as calculating overlapping during training. It also avoids all hyper-parameters related to anchor boxes, which are often very sensitive to the final detection performance." ;
    skos:prefLabel "FCOS" .

:FCPose a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2105.14185v1> ;
    skos:definition """**FCPose** is a fully convolutional multi-person [pose estimation framework](https://paperswithcode.com/methods/category/pose-estimation-models) using dynamic instance-aware convolutions. Different from existing methods, which often require ROI (Region of Interest) operations and/or grouping post-processing, FCPose eliminates the ROIs and grouping pre-processing with dynamic instance aware keypoint estimation heads. The dynamic keypoint heads are conditioned on each instance (person), and can encode the instance concept in the dynamically-generated weights of their filters. \r
\r
Overall, FCPose is built upon the one-stage object detector [FCOS](https://paperswithcode.com/method/fcos). The controller that generates the weights of the keypoint heads is attached to the FCOS heads. The weights $\\theta\\_{i}$ generated by the controller is used to fulfill the keypoint head $f$ for the instance $i$. Moreover, a keypoint refinement module is introduced to predict the offsets from each location of the heatmaps to the ground-truth keypoints. Finally, the coordinates derived from the predicted heatmaps are refined by the offsets predicted by the keypoint refinement module, resulting in the final keypoint results. "Rel. coord." is a map of the relative coordinates from all the locations of the feature maps $F$ to the location where the weights are generated. The relative coordinate map is concatenated to $F$ as the input to the keypoint head.""" ;
    skos:prefLabel "FCPose" .

:FEFM a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2009.09931v2> ;
    skos:altLabel "Field Embedded Factorization Machine" ;
    skos:definition """**Field Embedded Factorization Machine**, or **FEFM**, is a factorization machine variant. For each field pair, FEFM introduces symmetric matrix embeddings along with the usual feature vector embeddings that are present in FM. Like FM, $v\\_{i}$ is the vector embedding of the $i^{t h}$ feature. However, unlike Field-Aware Factorization Machines (FFMs), FEFM doesn't explicitly learn field-specific feature embeddings. The learnable symmetric matrix $W\\_{F(i), F(j)}$ is the embedding for the field pair $F(i)$ and $F(j) .$ The interaction between the $i^{t h}$ feature and the $j^{t h}$ feature is mediated through $W_{F(i), F(j)} .$\r
\r
$$\r
\\phi(\\theta, x)=\\phi\\_{F E F M}((w, v, W), x)=w\\_{0}+\\sum\\_{i=1}^{m} w_{i} x_{i}+\\sum\\_{i=1}^{m} \\sum\\_{j=i+1}^{m} v\\_{i}^{T} W\\_{F(i), F(j)} v\\_{j} x\\_{i} x\\_{j}\r
$$\r
\r
where $W\\_{F(i), F(j)}$ is a $k \\times k$ symmetric matrix ( $k$ is the dimension of the feature vector embedding space containing feature vectors $v\\_{i}$ and $v\\_{j}$ ).\r
\r
The symmetric property of the learnable matrix $W\\_{F(i), F(j)}$ is ensured by reparameterizing $W\\_{F(i), F(j)}$ as $U\\_{F(i), F(j)}+$ $U\\_{F(i), F(j)}^{T}$, where $U\\_{F(i), F(j)}^{T}$ is the transpose of the learnable matrix $U\\_{F(i), F(j)} .$ Note that $W_{F(i), F(j)}$ can also be interpreted as a vector transformation matrix which transforms a feature embedding when interacting with a specific field.""" ;
    skos:prefLabel "FEFM" .

:FFB6D a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2103.02242v1> ;
    skos:definition "**FFB6D** is a full flow bidirectional fusion network for 6D pose estimation of known objects from a single RGBD image. Unlike previous works that extract the RGB and point cloud features independently and fuse them in the final stage, FFB6D builds bidirectional fusion modules as communication bridges in the full flow of the two networks. In this way, the two networks can obtain complementary information from the other and learn representations containing rich appearance and geometry information of the scene." ;
    skos:prefLabel "FFB6D" .

:FFF a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2308.14711v2> ;
    skos:altLabel "Fast Feedforward Networks" ;
    skos:definition "A log-time alternative to feedforward layers outperforming both the vanilla feedforward and mixture-of-experts approaches." ;
    skos:prefLabel "FFF" .

:FFMv1 a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1811.04533v3> ;
    rdfs:seeAlso <https://github.com/qijiezhao/M2Det/blob/ade4f3d12979800c367bf1e46d2e316e73a87514/m2det.py> ;
    skos:altLabel "Feature Fusion Module v1" ;
    skos:definition "**Feature Fusion Module v1** is a feature fusion module from the [M2Det](https://paperswithcode.com/method/m2det) object detection model, and feature fusion modules are crucial for constructing the final multi-level feature pyramid. They use [1x1 convolution](https://paperswithcode.com/method/1x1-convolution) layers to compress the channels of the input features and use concatenation operation to aggregate these feature map. FFMv1 takes two feature maps with different scales in backbone as input, it adopts one upsample operation to rescale the deep features to the same scale before the concatenation operation." ;
    skos:prefLabel "FFMv1" .

:FFMv2 a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1811.04533v3> ;
    rdfs:seeAlso <https://github.com/qijiezhao/M2Det/blob/de4a6241bf22f7e7f46cb5cb1eb95615fd0a5e12/m2det.py#L44> ;
    skos:altLabel "Feature Fusion Module v2" ;
    skos:definition "**Feature Fusion Module v2** is a feature fusion module from the [M2Det](https://paperswithcode.com/method/m2det) object detection model, and is crucial for constructing the final multi-level feature pyramid. They use [1x1 convolution](https://paperswithcode.com/method/1x1-convolution) layers to compress the channels of the input features and use a concatenation operation to aggregate these feature map. FFMv2 takes the base feature and the largest output feature map of the previous [Thinned U-Shape Module](https://paperswithcode.com/method/tum) (TUM) – these two are of the same scale – as input, and produces the fused feature for the next TUM." ;
    skos:prefLabel "FFMv2" .

:FGA a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1904.05880v3> ;
    rdfs:seeAlso <https://github.com/idansc/fga/blob/master/atten.py> ;
    skos:altLabel "Factor Graph Attention" ;
    skos:definition "A general multimodal attention unit for any number of modalities. Graphical models inspire it, i.e., it infers several attention beliefs via aggregated interaction messages." ;
    skos:prefLabel "FGA" .

:FIERCE a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2208.03684v1> ;
    skos:altLabel "Feature Information Entropy Regularized Cross Entropy" ;
    skos:definition "FIERCE is an entropic regularization on the **feature** space" ;
    skos:prefLabel "FIERCE" .

:FINCHClustering a skos:Concept ;
    dcterms:source <http://openaccess.thecvf.com/content_CVPR_2019/html/Sarfraz_Efficient_Parameter-Free_Clustering_Using_First_Neighbor_Relations_CVPR_2019_paper.html> ;
    rdfs:seeAlso <https://github.com/ssarfraz/FINCH-Clustering> ;
    skos:altLabel "First Integer Neighbor Clustering Hierarchy (FINCH))" ;
    skos:definition "FINCH is a parameter-free fast and scalable clustering algorithm. it stands out for its speed and clustering quality." ;
    skos:prefLabel "FINCH Clustering" .

:FLAVA a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2112.04482v3> ;
    skos:definition "FLAVA aims at building a single holistic universal model that targets all modalities at once. FLAVA is a language vision alignment model that learns strong representations from multimodal data (image-text pairs) and unimodal data (unpaired images and text). The model consists of an image encode transformer to capture unimodal image representations, a text encoder transformer to process unimodal text information, and a multimodal encode transformer that takes as input the encoded unimodal image and text and integrates their representations for multimodal reasoning. During pretraining, masked image modeling (MIM) and mask language modeling (MLM) losses are applied onto the image and text encoders over a single image or a text piece, respectively, while contrastive, masked multimodal modeling (MMM), and image-text matching (ITM) loss are used over paired image-text data. For downstream tasks, classification heads are applied on the outputs from the image, text, and multimodal encoders respectively for visual recognition, language understanding, and multimodal reasoning tasks It can be applied to broad scope of tasks from three domains (visual recognition, language understanding, and multimodal reasoning) under a common transformer model architecture." ;
    skos:prefLabel "FLAVA" .

:FLAVR a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2012.08512v3> ;
    skos:definition """**FLAVR** is an architecture for video frame interpolation. It uses 3D space-time convolutions to enable end-to-end learning and inference for video frame interpolation. Overall, it consists of a [U-Net](https://paperswithcode.com/method/u-net) style architecture with 3D space-time convolutions and\r
deconvolutions (yellow blocks). Channel gating is used after all (de-)[convolution](https://paperswithcode.com/method/convolution) layers (blue blocks). The final prediction layer (the purple block) is implemented as a convolution layer to project the 3D feature maps into $(k−1)$ frame predictions. This design allows FLAVR to predict multiple frames in one inference forward pass.""" ;
    skos:prefLabel "FLAVR" .

:FLICA a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1603.01570v2> ;
    rdfs:seeAlso <https://github.com/DarkEyes/mFLICA> ;
    skos:altLabel "A Framework for Leader Identification in Coordinated Activity" ;
    skos:definition """An agreement of a group to follow a common purpose is manifested by its coalescence into a coordinated behavior. The process of initiating this behavior and the period of decision-making by the group members necessarily precedes the coordinated behavior. Given time series of group members’ behavior, the goal is to find these periods of decision-making and identify the initiating individual, if one exists.\r
\r
Image Source:  [Amornbunchornvej et al.](https://arxiv.org/pdf/1603.01570v2.pdf)""" ;
    skos:prefLabel "FLICA" .

:FMix a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2002.12047v3> ;
    skos:definition "A variant of [CutMix](https://paperswithcode.com/method/cutmix) which randomly samples masks from Fourier space." ;
    skos:prefLabel "FMix" .

:FMwithsplines a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2305.14528v1> ;
    skos:altLabel "Factorization machines with cubic splines for numerical features" ;
    skos:definition "Using cubic splines to improve factorization machine accuracy with numerical features" ;
    skos:prefLabel "FM with splines" .

:FORK a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2010.01652v3> ;
    skos:altLabel "Forward-Looking Actor" ;
    skos:definition """**FORK**, or **Forward Looking Actor** is a type of actor for actor-critic algorithms. In particular, FORK includes a neural network that forecasts the next state given the current state and current action, called system network; and a neural network that forecasts the\r
reward given a (state, action) pair, called reward network. With the system network and reward network, FORK can forecast the next state and consider the value of the next state when improving the policy.""" ;
    skos:prefLabel "FORK" .

:FPG a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2004.03580v1> ;
    skos:altLabel "Feature Pyramid Grid" ;
    skos:definition """**Feature Pyramid Grids**, or **FPG**, is a deep multi-pathway feature pyramid, that represents the feature scale-space as a regular grid of parallel bottom-up pathways which are fused by multi-directional lateral connections. It connects the backbone features, $C$, of a ConvNet with a regular structure of $p$ parallel top-down pyramid pathways which are fused by multi-directional lateral connections, AcrossSame, AcrossUp, AcrossDown, and AcrossSkip. AcrossSkip are direct connections while all other types use [convolutional](https://paperswithcode.com/method/convolution) and [ReLU](https://paperswithcode.com/method/relu) layers.\r
\r
On a high-level, FPG is a deep generalization of [FPN](https://paperswithcode.com/method/fpn) from one to $p$ pathways under a dense lateral connectivity structure.""" ;
    skos:prefLabel "FPG" .

:FPN a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1612.03144v2> ;
    rdfs:seeAlso <https://github.com/facebookresearch/Detectron/blob/8170b25b425967f8f1c7d715bea3c5b8d9536cd8/detectron/modeling/FPN.py#L117> ;
    skos:altLabel "Feature Pyramid Network" ;
    skos:definition """A **Feature Pyramid Network**, or **FPN**, is a feature extractor that takes a single-scale image of an arbitrary size as input, and outputs proportionally sized feature maps at multiple levels, in a fully convolutional fashion. This process is independent of the backbone convolutional architectures. It therefore acts as a generic solution for building feature pyramids inside deep convolutional networks to be used in tasks like object detection.\r
\r
The construction of the pyramid involves a bottom-up pathway and a top-down pathway.\r
\r
The bottom-up pathway is the feedforward computation of the backbone ConvNet, which computes a feature hierarchy consisting of feature maps at several scales with a scaling step of 2. For the feature\r
pyramid, one pyramid level is defined for each stage. The output of the last layer of each stage is used as a reference set of feature maps. For [ResNets](https://paperswithcode.com/method/resnet) we use the feature activations output by each stage’s last [residual block](https://paperswithcode.com/method/residual-block). \r
\r
The top-down pathway hallucinates higher resolution features by upsampling spatially coarser, but semantically stronger, feature maps from higher pyramid levels. These features are then enhanced with features from the bottom-up pathway via lateral connections. Each lateral connection merges feature maps of the same spatial size from the bottom-up pathway and the top-down pathway. The bottom-up feature map is of lower-level semantics, but its activations are more accurately localized as it was subsampled fewer times.""" ;
    skos:prefLabel "FPN" .

:FRILL a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2011.04609v5> ;
    skos:definition "**FRILL** is a non-semantic speech embedding model trained via knowledge distillation that is fast enough to be run in real-time on a mobile device. The fastest model runs at 0.9 ms, which is 300x faster than TRILL and 25x faster than TRILL-distilled." ;
    skos:prefLabel "FRILL" .

:FSAF a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1903.00621v1> ;
    rdfs:seeAlso <https://github.com/hdjang/Feature-Selective-Anchor-Free-Module-for-Single-Shot-Object-Detection/blob/d6ba356e0e9b65d658752fa7b712368addec7e31/mmdet/models/detectors/fsaf.py#L8> ;
    skos:definition """**FSAF**, or Feature Selective Anchor-Free, is a building block for single-shot object detectors. It can be plugged into single-shot detectors with feature pyramid structure. The FSAF module addresses two limitations brought up by the conventional anchor-based detection: 1) heuristic-guided feature selection; 2) overlap-based anchor sampling. The general concept of the FSAF module is online feature selection applied to the training of multi-level anchor-free branches. Specifically, an anchor-free branch is attached to each level of the feature pyramid, allowing box encoding and decoding in the anchor-free manner at an arbitrary level. During training, we dynamically assign each instance to the most suitable feature level. At the time of inference, the FSAF module can work jointly with anchor-based branches by outputting predictions in parallel. We instantiate this concept with simple implementations of anchor-free branches and online feature selection strategy\r
\r
The general concept is presented in the Figure to the right. An anchor-free branch is built per level of feature pyramid, independent to the anchor-based branch. Similar to the anchor-based branch, it consists of a classification subnet and a regression subnet (not shown in figure). An instance can be assigned to arbitrary level of the anchor-free branch. During training, we dynamically select the most suitable level of feature for each instance based on the instance content instead of just the size of instance box. The selected level of feature then learns to detect the assigned instances. At inference, the FSAF module can run independently or jointly with anchor-based branches. The FSAF module is agnostic to the backbone network and can be applied to single-shot detectors with a structure of feature pyramid. Additionally, the instantiation of anchor-free branches and online feature selection can be various.""" ;
    skos:prefLabel "FSAF" .

:FT-Transformer a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2106.11959v3> ;
    skos:definition "FT-Transformer (Feature Tokenizer + Transformer) is a simple adaptation of the [Transformer](/method/transformer) architecture for the tabular domain. The model (Feature Tokenizer component) transforms all features (categorical and numerical) to tokens and runs a stack of Transformer layers over the tokens, so every Transformer layer operates on the feature level of one object. (This model is similar to [AutoInt](/method/autoint)). In the Transformer component, the `[CLS]` token is appended to $T$. Then $L$ Transformer layers are applied. PreNorm is used for easier optimization and good performance. The final representation of the `[CLS]` token is used for prediction." ;
    skos:prefLabel "FT-Transformer" .

:FactorizedDenseSynthesizedAttention a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2005.00743v3> ;
    skos:definition """**Factorized Dense Synthesized Attention** is a synthesized attention mechanism, similar to [dense synthesized attention](https://paperswithcode.com/method/dense-synthesized-attention), but we factorize the outputs to reduce parameters and prevent overfitting. It was proposed as part of the [Synthesizer](https://paperswithcode.com/method/synthesizer) architecture. The factorized variant of the dense synthesizer can be expressed as follows:\r
\r
$$A, B = F\\_{A}\\left(X\\_{i}\\right), F\\_{B}\\left(X\\_{i}\\right)$$\r
\r
where $F\\_{A}\\left(.\\right)$ projects input $X\\_{i}$ into $a$ dimensions, $F\\_B\\left(.\\right)$ projects $X\\_{i}$ to $b$ dimensions, and $a \\text{ x } b = l$. The output of the factorized module is now written as:\r
\r
$$ Y = \\text{Softmax}\\left(C\\right)G\\left(X\\right) $$\r
\r
where $C = H\\_{A}\\left(A\\right) * H\\_{B}\\left(B\\right)$, where $H\\_{A}$, $H\\_{B}$ are tiling functions and $C \\in \\mathbb{R}^{l \\text{ x } l}$. The tiling function simply duplicates the vector $k$ times, i.e., $\\mathbb{R}^{l} \\rightarrow \\mathbb{R}^{lk}$. In this case, $H\\_{A}\\left(\\right)$ is a projection of $\\mathbb{R}^{a} \\rightarrow \\mathbb{R}^{ab}$ and $H\\_{B}\\left(\\right)$ is a projection of $\\mathbb{R}^{b} \\rightarrow \\mathbb{R}^{ba}$. To avoid having similar values within the same block, we compose the outputs of $H\\_{A}$ and $H\\_{B}$.""" ;
    skos:prefLabel "Factorized Dense Synthesized Attention" .

:FactorizedRandomSynthesizedAttention a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2005.00743v3> ;
    skos:definition """**Factorized Random Synthesized Attention**, introduced with the [Synthesizer](https://paperswithcode.com/method/synthesizer) architecture, is similar to [factorized dense synthesized attention](https://paperswithcode.com/method/factorized-dense-synthesized-attention) but for random synthesizers. Letting $R$ being a randomly initialized matrix, we factorize $R$ into low rank matrices $R\\_{1}, R\\_{2} \\in \\mathbb{R}^{l\\text{ x}k}$ in the attention function:\r
\r
$$ Y = \\text{Softmax}\\left(R\\_{1}R\\_{2}^{T}\\right)G\\left(X\\right) . $$\r
\r
Here $G\\left(.\\right)$ is a parameterized function that is equivalent to $V$ in [Scaled Dot-Product Attention](https://paperswithcode.com/method/scaled).\r
\r
For each head, the factorization reduces the parameter costs from $l^{2}$ to $2\\left(lk\\right)$ where\r
$k << l$ and hence helps prevent overfitting. In practice, we use a small value of $k = 8$.\r
\r
The basic idea of a  Random Synthesizer is to not rely on pairwise token interactions or any information from individual token but rather to learn a task-specific alignment that works well globally across many samples.""" ;
    skos:prefLabel "Factorized Random Synthesized Attention" .

:FairMOT a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2004.01888v6> ;
    skos:definition "**FairMOT** is a model for multi-object tracking which consists of two homogeneous branches to predict pixel-wise objectness scores and re-ID features. The achieved fairness between the tasks is used to achieve high levels of detection and tracking accuracy. The detection branch is implemented in an anchor-free style which estimates object centers and sizes represented as position-aware measurement maps. Similarly, the re-ID branch estimates a re-ID feature for each pixel to characterize the object centered at the pixel. Note that the two branches are completely homogeneous which essentially differs from the previous methods which perform detection and re-ID in a cascaded style. It is also worth noting that FairMOT operates on high-resolution feature maps of strides four while the previous anchor-based methods operate on feature maps of stride 32. The elimination of anchors as well as the use of high-resolution feature maps better aligns re-ID features to object centers which significantly improves the tracking accuracy." ;
    skos:prefLabel "FairMOT" .

:FashionCLIP a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2204.03972v4> ;
    rdfs:seeAlso <https://github.com/patrickjohncyh/fashion-clip> ;
    skos:definition "FashionCLIP is a fine-tuned CLIP model on fashion data (more than 800K pairs). It is the first foundation model for Fashion." ;
    skos:prefLabel "FashionCLIP" .

:Fast-OCR a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2009.10181v5> ;
    skos:definition "Fast-OCR is a new lightweight detection network that incorporates features from existing models focused on the speed/accuracy trade-off, such as [YOLOv2](https://paperswithcode.com/method/yolov2), [CR-NET](https://paperswithcode.com/method/cr-net), and Fast-[YOLOv4](https://paperswithcode.com/method/yolov4)." ;
    skos:prefLabel "Fast-OCR" .

:Fast-YOLOv2 a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1612.08242v1> ;
    skos:definition "" ;
    skos:prefLabel "Fast-YOLOv2" .

:Fast-YOLOv3 a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1804.02767v1> ;
    skos:definition "" ;
    skos:prefLabel "Fast-YOLOv3" .

:Fast-YOLOv4-SmallObj a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2009.10181v5> ;
    skos:definition "The Fast-YOLOv4-SmallObj model is a modified version of Fast-[YOLOv4](https://paperswithcode.com/method/yolov4) to improve the detection of small objects. Seven layers were added so that it predicts bounding boxes at 3 different scales instead of 2." ;
    skos:prefLabel "Fast-YOLOv4-SmallObj" .

:FastAutoAugment a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1905.00397v2> ;
    rdfs:seeAlso <https://github.com/kakaobrain/fast-autoaugment> ;
    skos:definition "**Fast AutoAugment** is an image data augmentation algorithm that finds effective augmentation policies via a search strategy based on density matching, motivated by Bayesian DA. The strategy is to improve the generalization performance of a given network by learning the augmentation policies which treat augmented data as missing data points of training data. However, different from Bayesian DA, the proposed method recovers those missing data points by the exploitation-and-exploration of a family of inference-time augmentations via Bayesian optimization in the policy search phase. This is realized by using an efficient density matching algorithm that does not require any back-propagation for network training for each policy evaluation." ;
    skos:prefLabel "Fast AutoAugment" .

:FastGCN a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1801.10247v1> ;
    skos:definition """FastGCN is a fast improvement of the GCN model recently proposed by Kipf & Welling (2016a) for learning graph embeddings. It generalizes transductive training to an inductive manner and also addresses the memory bottleneck issue of GCN caused by recursive expansion of neighborhoods. The crucial ingredient is a sampling scheme in the reformulation of the loss and the gradient, well justified through an alternative view of graph convoluntions in the form of integral transforms of embedding functions.\r
\r
Description and image from: [FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling](https://arxiv.org/pdf/1801.10247.pdf)""" ;
    skos:prefLabel "FastGCN" .

:FastMinimum-NormAttack a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2102.12827v3> ;
    skos:definition "**Fast Minimum-Norm Attack**, or **FNM**, is a type of adversarial attack that works with different $\\ell_{p}$-norm perturbation models ($p=0,1,2,\\infty$), is robust to hyperparameter choices, does not require adversarial starting points, and converges within few lightweight steps. It works by iteratively finding the sample misclassified with maximum confidence within an $\\ell_{p}$-norm constraint of size $\\epsilon$, while adapting $\\epsilon$ to minimize the distance of the current sample to the decision boundary." ;
    skos:prefLabel "Fast Minimum-Norm Attack" .

:FastMoE a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2103.13262v1> ;
    skos:definition "**FastMoE ** is a distributed MoE training system based on PyTorch with common accelerators. The system provides a hierarchical interface for both flexible model design and adaption to different applications, such as [Transformer-XL](https://paperswithcode.com/method/transformer-xl) and Megatron-LM." ;
    skos:prefLabel "FastMoE" .

:FastPitch a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2006.06873v2> ;
    skos:definition """**FastPitch** is a fully-parallel text-to-speech model based on FastSpeech, conditioned on fundamental frequency contours. The architecture of FastPitch is shown in the Figure. It is based on FastSpeech and composed mainly of two feed-forward [Transformer](https://paperswithcode.com/method/transformer) (FFTr) stacks. The first one operates in the resolution of input tokens, the second one in the resolution of the output frames. Let $x=\\left(x\\_{1}, \\ldots, x\\_{n}\\right)$ be the sequence of input lexical units, and $\\mathbf{y}=\\left(y\\_{1}, \\ldots, y\\_{t}\\right)$ be the sequence of target mel-scale spectrogram frames. The first FFTr stack produces the hidden representation $\\mathbf{h}=\\operatorname{FFTr}(\\mathbf{x})$. The hidden representation $h$ is used to make predictions about the duration and average pitch of every character with a 1-D CNN \r
\r
$$\r
\\hat{\\mathbf{d}}=\\text { DurationPredictor }(\\mathbf{h}), \\quad \\hat{\\mathbf{p}}=\\operatorname{PitchPredictor}(\\mathbf{h})\r
$$\r
\r
where $\\hat{\\mathbf{d}} \\in \\mathbb{N}^{n}$ and $\\hat{\\mathbf{p}} \\in \\mathbb{R}^{n}$. Next, the pitch is projected to match the dimensionality of the hidden representation $h \\in$ $\\mathbb{R}^{n \\times d}$ and added to $\\mathbf{h}$. The resulting sum $\\mathbf{g}$ is discretely upsampled and passed to the output FFTr, which produces the output mel-spectrogram sequence\r
\r
$$\r
\\mathbf{g}=\\mathbf{h}+\\operatorname{PitchEmbedding}(\\mathbf{p})\r
$$\r
\r
$$\r
\\hat{\\mathbf{y}}=\\operatorname{FFTr}\\left([\\underbrace{g\\_{1}, \\ldots, g\\_{1}}\\_{d\\_{1}}, \\ldots \\underbrace{g\\_{n}, \\ldots, g\\_{n}}_{d\\_{n}}]\\right)\r
$$\r
\r
\r
Ground truth $\\mathbf{p}$ and $\\mathbf{d}$ are used during training, and predicted $\\hat{\\mathbf{p}}$ and $\\hat{\\mathbf{d}}$ are used during inference. The model optimizes mean-squared error (MSE) between the predicted and ground-truth modalities\r
\r
$$\r
\\mathcal{L}=\\|\\hat{\\mathbf{y}}-\\mathbf{y}\\|\\_{2}^{2}+\\alpha\\|\\hat{\\mathbf{p}}-\\mathbf{p}\\|\\_{2}^{2}+\\gamma\\|\\hat{\\mathbf{d}}-\\mathbf{d}\\|\\_{2}^{2}\r
$$""" ;
    skos:prefLabel "FastPitch" .

:FastR-CNN a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1504.08083v2> ;
    skos:definition "**Fast R-CNN** is an object detection model that improves in its predecessor [R-CNN](https://paperswithcode.com/method/r-cnn) in a number of ways. Instead of extracting CNN features independently for each region of interest, Fast R-CNN aggregates them into a single forward pass over the image; i.e. regions of interest from the same image share computation and memory in the forward and backward passes." ;
    skos:prefLabel "Fast R-CNN" .

:FastSGT a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2008.12335v1> ;
    skos:definition """**Fast Schema Guided Tracker**, or **FastSGT**, is a fast and robust [BERT](https://paperswithcode.com/method/bert)-based model for state tracking in goal-oriented dialogue systems. The model employs carry-over mechanisms for transferring the values between slots, enabling switching between services and accepting the values offered by the system during dialogue. It also uses [multi-head attention](https://paperswithcode.com/method/multi-head-attention) projections in some of the decoders to have a better modelling of the encoder outputs.\r
\r
The model architecture is illustrated in the Figure. It consists of four main modules: 1-Utterance Encoder, 2-Schema Encoder, 3-State Decoder, and 4-State Tracker. The first three modules constitute the NLU component and are based on neural networks, whereas the state tracker is a rule-based module. [BERT](https://paperswithcode.com/method/bert) was used for both encoders in the model.\r
\r
The Utterance Encoder is a BERT model which encodes the user and system utterances at each turn. The Schema Encoder is also a BERT model which encodes the schema descriptions of intents, slots, and values into schema embeddings. These schema embeddings help the decoders to transfer or share knowledge between different services by having some language understanding of each slot, intent, or value. The schema and utterance embeddings are passed to the State Decoder - a multi-task module. This module consists of five sub-modules producing the information necessary to track the state of the dialogue. Finally, the State Tracker module takes the previous state along with the current outputs of the State Decoder and predicts the current state of the dialogue by aggregating and summarizing the information across turns.""" ;
    skos:prefLabel "FastSGT" .

:FastSampleRe-Weighting a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2109.03216v1> ;
    skos:definition "**Fast Sample Re-Weighting**, or **FSR**, is a sample re-weighting strategy to tackle problems such as dataset biases, noisy labels and imbalanced classes. It leverages a dictionary (essentially an extra buffer) to monitor the training history reflected by the model updates during meta optimization periodically, and utilises a valuation function to discover meaningful samples from training data as the proxy of reward data. The unbiased dictionary keeps being updated and provides reward signals to optimize sample weights. Additionally, instead of maintaining model states for both model and sample weight updates separately, feature sharing is enabled for saving the computation cost used for maintaining respective states." ;
    skos:prefLabel "Fast Sample Re-Weighting" .

:FastSpeech2 a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2006.04558v8> ;
    skos:definition """**FastSpeech2** is a text-to-speech model that aims to improve upon FastSpeech by better solving the one-to-many mapping problem in TTS, i.e., multiple speech variations corresponding to the same text. It attempts to solve this problem by 1) directly training the model with ground-truth target instead of the simplified output from teacher, and 2) introducing more variation information of speech (e.g., pitch, energy and more accurate duration) as conditional inputs. Specifically, in FastSpeech 2, we extract duration, pitch and energy from speech waveform and directly take them as conditional inputs in training and use predicted values in inference.\r
\r
The encoder converts the phoneme embedding sequence into the phoneme hidden sequence, and then the variance adaptor adds different variance information such as duration, pitch and energy into the hidden sequence, finally the mel-spectrogram decoder converts the adapted hidden sequence into mel-spectrogram sequence in parallel. FastSpeech 2 uses a feed-forward [Transformer](https://paperswithcode.com/method/transformer) block, which is a stack of [self-attention](https://paperswithcode.com/method/multi-head-attention) and 1D-[convolution](https://paperswithcode.com/method/convolution) as in FastSpeech, as the basic structure for the encoder and mel-spectrogram decoder.""" ;
    skos:prefLabel "FastSpeech 2" .

:FastSpeech2s a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2006.04558v8> ;
    skos:definition """**FastSpeech 2s** is a text-to-speech model that abandons mel-spectrograms as intermediate output completely and directly generates speech waveform from text during inference. In other words there is no cascaded mel-spectrogram generation (acoustic model) and waveform generation (vocoder). FastSpeech 2s generates waveform conditioning on intermediate hidden, which makes it more compact in inference by discarding the mel-spectrogram decoder.\r
\r
Two main design changes are made to the waveform decoder. \r
\r
First, considering that the phase information is difficult to predict using a variance predictor, [adversarial training](https://paperswithcode.com/methods/category/adversarial-training)  is used in the waveform decoder to force it to implicitly recover the phase information by itself. \r
\r
Secondly, the mel-spectrogram decoder of [FastSpeech 2](https://paperswithcode.com/method/fastspeech-2) is leveraged, which is trained on the full text sequence to help on the text feature extraction. As shown in the Figure, the waveform decoder is based on the structure of [WaveNet](https://paperswithcode.com/method/wavenet) including non-causal convolutions and gated activation. The waveform decoder takes a sliced hidden sequence corresponding to a short audio clip as input and upsamples it with transposed 1D-convolution to match the length of audio clip. The discriminator in the adversarial training adopts the same structure in Parallel WaveGAN, which consists of ten layers of non-causal [dilated 1-D convolutions](https://paperswithcode.com/method/dilated-convolution) with [leaky ReLU](https://paperswithcode.com/method/leaky-relu) activation function. The waveform decoder is optimized by the multi-resolution STFT loss and the [LSGAN discriminator](https://paperswithcode.com/method/lsgan) loss following Parallel WaveGAN. \r
\r
In inference, the mel-spectrogram decoder is discarded and only the waveform decoder is used to synthesize speech audio.""" ;
    skos:prefLabel "FastSpeech 2s" .

:FastVoxelQuery a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2109.02497v2> ;
    skos:definition "**Fast Voxel Query** is a module used in the [Voxel Transformer](https://paperswithcode.com/method/votr) 3D object detection model implementation of self-attention, specifically Local and Dilated Attention. For each querying index $v\\_{i}$, an attending voxel index $v\\_{j}$ is determined by Local and Dilated Attention. Then we can lookup the non-empty index $j$ in the hash table with hashed $v\\_{j}$ as the key. Finally, the non-empty index $j$ is used to gather the attending feature $f\\_{j}$ from $\\mathcal{F}$ for [multi-head attention](https://paperswithcode.com/method/multi-head-attention)." ;
    skos:prefLabel "Fast Voxel Query" .

:Fast_BAT a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2112.12376v6> ;
    skos:altLabel "Fast Bi-level Adversarial Training" ;
    skos:definition "Fast-BAT is a new method for accelerated adversarial training." ;
    skos:prefLabel "Fast_BAT" .

:FasterR-CNN a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1506.01497v3> ;
    rdfs:seeAlso <https://github.com/chenyuntc/simple-faster-rcnn-pytorch/blob/367db367834efd8a2bc58ee0023b2b628a0e474d/model/faster_rcnn.py#L22> ;
    skos:definition """**Faster R-CNN** is an object detection model that improves on [Fast R-CNN](https://paperswithcode.com/method/fast-r-cnn) by utilising a region proposal network ([RPN](https://paperswithcode.com/method/rpn)) with the CNN model. The RPN shares full-image convolutional features with the detection network, enabling nearly cost-free region proposals. It is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by [Fast R-CNN](https://paperswithcode.com/method/fast-r-cnn) for detection. RPN and Fast [R-CNN](https://paperswithcode.com/method/r-cnn) are merged into a single network by sharing their convolutional features: the RPN component tells the unified network where to look.\r
\r
As a whole, Faster R-CNN consists of two modules. The first module is a deep fully convolutional network that proposes regions, and the second module is the Fast R-CNN detector that uses the proposed regions.""" ;
    skos:prefLabel "Faster R-CNN" .

:Fastformer a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2108.09084v6> ;
    skos:definition "**Fastformer** is an type of [Transformer](https://paperswithcode.com/method/transformer) which uses [additive attention](https://www.paperswithcode.com/method/additive-attention) as a building block. Instead of modeling the pair-wise interactions between tokens, [additive attention](https://paperswithcode.com/method/additive-attention) is used to model global contexts, and then each token representation is further transformed based on its interaction with global context representations." ;
    skos:prefLabel "Fastformer" .

:Fawkes a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2002.08327v2> ;
    skos:definition "**Fawkes** is an image cloaking system that helps individuals inoculate their images against unauthorized facial recognition models. Fawkes achieves this by helping users add imperceptible pixel-level changes (\"cloaks\") to their own photos before releasing them. When used to train facial recognition models, these \"cloaked\" images produce functional models that consistently cause normal images of the user to be misidentified." ;
    skos:prefLabel "Fawkes" .

:FcaNet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2012.11879v4> ;
    skos:altLabel "Frequency channel attention networks" ;
    skos:definition """FCANet contains a novel multi-spectral channel attention module. Given an input feature map $X \\in \\mathbb{R}^{C \\times H \\times W}$, multi-spectral channel attention first splits $X$ into many parts $x^{i} \\in \\mathbb{R}^{C' \\times H \\times W}$. Then it applies a 2D DCT to each part $x^{i}$. Note that a 2D DCT can use pre-processing results to reduce computation. After processing each part,  all results are concatenated into a vector. Finally, fully connected layers, ReLU activation and a sigmoid are used to get the attention vector as in an SE block. This can be formulated as:\r
\\begin{align}\r
    s = F_\\text{fca}(X, \\theta) & = \\sigma (W_{2} \\delta (W_{1}[(\\text{DCT}(\\text{Group}(X)))]))\r
\\end{align}\r
\\begin{align}\r
    Y & = s  X\r
\\end{align}\r
where $\\text{Group}(\\cdot)$ indicates dividing the input into many groups and $\\text{DCT}(\\cdot)$ is the 2D discrete cosine transform. \r
\r
This work based on information compression and discrete cosine transforms achieves excellent performance on the classification task.""" ;
    skos:prefLabel "FcaNet" .

:Feature-CentricVoting a skos:Concept ;
    dcterms:source <https://www.semanticscholar.org/paper/Voting-for-Voting-in-Online-Point-Cloud-Object-Wang-Posner/8154027ed2e0c1772b54e79c40d30ae9ee468331> ;
    skos:definition "" ;
    skos:prefLabel "Feature-Centric Voting" .

:FeatureIntertwiner a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1903.11851v1> ;
    rdfs:seeAlso <https://github.com/hli2020/feature_intertwiner/blob/45f049c56dde1b5b2b3f585d9da55c8c05009e96/lib/sub_module.py#L286> ;
    skos:definition "**Feature Intertwiner** is an object detection module that leverages the features from a more reliable set to help guide the feature learning of another less reliable set. The mutual learning process helps two sets to have closer distance within the cluster in each class. The intertwiner is applied on the object detection task, where a historical buffer is proposed to address the sample missing problem during one mini-batch and the optimal transport (OT) theory is introduced to enforce the similarity among the two sets." ;
    skos:prefLabel "Feature Intertwiner" .

:FeatureNMS a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2002.07662v2> ;
    skos:definition "**Feature Non-Maximum Suppression**, or **FeatureNMS**, is a post-processing step for object detection models that removes duplicates where there are multiple detections outputted per object. FeatureNMS recognizes duplicates not only based on the intersection over union between the bounding boxes, but also based on the difference of feature vectors. These feature vectors can encode more information like visual appearance." ;
    skos:prefLabel "FeatureNMS" .

:FeatureSelection a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1905.02845v1> ;
    skos:definition "Feature selection, also known as variable selection, attribute selection or variable subset selection, is the process of selecting a subset of relevant features (variables, predictors) for use in model construction." ;
    skos:prefLabel "Feature Selection" .

:FeedbackMemory a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2002.09402v3> ;
    skos:definition """**Feedback Memory** is a type of attention module used in the [Feedback Transformer](https://paperswithcode.com/method/feedback-transformer) architecture. It allows a [transformer](https://paperswithcode.com/method/transformer) to to use the most abstract representations from the past directly as inputs for the current timestep. This means that the model does not form its representation in parallel, but sequentially token by token. More precisely, we replace the context inputs to attention modules with memory vectors that are computed over the past, i.e.:\r
\r
$$ \\mathbf{z}^{l}\\_{t} = \\text{Attn}\\left(\\mathbf{x}^{l}\\_{t},  \\left[\\mathbf{m}\\_{t−\\tau}, \\dots, \\mathbf{m}\\_{t−1}\\right]\\right) $$\r
\r
where a memory vector $\\mathbf{m}\\_{t}$ is computed by summing the representations of each layer at the $t$-th time step:\r
\r
$$ \\mathbf{m}\\_{t} = \\sum^{L}\\_{l=0}\\text{Softmax}\\left(w^{l}\\right)\\mathbf{x}\\_{t}^{l} $$\r
\r
where $w^{l}$ are learnable scalar parameters. Here $l = 0$ corresponds to token embeddings. The weighting of different layers by a [softmax](https://paperswithcode.com/method/softmax) output gives the model more flexibility as it can average them or select one of them. This modification of the self-attention input adapts the computation of the Transformer from parallel to sequential, summarized in the Figure. Indeed, it gives the ability to formulate the representation $\\mathbf{x}^{l}\\_{t+1}$ based on past representations from any layer $l'$, while in a standard Transformer this is only true for $l > l'$. This change can be viewed as exposing all previous computations to all future computations, providing better representations of the input. Such capacity would allow much shallower models to capture the same level of abstraction as a deeper architecture.""" ;
    skos:prefLabel "Feedback Memory" .

:FeedbackTransformer a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2002.09402v3> ;
    skos:definition "A **Feedback Transformer** is a type of sequential transformer that exposes all previous representations to all future representations, meaning the lowest representation of the current timestep is formed from the highest-level abstract representation of the past. This feedback nature allows this architecture to perform recursive computation, building stronger representations iteratively upon previous states. To achieve this, the self-attention mechanism of the standard [Transformer](https://paperswithcode.com/method/transformer) is modified so it attends to higher level representations rather than lower ones." ;
    skos:prefLabel "Feedback Transformer" .

:FeedforwardNetwork a skos:Concept ;
    skos:definition """A **Feedforward Network**, or a **Multilayer Perceptron (MLP)**, is a neural network with solely densely connected layers. This is the classic neural network architecture of the literature. It consists of inputs $x$ passed through units $h$ (of which there can be many layers) to predict a target $y$. Activation functions are generally chosen to be non-linear to allow for flexible functional approximation.\r
\r
Image Source: Deep Learning, Goodfellow et al""" ;
    skos:prefLabel "Feedforward Network" .

:FiLMModule a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2009.00713v2> ;
    skos:definition """The **Feature-wise linear modulation** (**FiLM**) module combines information from both noisy waveform and input mel-spectrogram. It is used in the [WaveGrad](https://paperswithcode.com/method/wavegrad) model. The authors also added iteration index $n$ which indicates the noise level of the input waveform by using the [Transformer](https://paperswithcode.com/method/transformer) sinusoidal positional embedding. To condition on the noise level directly, $n$ is replaced by $\\sqrt{\\bar{\\alpha}}$ and a linear scale $C = 5000$ is applied. The FiLM module produces both scale and bias vectors given inputs, which are used in a UBlock for feature-wise affine transformation as:\r
\r
$$ \\gamma\\left(D, \\sqrt{\\bar{\\alpha}}\\right) \\odot U + \\zeta\\left(D, \\sqrt{\\bar{\\alpha}}\\right) $$\r
\r
where $\\gamma$ and $\\zeta$ correspond to the scaling and shift vectors from the FiLM module, $D$ is the output from corresponding [DBlock](https://paperswithcode.com/method/dblock), $U$ is an intermediate output in the UBlock.""" ;
    skos:prefLabel "FiLM Module" .

:FilterResponseNormalization a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1911.09737v2> ;
    rdfs:seeAlso <https://github.com/gupta-abhay/pytorch-frn/blob/cc86a984fcbae61431ed41f37695e78f5e4b196e/frn.py#L9> ;
    skos:definition """**Filter Response Normalization (FRN)** is a type of normalization that combines normalization and an activation function, which can be used as a replacement for other normalizations and activations. It operates on each activation channel of each batch element independently, eliminating the dependency on other batch elements. \r
\r
To demonstrate, assume we are dealing with the feed-forward convolutional neural network. We follow the usual convention that the filter responses (activation maps) produced after a [convolution](https://paperswithcode.com/method/convolution) operation are a [4D ](https://paperswithcode.com/method/4d-a)tensor $X$ with shape $[B, W, H, C]$, where $B$ is the mini-batch size, $W, H$ are the spatial extents of the map, and $C$ is the number of filters used in convolution. $C$ is also referred to as output channels. Let $x = X_{b,:,:,c} \\in \\mathcal{R}^{N}$, where $N = W \\times H$, be the vector of filter responses for the $c^{th}$ filter for the $b^{th}$ batch point. \r
Let $\\nu^2 = \\sum\\_i x_i^2/N$, be the mean squared norm of $x$. \r
\r
Then Filter Response Normalization is defined as the following:\r
\r
$$\r
\\hat{x} = \\frac{x}{\\sqrt{\\nu^2 + \\epsilon}},\r
$$\r
\r
where $\\epsilon$ is a small positive constant to prevent division by zero.  \r
\r
A lack of mean centering in FRN can lead to activations having an arbitrary bias away from zero. Such a bias in conjunction with [ReLU](https://paperswithcode.com/method/relu) can have a detrimental effect on learning and lead to poor performance and dead units. To address this the authors augment ReLU with a learned threshold $\\tau$ to yield:\r
\r
$$\r
z = \\max(y, \\tau)\r
$$\r
\r
Since $\\max(y, \\tau){=}\\max(y-\\tau,0){+}\\tau{=}\\text{ReLU}{(y{-}\\tau)}{+}\\tau$, the effect of this activation is the same as having a shared bias before and after ReLU.""" ;
    skos:prefLabel "Filter Response Normalization" .

:FireModule a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1602.07360v4> ;
    rdfs:seeAlso <https://github.com/pytorch/vision/blob/6db1569c89094cf23f3bc41f79275c45e9fcb3f3/torchvision/models/squeezenet.py#L14> ;
    skos:definition "A **Fire Module** is a building block for convolutional neural networks, notably used as part of [SqueezeNet](https://paperswithcode.com/method/squeezenet). A Fire module is comprised of: a squeeze [convolution](https://paperswithcode.com/method/convolution) layer (which has only 1x1 filters), feeding into an expand layer that has a mix of 1x1 and 3x3 convolution filters.  We expose three tunable dimensions (hyperparameters) in a Fire module: $s\\_{1x1}$, $e\\_{1x1}$, and $e\\_{3x3}$. In a Fire module, $s\\_{1x1}$ is the number of filters in the squeeze layer (all 1x1), $e\\_{1x1}$ is the number of 1x1 filters in the expand layer, and $e\\_{3x3}$ is the number of 3x3 filters in the expand layer. When we use Fire modules we set $s\\_{1x1}$ to be less than ($e\\_{1x1}$ + $e\\_{3x3}$), so the squeeze layer helps to limit the number of input channels to the 3x3 filters." ;
    skos:prefLabel "Fire Module" .

:Fireflyalgorithm a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1602.07884v1> ;
    skos:definition "Metaheuristic algorithm" ;
    skos:prefLabel "Firefly algorithm" .

:Fisher-BRC a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2103.08050v1> ;
    skos:definition "**Fisher-BRC** is an actor critic algorithm for offline reinforcement learning that encourages the learned policy to stay close to the data, namely parameterizing the critic as the $\\log$-behavior-policy, which generated the offline dataset, plus a state-action value offset term, which can be learned using a neural network. Behavior regularization then corresponds to an appropriate regularizer on the offset term. A gradient penalty regularizer is used for the offset term, which is equivalent to Fisher divergence regularization, suggesting connections to the score matching and generative energy-based model literature." ;
    skos:prefLabel "Fisher-BRC" .

:Fishr a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2109.02934v3> ;
    skos:definition "**Fishr** is a learning scheme to enforce domain invariance in the space of the gradients of the loss function: specifically, it introduces a regularization term that matches the domain-level variances of gradients across training domains. Critically, the strategy exhibits close relations with the Fisher Information and the Hessian of the loss. Forcing domain-level gradient covariances to be similar during the learning procedure eventually aligns the domain-level loss landscapes locally around the final weights." ;
    skos:prefLabel "Fishr" .

:FixMatch a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2001.07685v2> ;
    skos:definition """FixMatch is an algorithm that first generates pseudo-labels using the model's predictions on weakly-augmented unlabeled images. For a given image, the pseudo-label is only retained if the model produces a high-confidence prediction. The model is then trained to predict the pseudo-label when fed a strongly-augmented version of the same image.\r
\r
Description from: [FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence](https://paperswithcode.com/paper/fixmatch-simplifying-semi-supervised-learning)\r
\r
Image credit:  [FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence](https://paperswithcode.com/paper/fixmatch-simplifying-semi-supervised-learning)""" ;
    skos:prefLabel "FixMatch" .

:FixRes a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1906.06423v4> ;
    skos:definition "**FixRes** is an image scaling strategy that seeks to optimize classifier performance. It is motivated by the observation that data augmentations induce a significant discrepancy between the size of the objects seen by the classifier at train and test time: in fact, a lower train resolution improves the classification at test time! FixRes is a simple strategy to optimize the classifier performance, that employs different train and test resolutions. The calibrations are: (a) calibrating the object sizes by adjusting the crop size and (b) adjusting statistics before spatial pooling." ;
    skos:prefLabel "FixRes" .

:FixedFactorizedAttention a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1904.10509v1> ;
    rdfs:seeAlso <https://github.com/openai/sparse_attention/blob/b68b367288ce5cabb3acec21afea2db54feedb9a/attention.py#L239> ;
    skos:definition """**Fixed Factorized Attention** is a factorized attention pattern where specific cells summarize previous locations and propagate that information to all future cells. It was proposed as part of the [Sparse Transformer](https://paperswithcode.com/method/sparse-transformer) architecture.\r
\r
\r
A self-attention layer maps a matrix of input embeddings $X$ to an output matrix and is parameterized by a connectivity pattern $S = \\text{set}\\left(S\\_{1}, \\dots, S\\_{n}\\right)$, where $S\\_{i}$ denotes the set of indices of the input vectors to which the $i$th output vector attends. The output vector is a weighted sum of transformations of the input vectors:\r
\r
$$ \\text{Attend}\\left(X, S\\right) = \\left(a\\left(\\mathbf{x}\\_{i}, S\\_{i}\\right)\\right)\\_{i\\in\\text{set}\\left(1,\\dots,n\\right)}$$\r
\r
$$ a\\left(\\mathbf{x}\\_{i}, S\\_{i}\\right) = \\text{softmax}\\left(\\frac{\\left(W\\_{q}\\mathbf{x}\\_{i}\\right)K^{T}\\_{S\\_{i}}}{\\sqrt{d}}\\right)V\\_{S\\_{i}} $$\r
\r
$$ K\\_{Si} = \\left(W\\_{k}\\mathbf{x}\\_{j}\\right)\\_{j\\in{S\\_{i}}} $$\r
\r
$$ V\\_{Si} = \\left(W\\_{v}\\mathbf{x}\\_{j}\\right)\\_{j\\in{S\\_{i}}} $$\r
\r
Here $W\\_{q}$, $W\\_{k}$, and $W\\_{v}$ represent the weight matrices which transform a given $x\\_{i}$ into a query, key, or value, and $d$ is the inner dimension of the queries and keys. The output at each position is a sum of the values weighted by the scaled dot-product similarity of the keys and queries.\r
\r
Full self-attention for autoregressive models defines $S\\_{i} = \\text{set}\\left(j : j \\leq i\\right)$, allowing every element to attend to all previous positions and its own position.\r
\r
Factorized self-attention instead has $p$ separate attention heads, where the $m$th head defines a subset of the indices $A\\_{i}^{(m)} ⊂ \\text{set}\\left(j : j \\leq i\\right)$ and lets $S\\_{i} = A\\_{i}^{(m)}$. The goal with the Sparse [Transformer](https://paperswithcode.com/method/transformer) was to find efficient choices for the subset $A$.\r
\r
Formally for Fixed Factorized Attention, $A^{(1)}\\_{i} = ${$j : \\left(\\lfloor{j/l\\rfloor}=\\lfloor{i/l\\rfloor}\\right)$}, where the brackets denote the floor operation, and $A^{(2)}\\_{i} = ${$j : j \\mod l \\in ${$t, t+1, \\ldots, l$}}, where $t=l-c$ and $c$ is a hyperparameter. The $i$-th output vector of the attention head attends to all input vectors either from $A^{(1)}\\_{i}$ or $A^{(2)}\\_{i}$. This pattern can be visualized in the figure to the right.\r
\r
If the stride is 128 and $c = 8$, then all future positions greater than 128 can attend to positions 120-128, all positions greater than 256 can attend to 248-256, and so forth. \r
\r
A fixed-attention pattern with $c = 1$ limits the expressivity of the network significantly, as many representations in the network are only used for one block whereas a small number of locations are used by all blocks. The authors found choosing $c \\in ${$8, 16, 32$} for typical values of $l \\in\r
{128, 256}$ performs well, although this increases the computational cost of this method by $c$ in comparison to the [strided attention](https://paperswithcode.com/method/strided-attention).\r
\r
Additionally, the authors found that when using multiple heads, having them attend to distinct subblocks of length $c$ within the block of size $l$ was preferable to having them attend to the same subblock.""" ;
    skos:prefLabel "Fixed Factorized Attention" .

:FixupInitialization a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1901.09321v2> ;
    skos:definition """**FixUp Initialization**, or **Fixed-Update Initialization**, is an initialization method that rescales the standard initialization of [residual branches](https://paperswithcode.com/method/residual-block) by adjusting for the network architecture. Fixup aims to enables training very deep [residual networks](https://paperswithcode.com/method/resnet) stably at a maximal learning rate without [normalization](https://paperswithcode.com/methods/category/normalization).\r
\r
The steps are as follows:\r
\r
1. Initialize the classification layer and the last layer of each residual branch to 0.\r
\r
2. Initialize every other layer using a standard method, e.g. [Kaiming Initialization](https://paperswithcode.com/method/he-initialization), and scale only the weight layers inside residual branches by $L^{\\frac{1}{2m-2}}$.\r
\r
3. Add a scalar multiplier (initialized at 1) in every branch and a scalar bias (initialized at 0) before each [convolution](https://paperswithcode.com/method/convolution), [linear](https://paperswithcode.com/method/linear-layer), and element-wise activation layer.""" ;
    skos:prefLabel "Fixup Initialization" .

:Flan-T5 a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2210.11416v5> ;
    skos:definition "**Flan-T5** is the instruction fine-tuned version of **T5** or **Text-to-Text Transfer Transformer** Language Model." ;
    skos:prefLabel "Flan-T5" .

:FlexFlow a skos:Concept ;
    skos:definition """**FlexFlow** is a deep learning engine that uses guided randomized search of the SOAP (Sample, Operator, Attribute, and Parameter) space to find a fast parallelization strategy for a specific parallel machine. To accelerate this search, FlexFlow introduces a novel execution simulator that can accurately predict a parallelization strategy’s performance and is three orders of magnitude faster than prior approaches that execute each strategy. \r
\r
FlexFlow uses two main components: a fast, incremental execution simulator to evaluate different parallelization strategies, and a Markov Chain Monte Carlo (MCMC) search algorithm that takes advantage of the incremental simulator to rapidly explore the large search space.""" ;
    skos:prefLabel "FlexFlow" .

:Florence a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2111.11432v1> ;
    skos:definition "Florence is a computer vision foundation model aiming to learn universal visual-language representations that be adapted to various computer vision tasks, visual question answering, image captioning, video retrieval, among other tasks. Florence's workflow consists of data curation, unified learning, Transformer architectures and adaption. Florence is pre-trained in an image-label-description space, utilizing a unified image-text contrastive learning. It involves a two-tower architecture: 12-layer Transformer for the language encoder, and a Vision Transformer for the image encoder. Two linear projection layers are added on top of the image encoder and language encoder to match the dimensions of image and language features. Compared to previous methods for cross-modal shared representations, Florence expands beyond simple classification and retrieval capabilities to advanced representations that support object level, multiple modality, and videos respectively." ;
    skos:prefLabel "Florence" .

:FlowAlignmentModule a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2002.10120v3> ;
    skos:definition """**Flow Alignment Module**, or **FAM**, is a flow-based align module for scene parsing to learn Semantic Flow between feature maps of adjacent levels and broadcast high-level features to high resolution features effectively and efficiently. The concept of Semantic Flow is inspired from optical flow, which is widely used in video processing task to represent the pattern of apparent motion of objects, surfaces, and edges in a visual scene caused by relative motion. The authors postulate that the relatinship between two feature maps of arbitrary resolutions from the same image can also be represented with the “motion” of every pixel from one feature map to the other one. Once precise Semantic Flow is obtained, the network is able to propagate semantic features with minimal information loss.\r
\r
In the FAM module, the transformed high-resolution feature map are combined with the low-resolution feature map to generate the semantic flow field, which is utilized to warp the low-resolution feature map to high-resolution feature map.""" ;
    skos:prefLabel "Flow Alignment Module" .

:FocalLoss a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1708.02002v2> ;
    rdfs:seeAlso <https://github.com/clcarwin/focal_loss_pytorch/blob/e11e75bad957aecf641db6998a1016204722c1bb/focalloss.py#L6> ;
    skos:definition """A **Focal Loss** function addresses class imbalance during training in tasks like object detection. Focal loss applies a modulating term to the cross entropy loss in order to focus learning on hard misclassified examples. It is a dynamically scaled cross entropy loss, where the scaling factor decays to zero as confidence in the correct class increases. Intuitively, this scaling factor can automatically down-weight the contribution of easy examples during training and rapidly focus the model on hard examples. \r
\r
Formally, the Focal Loss adds a factor $(1 - p\\_{t})^\\gamma$ to the standard cross entropy criterion. Setting $\\gamma>0$ reduces the relative loss for well-classified examples ($p\\_{t}>.5$), putting more focus on hard, misclassified examples. Here there is tunable *focusing* parameter $\\gamma \\ge 0$. \r
\r
$$ {\\text{FL}(p\\_{t}) = - (1 - p\\_{t})^\\gamma \\log\\left(p\\_{t}\\right)} $$""" ;
    skos:prefLabel "Focal Loss" .

:FocalTransformers a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2107.00641v1> ;
    skos:definition "The **focal self-attention** is built to make Transformer layers scalable to high-resolution inputs.  Instead of attending all tokens at fine-grain, the approach attends the fine-grain tokens only locally, but the summarized ones globally. As such, it can cover as many regions as standard self-attention but with much less cost. An image is first partitioned into patches, resulting in visual tokens. Then a patch embedding layer, consisting of a convolutional layer with filter and stride of same size, to project the patches into hidden features. This spatial feature map in then passed to four stages of focal Transformer blocks. Each focal Transformer block consists of $N_i$ focal Transformer layers. Patch embedding layers are used in between to reduce spatial size of feature map by factor 2, while feature dimension increased by 2." ;
    skos:prefLabel "Focal Transformers" .

:Focus a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2305.14952v1> ;
    skos:definition "" ;
    skos:prefLabel "Focus" .

:Forwardgradient a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2202.08587v1> ;
    skos:definition """Forward gradients are unbiased estimators of the gradient $\\nabla f(\\theta)$ for a function $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$, given by $g(\\theta) = \\langle \\nabla f(\\theta) , v \\rangle v$. \r
\r
Here $v = (v_1, \\ldots, v_n)$ is a random vector, which must satisfy the following conditions in order for $g(\\theta)$ to be an unbiased estimator of $\\nabla f(\\theta)$\r
\r
* $v_i \\perp v_j$ for all $i \\neq j$\r
* $\\mathbb{E}[v_i] = 0$ for all $i$\r
* $\\mathbb{V}[v_i] = 1$ for all $i$\r
\r
Forward gradients can be computed with a single jvp (Jacobian Vector Product), which enables the use of the forward mode of autodifferentiation instead of the usual reverse mode, which has worse computational characteristics.""" ;
    skos:prefLabel "Forward gradient" .

:FourierContourEmbedding a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2104.10442v2> ;
    skos:definition "**Fourier Contour Embedding** is a text instance representation that allows networks to learn diverse text geometry variances. Most of existing methods model text instances in image spatial domain via masks or contour point sequences in the Cartesian or the polar coordinate system. However, the mask representation might lead to expensive post-processing, while the point sequence one may have limited capability to model texts with highly-curved shapes. This motivates modeling text instances in the Fourier domain." ;
    skos:prefLabel "Fourier Contour Embedding" .

:FoveaBox a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1904.03797v2> ;
    rdfs:seeAlso <https://github.com/taokong/FoveaBox> ;
    skos:definition """**FoveaBox** is anchor-free framework for object detection. Instead of using predefined anchors to enumerate possible locations, scales and aspect ratios for the search of the objects, FoveaBox directly learns the object existing possibility and the bounding box coordinates without anchor reference. This is achieved by: (a) predicting category-sensitive semantic maps for the object existing possibility, and (b) producing category-agnostic bounding box for each position that potentially contains an object. The scales of target boxes are naturally associated with feature pyramid representations for each input image\r
\r
It is a single, unified network composed of a backbone network and two task-specific subnetworks. The backbone is responsible for computing a convolutional feature map over an entire input image and is an off-the-shelf convolutional network. The first subnet performs per pixel classification on the backbone’s output; the second subnet performs bounding box prediction for the corresponding\r
position.""" ;
    skos:prefLabel "FoveaBox" .

:FractalBlock a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1605.07648v4> ;
    rdfs:seeAlso <https://github.com/osmr/imgclsmob/blob/68335927ba27f2356093b985bada0bc3989836b1/pytorch/pytorchcv/models/fractalnet_cifar.py#L103> ;
    skos:definition """A **Fractal Block** is an image model block that utilizes an expansion rule that yields a structural layout of truncated fractals. For the base case where $f\\_{1}\\left(z\\right) = \\text{conv}\\left(z\\right)$ is a convolutional layer, we then have recursive fractals of the form:\r
\r
$$ f\\_{C+1}\\left(z\\right) = \\left[\\left(f\\_{C}\\circ{f\\_{C}}\\right)\\left(z\\right)\\right] \\oplus \\left[\\text{conv}\\left(z\\right)\\right]$$\r
\r
Where $C$ is the number of columns. For the join layer (green in Figure), we use the element-wise mean rather than concatenation or addition.""" ;
    skos:prefLabel "Fractal Block" .

:FractalNet a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1605.07648v4> ;
    rdfs:seeAlso <https://github.com/osmr/imgclsmob/blob/68335927ba27f2356093b985bada0bc3989836b1/pytorch/pytorchcv/models/fractalnet_cifar.py#L306> ;
    skos:definition "**FractalNet** is a type of convolutional neural network that eschews [residual connections](https://paperswithcode.com/method/residual-connection) in favour of a \"fractal\" design. They involve repeated application of a simple expansion rule to generate deep networks whose structural layouts are precisely truncated fractals. These networks contain interacting subpaths of different lengths, but do not include any pass-through or residual connections; every internal signal is transformed by a filter and nonlinearity before being seen by subsequent layers." ;
    skos:prefLabel "FractalNet" .

:Fragmentation a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2305.19659v2> ;
    skos:definition "Given a pattern $P,$ that is more complicated than the patterns, we fragment $P$ into simpler patterns such that their exact count is known. In the subgraph GNN proposed earlier, look into the subgraph of the host graph. We have seen that this technique is scalable on large graphs. Also, we have seen that subgraph GNN is more expressive and efficient than traditional GNN. So, we tried to explore the expressibility when the pattern is fragmented into smaller subpatterns." ;
    skos:prefLabel "Fragmentation" .

:FraternalDropout a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1711.00066v4> ;
    rdfs:seeAlso <https://github.com/kondiz/fraternal-dropout> ;
    skos:definition "**Fraternal Dropout** is a regularization method for recurrent neural networks that trains two identical copies of an RNN (that share parameters) with different [dropout](https://paperswithcode.com/method/dropout) masks while minimizing the difference between their (pre-[softmax](https://paperswithcode.com/method/softmax)) predictions. This encourages the representations of RNNs to be invariant to dropout mask, thus being robust." ;
    skos:prefLabel "Fraternal Dropout" .

:FreeAnchor a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1909.02466v2> ;
    skos:definition "**FreeAnchor** is an anchor supervision method for object detection. Many CNN-based object detectors assign anchors for ground-truth objects under the restriction of object-anchor Intersection-over-Unit (IoU). In contrast, FreeAnchor is a learning-to-match approach that breaks the IoU restriction, allowing objects to match anchors in a flexible manner. It updates hand-crafted anchor assignment to free anchor matching by formulating detector training as a maximum likelihood estimation (MLE) procedure. FreeAnchor targets at learning features which best explain a class of objects in terms of both classification and localization." ;
    skos:prefLabel "FreeAnchor" .

:FunnelTransformer a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2006.03236v1> ;
    skos:definition """**Funnel Transformer** is a type of [Transformer](https://paperswithcode.com/methods/category/transformers) that gradually compresses the sequence of hidden states to a shorter one and hence reduces the computation cost. By re-investing the saved FLOPs from length reduction in constructing a deeper or wider model, the model capacity is further improved. In addition, to perform token-level predictions as required by common pretraining objectives, Funnel-[transformer](https://paperswithcode.com/method/transformer) is able to recover a deep representation for each token from the reduced hidden sequence via a decoder.\r
\r
The proposed model keeps the same overall skeleton of interleaved S-[Attn](https://paperswithcode.com/method/scaled) and P-[FFN](https://paperswithcode.com/method/dense-connections) sub-modules wrapped by [residual connection](https://paperswithcode.com/method/residual-connection) and [layer normalization](https://paperswithcode.com/method/layer-normalization). But differently, to achieve representation compression and computation reduction, THE model employs an encoder that gradually reduces the sequence length of the hidden states as the layer gets deeper. In addition, for tasks involving per-token predictions like pretraining, a simple decoder is used to reconstruct a full sequence of token-level representations from the compressed encoder output. Compression is achieved via a pooling operation,""" ;
    skos:prefLabel "Funnel Transformer" .

:FuseFormer a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2109.02974v1> ;
    skos:definition "**FuseFormer** is a [Transformer](https://paperswithcode.com/method/transformer)-based model designed for video inpainting via fine-grained feature fusion based on novel [Soft Split and Soft Composition](https://paperswithcode.com/method/soft-split-and-soft-composition) operations. The soft split divides feature map into many patches with given overlapping interval while the soft composition stitches them back into a whole feature map where pixels in overlapping regions are summed up. FuseFormer builds soft composition and soft split into its [feedforward network](https://paperswithcode.com/method/feedforward-network) for further enhancing subpatch level feature fusion." ;
    skos:prefLabel "FuseFormer" .

:FuseFormerBlock a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2109.02974v1> ;
    skos:definition "A **FuseFormer block** is used in the [FuseFormer](https://paperswithcode.com/method/fuseformer) model for video inpainting. It is the same to standard [Transformer](https://paperswithcode.com/method/transformer) block except that feed forward network is replaced with a Fusion Feed Forward Network (F3N). F3N brings no extra parameter into the standard feed forward net and the difference is that F3N inserts a soft-split and a soft composite operation between the two layer of MLPs." ;
    skos:prefLabel "FuseFormer Block" .

:G-GLN a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2006.05964v2> ;
    skos:altLabel "Gaussian Gated Linear Network" ;
    skos:definition """**Gaussian Gated Linear Network**, or **G-GLN**, is a multi-variate extension to the recently proposed [GLN](https://paperswithcode.com/method/gln) family of deep neural networks by reformulating the GLN neuron as a gated product of Gaussians. This Gaussian Gated Linear Network (G-GLN) formulation exploits the fact that exponential family densities are closed under multiplication, a property that has seen much use in [Gaussian Process](https://paperswithcode.com/method/gaussian-process) and related literature. Similar to the Bernoulli GLN, every neuron in the G-GLN directly predicts the target distribution.  \r
\r
Precisely, a G-GLN is a feed-forward network of data-dependent distributions. Each neuron calculates the sufficient statistics $\\left(\\mu, \\sigma\\_{2}\\right)$ for its associated PDF using its active weights, given those emitted by neurons in the preceding layer. It consists of consists of $L+1$ layers indexed by $i \\in\\{0, \\ldots, L\\}$ with $K\\_{i}$ neurons in each layer. The weight space for a neuron in layer $i$ is denoted by $\\mathcal{W}\\_{i}$; the subscript is needed since the dimension of the weight space depends on $K_{i-1}$. Each neuron/distribution is indexed by its position in the network when laid out on a grid; for example, $f\\_{i k}$ refers to the family of PDFs defined by the $k$ th neuron in the $i$ th layer. Similarly, $c\\_{i k}$ refers to the context function associated with each neuron in layers $i \\geq 1$, and $\\mu\\_{i k}$ and $\\sigma\\_{i k}^{2}$ (or $\\Sigma\\_{i k}$ in the multivariate case) referring to the sufficient statistics for each Gaussian PDF.\r
\r
There are two types of input to neurons in the network. The first is the side information, which can be thought of as the input features, and is used to determine the weights used by each neuron via half-space gating. The second is the input to the neuron, which is the PDFs output by the previous layer, or in the case of layer 0, some provided base models. To apply a G-GLN in a supervised learning setting, we need to map the sequence of input-label pairs $\\left(x\\_{t}, y\\_{t}\\right)$ for $t=1,2, \\ldots$ onto a sequence of (side information, base Gaussian PDFs, label) triplets $\\left(z\\_{t},\\left\\(f\\_{0 i}\\right\\)\\_{i}, y\\_{t}\\right)$. The side information $z\\_{t}$ is set to the (potentially normalized) input features $x\\_{t}$. The Gaussian PDFs for layer 0 will generally include the necessary base Gaussian PDFs to span the target range, and optionally some base prediction PDFs that capture domain-specific knowledge.""" ;
    skos:prefLabel "G-GLN" .

:G-GLNNeuron a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2006.05964v2> ;
    skos:definition """A **G-GLN Neuron** is a type of neuron used in the [G-GLN](https://paperswithcode.com/method/g-gln) architecture. G-GLN. The key idea is that further representational power can be added to a weighted product of Gaussians via a contextual gating procedure. This is achieved by extending a weighted product of Gaussians model with an additional type of input called side information. The side information will be used by a neuron to select a weight vector to apply for a given example from a table of weight vectors. In typical applications to regression, the side information is defined as the (normalized) input features for an input example: i.e. $z=(x-\\bar{x}) / \\sigma\\_{x}$.\r
\r
More formally, associated with each neuron is a context function $c: \\mathcal{Z} \\rightarrow \\mathcal{C}$, where $\\mathcal{Z}$ is the set of possible side information and $\\mathcal{C}=\\{0, \\ldots, k-1\\}$ for some $k \\in \\mathbb{N}$ is the context space. Each neuron $i$ is now parameterized by a weight matrix $W\\_{i}=\\left[w\\_{i, 0} \\ldots w\\_{i, k-1}\\right]^{\\top}$ with each row vector $w\\_{i j} \\in \\mathcal{W}$ for $0 \\leq j<k$. The context function $c$ is responsible for mapping side information $z \\in \\mathcal{Z}$ to a particular row $w\\_{i, c(z)}$ of $W_{i}$, which we then use to weight the Product of Gaussians. In other words, a G-GLN neuron can be defined by:\r
\r
$$\r
\\operatorname{PoG}\\_{W}^{c}\\left(y ; f_{1}(\\cdot), \\ldots, f\\_{m}(\\cdot), z\\right):=\\operatorname{PoG}\\_{w^{c(z)}}\\left(y ; f\\_{1}(\\cdot), \\ldots, f\\_{m}(\\cdot)\\right)\r
$$\r
\r
with the associated loss function $-\\log \\left(\\operatorname{PoG}\\_{W}^{c}\\left(y ; f\\_{1}(y), \\ldots, f\\_{m}(y), z\\right)\\right)$ inheriting all the properties needed to apply Online Convex Programming.""" ;
    skos:prefLabel "G-GLN Neuron" .

:G-NIA a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2108.13049v2> ;
    skos:altLabel "Generalizable Node Injection Attack" ;
    skos:definition """**Generalizable Node Injection Attack**, or **G-NIA**, is an attack scenario for graph neural networks where the attacker injects malicious nodes rather than modifying original nodes or edges to affect the performance of GNNs. G-NIA generates the discrete edges also by Gumbel-Top-𝑘 following OPTI and captures the coupling effect between network structure and node features by a sophisticated designed model. \r
\r
 G-NIA explicitly models the most critical feature propagation via jointly modeling. Specifically, the malicious attributes are adopted to guide the generation of edges, modeling the influence of attributes and edges. G-NIA also adopts a model-based framework, utilizing useful information of attacking during model training, as well as saving computational cost during inference without re-optimization.""" ;
    skos:prefLabel "G-NIA" .

:G3D a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2003.14111v2> ;
    skos:definition "**G3D** is a unified spatial-temporal graph convolutional operator that directly models cross-spacetime joint dependencies. It leverages dense cross-spacetime edges as skip connections for direct information propagation across the 3D spatial-temporal graph." ;
    skos:prefLabel "G3D" .

:GA a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1901.05737v1> ;
    skos:altLabel "Genetic Algorithms" ;
    skos:definition "Genetic Algorithms are search algorithms that mimic Darwinian biological evolution in order to select and propagate better solutions." ;
    skos:prefLabel "GA" .

:GAGNN a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2108.12238v1> ;
    skos:altLabel "Group-Aware Neural Network" ;
    skos:definition "**GAGNN**, or **Group-aware Graph Neural Network**, is a hierarchical model for nationwide city air quality forecasting. The model constructs a city graph and a city group graph to model the spatial and latent dependencies between cities, respectively. GAGNN introduces differentiable grouping network to discover the latent dependencies among cities and generate city groups. Based on the generated city groups, a group correlation encoding module is introduced to learn the correlations between them, which can effectively capture the dependencies between city groups. After the graph construction, GAGNN implements message passing mechanism to model the dependencies between cities and city groups." ;
    skos:prefLabel "GAGNN" .

:GAIL a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1606.03476v1> ;
    skos:altLabel "Generative Adversarial Imitation Learning" ;
    skos:definition "**Generative Adversarial Imitation Learning** presents a new general framework for directly extracting a policy from data, as if it were obtained by reinforcement learning following inverse reinforcement learning." ;
    skos:prefLabel "GAIL" .

:GALA a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1805.08819v4> ;
    skos:altLabel "Global-and-Local attention" ;
    skos:definition """Most attention mechanisms learn where to focus using only weak supervisory signals from class labels, which inspired Linsley et al. to investigate how explicit human supervision can affect the performance and interpretability of attention models. As a proof of concept, Linsley et al. proposed the global-and-local attention (GALA) module, which extends an SE block with a spatial attention mechanism.\r
\r
Given the input feature map $X$, GALA uses an attention mask that combines global and local attention to tell the network where and on what to focus. As in SE blocks, global attention aggregates global information by global average pooling and then produces a channel-wise attention weight vector using a multilayer perceptron. In local attention, two consecutive $1\\times 1$ convolutions are conducted on the input to produce a positional weight map. The outputs of the local and global pathways are combined by addition and multiplication. Formally, GALA can be represented as:\r
\\begin{align}\r
    s_g &= W_{2} \\delta (W_{1}\\text{GAP}(x))\r
\\end{align}\r
\r
\\begin{align}\r
    s_l &= Conv_2^{1\\times 1} (\\delta(Conv_1^{1\\times1}(X)))\r
\\end{align}\r
\r
\\begin{align}\r
    s_g^* &= \\text{Expand}(s_g)\r
\\end{align}\r
\r
\\begin{align}\r
    s_l^* &= \\text{Expand}(s_l) \r
\\end{align}\r
\r
\\begin{align}\r
    s &= \\tanh(a(s_g^\\* + s_l^\\*) +m \\cdot (s_g^\\* s_l^\\*) )\r
\\end{align}\r
\r
\\begin{align}\r
    Y &= sX\r
\\end{align}\r
\r
where $a,m \\in \\mathbb{R}^{C}$ are learnable parameters representing channel-wise weight vectors. \r
\r
Supervised by human-provided feature importance maps, GALA has significantly improved representational power and can be combined with any CNN backbone.""" ;
    skos:prefLabel "GALA" .

:GAM a skos:Concept ;
    skos:altLabel "Generalized additive models" ;
    skos:definition "Please enter a description about the method here" ;
    skos:prefLabel "GAM" .

:GAN-TTS a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1909.11646v2> ;
    skos:definition """**GAN-TTS** is a generative adversarial network for text-to-speech synthesis. The architecture is composed of a conditional feed-forward generator producing raw speech audio, and an ensemble of discriminators which operate on random windows of different sizes. The discriminators analyze the audio both in terms of general realism, as well as how well the audio corresponds to the utterance that should be pronounced.\r
\r
The generator architecture consists of several GBlocks, which are residual based (dilated) [convolution](https://paperswithcode.com/method/convolution) blocks. GBlocks 3–7 gradually upsample the temporal dimension of hidden representations by factors of 2, 2, 2, 3, 5, while the number of channels is reduced by GBlocks 3, 6 and 7 (by a factor of 2 each). The final convolutional layer with [Tanh activation](https://paperswithcode.com/method/tanh-activation) produces a single-channel audio waveform.\r
\r
Instead of a single discriminator, GAN-TTS uses an ensemble of Random Window Discriminators (RWDs) which operate on randomly sub-sampled fragments of the real or generated samples. The ensemble allows for the evaluation of audio in different complementary ways.""" ;
    skos:prefLabel "GAN-TTS" .

:GANFeatureMatching a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1606.03498v1> ;
    rdfs:seeAlso <https://github.com/eli5168/improved_gan_pytorch/blob/f93520646f150a19084efd65215b6c082d3e9d20/improved_GAN.py#L248> ;
    skos:definition """**Feature Matching** is a regularizing objective for a generator in [generative adversarial networks](https://paperswithcode.com/methods/category/generative-adversarial-networks) that prevents it from overtraining on the current discriminator. Instead of directly maximizing the output of the discriminator, the new objective requires the generator to generate data that matches the statistics of the real data, where we use the discriminator only to specify the statistics that we think are worth matching. Specifically, we train the generator to match the expected value of the features on an intermediate layer of the discriminator. This is a natural choice of statistics for the generator to match, since by training the discriminator we ask it to find those features that are most discriminative of real data versus data generated by the current model.\r
\r
Letting $\\mathbf{f}\\left(\\mathbf{x}\\right)$ denote activations on an intermediate layer of the discriminator, our new objective for the generator is defined as: $ ||\\mathbb{E}\\_{x\\sim p\\_{data} } \\mathbf{f}\\left(\\mathbf{x}\\right) − \\mathbb{E}\\_{\\mathbf{z}∼p\\_{\\mathbf{z}}\\left(\\mathbf{z}\\right)}\\mathbf{f}\\left(G\\left(\\mathbf{z}\\right)\\right)||^{2}\\_{2} $. The discriminator, and hence\r
$\\mathbf{f}\\left(\\mathbf{x}\\right)$, are trained as with vanilla GANs. As with regular [GAN](https://paperswithcode.com/method/gan) training, the objective has a fixed point where G exactly matches the distribution of training data.""" ;
    skos:prefLabel "GAN Feature Matching" .

:GANHingeLoss a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1705.02894v2> ;
    rdfs:seeAlso <https://github.com/lim0606/pytorch-geometric-gan/blob/eb84feb5cae1d6963c075aa6fb4c0c3a18eeec41/main.py#L303> ;
    skos:definition """The **GAN Hinge Loss** is a hinge loss based loss function for [generative adversarial networks](https://paperswithcode.com/methods/category/generative-adversarial-networks):\r
\r
$$ L\\_{D} = -\\mathbb{E}\\_{\\left(x, y\\right)\\sim{p}\\_{data}}\\left[\\min\\left(0, -1 + D\\left(x, y\\right)\\right)\\right] -\\mathbb{E}\\_{z\\sim{p\\_{z}}, y\\sim{p\\_{data}}}\\left[\\min\\left(0, -1 - D\\left(G\\left(z\\right), y\\right)\\right)\\right] $$\r
\r
$$ L\\_{G} = -\\mathbb{E}\\_{z\\sim{p\\_{z}}, y\\sim{p\\_{data}}}D\\left(G\\left(z\\right), y\\right) $$""" ;
    skos:prefLabel "GAN Hinge Loss" .

:GANLeastSquaresLoss a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1611.04076v3> ;
    rdfs:seeAlso <https://github.com/eriklindernoren/PyTorch-GAN/blob/a163b82beff3d01688d8315a3fd39080400e7c01/implementations/lsgan/lsgan.py#L102> ;
    skos:definition """**GAN Least Squares Loss** is a least squares loss function for generative adversarial networks. Minimizing this objective function is equivalent to minimizing the Pearson $\\chi^{2}$ divergence. The objective function (here for [LSGAN](https://paperswithcode.com/method/lsgan)) can be defined as:\r
\r
$$ \\min\\_{D}V\\_{LS}\\left(D\\right) = \\frac{1}{2}\\mathbb{E}\\_{\\mathbf{x} \\sim p\\_{data}\\left(\\mathbf{x}\\right)}\\left[\\left(D\\left(\\mathbf{x}\\right) - b\\right)^{2}\\right] + \\frac{1}{2}\\mathbb{E}\\_{\\mathbf{z}\\sim p\\_{data}\\left(\\mathbf{z}\\right)}\\left[\\left(D\\left(G\\left(\\mathbf{z}\\right)\\right) - a\\right)^{2}\\right] $$\r
\r
$$ \\min\\_{G}V\\_{LS}\\left(G\\right) = \\frac{1}{2}\\mathbb{E}\\_{\\mathbf{z} \\sim p\\_{\\mathbf{z}}\\left(\\mathbf{z}\\right)}\\left[\\left(D\\left(G\\left(\\mathbf{z}\\right)\\right) - c\\right)^{2}\\right] $$\r
\r
where $a$ and $b$ are the labels for fake data and real data and $c$ denotes the value that $G$ wants $D$ to believe for fake data.""" ;
    skos:prefLabel "GAN Least Squares Loss" .

:GANformer a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2103.01209v4> ;
    skos:altLabel "Generative Adversarial Transformer" ;
    skos:definition """GANformer is a novel and efficient type of [transformer](https://paperswithcode.com/method/transformer) which can be used for visual generative modeling. The network employs a bipartite structure that enables long-range interactions across an image, while maintaining computation of linearly efficiency, that can readily scale to high-resolution synthesis. It iteratively propagates information from a set of latent variables to the evolving visual features and vice versa, to support the refinement of each in light of the other and encourage the emergence of compositional representations of objects and scenes.\r
\r
Source: [Generative Adversarial Transformers](https://arxiv.org/pdf/2103.01209v2.pdf)\r
\r
Image source: [Generative Adversarial Transformers](https://arxiv.org/pdf/2103.01209v2.pdf)""" ;
    skos:prefLabel "GANformer" .

:GAP-Layer a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2206.07369v3> ;
    rdfs:seeAlso <https://anonymous.4open.science/r/DiffWireNeurIPS22/GAP_layer.py> ;
    skos:altLabel "Spectral Gap Rewiring Layer" ;
    skos:definition """**TL;DR: GAP-Layer is a GNN Layer which is able to rewire a graph in an inductive an parameter-free way optimizing the spectral gap (minimizing or maximizing the bottleneck size), learning a differentiable way to compute the Fiedler vector and the Fiedler value of the graph.**\r
\r
## Summary\r
 **GAP-Layer** is a rewiring layer based on minimizing or maximizing the spectral gap (or graph bottleneck size) in an inductive way. Depending on the mining task we want to perform in our graph, we would like to maximize or minimize the size of the bottleneck, aiming to more connected or more separated communities. \r
\r
## GAP-Layer: Spectral Gap Rewiring\r
\r
#### Loss and derivatives using $\\mathbf{L}$ or $\\mathbf{\\cal L}$\r
For this explanation, we are going to suppose we want to minimize the spectral gap, i.e. make the graph bottleneck size smaller. For minimizing the spectral GAP we minimize this loss:\r
\r
$$\r
L\\_{Fiedler} = \\|\\tilde{\\mathbf{A}}-\\mathbf{A}\\| \\_F + \\alpha(\\lambda\\_2)^2\r
$$\r
\r
The gradients of this cost function w.r.t each element of $\\mathbf{A}$ are not trivial. Depending on if we use the Laplacian, $\\mathbf{L}$, or the normalized Laplacian, $\\cal L$, the derivatives are going to be different. For the former case ($\\mathbf{L}$), we will use the derivatives presented in Kang et al. 2019. In the latter scenario ($\\cal L$), we present the **Spectral Gradients**: derivatives from the spectral gap w.r.t. the Normalized Laplacian. However, whatever option we choose, $\\lambda_2$ can seen as a function of  $\\tilde{\\mathbf{A}}$ and , hence, $\\nabla\\_{\\tilde{\\mathbf{A}}}\\lambda\\_2$, the gradient of $\\lambda\\_2$ wrt each component of $\\tilde{\\mathbf{A}}$ (*how does the bottleneck change with each change in our graph?*),  comes from the chain rule of the matrix derivative $Tr\\left[\\left(\\nabla\\_{\\tilde{\\mathbf{L}}}\\lambda\\_2\\right)^T\\cdot\\nabla\\_{\\tilde{\\mathbf{A}}}\\tilde{\\mathbf{L}}\\right]$ if using the Laplacian or $Tr\\left[\\left(\\nabla\\_{\\tilde{\\mathbf{\\cal L}}}\\lambda\\_2\\right)^T\\cdot\\nabla\\_{\\tilde{\\mathbf{A}}}\\tilde{\\mathbf{\\cal L}}\\right]$ if using the normalized Laplacian. Both of this derivatives, relies on the Fiedler vector (2nd eigenvector: $\\mathbf{f}\\_2$ if we use $\\mathbf{L}$ and $\\mathbf{g}\\_2$ if using $\\mathbf{\\cal L}$ instead). For more details on those derivatives, and for the sake of simplicity in this blog explanation, I suggest go to the original paper.\r
\r
#### Differentiable approximation of $\\mathbf{f}_2$ and $\\lambda_2$\r
Once we have those derivatives, the problem is still not that trivial. Note that our cost function $L\\_{Fiedler}$, relies on an eigenvalue $\\lambda\\_2$. In addition, the derivatives also depends on the Fiedler vector $\\mathbf{f}\\_2$ or $\\mathbf{g}\\_2$, which is the eigenvector corresponding to the aforementioned eigenvalue. However, we **DO NOT COMPUTE IT SPECTRALLY**, as its computation has a complexity of $O(n^3)$ and would need to be computed in every learning iteration. Instead, **we learn an approximation of $\\mathbf{f}\\_2$ and use its Dirichlet energy ${\\cal E}(\\mathbf{f}\\_2)$ to approximate the $\\lambda_2$**. \r
$$\r
\\mathbf{f}\\_2(u) =  \\begin{array}{cl}\r
       +1/\\sqrt{n}  & \\text{if}\\;\\; u\\;\\; \\text{belongs to the first cluster} \\\\\r
       -1/\\sqrt{n}  & \\text{if}\\;\\; u\\;\\; \\text{belongs to the second cluster} \r
\\end{array} \r
$$\r
In addition, if using $\\mathbf{\\cal L}$, since $\\mathbf{g}\\_2=\\mathbf{D}^{1/2}\\mathbf{f}_2$, we first approximate $\\mathbf{g}_2$ and then approximate $\\lambda_2$ from ${\\cal E}(\\mathbf{g}\\_2)$. With this approximation, we can easily compute the node belonging to each cluster with a simple MLP. In addition, such as the Fiedler value must satisfy orthogonality and normality, restrictions must be added to that MLP Clustering.\r
\r
### GAP-Layer\r
To sum up, **GAP-Layer** can be defined as the following. Given the matrix $\\mathbf{X}\\_{n\\times F}$ encoding the features of the nodes after any message passing (MP) layer, $\\mathbf{S}\\_{n\\times 2}=\\textrm{Softmax}(\\textrm{MLP}(\\mathbf{X}))$ learns the association $\\mathbf{X}\\rightarrow \\mathbf{S}$ while $\\mathbf{S}$ is optimized according to the loss:\r
  \r
$$\r
L\\_{Cut} = -\\frac{Tr[\\mathbf{S}^T\\mathbf{A}\\mathbf{S}]}{Tr[\\mathbf{S}^T\\mathbf{D}\\mathbf{S}]} + \\left\\|\\frac{\\mathbf{S}^T\\mathbf{S}}{\\|\\mathbf{S}^T\\mathbf{S}\\|\\_F} - \\frac{\\mathbf{I}\\_n}{\\sqrt{2}}\\right\\|\\_F\r
$$\r
Then, the $\\mathbf{f}\\_2$ is approximated from $\\mathbf{S}$ using $\\mathbf{f}\\_2(u)$ equation. Once calculated  $\\mathbf{f}\\_2$ and  $\\lambda\\_2$ we consider the loss:\r
\r
$$\r
L\\_{Fiedler} = \\|\\tilde{\\mathbf{A}}-\\mathbf{A}\\|\\_F + \\alpha(\\lambda\\_2)^2\r
$$\r
$$\\mathbf{\\tilde{A}} = \\mathbf{A} - \\mu \\nabla_\\mathbf{\\tilde{A}}\\lambda\\_2$$\r
returning $\\tilde{\\mathbf{A}}$. Then the GAP diffusion $\\mathbf{T}^{GAP} = \\tilde{\\mathbf{A}}(\\mathbf{S}) \\odot \\mathbf{A}$ results from minimizing \r
\r
$$L_{GAP}= L\\_{Cut} + L\\_{Fiedler}$$\r
\r
\r
**References**\r
(Kang et al. 2019) Kang, J., & Tong, H. (2019, November). N2n: Network derivative mining. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management (pp. 861-870).""" ;
    skos:prefLabel "GAP-Layer" .

:GAT a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1710.10903v3> ;
    skos:altLabel "Graph Attention Network" ;
    skos:definition """A **Graph Attention Network (GAT)** is a neural network architecture that operates on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods’ features, a GAT enables (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront.\r
\r
See [here](https://docs.dgl.ai/en/0.4.x/tutorials/models/1_gnn/9_gat.html) for an explanation by DGL.""" ;
    skos:prefLabel "GAT" .

:GATv2 a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2105.14491v3> ;
    skos:altLabel "Graph Attention Network v2" ;
    skos:definition """The __GATv2__ operator from the [“How Attentive are Graph Attention Networks?”](https://arxiv.org/abs/2105.14491) paper, which fixes the static attention problem of the standard [GAT](https://paperswithcode.com/method/gat) layer: since the linear layers in the standard GAT are applied right after each other, the ranking of attended nodes is unconditioned on the query node. In contrast, in GATv2, every node can attend to any other node.\r
\r
GATv2 scoring function:\r
\r
$e_{i,j} =\\mathbf{a}^{\\top}\\mathrm{LeakyReLU}\\left(\\mathbf{W}[\\mathbf{h}_i \\, \\Vert \\,\\mathbf{h}_j]\\right)$""" ;
    skos:prefLabel "GATv2" .

:GBO a skos:Concept ;
    rdfs:seeAlso <https://imanahmadianfar.com/codes/> ;
    skos:altLabel "Gradient-based optimization" ;
    skos:definition """GBO is a novel metaheuristic optimization algorithm. The GBO, inspired by the gradient-based Newton’s method, uses two main operators: gradient search rule (GSR) and local escaping operator (LEO) and a set of vectors to explore the search space. The GSR employs the gradient-based method to enhance the exploration tendency and accelerate the convergence rate to achieve better positions in the search space. The LEO enables the proposed GBO to escape from local optima. The performance of the new algorithm was evaluated in two phases. 28 mathematical test functions were first used to evaluate various characteristics of the GBO, and then six engineering problems were optimized by the GBO. In the first phase, the GBO was compared with five existing optimization algorithms, indicating that the GBO yielded very promising results due to its enhanced capabilities of exploration, exploitation, convergence, and effective avoidance of local optima. The second phase also demonstrated the superior performance of the GBO in solving complex real-world engineering problems. \r
\r
* The source codes of GBO are publicly available at https://imanahmadianfar.com/codes/.""" ;
    skos:prefLabel "GBO" .

:GBlock a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1909.11646v2> ;
    rdfs:seeAlso <https://github.com/yanggeng1995/GAN-TTS/blob/4675fa108c4c52f190d27a32a8d9e9ce1c68d7a1/models/generator.py#L38> ;
    skos:definition """**GBlock** is a type of [residual block](https://paperswithcode.com/method/residual-block) used in the [GAN-TTS](https://paperswithcode.com/method/gan-tts) text-to-speech architecture - it is a stack of two residual blocks. As the generator is producing raw audio (e.g. a 2s training clip corresponds\r
to a sequence of 48000 samples), dilated convolutions are used to ensure that the receptive field of $G$ is large enough to capture long-term dependencies. The four kernel size-3 convolutions in each GBlock have increasing dilation factors: 1, 2, 4, 8. Convolutions are preceded by Conditional Batch Normalisation, conditioned on the linear embeddings of the noise term $z \\sim N\\left(0, \\mathbf{I}\\_{128}\\right)$ in the single-speaker case, or the concatenation of $z$ and a one-hot representation of the speaker ID in the multi-speaker case. The embeddings are different for\r
each BatchNorm instance. \r
\r
A GBlock contains two skip connections, the first of which in [GAN](https://paperswithcode.com/method/gan)-TTS performs upsampling if the output frequency is higher than the input, and it also contains a size-1 [convolution](https://paperswithcode.com/method/convolution)\r
if the number of output channels is different from the input.""" ;
    skos:prefLabel "GBlock" .

:GCN a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1609.02907v4> ;
    skos:altLabel "Graph Convolutional Network" ;
    skos:definition "A **Graph Convolutional Network**, or **GCN**, is an approach for semi-supervised learning on graph-structured data. It is based on an efficient variant of [convolutional neural networks](https://paperswithcode.com/methods/category/convolutional-neural-networks) which operate directly on graphs. The choice of convolutional architecture is motivated via a localized first-order approximation of spectral graph convolutions. The model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes." ;
    skos:prefLabel "GCN" .

:GCNFN a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1902.06673v1> ;
    skos:altLabel "Graph Convolutional Networks for Fake News Detection" ;
    skos:definition "Social media are nowadays one of the main news sources for millions of people around the globe due to their low cost, easy access and rapid dissemination. This however comes at the cost of dubious trustworthiness and significant risk of exposure to 'fake news', intentionally written to mislead the readers. Automatically detecting fake news poses challenges that defy existing content-based analysis approaches. One of the main reasons is that often the interpretation of the news requires the knowledge of political or social context or 'common sense', which current NLP algorithms are still missing. Recent studies have shown that fake and real news spread differently on social media, forming propagation patterns that could be harnessed for the automatic fake news detection. Propagation-based approaches have multiple advantages compared to their content-based counterparts, among which is language independence and better resilience to adversarial attacks. In this paper we show a novel automatic fake news detection model based on geometric deep learning. The underlying core algorithms are a generalization of classical CNNs to graphs, allowing the fusion of heterogeneous data such as content, user profile and activity, social graph, and news propagation. Our model was trained and tested on news stories, verified by professional fact-checking organizations, that were spread on Twitter. Our experiments indicate that social network structure and propagation are important features allowing highly accurate (92.7% ROC AUC) fake news detection. Second, we observe that fake news can be reliably detected at an early stage, after just a few hours of propagation. Third, we test the aging of our model on training and testing data separated in time. Our results point to the promise of propagation-based approaches for fake news detection as an alternative or complementary strategy to content-based approaches." ;
    skos:prefLabel "GCNFN" .

:GCNII a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2007.02133v1> ;
    skos:definition "**GCNII** is an extension of a [Graph Convolution Networks](https://www.paperswithcode.com/method/gcn) with two new techniques, initial residual and identify mapping, to tackle the problem of oversmoothing -- where stacking more layers and adding non-linearity tends to degrade performance. At each layer, initial residual constructs a skip connection from the input layer, while identity mapping adds an identity matrix to the weight matrix." ;
    skos:prefLabel "GCNII" .

:GCNet a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1904.11492v1> ;
    rdfs:seeAlso <https://github.com/xvjiarui/GCNet> ;
    skos:definition "A **Global Context Network**, or **GCNet**, utilises global context blocks to model long-range dependencies in images. It is based on the [Non-Local Network](https://paperswithcode.com/method/non-local-block), but it modifies the architecture so less computation is required. Global context blocks are applied to multiple layers in a backbone network to construct the GCNet." ;
    skos:prefLabel "GCNet" .

:GCT a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1909.11519v2> ;
    skos:altLabel "Gated Channel Transformation" ;
    skos:definition """GCT first collects global information by computing the l2-norm of each channel. Next, a learnable vector $ \\alpha $ is applied to scale the feature. Then a competition mechanism is adopted by channel normalization to interact between channels.\r
\r
Unlike previous methods, GCT first collects global information by computing the $l_{2}$-norm of each channel. \r
Next, a learnable vector $\\alpha$ is applied to scale the feature.\r
Then a competition mechanism is adopted by \r
channel normalization to interact between channels. \r
Like other common normalization methods, \r
a learnable scale parameter $\\gamma$ and bias $\\beta$ are applied to \r
rescale the normalization.\r
However, unlike previous methods,\r
GCT adopts tanh activation to control the attention vector.\r
Finally, it not only multiplies the input by the attention vector but also adds an identity connection. GCT can be written as: \r
\\begin{align}\r
    s = F_\\text{gct}(X, \\theta) & = \\tanh (\\gamma CN(\\alpha \\text{Norm}(X)) + \\beta)\r
\\end{align}\r
\\begin{align}\r
    Y & = s  X + X\r
\\end{align}\r
\r
where $\\alpha$, $\\beta$ and $\\gamma$ are trainable parameters. $\\text{Norm}(\\cdot)$ indicates the $L2$-norm of each channel. $CN$ is  channel normalization.\r
\r
A GCT block has fewer parameters than an SE block, and as it is  lightweight, \r
 can be added after each convolutional layer of a CNN.""" ;
    skos:prefLabel "GCT" .

:GCU a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2108.12943v3> ;
    skos:altLabel "Growing Cosine Unit" ;
    skos:definition "An oscillatory function defined as $x \\cdot cos(x)$ that reports better performance than Sigmoid, Mish, Swish, and ReLU on several benchmarks." ;
    skos:prefLabel "GCU" .

:GECO a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1810.00597v1> ;
    skos:altLabel "Generalized ELBO with Constrained Optimization" ;
    skos:definition "" ;
    skos:prefLabel "GECO" .

:GEE a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2109.08828v2> ;
    skos:altLabel "Generative Emotion Estimator" ;
    skos:definition "" ;
    skos:prefLabel "GEE" .

:GELU a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1606.08415v5> ;
    rdfs:seeAlso <https://github.com/pytorch/pytorch/blob/96aaa311c0251d24decb9dc5da4957b7c590af6f/torch/nn/modules/activation.py#L584> ;
    skos:altLabel "Gaussian Error Linear Units" ;
    skos:definition """The **Gaussian Error Linear Unit**, or **GELU**,  is an activation function. The GELU activation function is $x\\Phi(x)$, where $\\Phi(x)$ the standard Gaussian cumulative distribution function. The GELU nonlinearity weights inputs by their percentile, rather than gates inputs by their sign as in [ReLUs](https://paperswithcode.com/method/relu) ($x\\mathbf{1}_{x>0}$). Consequently the GELU can be thought of as a smoother ReLU.\r
\r
$$\\text{GELU}\\left(x\\right) = x{P}\\left(X\\leq{x}\\right) = x\\Phi\\left(x\\right) = x \\cdot \\frac{1}{2}\\left[1 + \\text{erf}(x/\\sqrt{2})\\right],$$\r
if $X\\sim \\mathcal{N}(0,1)$.\r
\r
One can approximate the GELU with\r
$0.5x\\left(1+\\tanh\\left[\\sqrt{2/\\pi}\\left(x + 0.044715x^{3}\\right)\\right]\\right)$ or $x\\sigma\\left(1.702x\\right),$\r
but PyTorch's exact implementation is sufficiently fast such that these approximations may be unnecessary. (See also the [SiLU](https://paperswithcode.com/method/silu) $x\\sigma(x)$ which was also coined in the paper that introduced the GELU.)\r
\r
GELUs are used in [GPT-3](https://paperswithcode.com/method/gpt-3), [BERT](https://paperswithcode.com/method/bert), and most other Transformers.""" ;
    skos:prefLabel "GELU" .

:GENet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2006.14090v4> ;
    skos:altLabel "GPU-Efficient Network" ;
    skos:definition "**GENets**, or **GPU-Efficient Networks**, are a family of efficient models found through [neural architecture search](https://paperswithcode.com/methods/category/neural-architecture-search). The search occurs over several types of convolutional block, which include [depth-wise convolutions](https://paperswithcode.com/method/depthwise-convolution), [batch normalization](https://paperswithcode.com/method/batch-normalization), [ReLU](https://paperswithcode.com/method/relu), and an [inverted bottleneck](https://paperswithcode.com/method/inverted-residual-block) structure." ;
    skos:prefLabel "GENet" .

:GEOMANCER a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2006.12982v2> ;
    skos:altLabel "Geometric Manifold Component Estimator" ;
    skos:definition "**Geomancer** is a nonparametric algorithm for symmetry-based disentangling of data manifolds. It learns a set of subspaces to assign to each point in the dataset, where each subspace is the tangent space of one disentangled submanifold. This means that geomancer can be used to disentangle manifolds for which there may not be a global axis-aligned coordinate system." ;
    skos:prefLabel "GEOMANCER" .

:GER a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1910.12906v2> ;
    rdfs:seeAlso <https://github.com/UttaranB127/STEP> ;
    skos:altLabel "Gait Emotion Recognition" ;
    skos:definition """We present a novel classifier network called STEP, to classify perceived human emotion from gaits, based on a Spatial Temporal Graph Convolutional Network (ST-[GCN](https://paperswithcode.com/method/gcn)) architecture. Given an RGB video of an individual walking, our formulation implicitly exploits the gait features to classify the perceived emotion of the human into one of four emotions: happy, sad, angry, or neutral. We train STEP on annotated real-world gait videos, augmented with annotated synthetic gaits generated using a novel generative network called STEP-Gen, built on an ST-GCN based Conditional Variational Autoencoder (CVAE). We incorporate a novel push-pull regularization loss in the CVAE formulation of STEP-Gen to generate realistic gaits and improve the classification accuracy of STEP.\r
We also release a novel dataset (E-Gait), which consists of 4,227 human gaits annotated with perceived emotions along with thousands of synthetic gaits. In practice, STEP can learn the affective features and exhibits classification accuracy of 88\\% on E-Gait, which is 14--30\\% more accurate over prior methods.""" ;
    skos:prefLabel "GER" .

:GFP-GAN a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2101.04061v2> ;
    skos:definition "**GFP-GAN** is a generative adversarial network for blind face restoration that leverages a generative facial prior (GFP). This Generative Facial Prior (GFP) is incorporated into the face restoration process via channel-split spatial feature transform layers, which allow for a good balance between realness and fidelity. As a whole, the GFP-GAN consists of a degradation removal module ([U-Net](https://paperswithcode.com/method/u-net)) and a pretrained face  [StyleGAN](https://paperswithcode.com/method/stylegan) as a facial prior. They are bridged by a latent code mapping and several Channel-Split [Spatial Feature Transform](https://paperswithcode.com/method/spatial-feature-transform) (CS-SFT) layers. During training, 1) intermediate restoration losses are employed to remove complex degradation, 2) Facial component loss with discriminators is used to enhance facial details, and 3) identity preserving loss is used to retain face identity." ;
    skos:prefLabel "GFP-GAN" .

:GFSA a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2007.04929v2> ;
    skos:altLabel "Graph Finite-State Automaton" ;
    skos:definition "**Graph Finite-State Automaton**, or **GFSA**, is a differentiable layer for learning graph structure that adds a new edge type (expressed as a weighted adjacency matrix) to a base graph. This layer can be trained end-to-end to add derived relationships (edges) to arbitrary graph-structured data based on performance on a downstream task." ;
    skos:prefLabel "GFSA" .

:GGS-NNs a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1511.05493v4> ;
    skos:altLabel "Gated Graph Sequence Neural Networks" ;
    skos:definition """Gated Graph Sequence Neural Networks (GGS-NNs) is a novel graph-based neural network model. GGS-NNs modifies Graph Neural Networks (Scarselli et al., 2009) to use gated recurrent units and modern optimization techniques and then extend to output sequences.\r
\r
Source: [Li et al.](https://arxiv.org/pdf/1511.05493v4.pdf)\r
\r
Image source: [Li et al.](https://arxiv.org/pdf/1511.05493v4.pdf)""" ;
    skos:prefLabel "GGS-NNs" .

:GHM-C a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1811.05181v1> ;
    skos:altLabel "Gradient Harmonizing Mechanism C" ;
    skos:definition "**GHM-C** is a loss function designed to balance the gradient flow for anchor classification. The GHM first performs statistics on the number of examples with similar attributes w.r.t their gradient density and then attaches a harmonizing parameter to the gradient of each example according to the density. The modification of gradient can be equivalently implemented by reformulating the loss function. Embedding the GHM into the classification loss is denoted as GHM-C loss. Since the gradient density is a statistical variable depending on the examples distribution in a mini-batch, GHM-C is a dynamic loss that can adapt to the change of data distribution in each batch as well as to the updating of the model." ;
    skos:prefLabel "GHM-C" .

:GHM-R a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1811.05181v1> ;
    skos:altLabel "Gradient Harmonizing Mechanism R" ;
    skos:definition "**GHM-R** is a loss function designed to balance the gradient flow for bounding box refinement. The GHM first performs statistics on the number of examples with similar attributes w.r.t their gradient density and then attaches a harmonizing parameter to the gradient of each example according to the density. The modification of gradient can be equivalently implemented by reformulating the loss function. Embedding the GHM into the bounding box regression branch is denoted as GHM-R loss." ;
    skos:prefLabel "GHM-R" .

:GIC a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2009.06946v1> ;
    skos:altLabel "Graph InfoClust" ;
    skos:definition "" ;
    skos:prefLabel "GIC" .

:GIN a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1810.00826v3> ;
    skos:altLabel "Graph Isomorphism Network" ;
    skos:definition "Per the authors, Graph Isomorphism Network (GIN) generalizes the WL test and hence achieves maximum discriminative power among GNNs." ;
    skos:prefLabel "GIN" .

:GLIDE a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2112.10741v3> ;
    skos:altLabel "Guided Language to Image Diffusion for Generation and Editing" ;
    skos:definition "GLIDE is a generative model based on text-guided diffusion models for more photorealistic image generation. Guided diffusion is applied to text-conditional image synthesis and the model is able to handle free-form prompts. The diffusion model uses a text encoder to condition on natural language descriptions. The model is provided with editing capabilities in addition to zero-shot generation, allowing for iterative improvement of model samples to match more complex prompts. The model is fine-tuned to perform image inpainting." ;
    skos:prefLabel "GLIDE" .

:GLM a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2210.02414v1> ;
    skos:definition "**GLM** is a bilingual (English and Chinese) pre-trained transformer-based language model that follow the traditional architecture of decoder-only autoregressive language modeling. It leverages autoregressive blank infilling as its training objective." ;
    skos:prefLabel "GLM" .

:GLN a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1910.01526v2> ;
    skos:altLabel "Gated Linear Network" ;
    skos:definition """A **Gated Linear Network**, or **GLN**, is a type of backpropagation-free neural architecture. What distinguishes GLNs from contemporary neural networks is the distributed and local nature of their credit assignment mechanism; each neuron directly predicts the target, forgoing the ability to learn feature representations in favor of rapid online learning. Individual neurons can model nonlinear functions via the use of data-dependent gating in conjunction with online convex optimization. \r
\r
GLNs are feedforward networks composed of many layers of gated geometric mixing neurons as shown in the Figure . Each neuron in a given layer outputs a gated geometric mixture of the predictions from the previous layer, with the final layer consisting of just a single neuron. In a supervised learning setting, a $\\mathrm{GLN}$ is trained on (side information, base predictions, label) triplets $\\left(z\\_{t}, p\\_{t}, x\\_{t}\\right)_{t=1,2,3, \\ldots}$ derived from input-label pairs $\\left(z\\_{t}, x\\_{t}\\right)$. There are two types of input to neurons in the network: the first is the side information $z\\_{t}$, which can be thought of as the input features; the second is the input to the neuron, which will be the predictions output by the previous layer, or in the case of layer 0 , some (optionally) provided base predictions $p\\_{t}$ that typically will be a function of $z\\_{t} .$ Each neuron will also take in a constant bias prediction, which helps empirically and is essential for universality guarantees.\r
\r
Weights are learnt in a Gated Linear Network using Online Gradient Descent (OGD) locally at each neuron. They key observation is that as each neuron $(i, k)$ in layers $i>0$ is itself a gated geometric mixture, all of these neurons can be thought of as individually predicting the target. Given side information $z$ , each neuron $(i, k)$ suffers a loss convex in its active weights $u:=w\\_{i k c\\_{i k}(z)}$ of\r
$$\r
\\ell\\_{t}(u):=-\\log \\left(\\operatorname{GEO}\\_{u}\\left(x_{t} ; p\\_{i-1}\\right)\\right)\r
$$""" ;
    skos:prefLabel "GLN" .

:GLOW a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1807.03039v2> ;
    rdfs:seeAlso <https://github.com/openai/glow/blob/master/model.py> ;
    skos:definition "**GLOW** is a type of flow-based generative model that is based on an invertible $1 \\times 1$ [convolution](https://paperswithcode.com/method/convolution). This builds on the flows introduced by [NICE](https://paperswithcode.com/method/nice) and [RealNVP](https://paperswithcode.com/method/realnvp). It consists of a series of steps of flow, combined in a multi-scale architecture; see the Figure to the right. Each step of flow consists of Act Normalization followed by an *invertible $1 \\times 1$ convolution* followed by an [affine coupling](https://paperswithcode.com/method/affine-coupling) layer." ;
    skos:prefLabel "GLOW" .

:GLU a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1612.08083v3> ;
    rdfs:seeAlso <https://github.com/pytorch/pytorch/blob/96aaa311c0251d24decb9dc5da4957b7c590af6f/torch/nn/modules/activation.py#L551> ;
    skos:altLabel "Gated Linear Unit" ;
    skos:definition """A **Gated Linear Unit**, or **GLU** computes:\r
\r
$$ \\text{GLU}\\left(a, b\\right) = a\\otimes \\sigma\\left(b\\right) $$\r
\r
It is used in natural language processing architectures, for example the [Gated CNN](https://paperswithcode.com/method/gated-convolution-network), because here $b$ is the gate that control what information from $a$ is passed up to the following layer. Intuitively, for a language modeling task, the gating mechanism allows selection of words or features that are important for predicting the next word. The GLU also has non-linear capabilities, but has a linear path for the gradient so diminishes the vanishing gradient problem.""" ;
    skos:prefLabel "GLU" .

:GMI a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2002.01169v1> ;
    skos:altLabel "Graphic Mutual Information" ;
    skos:definition "**Graphic Mutual Information**, or **GMI**, measures the correlation between input graphs and high-level hidden representations. GMI generalizes the idea of conventional mutual information computations from vector space to the graph domain where measuring mutual information from two aspects of node features and topological structure is indispensable. GMI exhibits several benefits: First, it is invariant to the isomorphic transformation of input graphs---an inevitable constraint in many existing graph representation learning algorithms; Besides, it can be efficiently estimated and maximized by current mutual information estimation methods such as MINE." ;
    skos:prefLabel "GMI" .

:GMVAE a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2108.10764v1> ;
    skos:altLabel "Gaussian Mixture Variational Autoencoder" ;
    skos:definition "**GMVAE**, or **Gaussian Mixture Variational Autoencoder**, is a stochastic regularization layer for [transformers](https://paperswithcode.com/methods/category/transformers). A GMVAE layer is trained using a 700-dimensional internal representation of the first MLP layer. For every output from the first MLP layer, the GMVAE layer first computes a latent low-dimensional representation sampling from the GMVAE posterior distribution to then provide at the output a reconstruction sampled from a generative model." ;
    skos:prefLabel "GMVAE" .

:GNNCL a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2007.03316v2> ;
    skos:altLabel "Graph Neural Networks with Continual Learning" ;
    skos:definition "Although significant effort has been applied to fact-checking, the prevalence of fake news over social media, which has profound impact on justice, public trust and our society, remains a serious problem. In this work, we focus on propagation-based fake news detection, as recent studies have demonstrated that fake news and real news spread differently online. Specifically, considering the capability of graph neural networks (GNNs) in dealing with non-Euclidean data, we use GNNs to differentiate between the propagation patterns of fake and real news on social media. In particular, we concentrate on two questions: (1) Without relying on any text information, e.g., tweet content, replies and user descriptions, how accurately can GNNs identify fake news? Machine learning models are known to be vulnerable to adversarial attacks, and avoiding the dependence on text-based features can make the model less susceptible to the manipulation of advanced fake news fabricators. (2) How to deal with new, unseen data? In other words, how does a GNN trained on a given dataset perform on a new and potentially vastly different dataset? If it achieves unsatisfactory performance, how do we solve the problem without re-training the model on the entire data from scratch? We study the above questions on two datasets with thousands of labelled news items, and our results show that: (1) GNNs can achieve comparable or superior performance without any text information to state-of-the-art methods. (2) GNNs trained on a given dataset may perform poorly on new, unseen data, and direct incremental training cannot solve the problem---this issue has not been addressed in the previous work that applies GNNs for fake news detection. In order to solve the problem, we propose a method that achieves balanced performance on both existing and new datasets, by using techniques from continual learning to train GNNs incrementally." ;
    skos:prefLabel "GNNCL" .

:GNS a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2002.09405v2> ;
    skos:altLabel "Graph Network-based Simulators" ;
    skos:definition "**Graph Network-Based Simulators** is a type of graph neural network that represents the state of a physical system with particles, expressed as nodes in a graph, and computes dynamics via learned message-passing." ;
    skos:prefLabel "GNS" .

:GPFL a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2003.06071v2> ;
    skos:altLabel "Graph Path Feature Learning" ;
    skos:definition "**Graph Path Feature Learning** is a probabilistic rule learner optimized to mine instantiated first-order logic rules from knowledge graphs. Instantiated rules contain constants extracted from KGs. Compared to abstract rules that contain no constants, instantiated rules are capable of explaining and expressing concepts in more detail. GPFL utilizes a novel two-stage rule generation mechanism that first generalizes extracted paths into templates that are acyclic abstract rules until a certain degree of template saturation is achieved, then specializes the generated templates into instantiated rules." ;
    skos:prefLabel "GPFL" .

:GPS a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2002.09103v2> ;
    skos:altLabel "Greedy Policy Search" ;
    skos:definition "**Greedy Policy Search** (GPS) is a simple algorithm that learns a policy for test-time data augmentation based on the predictive performance on a validation set. GPS starts with an empty policy and builds it in an iterative fashion. Each step selects a sub-policy that provides the largest improvement in calibrated log-likelihood of ensemble predictions and adds it to the current policy." ;
    skos:prefLabel "GPS" .

:GPSA a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2103.10697v2> ;
    rdfs:seeAlso <https://github.com/facebookresearch/convit/blob/60f2c8dd8cab4b96f46f781275a22b654d6cd4ad/convit.py#L54> ;
    skos:altLabel "Gated Positional Self-Attention" ;
    skos:definition "**Gated Positional Self-Attention (GPSA)** is a self-attention module for vision transformers, used in the [ConViT](https://paperswithcode.com/method/convit) architecture, that can be initialized as a convolutional layer -- helping a ViT learn inductive biases about locality." ;
    skos:prefLabel "GPSA" .

:GPT a skos:Concept ;
    dcterms:source <https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf> ;
    skos:definition """**GPT** is a [Transformer](https://paperswithcode.com/method/transformer)-based architecture and training procedure for natural language processing tasks. Training follows a two-stage procedure. First, a language modeling objective is used on\r
the unlabeled data to learn the initial parameters of a neural network model. Subsequently, these parameters are adapted to a target task using the corresponding supervised objective.""" ;
    skos:prefLabel "GPT" .

:GPT-2 a skos:Concept ;
    dcterms:source <https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf> ;
    skos:definition """**GPT-2** is a [Transformer](https://paperswithcode.com/methods/category/transformers) architecture that was notable for its size (1.5 billion parameters) on its release. The model is pretrained on a WebText dataset - text from 45 million website links. It largely follows the previous [GPT](https://paperswithcode.com/method/gpt) architecture with some modifications:\r
\r
- [Layer normalization](https://paperswithcode.com/method/layer-normalization) is moved to the input of each sub-block, similar to a\r
pre-activation residual network and an additional layer normalization was added after the final self-attention block. \r
\r
- A modified initialization which accounts for the accumulation on the residual path with model depth\r
is used. Weights of residual layers are scaled at initialization by a factor of $1/\\sqrt{N}$ where $N$ is the number of residual layers. \r
\r
- The vocabulary is expanded to 50,257. The context size is expanded from 512 to 1024 tokens and\r
a larger batch size of 512 is used.""" ;
    skos:prefLabel "GPT-2" .

:GPT-3 a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2005.14165v4> ;
    rdfs:seeAlso <https://github.com/EleutherAI/gpt-neox> ;
    skos:definition """**GPT-3** is an autoregressive [transformer](https://paperswithcode.com/methods/category/transformers)  model with 175 billion\r
parameters. It uses the same architecture/model as [GPT-2](https://paperswithcode.com/method/gpt-2), including the modified initialization, pre-normalization, and reversible tokenization, with the exception that GPT-3 uses alternating dense and locally banded sparse attention patterns in the layers of the [transformer](https://paperswithcode.com/method/transformer), similar to the [Sparse Transformer](https://paperswithcode.com/method/sparse-transformer).""" ;
    skos:prefLabel "GPT-3" .

:GPT-4 a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2303.08774v3> ;
    skos:definition "**GPT-4** is a transformer based model pre-trained to predict the next token in a document." ;
    skos:prefLabel "GPT-4" .

:GPT-Neo a skos:Concept ;
    skos:definition """An implementation of model & data parallel [GPT3-like](https://paperswithcode.com/method/gpt-3) models using the [mesh-tensorflow](https://github.com/tensorflow/mesh) library.\r
\r
Source: [EleutherAI/GPT-Neo](https://github.com/EleutherAI/gpt-neo)""" ;
    skos:prefLabel "GPT-Neo" .

:GPT-NeoX a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2204.06745v1> ;
    skos:definition "**GPT-NeoX** is an autoregressive transformer decoder model whose architecture largely follows that of GPT-3, with a few notable deviations. The model has 20 billion parameters with 44 layers, a hidden dimension size of 6144, and 64 heads. The main difference with GPT-3 is the change in tokenizer, the addition of Rotary Positional Embeddings, the parallel computation of attention and feed-forward layers, and a different initialization scheme and hyperparameters." ;
    skos:prefLabel "GPT-NeoX" .

:GPipe a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1811.06965v5> ;
    rdfs:seeAlso <https://github.com/kakaobrain/torchgpipe/blob/a1b4ee25574864e7650e7905a69ce156da9752ec/torchgpipe/gpipe.py#L134> ;
    skos:definition "**GPipe** is a distributed model parallel method for neural networks. With GPipe, each model can be specified as a sequence of layers, and consecutive groups of layers can be partitioned into cells. Each cell is then placed on a separate accelerator. Based on this partitioned setup, batch splitting is applied. A mini-batch of training examples is split into smaller micro-batches, then the execution of each set of micro-batches is pipelined over cells. Synchronous mini-batch gradient descent is applied for training, where gradients are accumulated across all micro-batches in a mini-batch and applied at the end of a mini-batch." ;
    skos:prefLabel "GPipe" .

:GRIN a skos:Concept ;
    skos:altLabel "Graph Recurrent Imputation Network" ;
    skos:definition "" ;
    skos:prefLabel "GRIN" .

:GRLIA a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2108.12179v1> ;
    skos:definition "**GRLIA** is an incident aggregation framework for online service systems based on graph representation learning over the cascading graph of cloud failures. A representation vector is learned for each unique type of incident in an unsupervised and unified manner, which is able to simultaneously encode the topological and temporal correlations among incidents." ;
    skos:prefLabel "GRLIA" .

:GRU a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1406.1078v3> ;
    skos:altLabel "Gated Recurrent Unit" ;
    skos:definition """A **Gated Recurrent Unit**, or **GRU**, is a type of recurrent neural network. It is similar to an [LSTM](https://paperswithcode.com/method/lstm), but only has two gates - a reset gate and an update gate - and notably lacks an output gate. Fewer parameters means GRUs are generally easier/faster to train than their LSTM counterparts.\r
\r
Image Source: [here](https://www.google.com/url?sa=i&url=https%3A%2F%2Fcommons.wikimedia.org%2Fwiki%2FFile%3AGated_Recurrent_Unit%2C_type_1.svg&psig=AOvVaw3EmNX8QXC5hvyxeenmJIUn&ust=1590332062671000&source=images&cd=vfe&ved=0CA0QjhxqFwoTCMiev9-eyukCFQAAAAAdAAAAABAR)""" ;
    skos:prefLabel "GRU" .

:GRoIE a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2004.13665v2> ;
    skos:altLabel "Generic RoI Extractor" ;
    skos:definition """**GroIE** is an RoI extractor which intends to overcome the limitation of existing extractors which select only one (the best) layer from the [FPN](https://paperswithcode.com/method/fpn). The intuition is that all the layers of FPN retain useful\r
information. Therefore, the proposed layer introduces non-local building blocks and attention mechanisms to boost the performance.""" ;
    skos:prefLabel "GRoIE" .

:GShard a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2006.16668v1> ;
    skos:definition "**GShard** is a intra-layer parallel distributed method. It consists of set of simple APIs for annotations, and a compiler extension in XLA for automatic parallelization." ;
    skos:prefLabel "GShard" .

:GSoP-Net a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1811.12006v2> ;
    skos:altLabel "Global second-order pooling convolutional networks" ;
    skos:definition """A Gsop block has a squeeze module and an excitation module, and uses a second-order pooling to model high-order statistics while gathering global information.\r
In the squeeze module, a GSoP block firstly reduces the number of channels from $c$ to $c'$ ($c' < c$) using a $1 \\times 1$ convolution,  then  computes a $c' \\times c'$ covariance matrix for the different channels to obtain their correlation.  Next, row-wise normalization is performed on the covariance matrix.  Each $(i, j)$ in the normalized covariance matrix explicitly relates channel $i$ to channel $j$. \r
\r
In the excitation module, a GSoP block performs row-wise convolution to  maintain structural information and output a vector. Then a fully-connected layer and a sigmoid function are applied  to get a $c$-dimensional attention vector. Finally, it multiplies the input features by the attention vector, as in an SE block. A GSoP block can be formulated as:\r
\\begin{align}\r
    s = F_\\text{gsop}(X, \\theta) & = \\sigma (W \\text{RC}(\\text{Cov}(\\text{Conv}(X))))\r
\\end{align}\r
\\begin{align}\r
    Y & = s  X\r
\\end{align}\r
Here, $\\text{Conv}(\\cdot)$ reduces the number of channels,\r
$\\text{Cov}(\\cdot)$ computes the covariance matrix and\r
$\\text{RC}(\\cdot)$ means row-wise convolution.""" ;
    skos:prefLabel "GSoP-Net" .

:GTS a skos:Concept ;
    dcterms:source <https://www.ijcai.org/Proceedings/2019/736> ;
    skos:altLabel "Goal-Driven Tree-Structured Neural Model" ;
    skos:definition "" ;
    skos:prefLabel "GTS" .

:GTrXL a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1910.06764v1> ;
    skos:altLabel "Gated Transformer-XL" ;
    skos:definition """**Gated Transformer-XL**, or **GTrXL**, is a [Transformer](https://paperswithcode.com/methods/category/transformers)-based architecture for reinforcement learning. It introduces architectural modifications that improve the stability and learning speed of the original Transformer and XL variant. Changes include:\r
\r
- Placing the [layer normalization](https://paperswithcode.com/method/layer-normalization) on only the input stream of the submodules. A key benefit to this reordering is that it now enables an identity map from the input of the transformer at the first layer to the output of the transformer after the last layer. This is in contrast to the canonical transformer, where there are a series of layer normalization operations that non-linearly transform the state encoding.\r
- Replacing [residual connections](https://paperswithcode.com/method/residual-connection) with gating layers. The authors' experiments found that [GRUs](https://www.paperswithcode.com/method/gru) were the most effective form of gating.""" ;
    skos:prefLabel "GTrXL" .

:GaAN a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1803.07294v1> ;
    skos:altLabel "Gated Attention Networks" ;
    skos:definition """Gated Attention Networks (GaAN) is a new architecture for learning on graphs. Unlike the traditional multi-head attention mechanism, which equally consumes all attention heads, GaAN uses a convolutional sub-network to control each attention head’s importance.\r
\r
Image credit: [GaAN: Gated Attention Networks for Learning on Large and Spatiotemporal Graphs](https://paperswithcode.com/paper/gaan-gated-attention-networks-for-learning-on)""" ;
    skos:prefLabel "GaAN" .

:Galactica a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2211.09085v1> ;
    skos:definition """Galactica is a language model which uses a Transformer architecture in a decoder-only setup with the following modifications:\r
\r
- It uses GeLU activations on all model sizes\r
- It uses a 2048 length context window for all model sizes\r
- It does not use biases in any of the dense kernels or layer norms\r
- It uses learned positional embeddings for the model\r
- A vocabulary of 50k tokens was constructed using BPE. The vocabulary was generated from a randomly selected 2% subset of the training data""" ;
    skos:prefLabel "Galactica" .

:GatedConvolution a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1612.08083v3> ;
    skos:definition "A **Gated Convolution** is a type of temporal [convolution](https://paperswithcode.com/method/convolution) with a gating mechanism. Zero-padding is used to ensure that future context can not be seen." ;
    skos:prefLabel "Gated Convolution" .

:GatedConvolutionNetwork a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1612.08083v3> ;
    skos:definition "A **Gated Convolutional Network** is a type of language model that combines convolutional networks with a gating mechanism. Zero padding is used to ensure future context can not be seen. Gated convolutional layers can be stacked on top of other hierarchically. Model predictions are then obtained with an [adaptive softmax](https://paperswithcode.com/method/adaptive-softmax) layer." ;
    skos:prefLabel "Gated Convolution Network" .

:Gather-ExciteNetworks a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1810.12348v3> ;
    skos:definition "GENet combines part gathering and excitation operations. In the first step, it aggregates input features over large neighborhoods and models the relationship between different spatial locations. In the second step, it first generates an attention map of the same size as the input feature map, using interpolation. Then each position in the input feature map is scaled by multiplying by the corresponding element in the attention map." ;
    skos:prefLabel "Gather-Excite Networks" .

:GaussianAffinity a skos:Concept ;
    rdfs:seeAlso <https://github.com/tea1528/Non-Local-NN-Pytorch/blob/986937674eb3b85d3d3fbaaa8f384c0a26624121/models/non_local.py#L93> ;
    skos:definition """**Gaussian Affinity** is a type of affinity or self-similarity function between two points $\\mathbb{x\\_{i}}$ and $\\mathbb{x\\_{j}}$ that uses a Gaussian function:\r
\r
$$ f\\left(\\mathbb{x\\_{i}}, \\mathbb{x\\_{j}}\\right) = e^{\\mathbb{x^{T}\\_{i}}\\mathbb{x\\_{j}}} $$\r
\r
Here $\\mathbb{x^{T}\\_{i}}\\mathbb{x\\_{j}}$ is dot-product similarity.""" ;
    skos:prefLabel "Gaussian Affinity" .

:GaussianProcess a skos:Concept ;
    skos:definition """**Gaussian Processes** are non-parametric models for approximating functions. They rely upon a measure of similarity between points (the kernel function) to predict the value for an unseen point from training data. The models are fully probabilistic so uncertainty bounds are baked in with the model.\r
\r
Image Source: Gaussian Processes for Machine Learning, C. E. Rasmussen & C. K. I. Williams""" ;
    skos:prefLabel "Gaussian Process" .

:GeGLU a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2002.05202v1> ;
    skos:definition """**GeGLU** is an activation function which is a variant of [GLU](https://paperswithcode.com/method/glu). The definition is as follows:\r
\r
$$ \\text{GeGLU}\\left(x, W, V, b, c\\right) = \\text{GELU}\\left(xW + b\\right) \\otimes \\left(xV + c\\right) $$""" ;
    skos:prefLabel "GeGLU" .

:GeneralizedFocalLoss a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2006.04388v1> ;
    skos:definition "**Generalized Focal Loss (GFL)** is a loss function for object detection that combines Quality [Focal Loss](https://paperswithcode.com/method/focal-loss) and Distribution Focal Loss into a general form." ;
    skos:prefLabel "Generalized Focal Loss" .

:GeneralizedMeanPooling a skos:Concept ;
    skos:definition """**Generalized Mean Pooling (GeM)** computes the generalized mean of each channel in a tensor. Formally:\r
\r
$$ \\textbf{e} = \\left[\\left(\\frac{1}{|\\Omega|}\\sum\\_{u\\in{\\Omega}}x^{p}\\_{cu}\\right)^{\\frac{1}{p}}\\right]\\_{c=1,\\cdots,C} $$\r
\r
where $p > 0$ is a parameter. Setting this exponent as $p > 1$ increases the contrast of the pooled feature map and focuses on the salient features of the image. GeM is a generalization of the [average pooling](https://paperswithcode.com/method/average-pooling) commonly used in classification networks ($p = 1$) and of spatial max-pooling layer ($p = \\infty$).\r
\r
Source: [MultiGrain](https://paperswithcode.com/method/multigrain)\r
\r
Image Source: [Eva Mohedano](https://www.google.com/url?sa=i&url=https%3A%2F%2Fwww.slideshare.net%2Fxavigiro%2Fd1l5-contentbased-image-retrieval-upc-2018-deep-learning-for-computer-vision&psig=AOvVaw2-9Hx23FNGFDe4GHU22Oo5&ust=1591798200590000&source=images&cd=vfe&ved=0CA0QjhxqFwoTCOiP-9P09OkCFQAAAAAdAAAAABAD)""" ;
    skos:prefLabel "Generalized Mean Pooling" .

:GeniePath a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1802.00910v3> ;
    skos:definition """GeniePath is a scalable approach for learning adaptive receptive fields of neural networks defined on permutation invariant graph data. In GeniePath, we propose an adaptive path layer consists of two complementary functions designed for breadth and depth exploration respectively, where the former learns the importance of different sized neighborhoods, while the latter extracts and filters signals aggregated from neighbors of different hops away.\r
\r
Description and image from: [GeniePath: Graph Neural Networks with Adaptive Receptive Paths](https://arxiv.org/pdf/1802.00910.pdf)""" ;
    skos:prefLabel "GeniePath" .

:GhostBottleneck a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1911.11907v2> ;
    rdfs:seeAlso <https://github.com/iamhankai/ghostnet.pytorch/blob/a74f64e74014cab190b4c56607c78abc540b0e5d/ghost_net.py#L81> ;
    skos:definition """A **Ghost BottleNeck** is a skip connection block, similar to the basic [residual block](https://paperswithcode.com/method/residual-block) in [ResNet](https://paperswithcode.com/method/resnet) in which several convolutional layers and shortcuts are integrated, but stacks [Ghost Modules](https://paperswithcode.com/method/ghost-module) instead (two stacked Ghost modules). It was proposed as part of the [GhostNet](https://paperswithcode.com/method/ghostnet) CNN architecture.\r
\r
The first Ghost module acts as an expansion layer increasing the number of channels. The ratio between the number of the output channels and that of the input is referred to as the *expansion ratio*. The second Ghost module reduces the number of channels to match the shortcut path. Then the shortcut is connected between the inputs and the outputs of these two Ghost modules. The [batch normalization](https://paperswithcode.com/method/batch-normalization) (BN) and [ReLU](https://paperswithcode.com/method/relu) nonlinearity are applied after each layer, except that ReLU is not used after the second Ghost module as suggested by [MobileNetV2](https://paperswithcode.com/method/mobilenetv2). The Ghost bottleneck described above is for stride=1. As for the case where stride=2, the shortcut path is implemented by a downsampling layer and a [depthwise convolution](https://paperswithcode.com/method/depthwise-convolution) with stride=2 is inserted between the two Ghost modules. In practice, the primary [convolution](https://paperswithcode.com/method/convolution) in Ghost module here is [pointwise convolution](https://paperswithcode.com/method/pointwise-convolution) for its efficiency.""" ;
    skos:prefLabel "Ghost Bottleneck" .

:GhostModule a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1911.11907v2> ;
    rdfs:seeAlso <https://github.com/huawei-noah/ghostnet/blob/e8c449210c833996e14ffcdd3946ecdbb7098fd1/pytorch/ghostnet.py#L76> ;
    skos:definition """A **Ghost Module** is an image block for convolutional neural network that aims to generate more features by using fewer parameters. Specifically, an ordinary convolutional layer in deep neural networks is split into two parts. The first part involves ordinary convolutions but their total number is controlled. Given the intrinsic feature maps from the first part, a series of simple linear operations are applied for generating more feature maps. \r
\r
Given the widely existing redundancy in intermediate feature maps calculated by mainstream CNNs, ghost modules aim to reduce them. In practice, given the input data $X\\in\\mathbb{R}^{c\\times h\\times w}$, where $c$ is the number of input channels and $h$ and $w$ are the height and width of the input data, respectively,  the operation of an arbitrary convolutional layer for producing $n$ feature maps can be formulated as\r
\r
$$\r
Y = X*f+b,\r
$$\r
\r
where $*$ is the [convolution](https://paperswithcode.com/method/convolution) operation, $b$ is the bias term, $Y\\in\\mathbb{R}^{h'\\times w'\\times n}$ is the output feature map with $n$ channels, and $f\\in\\mathbb{R}^{c\\times k\\times k \\times n}$ is the convolution filters in this layer. In addition, $h'$ and $w'$ are the height and width of the output data, and $k\\times k$ is the kernel size of convolution filters $f$, respectively. During this convolution procedure, the required number of FLOPs can be calculated as $n\\cdot h'\\cdot w'\\cdot c\\cdot k\\cdot k$, which is often as large as hundreds of thousands since the number of filters $n$ and the channel number $c$ are generally very large (e.g. 256 or 512).\r
\r
Here, the number of parameters (in $f$ and $b$) to be optimized is explicitly determined by the dimensions of input and output feature maps. The output feature maps of convolutional layers often contain much redundancy, and some of them could be similar with each other. We point out that it is unnecessary to generate these redundant feature maps one by one with large number of FLOPs and parameters. Suppose that the output feature maps are *ghosts* of a handful of intrinsic feature maps with some cheap transformations. These intrinsic feature maps are often of smaller size and produced by ordinary convolution filters. Specifically, $m$ intrinsic feature maps $Y'\\in\\mathbb{R}^{h'\\times w'\\times m}$ are generated using a primary convolution:\r
\r
$$\r
Y' = X*f',\r
$$\r
\r
where $f'\\in\\mathbb{R}^{c\\times k\\times k \\times m}$ is the utilized filters, $m\\leq n$ and the bias term is omitted for simplicity. The hyper-parameters such as filter size, stride, padding, are the same as those in the ordinary convolution to keep the spatial size (ie $h'$ and $w'$) of the output feature maps consistent. To further obtain the desired $n$ feature maps, we apply a series of cheap linear operations on each intrinsic feature in $Y'$ to generate $s$ ghost features according to the following function:\r
\r
$$\r
y_{ij} = \\Phi_{i,j}(y'_i),\\quad \\forall\\; i = 1,...,m,\\;\\; j = 1,...,s,\r
$$\r
\r
where $y'\\_i$ is the $i$-th intrinsic feature map in $Y'$, $\\Phi\\_{i,j}$ in the above function is the $j$-th (except the last one) linear operation for generating the $j$-th ghost feature map $y_{ij}$, that is to say, $y'\\_i$ can have one or more ghost feature maps $\\{y\\_{ij}\\}\\_{j=1}^{s}$. The last $\\Phi\\_{i,s}$ is the identity mapping for preserving the intrinsic feature maps. we can obtain $n=m\\cdot s$ feature maps $Y=[y\\_{11},y\\_{12},\\cdots,y\\_{ms}]$ as the output data of a Ghost module. Note that the linear operations $\\Phi$ operate on each channel whose computational cost is much less than the ordinary convolution. In practice, there could be several different linear operations in a Ghost module, eg $3\\times 3$ and $5\\times5$ linear kernels, which will be analyzed in the experiment part.""" ;
    skos:prefLabel "Ghost Module" .

:GhostNet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1911.11907v2> ;
    rdfs:seeAlso <https://github.com/osmr/imgclsmob/blob/c03fa67de3c9e454e9b6d35fe9cbb6b15c28fda7/pytorch/pytorchcv/models/ghostnet.py#L207> ;
    skos:definition """A **GhostNet** is a type of convolutional neural network that is built using Ghost modules, which aim to generate more features by using fewer parameters (allowing for greater efficiency). \r
\r
GhostNet mainly consists of a stack of Ghost bottlenecks with the Ghost modules as the building block. The first layer is a standard convolutional layer with 16 filters, then a series of Ghost bottlenecks with gradually increased channels follow. These Ghost bottlenecks are grouped into different stages according to the sizes of their input feature maps. All the Ghost bottlenecks are applied with stride=1 except that the last one in each stage is with stride=2. At last a [global average pooling](https://paperswithcode.com/method/global-average-pooling) and a convolutional layer are utilized to transform the feature maps to a 1280-dimensional feature vector for final classification. The squeeze and excite (SE) module is also applied to the residual layer in some ghost bottlenecks. \r
\r
In contrast to [MobileNetV3](https://paperswithcode.com/method/mobilenetv3), GhostNet does not use [hard-swish](https://paperswithcode.com/method/hard-swish) nonlinearity function due to its large latency.""" ;
    skos:prefLabel "GhostNet" .

:GloVe a skos:Concept ;
    dcterms:source <https://aclanthology.org/D14-1162> ;
    skos:altLabel "GloVe Embeddings" ;
    skos:definition """**GloVe Embeddings** are a type of word embedding that encode the co-occurrence probability ratio between two words as vector differences. GloVe uses a weighted least squares objective $J$ that minimizes the difference between the dot product of the vectors of two words and the logarithm of their number of co-occurrences:\r
\r
$$ J=\\sum\\_{i, j=1}^{V}f\\left(𝑋\\_{i j}\\right)(w^{T}\\_{i}\\tilde{w}_{j} + b\\_{i} + \\tilde{b}\\_{j} - \\log{𝑋}\\_{ij})^{2} $$\r
\r
where $w\\_{i}$ and $b\\_{i}$ are the word vector and bias respectively of word $i$, $\\tilde{w}_{j}$ and $b\\_{j}$ are the context word vector and bias respectively of word $j$, $X\\_{ij}$ is the number of times word $i$ occurs in the context of word $j$, and $f$ is a weighting function that assigns lower weights to rare and frequent co-occurrences.""" ;
    skos:prefLabel "GloVe" .

:Global-LocalAttention a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2004.08483v5> ;
    skos:definition "**Global-Local Attention** is a type of attention mechanism used in the [ETC](https://paperswithcode.com/method/etc) architecture. ETC receives two separate input sequences: the global input $x^{g} = (x^{g}\\_{1}, \\dots, x^{g}\\_{n\\_{g}})$ and the long input $x^{l} = (x^{l}\\_{1}, \\dots x^{l}\\_{n\\_{l}})$. Typically, the long input contains the input a [standard Transformer](https://paperswithcode.com/method/transformer) would receive, while the global input contains a much smaller number of auxiliary tokens ($n\\_{g}  \\ll n\\_{l}$). Attention is then split into four separate pieces: global-to-global (g2g), global-tolong (g2l), long-to-global (l2g), and long-to-long (l2l). Attention in the l2l piece (the most computationally expensive piece) is restricted to a fixed radius $r \\ll n\\_{l}$. To compensate for this limited attention span, the tokens in the global input have unrestricted attention, and thus long input tokens can transfer information to each other through global input tokens. Accordingly, g2g, g2l, and l2g pieces of attention are unrestricted." ;
    skos:prefLabel "Global-Local Attention" .

:GlobalAveragePooling a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1312.4400v3> ;
    rdfs:seeAlso <https://github.com/pytorch/vision/blob/baa592b215804927e28638f6a7f3318cbc411d49/torchvision/models/resnet.py#L157> ;
    skos:definition """**Global Average Pooling** is a pooling operation designed to replace fully connected layers in classical CNNs. The idea is to generate one feature map for each corresponding category of the classification task in the last mlpconv layer. Instead of adding fully connected layers on top of the feature maps, we take the average of each feature map, and the resulting vector is fed directly into the [softmax](https://paperswithcode.com/method/softmax) layer. \r
\r
One advantage of global [average pooling](https://paperswithcode.com/method/average-pooling) over the fully connected layers is that it is more native to the [convolution](https://paperswithcode.com/method/convolution) structure by enforcing correspondences between feature maps and categories. Thus the feature maps can be easily interpreted as categories confidence maps. Another advantage is that there is no parameter to optimize in the global average pooling thus overfitting is avoided at this layer. Furthermore, global average pooling sums out the spatial information, thus it is more robust to spatial translations of the input.""" ;
    skos:prefLabel "Global Average Pooling" .

:GlobalContextBlock a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1904.11492v1> ;
    rdfs:seeAlso <https://github.com/xvjiarui/GCNet/blob/a9fcc88c4bd3a0b89de3678b4629c9dfd190575f/mmdet/ops/gcb/context_block.py#L13> ;
    skos:definition """A **Global Context Block** is an image model block for global context modeling. The aim is to have both the benefits of the simplified [non-local block](https://paperswithcode.com/method/non-local-block) with effective modeling of long-range dependencies, and the [squeeze-excitation block](https://paperswithcode.com/method/squeeze-and-excitation-block) with lightweight computation. \r
\r
In the Global Context framework, we have (a) global attention pooling, which adopts a [1x1 convolution](https://paperswithcode.com/method/1x1-convolution) $W_{k}$ and [softmax](https://paperswithcode.com/method/softmax) function to obtain the attention weights, and then performs the attention pooling to obtain the global context features, (b) feature transform via a 1x1 [convolution](https://paperswithcode.com/method/convolution) $W\\_{v}$; (c) feature aggregation, which employs addition to aggregate the global context features to the features of each position. Taken as a whole, the GC block is proposed as a lightweight way to achieve global context modeling.""" ;
    skos:prefLabel "Global Context Block" .

:GlobalConvolutionalNetwork a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1703.02719v1> ;
    skos:definition """A **Global Convolutional Network**, or **GCN**, is a semantic segmentation building block that utilizes a large kernel to help perform classification and localization tasks simultaneously. It can be used in a [FCN](https://paperswithcode.com/method/fcn)-like structure, where the [GCN](https://paperswithcode.com/method/gcn) is used to generate semantic score maps. Instead of directly using larger kernels or global [convolution](https://paperswithcode.com/method/convolution), the GCN module employs a combination of $1 \\times k + k \\times 1$ and $k \\times 1 + 1 \\times k$ convolutions, which enables [dense connections](https://paperswithcode.com/method/dense-connections) within a large\r
$k\\times{k}$ region in the feature map""" ;
    skos:prefLabel "Global Convolutional Network" .

:GlobalLocalAttentionModule a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2107.08000v1> ;
    skos:definition """The Global Local Attention Module (GLAM) is an image model block that attends to the feature map's channels and spatial dimensions locally, and also attends to the feature map's channels and spatial dimensions globally. The locally attended feature maps, globally attended feature maps, and the original feature maps are then fused through a weighted sum (with learnable weights) to obtain the final feature map.\r
\r
Paper:\r
\r
Song, C. H., Han, H. J., & Avrithis, Y. (2022). All the attention you need: Global-local, spatial-channel attention for image retrieval. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (pp. 2754-2763).""" ;
    skos:prefLabel "Global Local Attention Module" .

:GlobalSub-SampledAttention a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2104.13840v4> ;
    skos:definition """**Global Sub-Sampled Attention**, or **GSA**, is a local [attention mechanism](https://paperswithcode.com/methods/category/attention-mechanisms-1) used in the [Twins-SVT](https://paperswithcode.com/method/twins-svt) architecture. \r
\r
A single representative is used to summarize the key information for each of $m \\times n$ subwindows and the representative is used to communicate with other sub-windows (serving as the key in self-attention), which can reduce the cost to $\\mathcal{O}(m n H W d)=\\mathcal{O}\\left(\\frac{H^{2} W^{2} d}{k\\_{1} k\\_{2}}\\right)$. This is essentially equivalent to using the sub-sampled feature maps as the key in attention operations, and thus it is termed global sub-sampled attention (GSA). \r
\r
If we alternatively use the [LSA](https://paperswithcode.com/method/locally-grouped-self-attention) and GSA like [separable convolutions](https://paperswithcode.com/method/depthwise-separable-convolution) (depth-wise + point-wise). The total computation cost is $\\mathcal{O}\\left(\\frac{H^{2} W^{2} d}{k\\_{1} k\\_{2}}+k\\_{1} k\\_{2} H W d\\right) .$ We have:\r
\r
$$\\frac{H^{2} W^{2} d}{k\\_{1} k\\_{2}}+k_{1} k_{2} H W d \\geq 2 H W d \\sqrt{H W} $$ \r
\r
The minimum is obtained when $k\\_{1} \\cdot k\\_{2}=\\sqrt{H W}$. Note that $H=W=224$ is popular in classification. Without loss of generality, square sub-windows are used, i.e., $k\\_{1}=k\\_{2}$. Therefore, $k\\_{1}=k\\_{2}=15$ is close to the global minimum for $H=W=224$. However, the network is designed to include several stages with variable resolutions. Stage 1 has feature maps of $56 \\times 56$, the minimum is obtained when $k\\_{1}=k\\_{2}=\\sqrt{56} \\approx 7$. Theoretically, we can calibrate optimal $k\\_{1}$ and $k\\_{2}$ for each of the stages. For simplicity, $k\\_{1}=k\\_{2}=7$ is used everywhere. As for stages with lower resolutions, the summarizing window-size of GSA is controlled to avoid too small amount of generated keys. Specifically, the sizes of 4,2 and 1 are used for the last three stages respectively.""" ;
    skos:prefLabel "Global Sub-Sampled Attention" .

:GlobalandSlidingWindowAttention a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2004.05150v2> ;
    rdfs:seeAlso <https://github.com/allenai/longformer/blob/da3902983f548a466a6e99988bcdeb333a644c84/longformer/longformer.py#L54> ;
    skos:definition """**Global and Sliding Window Attention** is an attention pattern for attention-based models. It is motivated by the fact that non-sparse attention in the original [Transformer](https://paperswithcode.com/method/transformer) formulation has a [self-attention component](https://paperswithcode.com/method/scaled) with $O\\left(n^{2}\\right)$ time and memory complexity where $n$ is the input sequence length and thus, is not efficient to scale to long inputs. \r
\r
Since [windowed](https://paperswithcode.com/method/sliding-window-attention) and [dilated](https://paperswithcode.com/method/dilated-sliding-window-attention) attention patterns are not flexible enough to learn task-specific representations, the authors of the [Longformer](https://paperswithcode.com/method/longformer) add “global attention” on few pre-selected input locations. This attention is operation symmetric: that is, a token with a global attention attends to all tokens across the sequence, and all tokens in the sequence attend to it. The Figure to the right shows an example of a sliding window attention with global attention at a few tokens at custom locations. For the example of classification, global attention is used for the [CLS] token, while in the example of Question Answering, global attention is provided on all question tokens.""" ;
    skos:prefLabel "Global and Sliding Window Attention" .

:Glow-TTS a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2005.11129v1> ;
    skos:definition "**Glow-TTS** is a flow-based generative model for parallel TTS that does not require any external aligner. By combining the properties of flows and dynamic programming, the proposed model searches for the most probable monotonic alignment between text and the latent representation of speech.  The model is directly trained to maximize the log-likelihood of speech with the alignment. Enforcing hard monotonic alignments helps enable robust TTS, which generalizes to long utterances, and employing flows enables fast, diverse, and controllable speech synthesis." ;
    skos:prefLabel "Glow-TTS" .

:Go-Explore a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1901.10995v4> ;
    skos:definition """**Go-Explore** is a family of algorithms aiming to tackle two challenges with effective exploration in reinforcement learning: algorithms forgetting how to reach previously visited states ("detachment") and from failing to first return to a state before exploring from it ("derailment").\r
\r
To avoid detachment, Go-Explore builds an archive of the different states it has visited in the environment, thus ensuring that states cannot be forgotten. Starting with an archive beginning with the initial state, the archive is built iteratively. In Go-Explore we:\r
\r
(a) Probabilistically select a state from the archive, preferring states associated with promising cells. \r
\r
(b) Return to the selected state, such as by restoring simulator state or by running a goal-conditioned policy. \r
\r
(c) Explore from that state by taking random actions or sampling from a trained policy. \r
\r
(d) Map every state encountered during returning and exploring to a low-dimensional cell representation. \r
\r
(e) Add states that map to new cells to the archive and update other archive entries.""" ;
    skos:prefLabel "Go-Explore" .

:GoodFeatureMatching a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2001.00714v1> ;
    skos:definition "**Good Feature Matching** is an active map-to-frame feature matching method. Feature matching effort is tied to submatrix selection, which has combinatorial time complexity and requires choosing a scoring metric. Via simulation, the Max-logDet matrix revealing metric is shown to perform best." ;
    skos:prefLabel "Good Feature Matching" .

:GoogLeNet a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1409.4842v1> ;
    rdfs:seeAlso <https://github.com/pytorch/vision/blob/6db1569c89094cf23f3bc41f79275c45e9fcb3f3/torchvision/models/googlenet.py#L62> ;
    skos:definition "**GoogLeNet** is a type of convolutional neural network based on the [Inception](https://paperswithcode.com/method/inception-module) architecture. It utilises Inception modules, which allow the network to choose between multiple convolutional filter sizes in each block. An Inception network stacks these modules on top of each other, with occasional max-pooling layers with stride 2 to halve the resolution of the grid." ;
    skos:prefLabel "GoogLeNet" .

:GraRep a skos:Concept ;
    dcterms:source <https://www.researchgate.net/publication/301417811_GraRep> ;
    skos:altLabel "Graph Representation with Global structure" ;
    skos:definition "" ;
    skos:prefLabel "GraRep" .

:Grab a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2001.01033v1> ;
    skos:definition "**Grab** is a sensor processing system for cashier-free shopping. Grab needs to accurately identify and track customers, and associate each shopper with items he or she retrieves from shelves. To do this, it uses a keypoint-based pose tracker as a building block for identification and tracking, develops robust feature-based face trackers, and algorithms for associating and tracking arm movements. It also uses a probabilistic framework to fuse readings from camera, weight and RFID sensors in order to accurately assess which shopper picks up which item." ;
    skos:prefLabel "Grab" .

:GradDrop a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2010.06808v1> ;
    skos:altLabel "Gradient Sign Dropout" ;
    skos:definition """**GradDrop**, or **Gradient Sign Dropout**, is a probabilistic masking procedure which samples gradients at an activation layer based on their level of consistency. It is applied as a layer in any standard network forward pass, usually on the final layer before the prediction head to save on compute overhead and maximize benefits during backpropagation. Below, we develop the GradDrop formalism. Throughout, o denotes elementwise multiplication after any necessary tiling operations (if any) are completed.\r
To implement GradDrop, we first define the Gradient Positive Sign Purity, $\\mathcal{P}$, as\r
\r
$$\r
\\mathcal{P}=\\frac{1}{2}\\left(1+\\frac{\\sum\\_{i} \\nabla L_\\{i}}{\\sum\\_{i}\\left|\\nabla L\\_{i}\\right|}\\right)\r
$$\r
\r
$\\mathcal{P}$ is bounded by $[0,1] .$ For multiple gradient values $\\nabla\\_{a} L\\_{i}$ at some scalar $a$, we see that $\\mathcal{P}=0$ if $\\nabla_{a} L\\_{i}<0 $ $\\forall i$, while $\\mathcal{P}=1$ if $\\nabla\\_{a} L\\_{i}>0$ $\\forall i $. Thus, $\\mathcal{P}$ is a measure of how many positive gradients are present at any given value. We then form a mask for each gradient $\\mathcal{M}\\_{i}$ as follows:\r
\r
$$\r
\\mathcal{M}\\_{i}=\\mathcal{I}[f(\\mathcal{P})>U] \\circ \\mathcal{I}\\left[\\nabla L\\_{i}>0\\right]+\\mathcal{I}[f(\\mathcal{P})<U] \\circ \\mathcal{I}\\left[\\nabla L\\_{i}<0\\right]\r
$$\r
\r
for $\\mathcal{I}$ the standard indicator function and $f$ some monotonically increasing function (often just the identity) that maps $[0,1] \\mapsto[0,1]$ and is odd around $(0.5,0.5)$. $U$ is a tensor composed of i.i.d $U(0,1)$ random variables. The $\\mathcal{M}\\_{i}$ is then used to produce a final gradient $\\sum \\mathcal{M}\\_{i} \\nabla L\\_{i}$""" ;
    skos:prefLabel "GradDrop" .

:Gradient-BasedSubwordTokenization a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2106.12672v3> ;
    rdfs:seeAlso <https://github.com/google-research/google-research/blob/master/charformer/lib/charformer_layers.py> ;
    skos:altLabel "GBST" ;
    skos:definition """**GBST**, or **Gradient-based Subword Tokenization Module**, is a soft gradient-based subword tokenization module that automatically learns latent subword representations from characters in a data-driven fashion. Concretely, GBST enumerates candidate subword blocks and learns to score them in a position-wise fashion using a block scoring network.  \r
\r
GBST learns a position-wise soft selection over candidate subword blocks by scoring them with a scoring network. In contrast to prior tokenization-free methods, GBST learns interpretable latent subwords, which enables easy inspection of lexical representations and is more efficient than other byte-based models.""" ;
    skos:prefLabel "Gradient-Based Subword Tokenization" .

:GradientCheckpointing a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1604.06174v2> ;
    skos:definition "**Gradient Checkpointing** is a method used for reducing the memory footprint when training deep neural networks, at the cost of having a small increase in computation time." ;
    skos:prefLabel "Gradient Checkpointing" .

:GradientClipping a skos:Concept ;
    skos:definition """One difficulty that arises with optimization of deep neural networks is that large parameter gradients can lead an [SGD](https://paperswithcode.com/method/sgd) optimizer to update the parameters strongly into a region where the loss function is much greater, effectively undoing much of the work that was needed to get to the current solution.\r
\r
**Gradient Clipping** clips the size of the gradients to ensure optimization performs more reasonably near sharp areas of the loss surface. It can be performed in a number of ways. One option is to simply clip the parameter gradient element-wise before a parameter update. Another option is to clip the norm ||$\\textbf{g}$|| of the gradient $\\textbf{g}$ before a parameter update:\r
\r
$$\\text{ if } ||\\textbf{g}||  > v \\text{ then } \\textbf{g} \\leftarrow \\frac{\\textbf{g}{v}}{||\\textbf{g}||}$$\r
\r
where $v$ is a norm threshold.\r
\r
Source: Deep Learning, Goodfellow et al\r
\r
Image Source: [Pascanu et al](https://arxiv.org/pdf/1211.5063.pdf)""" ;
    skos:prefLabel "Gradient Clipping" .

:GradientDICE a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2001.11113v7> ;
    skos:definition "**GradientDICE** is a density ratio learning method for estimating the density ratio between the state distribution of the target policy and the sampling distribution in off-policy reinforcement learning. It optimizes a different objective from [GenDICE](https://arxiv.org/abs/2002.09072) by using the Perron-Frobenius theorem and eliminating GenDICE’s use of divergence, such that nonlinearity in parameterization is not necessary for GradientDICE, which is provably convergent under linear function approximation." ;
    skos:prefLabel "GradientDICE" .

:GradientNormalization a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2109.02235v2> ;
    skos:definition "**Gradient Normalization** is a normalization method for [Generative Adversarial Networks](https://paperswithcode.com/methods/category/generative-adversarial-networks) to tackle the training instability of generative adversarial networks caused by the sharp gradient space. Unlike existing work such as [gradient penalty](https://paperswithcode.com/method/wgan-gp-loss) and [spectral normalization](https://paperswithcode.com/method/spectral-normalization), the proposed GN only imposes a hard 1-Lipschitz constraint on the discriminator function, which increases the capacity of the network." ;
    skos:prefLabel "Gradient Normalization" .

:GradientSparsification a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1710.09854v1> ;
    skos:definition "**Gradient Sparsification** is a technique for distributed training that sparsifies stochastic gradients to reduce the communication cost, with minor increase in the number of iterations. The key idea behind our sparsification technique is to drop some coordinates of the stochastic gradient and appropriately amplify the remaining coordinates to ensure the unbiasedness of the sparsified stochastic gradient. The sparsification approach can significantly reduce the coding length of the stochastic gradient and only slightly increase the variance of the stochastic gradient." ;
    skos:prefLabel "Gradient Sparsification" .

:GradualSelf-Training a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2002.11361v1> ;
    skos:definition """Gradual self-training is a method for semi-supervised domain adaptation. The goal is to adapt an initial classifier trained on a source domain given only unlabeled data that shifts gradually in distribution towards a target domain. \r
\r
This comes up for example in applications ranging from sensor networks and self-driving car perception modules to brain-machine interfaces, where machine learning systems must adapt to data distributions that evolve over time.\r
\r
The gradual self-training algorithm begins with a classifier $w_0$ trained on labeled examples from the source domain (Figure a). For each successive domain $P_t$, the algorithm generates pseudolabels for unlabeled examples from that domain, and then trains a regularized supervised classifier on the pseudolabeled examples. The intuition, visualized in the Figure, is that after a single gradual shift, most examples are pseudolabeled correctly so self-training learns a good classifier on the shifted data, but the shift from the source to the target can be too large for self-training to correct.""" ;
    skos:prefLabel "Gradual Self-Training" .

<http://w3id.org/mlso/vocab/ml_algorithm/Grammaticalevolution+Q-learning> a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2012.07723v3> ;
    skos:altLabel "Grammatical evolution and Q-learning" ;
    skos:definition """This method works as a two-levels optimization algorithm.\r
The outmost layer uses Grammatical evolution to evolve a grammar to build the agent.\r
Then, [Q-learning](https://paperswithcode.com/method/q-learning) is used the fitness evaluation phase to allow the agent to learn to perform online learning.""" ;
    skos:prefLabel "Grammatical evolution + Q-learning" .

:Graph2Tree a skos:Concept ;
    dcterms:source <https://aclanthology.org/2020.acl-main.362> ;
    skos:altLabel "Graph-to-Tree MWP Solver" ;
    skos:definition "" ;
    skos:prefLabel "Graph2Tree" .

:GraphContrastiveCoding a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2006.09963v3> ;
    skos:definition "**Graph Contrastive Coding** is a self-supervised graph neural network pre-training framework to capture the universal network topological properties across multiple networks. GCC's pre-training task is designed as subgraph instance discrimination in and across networks and leverages contrastive learning to empower graph neural networks to learn the intrinsic and transferable structural representations." ;
    skos:prefLabel "Graph Contrastive Coding" .

:GraphESN a skos:Concept ;
    skos:altLabel "Graph Echo State Network" ;
    skos:definition """**Graph Echo State Network** (**GraphESN**) model is a generalization of the Echo State Network (ESN) approach to graph domains. GraphESNs allow for an efficient approach to Recursive Neural Networks (RecNNs) modeling extended to deal with cyclic/acyclic, directed/undirected, labeled graphs. The recurrent reservoir of the network computes a fixed contractive encoding function over graphs and is left untrained after initialization, while a feed-forward readout implements an adaptive linear output function. Contractivity of the state transition function implies a Markovian characterization of state dynamics and stability of the state computation in presence of cycles. Due to the use of fixed (untrained) encoding, the model represents both an extremely efficient version and a baseline for the performance of recursive models with trained connections.\r
\r
Description from: [Graph Echo State Networks](https://ieeexplore.ieee.org/document/5596796)""" ;
    skos:prefLabel "GraphESN" .

:GraphSAGE a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1706.02216v4> ;
    skos:definition """GraphSAGE is a general inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data.\r
\r
Image from: [Inductive Representation Learning on Large Graphs](https://arxiv.org/pdf/1706.02216v4.pdf)""" ;
    skos:prefLabel "GraphSAGE" .

:GraphSAINT a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1907.04931v4> ;
    skos:altLabel "Graph sampling based inductive learning method" ;
    skos:definition "Scalable method to train large scale GNN models via sampling small subgraphs." ;
    skos:prefLabel "GraphSAINT" .

:GraphSelf-Attention a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1911.04070v1> ;
    rdfs:seeAlso <https://github.com/yzh119/BPT/blob/fa87c1c993eac4e4ad9997a60fc0052dcfc075da/modules/attention.py#L22> ;
    skos:definition """**Graph Self-Attention (GSA)** is a self-attention module used in the [BP-Transformer](https://paperswithcode.com/method/bp-transformer) architecture, and is based on the [graph attentional layer](https://paperswithcode.com/method/graph-attentional-layer).\r
\r
For a given node $u$, we update its representation according to its neighbour nodes, formulated as $\\mathbf{h}\\_{u} \\leftarrow \\text{GSA}\\left(\\mathcal{G}, \\mathbf{h}^{u}\\right)$.\r
\r
Let $\\mathbf{A}\\left(u\\right)$ denote the set of the neighbour nodes of $u$ in $\\mathcal{G}$, $\\text{GSA}\\left(\\mathcal{G}, \\mathbf{h}^{u}\\right)$ is detailed as follows:\r
\r
$$ \\mathbf{A}^{u} = \\text{concat}\\left(\\{\\mathbf{h}\\_{v} | v \\in \\mathcal{A}\\left(u\\right)\\}\\right) $$\r
\r
$$ \\mathbf{Q}^{u}\\_{i} = \\mathbf{H}\\_{k}\\mathbf{W}^{Q}\\_{i},\\mathbf{K}\\_{i}^{u} = \\mathbf{A}^{u}\\mathbf{W}^{K}\\_{i},\\mathbf{V}^{u}\\_{i} = \\mathbf{A}^{u}\\mathbf{W}\\_{i}^{V} $$\r
\r
$$ \\text{head}^{u}\\_{i} = \\text{softmax}\\left(\\frac{\\mathbf{Q}^{u}\\_{i}\\mathbf{K}\\_{i}^{uT}}{\\sqrt{d}}\\right)\\mathbf{V}\\_{i}^{u} $$\r
\r
$$ \\text{GSA}\\left(\\mathcal{G}, \\mathbf{h}^{u}\\right) = \\left[\\text{head}^{u}\\_{1}, \\dots, \\text{head}^{u}\\_{h}\\right]\\mathbf{W}^{O}$$\r
\r
where d is the dimension of h, and $\\mathbf{W}^{Q}\\_{i}$, $\\mathbf{W}^{K}\\_{i}$ and $\\mathbf{W}^{V}\\_{i}$ are trainable parameters of the $i$-th attention head.""" ;
    skos:prefLabel "Graph Self-Attention" .

:GraphTransformer a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2012.09699v2> ;
    rdfs:seeAlso <https://github.com/graphdeeplearning/graphtransformer> ;
    skos:definition """This is **Graph Transformer** method, proposed as a generalization of [Transformer](https://paperswithcode.com/method/transformer) Neural Network architectures, for arbitrary graphs.\r
\r
Compared to the original Transformer, the highlights of the presented architecture are:\r
\r
- The attention mechanism is a function of neighborhood connectivity for each node in the graph.  \r
- The position encoding is represented by Laplacian eigenvectors, which naturally generalize the sinusoidal positional encodings often used in NLP.  \r
- The [layer normalization](https://paperswithcode.com/method/layer-normalization) is replaced by a [batch normalization](https://paperswithcode.com/method/batch-normalization) layer.  \r
- The architecture is extended to have edge representation, which can be critical to tasks with rich information on the edges, or pairwise interactions (such as bond types in molecules, or relationship type in KGs. etc).""" ;
    skos:prefLabel "Graph Transformer" .

:Gravity a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2101.09192v1> ;
    skos:definition "Gravity is a kinematic approach to optimization based on gradients." ;
    skos:prefLabel "Gravity" .

:GreedyNAS a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2003.11236v1> ;
    skos:definition """**GreedyNAS** is a one-shot [neural architecture search](https://paperswithcode.com/method/neural-architecture-search) method. Previous methods held the assumption that a supernet should give a reasonable ranking over all paths. They thus treat all paths equally, and spare much effort to train paths. However, it is harsh for a single supernet to evaluate accurately on such a huge-scale search space (eg, $7^{21}$). GreedyNAS eases the burden of supernet by encouraging focus more on evaluation of potentially-good candidates, which are identified using a surrogate portion of validation data. \r
\r
Concretely, during training, GreedyNAS utilizes a multi-path sampling strategy with rejection, and greedily filters the weak paths. The training efficiency is thus boosted since the training space has been greedily shrunk from all paths to those potentially-good ones. An exploration and exploitation policy is adopted by introducing an empirical candidate path pool.""" ;
    skos:prefLabel "GreedyNAS" .

:GreedyNAS-A a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2003.11236v1> ;
    skos:definition "**GreedyNAS-A** is a convolutional neural network discovered using the [GreedyNAS](https://paperswithcode.com/method/greedynas) [neural architecture search](https://paperswithcode.com/method/neural-architecture-search) method. The basic building blocks used are inverted residual blocks (from [MobileNetV2](https://paperswithcode.com/method/mobilenetv2)) and squeeze-and-excitation blocks." ;
    skos:prefLabel "GreedyNAS-A" .

:GreedyNAS-B a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2003.11236v1> ;
    skos:definition "**GreedyNAS-B** is a convolutional neural network discovered using the [GreedyNAS](https://paperswithcode.com/method/greedynas) [neural architecture search](https://paperswithcode.com/method/neural-architecture-search) method. The basic building blocks used are inverted residual blocks (from [MobileNetV2](https://paperswithcode.com/method/mobilenetv2)) and squeeze-and-excitation blocks." ;
    skos:prefLabel "GreedyNAS-B" .

:GreedyNAS-C a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2003.11236v1> ;
    skos:definition "**GreedyNAS-C** is a convolutional neural network discovered using the [GreedyNAS](https://paperswithcode.com/method/greedynas) [neural architecture search](https://paperswithcode.com/method/neural-architecture-search) method. The basic building blocks used are inverted residual blocks (from [MobileNetV2](https://paperswithcode.com/method/mobilenetv2)) and squeeze-and-excitation blocks." ;
    skos:prefLabel "GreedyNAS-C" .

:GridMask a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2001.04086v2> ;
    skos:definition """**GridMask** is a data augmentation method that randomly removes some pixels of an input image. Unlike other methods, the region that the algorithm removes is neither a continuous region nor random pixels in dropout. Instead, the algorithm removes a region with disconnected pixel sets, as shown in the Figure.\r
\r
We express the setting as\r
\r
$$\r
\\tilde{\\mathbf{x}}=\\mathbf{x} \\times M\r
$$\r
\r
where $\\mathbf{x} \\in R^{H \\times W \\times C}$ represents the input image, $M \\in$ $\\{0,1\\}^{H \\times W}$ is the binary mask that stores pixels to be removed, and $\\tilde{\\mathbf{x}} \\in R^{H \\times W \\times C}$ is the result produced by the algorithm. For the binary mask $M$, if $M_{i, j}=1$ we keep pixel $(i, j)$ in the input image; otherwise we remove it. GridMask is applied after the image normalization operation.\r
\r
The shape of $M$ looks like a grid, as shown in the Figure . Four numbers $\\left(r, d, \\delta_{x}, \\delta_{y}\\right)$ are used to represent a unique $M$. Every mask is formed by tiling the units. $r$ is the ratio of the shorter gray edge in a unit. $d$ is the length of one unit. $\\delta\\_{x}$ and $\\delta\\_{y}$ are the distances between the first intact unit and boundary of the image.""" ;
    skos:prefLabel "GridMask" .

:GridR-CNN a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1811.12030v1> ;
    rdfs:seeAlso <https://github.com/STVIR/Grid-R-CNN> ;
    skos:definition """**Grid R-CNN** is an object detection framework, where the traditional regression\r
formulation is replaced by a grid point guided localization mechanism.\r
\r
Grid R-CNN divides the object bounding box region into grids and employs a fully convolutional network ([FCN](https://paperswithcode.com/method/fcn)) to predict the locations of grid points. Owing to the position sensitive property of fully convolutional architecture, Grid R-CNN maintains the explicit spatial information and grid points locations can be obtained in pixel level. When a certain number of grid points at specified location are known, the corresponding bounding box is definitely determined. Guided by the grid points, Grid R-CNN can determine more accurate object bounding box than regression method which lacks the guidance of explicit spatial information.""" ;
    skos:prefLabel "Grid R-CNN" .

:GridSensitive a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2004.10934v1> ;
    skos:definition """**Grid Sensitive** is a trick for object detection introduced by [YOLOv4](https://paperswithcode.com/method/yolov4). When we decode the coordinate of the bounding box center $x$ and $y$, in original [YOLOv3](https://paperswithcode.com/method/yolov3), we can get them by\r
\r
$$\r
\\begin{aligned}\r
&x=s \\cdot\\left(g\\_{x}+\\sigma\\left(p\\_{x}\\right)\\right) \\\\\r
&y=s \\cdot\\left(g\\_{y}+\\sigma\\left(p\\_{y}\\right)\\right)\r
\\end{aligned}\r
$$\r
\r
where $\\sigma$ is the sigmoid function, $g\\_{x}$ and $g\\_{y}$ are integers and $s$ is a scale factor. Obviously, $x$ and $y$ cannot be exactly equal to $s \\cdot g\\_{x}$ or $s \\cdot\\left(g\\_{x}+1\\right)$. This makes it difficult to predict the centres of bounding boxes that just located on the grid boundary. We can address this problem, by changing the equation to\r
\r
$$\r
\\begin{aligned}\r
&x=s \\cdot\\left(g\\_{x}+\\alpha \\cdot \\sigma\\left(p\\_{x}\\right)-(\\alpha-1) / 2\\right) \\\\\r
&y=s \\cdot\\left(g\\_{y}+\\alpha \\cdot \\sigma\\left(p\\_{y}\\right)-(\\alpha-1) / 2\\right)\r
\\end{aligned}\r
$$\r
\r
This makes it easier for the model to predict bounding box center exactly located on the grid boundary. The FLOPs added by Grid Sensitive are really small, and can be totally ignored.""" ;
    skos:prefLabel "Grid Sensitive" .

:Griffin-LimAlgorithm a skos:Concept ;
    skos:definition """The **Griffin-Lim Algorithm (GLA)** is a phase reconstruction method based on the redundancy of the short-time Fourier transform. It promotes the consistency of a spectrogram by iterating two projections, where a spectrogram is said to be consistent when its inter-bin dependency owing to the redundancy of STFT is retained.  GLA is based only on the consistency and does not take any prior knowledge about the target signal into account. \r
\r
This algorithm expects to recover a complex-valued spectrogram, which is consistent and maintains the given amplitude $\\mathbf{A}$, by the following alternative projection procedure:\r
\r
$$ \\mathbf{X}^{[m+1]} = P\\_{\\mathcal{C}}\\left(P\\_{\\mathcal{A}}\\left(\\mathbf{X}^{[m]}\\right)\\right) $$\r
\r
where $\\mathbf{X}$ is a complex-valued spectrogram updated through the iteration, $P\\_{\\mathcal{S}}$ is the metric projection onto a set $\\mathcal{S}$, and $m$ is the iteration index. Here, $\\mathcal{C}$ is the set of consistent spectrograms, and $\\mathcal{A}$ is the set of spectrograms whose amplitude is the same as the given one. The metric projections onto these sets $\\mathcal{C}$ and $\\mathcal{A}$ are given by:\r
\r
$$ P\\_{\\mathcal{C}}(\\mathbf{X}) = \\mathcal{GG}^{†}\\mathbf{X} $$\r
$$ P\\_{\\mathcal{A}}(\\mathbf{X}) = \\mathbf{A} \\odot \\mathbf{X} \\oslash |\\mathbf{X}| $$\r
\r
\r
where $\\mathcal{G}$ represents STFT, $\\mathcal{G}^{†}$ is the pseudo inverse of STFT (iSTFT), $\\odot$ and $\\oslash$ are element-wise multiplication and division, respectively, and division by zero is replaced by zero. GLA is obtained as an algorithm for the following optimization problem:\r
\r
$$ \\min\\_{\\mathbf{X}} || \\mathbf{X} - P\\_{\\mathcal{C}}\\left(\\mathbf{X}\\right) ||^{2}\\_{\\text{Fro}} \\text{ s.t. } \\mathbf{X} \\in \\mathcal{A} $$\r
\r
where $ || · ||\\_{\\text{Fro}}$ is the Frobenius norm. This equation minimizes the energy of the inconsistent components under the constraint on amplitude which must be equal to the given one. Although GLA has been widely utilized because of its simplicity, GLA often involves many iterations until it converges to a certain spectrogram and results in low reconstruction quality. This is because the cost function only requires the consistency, and the characteristics of the target signal are not taken into account.""" ;
    skos:prefLabel "Griffin-Lim Algorithm" .

:GroupDNet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2003.12697v3> ;
    skos:altLabel "Group Decreasing Network" ;
    skos:definition """**Group Decreasing Network**, or **GroupDNet**, is a type of convolutional neural network for multi-modal image synthesis. GroupDNet contains one encoder and one decoder. Inspired by the idea of [VAE](https://paperswithcode.com/method/vae) and SPADE, the encoder $E$ produces a\r
latent code $Z$ that is supposed to follow a Gaussian distribution $\\mathcal{N}(0,1)$ during training. While testing, the encoder $E$ is discarded. A randomly sampled code from the Gaussian distribution substitutes for $Z$. To fulfill this, the re-parameterization trick is used to enable a differentiable loss function during training. Specifically, the encoder predicts a mean vector and a variance vector through two fully connected layers to represent the encoded distribution. The gap between the encoded distribution and Gaussian distribution can be minimized by imposing a KL-divergence loss.""" ;
    skos:prefLabel "GroupDNet" .

:GroupNormalization a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1803.08494v3> ;
    rdfs:seeAlso <https://github.com/pytorch/pytorch/blob/1c5c289b6218eb1026dcb5fd9738231401cfccea/torch/nn/modules/normalization.py#L177> ;
    skos:definition """**Group Normalization** is a normalization layer that divides channels into groups and normalizes the features within each group. GN does not exploit the batch dimension, and its computation is independent of batch sizes. In the case where the group size is 1, it is equivalent to [Instance Normalization](https://paperswithcode.com/method/instance-normalization).\r
\r
As motivation for the method, many classical features like SIFT and HOG had *group-wise* features and involved *group-wise normalization*. For example, a HOG vector is the outcome of several spatial cells where each cell is represented by a normalized orientation histogram.\r
\r
Formally, Group Normalization is defined as:\r
\r
$$ \\mu\\_{i} = \\frac{1}{m}\\sum\\_{k\\in\\mathcal{S}\\_{i}}x\\_{k} $$\r
\r
$$ \\sigma^{2}\\_{i} = \\frac{1}{m}\\sum\\_{k\\in\\mathcal{S}\\_{i}}\\left(x\\_{k}-\\mu\\_{i}\\right)^{2} $$\r
\r
$$ \\hat{x}\\_{i} = \\frac{x\\_{i} - \\mu\\_{i}}{\\sqrt{\\sigma^{2}\\_{i}+\\epsilon}} $$\r
\r
Here $x$ is the feature computed by a layer, and $i$ is an index. Formally, a Group Norm layer computes $\\mu$ and $\\sigma$ in a set $\\mathcal{S}\\_{i}$ defined as: $\\mathcal{S}\\_{i} = ${$k \\mid k\\_{N} = i\\_{N} ,\\lfloor\\frac{k\\_{C}}{C/G}\\rfloor = \\lfloor\\frac{I\\_{C}}{C/G}\\rfloor $}.\r
\r
Here $G$ is the number of groups, which is a pre-defined hyper-parameter ($G = 32$ by default). $C/G$ is the number of channels per group. $\\lfloor$ is the floor operation, and the final term means that the indexes $i$ and $k$ are in the same group of channels, assuming each group of channels are stored in a sequential order along the $C$ axis.""" ;
    skos:prefLabel "Group Normalization" .

:Grouped-queryattention a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2305.13245v1> ;
    skos:definition "**Grouped-query attention** an interpolation of multi-query and multi-head attention that achieves quality close to multi-head at comparable speed to multi-query attention." ;
    skos:prefLabel "Grouped-query attention" .

:GroupedConvolution a skos:Concept ;
    dcterms:source <http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks> ;
    rdfs:seeAlso <https://github.com/prlz77/ResNeXt.pytorch/blob/39fb8d03847f26ec02fb9b880ecaaa88db7a7d16/models/model.py#L42> ;
    skos:definition "A **Grouped Convolution** uses a group of convolutions - multiple kernels per layer - resulting in multiple channel outputs per layer. This leads to wider networks helping a network learn a varied set of low level and high level features. The original motivation of using Grouped Convolutions in [AlexNet](https://paperswithcode.com/method/alexnet) was to distribute the model over multiple GPUs as an engineering compromise. But later, with models such as [ResNeXt](https://paperswithcode.com/method/resnext), it was shown this module could be used to improve classification accuracy. Specifically by exposing a new dimension through grouped convolutions, *cardinality* (the size of set of transformations), we can increase accuracy by increasing it." ;
    skos:prefLabel "Grouped Convolution" .

:GroupwisePointConvolution a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1811.11431v3> ;
    rdfs:seeAlso <https://github.com/sacmehta/ESPNetv2/blob/5463915da1f67d77eca9e3b00020ebc7997ba5a4/imagenet/Model.py#L75> ;
    skos:definition """A **Groupwise Point Convolution** is a type of [convolution](https://paperswithcode.com/method/convolution) where we apply a [point convolution](https://paperswithcode.com/method/pointwise-convolution) groupwise (using different set of convolution filter groups).\r
\r
Image Credit: [Chi-Feng Wang](https://towardsdatascience.com/a-basic-introduction-to-separable-convolutions-b99ec3102728)""" ;
    skos:prefLabel "Groupwise Point Convolution" .

:GrowNet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2002.07971v2> ;
    skos:definition "**GrowNet** is a novel approach to combine the power of gradient boosting to incrementally build complex deep neural networks out of shallow components. It introduces a versatile framework that can readily be adapted for a diverse range of machine learning tasks in a wide variety of domains." ;
    skos:prefLabel "GrowNet" .

:GuidedAnchoring a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1901.03278v2> ;
    skos:definition "**Guided Anchoring** is an anchoring scheme for object detection which leverages semantic features to guide the anchoring. The method is motivated by the observation that objects are not distributed evenly over the image. The scale of an object is also closely related to the imagery content, its location and geometry of the scene. Following this intuition, the method generates sparse anchors in two steps: first identifying sub-regions that may contain objects and then determining the shapes at different locations." ;
    skos:prefLabel "Guided Anchoring" .

:GumbelActivation a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2207.10936v2> ;
    skos:altLabel "Gumbel Cross Entropy" ;
    skos:definition """Gumbel activation function, is defined using the cumulative Gumbel distribution and it can be used to perform Gumbel regression. Gumbel activation is an alternative activation function to the sigmoid or softmax activation functions and can be used to transform the unormalised output of a model to probability. Gumbel activation  $\\eta_{Gumbel}$ is defined as follows:\r
\r
$\\eta_{Gumbel}(q_i) = exp(-exp(-q_i))$\r
\r
It can be combined with Cross Entropy loss function to solve long-tailed classification problems. Gumbel Cross Entropy (GCE) is defined as follows:\r
\r
$GCE(\\eta_{Gumbel}(q_i),y_i) = -y_i \\log(\\eta_{Gumbel}(q_i))+ (1-y_i) \\log(1-\\eta_{Gumbel}(q_i))$""" ;
    skos:prefLabel "Gumbel Activation" .

:GumbelSoftmax a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1611.01144v5> ;
    rdfs:seeAlso <https://github.com/ericjang/gumbel-softmax/blob/master/gumbel_softmax_vae_v2.ipynb> ;
    skos:definition "**Gumbel-Softmax** is a continuous distribution that has the property that it can be smoothly annealed into a categorical distribution, and whose parameter gradients can be easily computed via the reparameterization trick." ;
    skos:prefLabel "Gumbel Softmax" .

:H-BEMD a skos:Concept ;
    dcterms:source <https://ieeexplore.ieee.org/document/9159123> ;
    skos:altLabel "Hue — Bi-Dimensional Empirical Mode Decomposition" ;
    skos:definition "" ;
    skos:prefLabel "H-BEMD" .

:H3DNet a skos:Concept ;
    rdfs:seeAlso <https://github.com/open-mmlab/mmdetection3d/> ;
    skos:definition "Code for paper: H3DNet: 3D Object Detection Using Hybrid Geometric Primitives (ECCV 2020)" ;
    skos:prefLabel "H3DNet" .

:HANet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2003.05128v3> ;
    skos:altLabel "Height-driven Attention Network" ;
    skos:definition "**Height-driven Attention Network**, or **HANet**, is a general add-on module for improving semantic segmentation for urban-scene images. It emphasizes informative features or classes selectively according to the vertical position of a pixel. The pixel-wise class distributions are significantly different from each other among horizontally segmented sections in the urban-scene images. Likewise, urban-scene images have their own distinct characteristics, but most semantic segmentation networks do not reflect such unique attributes in the architecture. The proposed network architecture incorporates the capability exploiting the attributes to handle the urban scene dataset effectively." ;
    skos:prefLabel "HANet" .

:HAPPIER a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2207.04873v2> ;
    skos:altLabel "Hierarchical Average Precision training for Pertinent ImagE Retrieval" ;
    skos:definition "" ;
    skos:prefLabel "HAPPIER" .

:HBMP a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1808.08762v2> ;
    skos:altLabel "Hierarchical BiLSTM Max Pooling" ;
    skos:definition "HBMP is a hierarchy-like structure of [BiLSTM](https://paperswithcode.com/method/bilstm) layers with [max pooling](https://paperswithcode.com/method/max-pooling). All in all, this model improves the previous state of the art for SciTail and achieves strong results for the SNLI and MultiNLI." ;
    skos:prefLabel "HBMP" .

:HDCGAN a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1711.06491v12> ;
    rdfs:seeAlso <https://github.com/curto2/c> ;
    skos:altLabel "High-resolution Deep Convolutional Generative Adversarial Networks" ;
    skos:definition """**HDCGAN**, or **High-resolution Deep Convolutional Generative Adversarial Networks**, is a [DCGAN](https://paperswithcode.com/method/dcgan) based architecture that achieves high-resolution image generation through the proper use of [SELU](https://paperswithcode.com/method/selu) activations. Glasses, a mechanism to arbitrarily improve the final [GAN](https://paperswithcode.com/method/gan) generated results by enlarging the input size by a telescope ζ is also set forth. \r
\r
A video showing the training procedure on CelebA-hq can be found [here](https://youtu.be/1XZB87W0SaY).""" ;
    skos:prefLabel "HDCGAN" .

:HEGCN a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2108.09505v1> ;
    skos:altLabel "Hierarchical Entity Graph Convolutional Network" ;
    skos:definition """**HEGCN**, or **Hierarchical Entity Graph Convolutional Network** is a model for multi-hop relation extraction across documents. Documents in a document chain are encoded using a bi-directional long short-term memory ([BiLSTM](https://paperswithcode.com/method/bilstm)) layer. On top of the BiLSTM layer, two graph convolutional networks ([GCN](https://paperswithcode.com/method/gcn)) are used, one after another in a hierarchy. \r
\r
In the first level of the GCN hierarchy, a separate entity mention graph is constructed on each document of the chain using all the entities mentioned in that document. Each mention of an entity in a document is considered as a separate node in the graph. A graph convolutional network (GCN) is used to represent the entity mention graph of each document to capture the relations among the entity mentions in the document. A unified entity-level graph is then constructed across all the documents in the chain. Each node of this entity-level graph represents a unique entity in the document chain. Each common entity between two documents in the chain is represented by a single node in the graph. A GCN is used to represent this entity-level graph to capture the relations among the entities across the documents. \r
\r
The representations of the nodes of the subject entity and object entity are concatenated and passed to a feed-forward layer with [softmax](https://paperswithcode.com/method/softmax) for relation classification.""" ;
    skos:prefLabel "HEGCN" .

:HFPSO a skos:Concept ;
    rdfs:seeAlso <https://www.mathworks.com/matlabcentral/fileexchange/67768-a-hybrid-firefly-and-particle-swarm-optimization-hfpso> ;
    skos:altLabel "Hybrid Firefly and Particle Swarm Optimization" ;
    skos:definition """**Hybrid Firefly and Particle Swarm Optimization (HFPSO)** is a metaheuristic optimization algorithm that combines strong points of firefly and particle swarm optimization. HFPSO tries to determine the start of the local search process properly by checking the previous global best fitness values.\r
\r
[Click Here for the Paper](https://www.sciencedirect.com/science/article/abs/pii/S156849461830084X)\r
\r
[Codes (MATLAB)](https://www.mathworks.com/matlabcentral/fileexchange/67768-a-hybrid-firefly-and-particle-swarm-optimization-hfpso)""" ;
    skos:prefLabel "HFPSO" .

:HGS a skos:Concept ;
    rdfs:seeAlso <https://aliasgharheidari.com/HGS.html> ;
    skos:altLabel "Hunger Games Search" ;
    skos:definition """**Hunger Games Search (**HGS**)** is a general-purpose population-based optimization technique with a simple structure, special stability features and very competitive performance to realize the solutions of both constrained and unconstrained problems more effectively. HGS is designed according to the hunger-driven activities and behavioural choice of animals. This dynamic, fitness-wise search method follows a simple concept of “Hunger” as the most crucial homeostatic motivation and reason for behaviours, decisions, and actions in the life of all animals to make the process of optimization more understandable and consistent for new users and decision-makers. The Hunger Games Search incorporates the concept of hunger into the feature process; in other words, an adaptive weight based on the concept of hunger is designed and employed to simulate the effect of hunger on each search step. It follows the computationally logical rules (games) utilized by almost all animals and these rival activities and games are often adaptive evolutionary by securing higher chances of survival and food acquisition. This method's main feature is its dynamic nature, simple structure, and high performance in terms of convergence and acceptable quality of solutions, proving to be more efficient than the current optimization methods. \r
\r
Implementation of the HGS algorithm is available at [https://aliasgharheidari.com/HGS.html](https://aliasgharheidari.com/HGS.html).""" ;
    skos:prefLabel "HGS" .

:HINT a skos:Concept ;
    dcterms:source <https://link.springer.com/chapter/10.1007/978-3-031-28244-7_44> ;
    skos:altLabel "Hierarchical Information Threading" ;
    skos:definition """An unsupervised approach for identifying Hierarchical Information Threads by analysing the network of related articles in a collection. In particular, HINT leverages article timestamps and the 5W1H questions to identify related articles about an event or discussion. HINT then constructs a network representation of the articles,\r
and identify threads as strongly connected hierarchical network communities.""" ;
    skos:prefLabel "HINT" .

:HITNet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2007.12140v5> ;
    skos:definition """**HITNet** is a framework for neural network based depth estimation which overcomes the computational disadvantages of operating on a 3D volume by integrating image warping, spatial propagation and a fast high resolution initialization step into the network architecture, while keeping the flexibility of a learned representation by allowing features to flow through the network. The main idea of the approach is to represent image tiles as planar patches which have a learned compact feature descriptor attached to them. The basic principle of the approach is to fuse information from the high resolution initialization and the current hypotheses using spatial propagation. The propagation is implemented via a [convolutional neural network](https://paperswithcode.com/methods/category/convolutional-neural-networks) module that updates the estimate of the planar patches and their attached features. \r
\r
In order for the network to iteratively increase the accuracy of the disparity predictions, the network is provided a local cost volume in a narrow band (±1 disparity) around the planar patch using in-network image warping allowing the network to minimize image dissimilarity. To reconstruct fine details while also capturing large texture-less areas we start at low resolution and hierarchically upsample predictions to higher resolution. A critical feature of the architecture is that at each resolution, matches from the initialization module are provided to facilitate recovery of thin structures that cannot be represented at low resolution.""" ;
    skos:prefLabel "HITNet" .

:HMGNN a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2009.12710v1> ;
    skos:altLabel "Heterogeneous Molecular Graph Neural Network" ;
    skos:definition "As they carry great potential for modeling complex interactions, graph neural network (GNN)-based methods have been widely used to predict quantum mechanical properties of molecules. Most of the existing methods treat molecules as molecular graphs in which atoms are modeled as nodes. They characterize each atom's chemical environment by modeling its pairwise interactions with other atoms in the molecule. Although these methods achieve a great success, limited amount of works explicitly take many-body interactions, i.e., interactions between three and more atoms, into consideration. In this paper, we introduce a novel graph representation of molecules, heterogeneous molecular graph (HMG) in which nodes and edges are of various types, to model many-body interactions. HMGs have the potential to carry complex geometric information. To leverage the rich information stored in HMGs for chemical prediction problems, we build heterogeneous molecular graph neural networks (HMGNN) on the basis of a neural message passing scheme. HMGNN incorporates global molecule representations and an attention mechanism into the prediction process. The predictions of HMGNN are invariant to translation and rotation of atom coordinates, and permutation of atom indices. Our model achieves state-of-the-art performance in 9 out of 12 tasks on the QM9 dataset." ;
    skos:prefLabel "HMGNN" .

:HOC a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2102.05291v2> ;
    rdfs:seeAlso <https://github.com/UCSC-REAL/HOC> ;
    skos:altLabel "High-Order Consensuses" ;
    skos:definition "" ;
    skos:prefLabel "HOC" .

:HOPE a skos:Concept ;
    dcterms:source <https://dl.acm.org/doi/abs/10.1145/2939672.2939751> ;
    skos:altLabel "High-Order Proximity preserved Embedding" ;
    skos:definition "" ;
    skos:prefLabel "HOPE" .

:HPO a skos:Concept ;
    dcterms:source <http://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization> ;
    skos:altLabel "Hyper-parameter optimization" ;
    skos:definition "In machine learning, a hyperparameter is a parameter whose value is used to control learning process, and HPO is the problem of choosing a set of optimal hyperparameters for a learning algorithm." ;
    skos:prefLabel "HPO" .

:HRIpipeline a skos:Concept ;
    rdfs:seeAlso <https://github.com/anacmurillo/unizar_interactive_robotics> ;
    skos:altLabel "Human Robot Interaction Pipeline" ;
    skos:definition "The pipeline we propose consists of three parts: 1) recognizing the interaction type; 2) detecting the object that the interaction is targeting; and 3) learning incrementally the models from data recorded by the robot sensors. Our main contributions lie in the target object detection, guided by the recognized interaction, and in the incremental object learning. The novelty of our approach is the focus on natural, heterogeneous, and multimodal HRIs to incrementally learn new object models." ;
    skos:prefLabel "HRI pipeline" .

:HRNet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1908.07919v2> ;
    rdfs:seeAlso <https://github.com/HRNet/HRNet-Image-Classification/blob/8f158719e821836e21e6cba99a3241a12a13bc41/lib/models/cls_hrnet.py#L254> ;
    skos:definition """**HRNet**, or **High-Resolution Net**, is a general purpose convolutional neural network for tasks like semantic segmentation, object detection and image classification. It is able to maintain high resolution representations through the whole process. We start from a high-resolution [convolution](https://paperswithcode.com/method/convolution) stream, gradually add high-to-low resolution convolution streams one by one, and connect the multi-resolution streams in parallel. The resulting network consists of several ($4$ in the paper) stages and\r
the $n$th stage contains $n$ streams corresponding to $n$ resolutions. The authors conduct repeated multi-resolution fusions by exchanging the information across the parallel streams over and over.""" ;
    skos:prefLabel "HRNet" .

:HRank a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2002.10179v2> ;
    skos:definition "**HRank** is a filter pruning method that explores the High Rank of the feature map in each layer (HRank). The proposed HRank  is inspired by the discovery that the average rank of multiple feature maps generated by a single filter is always the same, regardless of the number of image batches CNNs receive. Based on HRank, the authors develop a method that is mathematically formulated to prune filters with low-rank feature maps." ;
    skos:prefLabel "HRank" .

:HS-ResNet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2010.07621v1> ;
    skos:definition "**HS-ResNet** is a [convolutional neural network](https://paperswithcode.com/methods/category/convolutional-neural-networks) that employs [Hierarchical-Split Block](https://paperswithcode.com/method/hierarchical-split-block) as its central building block within a [ResNet](https://paperswithcode.com/method/resnet)-like architecture." ;
    skos:prefLabel "HS-ResNet" .

:HTC a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1901.07518v2> ;
    rdfs:seeAlso <https://github.com/open-mmlab/mmdetection/blob/47d663f0fa97919cce60a64ffa977d9aed6cff45/mmdet/models/detectors/htc.py#L6> ;
    skos:altLabel "Hybrid Task Cascade" ;
    skos:definition "**Hybrid Task Cascade**, or **HTC**, is a framework for cascading in instance segmentation. It differs from [Cascade Mask R-CNN](https://paperswithcode.com/method/cascade-mask-r-cnn) in two important aspects:  (1) instead of performing cascaded refinement on the two tasks of detection and segmentation separately, it interweaves them for a joint multi-stage processing; (2) it adopts a fully convolutional branch to provide spatial context, which can help distinguishing hard foreground from cluttered background." ;
    skos:prefLabel "HTC" .

:HTCN a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2003.06297v1> ;
    skos:altLabel "Hierarchical Transferability Calibration Network" ;
    skos:definition "**Hierarchical Transferability Calibration Network** (HTCN) is an adaptive object detector that hierarchically (local-region/image/instance) calibrates the transferability of feature representations for harmonizing transferability and discriminability. The proposed model consists of three components: (1) Importance Weighted Adversarial Training with input Interpolation (IWAT-I), which strengthens the global discriminability by re-weighting the interpolated image-level features; (2) Context-aware Instance-Level Alignment (CILA) module, which enhances the local discriminability by capturing the complementary effect between the instance-level feature and the global context information for the instance-level feature alignment; (3) local feature masks that calibrate the local transferability to provide semantic guidance for the following discriminative pattern alignment." ;
    skos:prefLabel "HTCN" .

:HalluciNet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1912.04430v3> ;
    skos:altLabel "Approximating Spatiotemporal Representations Using a 2DCNN" ;
    skos:definition "Approximating Spatiotemporal Representations Using a 2DCNN" ;
    skos:prefLabel "HalluciNet" .

:HaloNet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2103.12731v3> ;
    skos:definition "A **HaloNet** is a self-attention based model for efficient image classification. It relies on a local self-attention architecture that efficiently maps to existing hardware with haloing. The formulation breaks translational equivariance, but the authors observe that it improves  throughput and accuracies over the centered local self-attention used in regular self-attention. The approach also utilises a strided self-attentive downsampling operation for multi-scale feature extraction." ;
    skos:prefLabel "HaloNet" .

:Hamburger a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2109.04553v2> ;
    skos:definition "**Hamburger** is a global context module that employs matrix decomposition to factorize the learned representation into sub-matrices so as to recover the clean low-rank signal subspace. The key idea is, if we formulate the inductive bias like the global context into an objective function, the optimization algorithm to minimize the objective function can construct a computational graph, i.e., the architecture we need in the networks." ;
    skos:prefLabel "Hamburger" .

:HardELiSH a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1808.00783v1> ;
    rdfs:seeAlso <https://github.com/digantamisra98/Echo/blob/ffac8cab7938d8a2ea1eee9a947602d1f577c9f3/echoAI/Activation/Torch/hard_elish.py#L19> ;
    skos:definition """**HardELiSH** is an activation function for neural networks.  The HardELiSH is a multiplication of the [HardSigmoid](https://paperswithcode.com/method/hard-sigmoid) and [ELU](https://paperswithcode.com/method/elu) in the negative part and a multiplication of the Linear and the HardSigmoid in the positive\r
part:\r
\r
$$f\\left(x\\right) = x\\max\\left(0, \\min\\left(1, \\left(\\frac{x+1}{2}\\right)\\right) \\right) \\text{ if } x \\geq 1$$\r
$$f\\left(x\\right) = \\left(e^{x}-1\\right)\\max\\left(0, \\min\\left(1, \\left(\\frac{x+1}{2}\\right)\\right)\\right) \\text{ if } x < 0 $$\r
\r
Source: [Activation Functions](https://arxiv.org/pdf/1811.03378.pdf)""" ;
    skos:prefLabel "HardELiSH" .

:HardSigmoid a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1511.00363v3> ;
    rdfs:seeAlso <https://github.com/tensorflow/tensorflow/blob/2b96f3662bd776e277f86997659e61046b56c315/tensorflow/python/keras/backend.py#L4716> ;
    skos:definition """The **Hard Sigmoid** is an activation function used for neural networks of the form:\r
\r
$$f\\left(x\\right) = \\max\\left(0, \\min\\left(1,\\frac{\\left(x+1\\right)}{2}\\right)\\right)$$\r
\r
Image Source: [Rinat Maksutov](https://towardsdatascience.com/deep-study-of-a-not-very-deep-neural-network-part-2-activation-functions-fd9bd8d406fc)""" ;
    skos:prefLabel "Hard Sigmoid" .

:HardSwish a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1905.02244v5> ;
    rdfs:seeAlso <https://github.com/pytorch/pytorch/blob/96aaa311c0251d24decb9dc5da4957b7c590af6f/torch/nn/modules/activation.py#L391> ;
    skos:definition """**Hard Swish** is a type of activation function based on [Swish](https://paperswithcode.com/method/swish), but replaces the computationally expensive sigmoid with a piecewise linear analogue:\r
\r
$$\\text{h-swish}\\left(x\\right) = x\\frac{\\text{ReLU6}\\left(x+3\\right)}{6} $$""" ;
    skos:prefLabel "Hard Swish" .

:HardtanhActivation a skos:Concept ;
    skos:definition """**Hardtanh** is an activation function used for neural networks:\r
\r
$$ f\\left(x\\right) = -1 \\text{ if } x < - 1 $$\r
$$ f\\left(x\\right) = x \\text{ if } -1 \\leq x \\leq 1 $$\r
$$ f\\left(x\\right) = 1 \\text{ if } x > 1 $$\r
\r
It is a cheaper and more computationally efficient version of the [tanh activation](https://paperswithcode.com/method/tanh-activation).\r
\r
Image Source: [Zhuan Lan](https://zhuanlan.zhihu.com/p/30385380)""" ;
    skos:prefLabel "Hardtanh Activation" .

:Harm-Net a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2001.06570v3> ;
    rdfs:seeAlso <https://github.com/matej-ulicny/harmonic-networks/blob/0fccf674806a0b876e641ef5271aad520ff90739/imagenet/models/resnet.py#L113> ;
    skos:definition "A **Harmonic Network**, or **Harm-Net**, is a type of convolutional neural network that replaces convolutional layers with \"harmonic blocks\" that use [Discrete Cosine Transform](https://paperswithcode.com/method/discrete-cosine-transform) (DCT) filters. These blocks can be useful in  truncating high-frequency information (possible due to the redundancies in the spectral domain)." ;
    skos:prefLabel "Harm-Net" .

:HarmonicBlock a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2001.06570v3> ;
    rdfs:seeAlso <https://github.com/matej-ulicny/harmonic-networks/blob/0fccf674806a0b876e641ef5271aad520ff90739/imagenet/models/resnet.py#L71> ;
    skos:definition """A **Harmonic Block** is an image model component that utilizes [Discrete Cosine Transform](https://paperswithcode.com/method/discrete-cosine-transform) (DCT) filters. Convolutional neural networks (CNNs) learn filters in order to capture local correlation patterns in feature space. In contrast, DCT has preset spectral filters, which can be better for compressing information (due to the presence of redundancy in the spectral domain).\r
\r
DCT has been successfully used for JPEG encoding to transform image blocks into spectral representations to capture the most information with a small number of coefficients. Harmonic blocks learn how to optimally combine spectral coefficients at every layer to produce a fixed size representation defined as a weighted sum of responses to DCT filters. The use of DCT filters allows to address the task of model compression.""" ;
    skos:prefLabel "Harmonic Block" .

:HarrisHawksoptimization\(HHO\) a skos:Concept ;
    rdfs:seeAlso <https://aliasgharheidari.com/HHO.html> ;
    skos:altLabel "Harris Hawks optimization" ;
    skos:definition """[HHO](https://aliasgharheidari.com/HHO.html) is a popular swarm-based, gradient-free optimization algorithm with several active and time-varying phases of exploration and exploitation. This algorithm initially published by the prestigious Journal of Future Generation Computer Systems (FGCS) in 2019, and from the first day, it has gained increasing attention among researchers due to its flexible structure, high performance, and high-quality results. The main logic of the HHO method is designed based on the cooperative behaviour and chasing styles of Harris' hawks in nature called "surprise pounce". Currently, there are many suggestions about how to enhance the functionality of HHO, and there are also several enhanced variants of the HHO in the leading Elsevier and IEEE transaction journals.\r
\r
From the algorithmic behaviour viewpoint, there are several effective features in HHO :\r
Escaping energy parameter has a dynamic randomized time-varying nature, which can further improve and harmonize the exploratory and exploitive patterns of HHO. This factor also supports HHO to conduct a smooth transition between exploration and exploitation.\r
Different exploration mechanisms with respect to the average location of hawks can increase the exploratory trends of HHO throughout initial iterations.\r
Diverse LF-based patterns with short-length jumps enrich the exploitative behaviours of HHO when directing a local search.\r
The progressive selection scheme supports search agents to progressively advance their position and only select a better position, which can improve the superiority of solutions and intensification powers of HHO throughout the optimization procedure.\r
HHO shows a series of searching strategies and then, it selects the best movement step. This feature has also a constructive influence on the exploitation inclinations of HHO.\r
The randomized jump strength can assist candidate solutions in harmonising the exploration and exploitation leanings.\r
The application of adaptive and time-varying components allows HHO to handle difficulties of a feature space including local optimal solutions, multi-modality, and deceptive optima.\r
\r
🔗 The source codes of HHO are publicly available at https://aliasgharheidari.com/HHO.html""" ;
    skos:prefLabel "Harris Hawks optimization (HHO)" .

:Heatmap a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1406.2984v2> ;
    skos:definition "" ;
    skos:prefLabel "Heatmap" .

:HermiteActivation a skos:Concept ;
    skos:altLabel "Hermite Polynomial Activation" ;
    skos:definition """A **Hermite Activations** is a type of activation function which uses a smooth finite Hermite polynomial base as a substitute for non-smooth [ReLUs](https://paperswithcode.com/method/relu). \r
\r
Relevant Paper: [Lokhande et al](https://arxiv.org/pdf/1909.05479.pdf)""" ;
    skos:prefLabel "Hermite Activation" .

:Herring a skos:Concept ;
    skos:definition "**Herring** is a parameter server based distributed training method. It combines AWS's Elastic Fabric [Adapter](https://paperswithcode.com/method/adapter) (EFA) with a novel parameter sharding technique that makes better use of the available network bandwidth.  Herring uses EFA and balanced fusion buffer to optimally use the total bandwidth available across all nodes in the cluster. Herring reduces gradients hierarchically, reducing them inside the node first and then reducing across nodes. This enables more efficient use of PCIe bandwidth in the node and helps keep the gradient averaging related burden on GPU low." ;
    skos:prefLabel "Herring" .

:HetPipe a skos:Concept ;
    skos:definition "**HetPipe** is a hybrid parallel method that integrates pipelined model parallelism (PMP) with data parallelism (DP). In HetPipe, a group of multiple GPUs, called a virtual worker, processes minibatches in a pipelined manner, and multiple such virtual workers employ data parallelism for higher performance." ;
    skos:prefLabel "HetPipe" .

:Hi-LANDER a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2107.01319v2> ;
    skos:definition "**Hi-LANDER** is a hierarchical [graph neural network](https://paperswithcode.com/methods/category/graph-models) (GNN) model that learns how to cluster a set of images into an unknown number of identities using an image annotated with labels belonging to a disjoint set of identities. The hierarchical GNN uses an approach to merge connected components predicted at each level of the hierarchy to form a new graph at the next level. Unlike fully unsupervised hierarchical clustering, the choice of grouping and complexity criteria stems naturally from supervision in the training set." ;
    skos:prefLabel "Hi-LANDER" .

:HiFi-GAN a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2010.05646v2> ;
    skos:definition """**HiFi-GAN** is a generative adversarial network for speech synthesis. HiFi-GAN consists of one generator and two discriminators: multi-scale and multi-period discriminators. The generator and discriminators are trained adversarially, along with two additional losses for improving training stability and model performance.\r
\r
The generator is a fully convolutional neural network. It uses a mel-spectrogram as input and upsamples it through transposed convolutions until the length of the output sequence matches the temporal resolution of raw waveforms. Every [transposed convolution](https://paperswithcode.com/method/transposed-convolution) is followed by a multi-receptive field fusion (MRF) module.\r
\r
For the discriminator, a multi-period discriminator (MPD) is used consisting of several sub-discriminators each handling a portion of periodic signals of input audio. Additionally, to capture consecutive patterns and long-term dependencies, the multi-scale discriminator (MSD) proposed in [MelGAN](https://paperswithcode.com/method/melgan) is used, which consecutively evaluates audio samples at different levels.""" ;
    skos:prefLabel "HiFi-GAN" .

:HiSD a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2103.01456v1> ;
    skos:altLabel "Hierarchical Style Disentanglement" ;
    skos:definition "**Hierarchical Style Disentanglement**, or **HiSD**,  aims to disentangle different styles in image-to-image translation models. It organizes the labels into a hierarchical structure, where independent tags, exclusive attributes, and disentangled styles are allocated from top to bottom. To make the styles identified to the tags and attributes, the authors carefully redesign the modules, phases, and objectives." ;
    skos:prefLabel "HiSD" .

:Hierarchical-SplitBlock a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2010.07621v1> ;
    skos:definition """**Hierarchical-Split Block** is a representational block for multi-scale feature representations. It contains many hierarchical split and concatenate connections within one single [residual block](https://paperswithcode.com/methods/category/skip-connection-blocks). \r
\r
Specifically, ordinary feature maps in deep neural networks are split into $s$ groups, each with $w$ channels. As shown in the Figure, only the first group of filters can be straightly connected to next layer. The second group of feature maps are sent to a convolution of $3 \\times 3$ filters to extract features firstly, then the output feature maps are split into two sub-groups in the channel dimension. One sub-group of feature maps straightly connected to next layer, while the other sub-group is concatenated with the next group of input feature maps in the channel dimension. The concatenated feature maps are operated by a set of $3 \\times 3$ convolutional filters. This process repeats several times until the rest of input feature maps are processed. Finally, features maps from all input groups are concatenated and sent to another layer of $1 \\times 1$ filters to rebuild the features.""" ;
    skos:prefLabel "Hierarchical-Split Block" .

:HierarchicalFeatureFusion a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1803.06815v3> ;
    skos:definition "**Hierarchical Feature Fusion (HFF)** is a feature fusion method employed in [ESP](https://paperswithcode.com/method/esp) and [EESP](https://paperswithcode.com/method/eesp) image model blocks for degridding. In the ESP module, concatenating the outputs of dilated convolutions gives the ESP module a large effective receptive field, but it introduces unwanted checkerboard or gridding artifacts. To address the gridding artifact in ESP, the feature maps obtained using kernels of different dilation rates are hierarchically added before concatenating them (HFF). This solution is simple and effective and does not increase the complexity of the ESP module." ;
    skos:prefLabel "Hierarchical Feature Fusion" .

:HierarchicalMTL a skos:Concept ;
    dcterms:source <https://aclanthology.org/P16-2038> ;
    skos:altLabel "Hierarchical Multi-Task Learning" ;
    skos:definition "Multi-task learning (MTL) introduces an inductive bias, based on a-priori relations between tasks: the trainable model is compelled to model more general dependencies by using the abovementioned relation as an important data feature. Hierarchical MTL, in which different tasks use different levels of the deep neural network, provides more effective inductive bias compared to “flat” MTL. Also, hierarchical MTL helps to solve the vanishing gradient problem in deep learning." ;
    skos:prefLabel "Hierarchical MTL" .

:HierarchicalNetworkDissection a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2108.10360v2> ;
    skos:definition "**Hierarchical Network Dissection** is a pipeline for interpreting the internal representation of face-centric inference models. Using a probabilistic formulation, Hierarchical Network Dissection pairs units of the model with concepts in a \"Face Dictionary\" (a collection of facial concepts with corresponding sample images). Interpretable units are discovered in a [convolution](https://paperswithcode.com/method/convolution) layer through HND to identify multiple instances of unit-concept affinity. The pipeline is inspired by [Network Dissection](https://paperswithcode.com/method/network-dissection), an interpretability model for object-centric and scene-centric models." ;
    skos:prefLabel "Hierarchical Network Dissection" .

:HierarchicalSoftmax a skos:Concept ;
    skos:definition """**Hierarchical Softmax** is a is an alternative to [softmax](https://paperswithcode.com/method/softmax) that is faster to evaluate: it is $O\\left(\\log{n}\\right)$ time to evaluate compared to $O\\left(n\\right)$ for softmax. It utilises a multi-layer binary tree, where the probability of a word is calculated through the product of probabilities on each edge on the path to that node. See the Figure to the right for an example of where the product calculation would occur for the word "I'm".\r
\r
(Introduced by Morin and Bengio)\r
\r
Image Credit: [Steven Schmatz](https://www.quora.com/profile/Steven-Schmatz)""" ;
    skos:prefLabel "Hierarchical Softmax" .

:HierarchicalVAE a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1602.02282v3> ;
    skos:altLabel "Hierarchical Variational Autoencoder" ;
    skos:definition "" ;
    skos:prefLabel "Hierarchical VAE" .

:High-levelbackbone a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2004.12186v2> ;
    skos:definition "" ;
    skos:prefLabel "High-level backbone" .

:High-resolutioninput a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2004.12186v2> ;
    skos:definition "" ;
    skos:prefLabel "High-resolution input" .

:HighwayLayer a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1505.00387v2> ;
    rdfs:seeAlso <https://github.com/kefirski/pytorch_Highway/blob/70b75db7a2d029f4bbe08fd4c7d69e36bf7b6d3a/highway/highway.py#L35> ;
    skos:definition """A **Highway Layer** contains an information highway to other layers that helps with information flow. It is characterised by the use of a gating unit to help this information flow. \r
\r
A plain feedforward neural network typically consists of $L$ layers where the $l$th layer ($l \\in ${$1, 2, \\dots, L$}) applies a nonlinear transform $H$ (parameterized by $\\mathbf{W\\_{H,l}}$) on its input $\\mathbf{x\\_{l}}$ to produce its output $\\mathbf{y\\_{l}}$. Thus, $\\mathbf{x\\_{1}}$ is the input to the network and $\\mathbf{y\\_{L}}$ is the network’s output. Omitting the layer index and biases for clarity,\r
\r
$$ \\mathbf{y} = H\\left(\\mathbf{x},\\mathbf{W\\_{H}}\\right) $$\r
\r
$H$ is usually an affine transform followed by a non-linear activation function, but in general it may take other forms. \r
\r
For a [highway network](https://paperswithcode.com/method/highway-network), we additionally define two nonlinear transforms $T\\left(\\mathbf{x},\\mathbf{W\\_{T}}\\right)$ and $C\\left(\\mathbf{x},\\mathbf{W\\_{C}}\\right)$ such that:\r
\r
$$ \\mathbf{y} = H\\left(\\mathbf{x},\\mathbf{W\\_{H}}\\right)·T\\left(\\mathbf{x},\\mathbf{W\\_{T}}\\right) + \\mathbf{x}·C\\left(\\mathbf{x},\\mathbf{W\\_{C}}\\right)$$\r
\r
We refer to T as the transform gate and C as the carry gate, since they express how much of the output is produced by transforming the input and carrying it, respectively. In the original paper, the authors set $C = 1 − T$, giving:\r
\r
$$ \\mathbf{y} = H\\left(\\mathbf{x},\\mathbf{W\\_{H}}\\right)·T\\left(\\mathbf{x},\\mathbf{W\\_{T}}\\right) + \\mathbf{x}·\\left(1-T\\left(\\mathbf{x},\\mathbf{W\\_{T}}\\right)\\right)$$\r
\r
The authors set:\r
\r
$$ T\\left(x\\right) = \\sigma\\left(\\mathbf{W\\_{T}}^{T}\\mathbf{x} + \\mathbf{b\\_{T}}\\right) $$\r
\r
Image: [Sik-Ho Tsang](https://towardsdatascience.com/review-highway-networks-gating-function-to-highway-image-classification-5a33833797b5)""" ;
    skos:prefLabel "Highway Layer" .

:HighwayNetwork a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1505.00387v2> ;
    rdfs:seeAlso <https://github.com/kefirski/pytorch_Highway/blob/70b75db7a2d029f4bbe08fd4c7d69e36bf7b6d3a/highway/highway.py#L5> ;
    skos:definition "A **Highway Network** is an architecture designed to ease gradient-based training of very deep networks. They allow unimpeded information flow across several layers on \"information highways\". The architecture is characterized by the use of gating units which learn to regulate the flow of information through a network. Highway networks with hundreds of layers can be trained directly using stochastic gradient descent and with a variety of activation functions." ;
    skos:prefLabel "Highway Network" .

:Highwaynetworks a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1505.00387v2> ;
    skos:definition "There is plenty of theoretical and empirical evidence that depth of neural networks is a crucial ingredient for their success. However, network training becomes more difficult with increasing depth and training of very deep networks remains an open problem. In this extended abstract, we introduce a new architecture designed to ease gradient-based training of very deep networks. We refer to networks with this architecture as highway networks, since they allow unimpeded information flow across several layers on \"information highways\". The architecture is characterized by the use of gating units which learn to regulate the flow of information through a network. Highway networks with hundreds of layers can be trained directly using stochastic gradient descent and with a variety of activation functions, opening up the possibility of studying extremely deep and efficient architectures." ;
    skos:prefLabel "Highway networks" .

:Hit-Detector a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2003.11818v1> ;
    rdfs:seeAlso <https://github.com/ggjy/HitDet.pytorch> ;
    skos:definition "**Hit-Detector** is a neural architectures search algorithm that simultaneously searches all components of an object detector in an end-to-end manner. It is a hierarchical approach to mine the proper subsearch space from the large volume of operation candidates. It consists of two main procedures. First, given a large search space containing all the operation candidates, we screen out the customized sub search space suitable for each part of detector with the help of group sparsity regularization. Secondly, we search the architectures for each part within the corresponding sub search space by adopting the differentiable manner." ;
    skos:prefLabel "Hit-Detector" .

:HolographicReducedRepresentation a skos:Concept ;
    skos:definition """**Holographic Reduced Representations** are a simple mechanism to represent an associative array of key-value pairs in a fixed-size vector. Each individual key-value pair is the same size as the entire associative array; the array is represented by the sum of the pairs. Concretely, consider a complex vector key $r = (a\\_{r}[1]e^{iφ\\_{r}[1]}, a\\_{r}[2]e^{iφ\\_{r}[2]}, \\dots)$, which is the same size as the complex vector value x. The pair is "bound" together by element-wise complex multiplication, which multiplies the moduli and adds the phases of the elements:\r
\r
$$ y = r \\otimes x $$\r
\r
$$ y =  \\left(a\\_{r}[1]a\\_{x}[1]e^{i(φ\\_{r}[1]+φ\\_{x}[1])}, a\\_{r}[2]a\\_{x}[2]e^{i(φ\\_{r}[2]+φ\\_{x}[2])}, \\dots\\right) $$\r
\r
Given keys $r\\_{1}$, $r\\_{2}$, $r\\_{3}$ and input vectors $x\\_{1}$, $x\\_{2}$, $x\\_{3}$, the associative array is:\r
\r
$$c = r\\_{1} \\otimes x\\_{1} + r\\_{2} \\otimes x\\_{2} + r\\_{3} \\otimes x\\_{3} $$\r
\r
where we call $c$ a memory trace. Define the key inverse:\r
\r
$$ r^{-1} = \\left(a\\_{r}[1]^{−1}e^{−iφ\\_{r}[1]}, a\\_{r}[2]^{−1}e^{−iφ\\_{r}[2]}, \\dots\\right) $$\r
\r
To retrieve the item associated with key $r\\_{k}$, we multiply the memory trace element-wise by the vector $r^{-1}\\_{k}$. For example: \r
\r
$$ r\\_{2}^{−1} \\otimes c = r\\_{2}^{-1} \\otimes \\left(r\\_{1} \\otimes x\\_{1} + r\\_{2} \\otimes x\\_{2} + r\\_{3} \\otimes x\\_{3}\\right) $$\r
\r
$$ r\\_{2}^{−1} \\otimes c = x\\_{2} + r^{-1}\\_{2} \\otimes \\left(r\\_{1} \\otimes x\\_{1} + r\\_{3} \\otimes x3\\right) $$\r
\r
$$ r\\_{2}^{−1} \\otimes c = x\\_{2} + noise $$\r
\r
The product is exactly $x\\_{2}$ together with a noise term. If the phases of the elements of the key vector are randomly distributed, the noise term has zero mean.\r
\r
Source: [Associative LSTMs](https://arxiv.org/pdf/1602.03032.pdf)""" ;
    skos:prefLabel "Holographic Reduced Representation" .

:HopfieldLayer a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2008.02217v3> ;
    rdfs:seeAlso <https://github.com/ml-jku/hopfield-layers/blob/19443f24d0c7bc23bd01249b9a5c9c50f30e2936/modules/activation.py#L16> ;
    skos:definition """A **Hopfield Layer** is a module that enables a network to associate two sets of vectors. This general functionality allows for [transformer](https://paperswithcode.com/method/transformer)-like self-attention, for decoder-encoder attention, for time series prediction (maybe with positional encoding), for sequence analysis, for multiple instance learning, for learning with point sets, for combining data sources by associations, for constructing a memory, for averaging and pooling operations, and for many more. \r
\r
In particular, the Hopfield layer can readily be used as plug-in replacement for existing layers like pooling layers ([max-pooling](https://paperswithcode.com/method/max-pooling) or [average pooling](https://paperswithcode.com/method/average-pooling), permutation equivariant layers, [GRU](https://paperswithcode.com/method/gru) & [LSTM](https://paperswithcode.com/method/lstm) layers, and attention layers. The Hopfield layer is based on modern Hopfield networks with continuous states that have very high storage capacity and converge after one update.""" ;
    skos:prefLabel "Hopfield Layer" .

:HourglassModule a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1603.06937v2> ;
    rdfs:seeAlso <https://github.com/bearpaw/pytorch-pose/blob/7ce6642f777e9da6249bd5b05330d57fa09ea37a/pose/models/hourglass.py#L54> ;
    skos:definition """An **Hourglass Module** is an image block module used mainly for pose estimation tasks. The design of the hourglass is motivated by the need to capture information at every scale. While local evidence is essential for identifying features like faces and hands, a final pose estimate requires a coherent understanding of the full body. The person’s orientation, the arrangement of their limbs, and the relationships of adjacent joints are among the many cues that are best recognized at different scales in the image. The hourglass is a simple, minimal design that has the capacity to capture all of these features and bring them together to output pixel-wise predictions.\r
\r
The network must have some mechanism to effectively process and consolidate features across scales. The Hourglass uses a single pipeline with skip layers to preserve spatial information at each resolution. The network reaches its lowest resolution at 4x4 pixels allowing smaller spatial filters to be applied that compare features across the entire space of the image.\r
\r
The hourglass is set up as follows: Convolutional and [max pooling](https://paperswithcode.com/method/max-pooling) layers are used to process features down to a very low resolution. At each max pooling step, the network branches off and applies more convolutions at the original pre-pooled resolution. After reaching the lowest resolution, the network begins the top-down sequence of upsampling and combination of features across scales. To bring together information across two adjacent resolutions, we do nearest neighbor upsampling of the lower resolution followed by an elementwise addition of the two sets of features. The topology of the hourglass is symmetric, so for every layer present on the way down there is a corresponding layer going up.\r
\r
After reaching the output resolution of the network, two consecutive rounds of 1x1 convolutions are applied to produce the final network predictions. The output of the network is a set of heatmaps where for a given [heatmap](https://paperswithcode.com/method/heatmap) the network predicts the probability of a joint’s presence at each and every pixel.""" ;
    skos:prefLabel "Hourglass Module" .

:Huberloss a skos:Concept ;
    skos:definition """The Huber loss function describes the penalty incurred by an estimation procedure f. Huber (1964) defines the loss function piecewise by[1]\r
\r
    L δ ( a ) = { 1 2 a 2 for  | a | ≤ δ , δ ⋅ ( | a | − 1 2 δ ) , otherwise. {\\displaystyle L_{\\delta }(a)={\\begin{cases}{\\frac {1}{2}}{a^{2}}&{\\text{for }}|a|\\leq \\delta ,\\\\\\delta \\cdot \\left(|a|-{\\frac {1}{2}}\\delta \\right),&{\\text{otherwise.}}\\end{cases}}}\r
\r
This function is quadratic for small values of a, and linear for large values, with equal values and slopes of the different sections at the two points where | a | = δ |a|=\\delta . The variable a often refers to the residuals, that is to the difference between the observed and predicted values a = y − f ( x ) a=y-f(x), so the former can be expanded to[2]\r
\r
    L δ ( y , f ( x ) ) = { 1 2 ( y − f ( x ) ) 2 for  | y − f ( x ) | ≤ δ , δ   ⋅ ( | y − f ( x ) | − 1 2 δ ) , otherwise. {\\displaystyle L_{\\delta }(y,f(x))={\\begin{cases}{\\frac {1}{2}}(y-f(x))^{2}&{\\text{for }}|y-f(x)|\\leq \\delta ,\\\\\\delta \\ \\cdot \\left(|y-f(x)|-{\\frac {1}{2}}\\delta \\right),&{\\text{otherwise.}}\\end{cases}}}\r
\r
The Huber loss is the convolution of the absolute value function with the rectangular function, scaled and translated. Thus it "smoothens out" the former's corner at the origin. \r
\r
.. math::\r
        \\ell(x, y) = L = \\{l_1, ..., l_N\\}^T\r
\r
    with\r
\r
    .. math::\r
        l_n = \\begin{cases}\r
        0.5 (x_n - y_n)^2, & \\text{if } |x_n - y_n| < delta \\\\\r
        delta * (|x_n - y_n| - 0.5 * delta), & \\text{otherwise }\r
        \\end{cases}""" ;
    skos:prefLabel "Huber loss" .

:Hydra a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2001.04694v2> ;
    skos:definition "**Hydra** is a multi-headed neural network for model distillation with a shared body network. The shared body network learns a joint feature representation that enables each head to capture the predictive behavior of each ensemble member.  Existing distillation methods often train a distillation network to imitate the prediction of a larger network. Hydra instead learns to distill the individual predictions of each ensemble member into separate light-weight head models while amortizing the computation through a shared heavy-weight body network. This retains the diversity of ensemble member predictions which is otherwise lost in knowledge distillation." ;
    skos:prefLabel "Hydra" .

:HypE a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2012.13023v3> ;
    skos:altLabel "Hyperboloid Embeddings" ;
    skos:definition "Hyperboloid Embeddings (HypE) is a novel self-supervised dynamic reasoning framework, that utilizes positive first-order existential queries on a KG to learn representations of its entities and relations as hyperboloids in a Poincaré ball. HypE models the positive first-order queries as geometrical translation (t), intersection ($\\cap$), and union ($\\cup$). For the problem of KG reasoning in real-world datasets, the proposed HypE model significantly outperforms the state-of-the art results. HypE is also applied to an anomaly detection task on a popular e-commerce website product taxonomy as well as hierarchically organized web articles and demonstrate significant performance improvements compared to existing baseline methods. Finally, HypE embeddings can also be visualized in a Poincaré ball to clearly interpret and comprehend the representation space." ;
    skos:prefLabel "HypE" .

:HyperDenseNet a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1804.02967v2> ;
    skos:definition "Recently, [dense connections](https://paperswithcode.com/method/dense-connections) have attracted substantial attention in computer vision because they facilitate gradient flow and implicit deep supervision during training. Particularly, [DenseNet](https://paperswithcode.com/method/densenet) that connects each layer to every other layer in a feed-forward fashion and has shown impressive performances in natural image classification tasks. We propose HyperDenseNet, a 3-D fully convolutional neural network that extends the definition of dense connectivity to multi-modal segmentation problems. Each imaging modality has a path, and dense connections occur not only between the pairs of layers within the same path but also between those across different paths. This contrasts with the existing multi-modal CNN approaches, in which modeling several modalities relies entirely on a single joint layer (or level of abstraction) for fusion, typically either at the input or at the output of the network. Therefore, the proposed network has total freedom to learn more complex combinations between the modalities, within and in-between all the levels of abstraction, which increases significantly the learning representation. We report extensive evaluations over two different and highly competitive multi-modal brain tissue segmentation challenges, iSEG 2017 and MRBrainS 2013, with the former focusing on six month infant data and the latter on adult images. HyperDenseNet yielded significant improvements over many state-of-the-art segmentation networks, ranking at the top on both benchmarks. We further provide a comprehensive experimental analysis of features re-use, which confirms the importance of hyper-dense connections in multi-modal representation learning." ;
    skos:prefLabel "HyperDenseNet" .

:HyperHyperNetwork a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2105.03838v1> ;
    skos:altLabel "Hyper HyperNetwork" ;
    skos:definition "" ;
    skos:prefLabel "HyperHyperNetwork" .

:HyperNetwork a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1609.09106v4> ;
    rdfs:seeAlso <https://github.com/g1910/HyperNetworks/blob/393e30a56c2ba14fa9efee5f2698d8c82e5821f6/hypernetwork_modules.py#L6> ;
    skos:definition "A **HyperNetwork** is a network that generates weights for a main network.  The behavior of the main network is the same with any usual neural network: it learns to map some raw inputs to their desired targets; whereas the hypernetwork takes a set of inputs that contain information about the structure of the weights and generates the weight for that layer." ;
    skos:prefLabel "HyperNetwork" .

:HyperSA a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2211.09590v5> ;
    skos:altLabel "HyperGraph Self-Attention" ;
    skos:definition """An extension of Self-Attention to hypergraph\r
Skeleton-based action recognition aims to recognize human actions given human joint coordinates with skeletal interconnections. By defining a graph with joints as vertices and their natural connections as edges, previous works successfully adopted Graph Convolutional networks (GCNs) to model joint co-occurrences and achieved superior performance. More recently, a limitation of GCNs is identified, i.e., the topology is fixed after training. To relax such a restriction, Self-Attention (SA) mechanism has been adopted to make the topology of GCNs adaptive to the input, resulting in the state-of-the-art hybrid models. Concurrently, attempts with plain Transformers have also been made, but they still lag behind state-of-the-art GCN-based methods due to the lack of structural prior. Unlike hybrid models, we propose a more elegant solution to incorporate the bone connectivity into Transformer via a graph distance embedding. Our embedding retains the information of skeletal structure during training, whereas GCNs merely use it for initialization. More importantly, we reveal an underlying issue of graph models in general, i.e., pairwise aggregation essentially ignores the high-order kinematic dependencies between body joints. To fill this gap, we propose a new self-attention (SA) mechanism on hypergraph, termed Hypergraph Self-Attention (HyperSA), to incorporate intrinsic higher-order relations into the model. We name the resulting model Hyperformer, and it beats state-of-the-art graph models w.r.t. accuracy and efficiency on NTU RGB+D, NTU RGB+D 120, and Northwestern-UCLA datasets.""" ;
    skos:prefLabel "HyperSA" .

:HyperTreeMetaModel a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1810.11714v2> ;
    skos:definition "Optimize combinations of various neural network models for multimodal data with bayseian optimization." ;
    skos:prefLabel "HyperTree MetaModel" .

:I-BERT a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2101.01321v3> ;
    skos:definition """**I-BERT** is a quantized version of [BERT](https://paperswithcode.com/method/bert) that quantizes the entire inference with integer-only arithmetic. Based on lightweight integer only approximation methods for nonlinear operations, e.g., [GELU](https://paperswithcode.com/method/gelu), [Softmax](https://paperswithcode.com/method/softmax), and [Layer Normalization](https://paperswithcode.com/method/layer-normalization), it performs an end-to-end integer-only [BERT](https://paperswithcode.com/method/bert) inference without any floating point calculation.\r
\r
In particular, GELU and Softmax are approximated with lightweight second-order polynomials, which can be evaluated with integer-only arithmetic. For LayerNorm, integer-only computation is performed by leveraging a known algorithm for integer calculation of\r
square root.""" ;
    skos:prefLabel "I-BERT" .

:I3DR-Net a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2106.05624v2> ;
    skos:altLabel "Inflated 3D ConvNet Retina Net" ;
    skos:definition "" ;
    skos:prefLabel "I3DR-Net" .

:IAN a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1609.07093v3> ;
    rdfs:seeAlso <https://github.com/ajbrock/Neural-Photo-Editor/blob/master/IAN.py> ;
    skos:altLabel "Introspective Adversarial Network" ;
    skos:definition """The **Introspective Adversarial Network (IAN)** is a hybridization of [GANs](https://paperswithcode.com/method/gan) and [VAEs](https://paperswithcode.com/method/vae) that leverages the power of the adversarial objective while maintaining the VAE’s efficient inference mechanism. It uses the discriminator of the GAN, $D$, as a feature extractor for an inference subnetwork, $E$, which is implemented as a fully-connected layer on top of the final convolutional layer of the discriminator. We infer latent values $Z \\sim E\\left(X\\right) = q\\left(Z\\mid{X}\\right)$ for reconstruction and sample random values $Z \\sim p\\left(Z\\right)$ from a standard normal for random image generation using the generator network, $G$.\r
\r
Three distinct loss functions are used:\r
\r
- $\\mathcal{L}\\_{img}$, the L1 pixel-wise reconstruction loss, which is preferred to the L2 reconstruction loss for its higher average gradient.\r
- $\\mathcal{L\\_{feature}}$, the feature-wise reconstruction loss, evaluated as the L2 difference between the original and reconstruction in the space of the hidden layers of the discriminator.\r
- $\\mathcal{L}\\_{adv}$, the ternary adversarial loss, a modification of the adversarial loss that forces the discriminator to label a sample as real, generated, or reconstructed (as opposed to a binary\r
real vs. generated label).\r
\r
Including the VAE’s KL divergence between the inferred latents $E\\left(X\\right)$ and the prior $p\\left(Z\\right)$, the loss function for the generator and encoder network is thus:\r
\r
$$\\mathcal{L}\\_{E, G} = \\lambda\\_{adv}\\mathcal{L}\\_{G\\_{adv}} + \\lambda\\_{img}\\mathcal{L}\\_{img}  + \\lambda\\_{feature}\\mathcal{L}\\_{feature}  + D\\_{KL}\\left(E\\left(X\\right) || p\\left(Z\\right)\\right) $$\r
\r
Where the $\\lambda$ terms weight the relative importance of each loss. We set $\\lambda\\_{img}$ to 3 and leave the other terms at 1. The discriminator is updated solely using the ternary adversarial loss. During each training step, the generator produces reconstructions $G\\left(E\\left(X\\right)\\right)$ (using the standard VAE reparameterization trick) from data $X$ and random samples $G\\left(Z\\right)$, while the discriminator observes $X$ as well as the reconstructions and random samples, and both networks are simultaneously updated.""" ;
    skos:prefLabel "IAN" .

:IB-BERT a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2004.02984v2> ;
    skos:altLabel "Inverted Bottleneck BERT" ;
    skos:definition "**IB-BERT**, or **Inverted Bottleneck BERT**, is a [BERT](https://paperswithcode.com/method/bert) variant that uses an [inverted bottleneck](https://paperswithcode.com/method/inverted-residual-block) structure. It is used as a teacher network to train the [MobileBERT](https://paperswithcode.com/method/mobilebert) models." ;
    skos:prefLabel "IB-BERT" .

:IC-SBP a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2104.09958v3> ;
    skos:altLabel "Instance Colouring Stick-Breaking Process" ;
    skos:definition "" ;
    skos:prefLabel "IC-SBP" .

:IFBlock a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2011.06294v11> ;
    skos:definition "**IFBlock** is a video model block used in the [IFNet](https://paperswithcode.com/method/ifnet) architecture for video frame interpolation. IFBlocks do not contain expensive operators like cost volume or forward warping and use 3 × 3 convolution and deconvolution as building blocks. Each IFBlock has a feed-forward structure consisting of several convolutional layers and an upsampling operator. Except for the layer that outputs the optical flow residuals and the fusion map, [PReLU](https://paperswithcode.com/method/prelu) activations are used." ;
    skos:prefLabel "IFBlock" .

:IFNet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2011.06294v11> ;
    skos:definition "**IFNet** is an architecture for video frame interpolation that adopts a coarse-to-fine strategy with progressively increased resolutions: it iteratively updates intermediate flows and soft fusion mask via successive [IFBlocks](https://paperswithcode.com/method/ifblock). Conceptually, according to the iteratively updated flow fields, we can move corresponding pixels from two input frames to the same location in a latent intermediate frame and use a fusion mask to combine pixels from two input frames. Unlike most previous optical flow models, IFBlocks do not contain expensive operators like cost volume or forward warping and use 3 × 3 [convolution](https://paperswithcode.com/method/convolution) and deconvolution as building blocks." ;
    skos:prefLabel "IFNet" .

:IGSA a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2003.00830v1> ;
    skos:altLabel "Improved Gravitational Search algorithm" ;
    skos:definition "Metaheuristic algorithm" ;
    skos:prefLabel "IGSA" .

:IICNet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2109.04242v1> ;
    skos:definition "**Invertible Image Conversion Net**, or **IICNet**, is a generic framework for reversible image conversion tasks. Unlike previous encoder-decoder based methods, IICNet maintains a highly invertible structure based on invertible neural networks (INNs) to better preserve the information during conversion. It uses a relation module and a channel squeeze layer to improve the INN nonlinearity to extract cross-image relations and the network flexibility, respectively." ;
    skos:prefLabel "IICNet" .

:ILVR a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2108.02938v2> ;
    skos:altLabel "Iterative Latent Variable Refinement" ;
    skos:definition "**Iterative Latent Variable Refinement**, or **ILVR**, is a method to guide the generative process in denoising diffusion probabilistic models (DDPMs) to generate high-quality images based on a given reference image. ILVR conditions the generation process in well-performing unconditional DDPM. Each transition in the generation process is refined utilizing a given reference image. By matching each latent variable, ILVR ensures the given condition in each transition thus enables sampling from a conditional distribution. Thus, ILVR generates high-quality images sharing desired semantics." ;
    skos:prefLabel "ILVR" .

:IMGEP a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1708.02190v3> ;
    skos:altLabel "Intrinsically Motivated Goal Exploration Processes" ;
    skos:definition "Population-based intrinsically motivated goal exploration algorithms applied to real world robot learning of complex skills like tool use." ;
    skos:prefLabel "IMGEP" .

:IMPALA a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1802.01561v3> ;
    skos:definition """**IMPALA**, or the **Importance Weighted Actor Learner Architecture**, is an off-policy actor-critic framework that decouples acting from learning and learns from experience trajectories using [V-trace](https://paperswithcode.com/method/v-trace). Unlike the popular [A3C](https://paperswithcode.com/method/a3c)-based agents, in which workers communicate gradients with respect to the parameters of the policy to a central parameter server, IMPALA actors communicate trajectories of experience (sequences of states, actions, and rewards) to a centralized learner. Since the learner in IMPALA has access to full trajectories of experience we use a GPU to perform updates on mini-batches of trajectories while aggressively parallelising all time independent operations. \r
\r
This type of decoupled architecture can achieve very high throughput. However, because the policy used to generate a trajectory can lag behind the policy on the learner by several updates at the time of gradient calculation, learning becomes off-policy. The V-trace off-policy actor-critic algorithm is used to correct for this harmful discrepancy.""" ;
    skos:prefLabel "IMPALA" .

:IPA-GNN a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2010.12621v1> ;
    skos:altLabel "Instruction Pointer Attention Graph Neural Network" ;
    skos:definition "**Instruction Pointer Attention Graph Neural Network**, or **IPA-GNN**, is a learning-interpreter neural network (LNN) based on GNNs for learning to execute programmes. It achieves improved systematic generalization on the task of learning to execute programs using control flow graphs. The model arises by considering RNNs operating on program traces with branch decisions as latent variables. The IPA-GNN can be seen either as a continuous relaxation of the RNN model or as a GNN variant more tailored to execution." ;
    skos:prefLabel "IPA-GNN" .

:IPBI a skos:Concept ;
    dcterms:source <https://ieeexplore.ieee.org/document/9222020> ;
    skos:altLabel "Instances-Pixels Balance Index" ;
    skos:definition """In a given dataset for semantic image segmentation, the number of samples per class should be the same, so that no classifier would be biased towards the majority class (here included the background). It is very difficult, if not impossible, to achieve a perfect balance between the several classes of objects of a dataset. Considering that the segmentation of the objects  is accomplished at the pixel level, the number of pixels for each class must be taken into account. As a matter of fact, in image semantic segmentation, \r
different classes and the background may have quite different\r
sizes. Therefore, the image segmentation problem is naturally unbalanced. The IPBI is based on the concept of entropy, a common measure used in many fields of science. In a general sense, it measures the amount of disorder of a system. For the sake of semantic image segmentation, the ideal dataset should have the same number of instances per class, as well as the same number of pixels in all classes. Similar reasoning can be done considering the number of pixels of all samples in a class, so that we can obtain the\r
pixels balance measure for the dataset. Overall, IPBI evaluates the balance of pixels and number of instances of an image semantic segmentation dataset and, so, it is usefull to compare different datasets.""" ;
    skos:prefLabel "IPBI" .

:IPL a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2005.09267v2> ;
    skos:altLabel "Iterative Pseudo-Labeling" ;
    skos:definition "**Iterative Pseudo-Labeling** (IPL) is a semi-supervised algorithm for speech recognition which efficiently performs multiple iterations of pseudo-labeling on unlabeled data as the acoustic model evolves. In particular, IPL fine tunes an existing model at each iteration using both labeled data and a subset of unlabeled data." ;
    skos:prefLabel "IPL" .

:IQ-Learn a skos:Concept ;
    skos:altLabel "Inverse Q-Learning" ;
    skos:definition """**Inverse Q-Learning (IQ-Learn)** is a a simple, stable & data-efficient framework for Imitation Learning (IL), that directly learns *soft Q-functions* from expert data. IQ-Learn enables **non-adverserial** imitation learning, working on both offline and online IL settings. It is performant even with very sparse expert data, and scales to complex image-based environments, surpassing prior methods by more than **3x**. \r
\r
It is very simple to implement requiring ~15 lines of code on top of existing RL methods.\r
\r
<span class="description-source">Source: [IQ-Learn: Inverse soft Q-Learning for Imitation](https://arxiv.org/abs/2106.12142)</span>""" ;
    skos:prefLabel "IQ-Learn" .

:IQL a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2110.06169v1> ;
    skos:altLabel "Implicit Q-Learning" ;
    skos:definition "" ;
    skos:prefLabel "IQL" .

:IRN a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2005.05650v1> ;
    rdfs:seeAlso <https://github.com/pkuxmq/Invertible-Image-Rescaling/blob/f74e8442bdd9d9064bae962267cc29546c4943b9/codes/models/IRN_model.py#L15> ;
    skos:altLabel "Invertible Rescaling Network" ;
    skos:definition """An **Invertible Rescaling Network (IRN)** is a network for image rescaling.  According to the Nyquist-Shannon sampling theorem, high-frequency contents are lost during downscaling. Ideally, we hope to keep all lost information to perfectly recover the original HR image, but storing or transferring the high-frequency information is unacceptable. In order to well address this challenge, the Invertible Rescaling Net (IRN) captures some knowledge on the lost information in the form of its distribution and embeds it into model’s parameters to mitigate the ill-posedness. Given an HR image $x$, IRN not only downscales it into a LR image y, but also embeds the case-specific high-frequency content into an auxiliary case-agnostic latent variable $z$, whose marginal distribution\r
obeys a fixed pre-specified distribution (e.g., isotropic Gaussian). Based on this model,\r
we use a randomly drawn sample of $z$ from the pre-specified distribution for the inverse upscaling procedure, which holds the most information that one could have in upscaling.""" ;
    skos:prefLabel "IRN" .

:ISPL a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2010.05508v1> ;
    skos:altLabel "Implicit Subspace Prior Learning" ;
    skos:definition "**Implicit Subspace Prior Learning**, or **ISPL**, is a framework to approach dual-blind face restoration, with two major distinctions from previous restoration methods: 1) Instead of assuming an explicit degradation function between LQ and HQ domain, it establishes an implicit correspondence between both domains via a mutual embedding space, thus avoid solving the pathological inverse problem directly. 2) A subspace prior decomposition and fusion mechanism to dynamically handle inputs at varying degradation levels with consistent high-quality restoration results." ;
    skos:prefLabel "ISPL" .

:IkshanaNet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2101.10837v4> ;
    skos:altLabel "The Ikshana Hypothesis of Human Scene Understanding Mechanism" ;
    skos:definition "" ;
    skos:prefLabel "IkshanaNet" .

:ImageScaleAugmentation a skos:Concept ;
    skos:definition "Image Scale Augmentation is an augmentation technique where we randomly pick the short size of a image within a dimension range. One use case of this augmentation technique is in object detectiont asks." ;
    skos:prefLabel "Image Scale Augmentation" .

:ImplicitPointRend a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2104.06404v2> ;
    skos:definition "**Implicit PointRend** is a modification to the [PointRend](https://paperswithcode.com/method/pointrend) module for instance segmentation. Instead of a coarse mask prediction used in [PointRend](https://paperswithcode.com/method/pointrend) to provide region-level context to distinguish objects, for each object Implicit PointRend generates different parameters for a function that makes the final pointwise mask prediction. The new model is more straightforward than PointRend: (1) it does not require an importance point sampling during training and (2) it uses a single point-level mask loss instead of two mask losses. Implicit PointRend can be trained directly with point supervision without any intermediate prediction interpolation steps." ;
    skos:prefLabel "Implicit PointRend" .

:InPlace-ABN a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1712.02616v3> ;
    rdfs:seeAlso <https://github.com/mapillary/inplace_abn/blob/24fc791e6d4796a1639e7a5dce6fa67377e51a3e/inplace_abn/abn.py#L88> ;
    skos:altLabel "In-Place Activated Batch Normalization" ;
    skos:definition "**In-Place Activated Batch Normalization**, or **InPlace-ABN**, substitutes the conventionally used succession of [BatchNorm](https://paperswithcode.com/method/batch-normalization) + Activation layers with a single plugin layer, hence avoiding invasive framework surgery while providing straightforward applicability for existing deep learning frameworks. It approximately halves the memory requirements during training of modern deep learning models." ;
    skos:prefLabel "InPlace-ABN" .

:Inception-A a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1602.07261v2> ;
    rdfs:seeAlso <https://github.com/kentsommer/keras-inceptionV4/blob/ef1db6f09b6511779c05fab47d374741bc89b5ee/inception_v4.py#L71> ;
    skos:definition "**Inception-A** is an image model block used in the [Inception-v4](https://paperswithcode.com/method/inception-v4) architecture." ;
    skos:prefLabel "Inception-A" .

:Inception-B a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1602.07261v2> ;
    rdfs:seeAlso <https://github.com/kentsommer/keras-inceptionV4/blob/ef1db6f09b6511779c05fab47d374741bc89b5ee/inception_v4.py#L111> ;
    skos:definition "**Inception-B** is an image model block used in the [Inception-v4](https://paperswithcode.com/method/inception-v4) architecture." ;
    skos:prefLabel "Inception-B" .

:Inception-C a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1602.07261v2> ;
    rdfs:seeAlso <https://github.com/kentsommer/keras-inceptionV4/blob/ef1db6f09b6511779c05fab47d374741bc89b5ee/inception_v4.py#L156> ;
    skos:definition "**Inception-C** is an image model block used in the [Inception-v4](https://paperswithcode.com/method/inception-v4) architecture." ;
    skos:prefLabel "Inception-C" .

:Inception-ResNet-v2 a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1602.07261v2> ;
    rdfs:seeAlso <https://github.com/Cadene/pretrained-models.pytorch/blob/8aae3d8f1135b6b13fed79c1d431e3449fdbf6e0/pretrainedmodels/models/inceptionresnetv2.py#L234> ;
    skos:definition "**Inception-ResNet-v2** is a convolutional neural architecture that builds on the Inception family of architectures but incorporates [residual connections](https://paperswithcode.com/method/residual-connection) (replacing the filter concatenation stage of the Inception architecture)." ;
    skos:prefLabel "Inception-ResNet-v2" .

:Inception-ResNet-v2-A a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1602.07261v2> ;
    rdfs:seeAlso <https://github.com/Cadene/pretrained-models.pytorch/blob/8aae3d8f1135b6b13fed79c1d431e3449fdbf6e0/pretrainedmodels/models/inceptionresnetv2.py#L86> ;
    skos:definition "**Inception-ResNet-v2-A** is an image model block for a 35 x 35 grid used in the [Inception-ResNet-v2](https://paperswithcode.com/method/inception-resnet-v2) architecture." ;
    skos:prefLabel "Inception-ResNet-v2-A" .

:Inception-ResNet-v2-B a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1602.07261v2> ;
    rdfs:seeAlso <https://github.com/tensorflow/models/blob/e54fcee236a1258302342bd703ee27cbba0c12e3/research/slim/nets/inception_resnet_v2.py#L58> ;
    skos:definition "**Inception-ResNet-v2-B** is an image model block for a 17 x 17 grid used in the [Inception-ResNet-v2](https://paperswithcode.com/method/inception-resnet-v2) architecture. It largely follows the idea of Inception modules - and grouped convolutions - but also includes residual connections." ;
    skos:prefLabel "Inception-ResNet-v2-B" .

:Inception-ResNet-v2-C a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1602.07261v2> ;
    rdfs:seeAlso <https://github.com/tensorflow/models/blob/e54fcee236a1258302342bd703ee27cbba0c12e3/research/slim/nets/inception_resnet_v2.py#L84> ;
    skos:definition "**Inception-ResNet-v2-C** is an image model block for an 8 x 8 grid used in the [Inception-ResNet-v2](https://paperswithcode.com/method/inception-resnet-v2) architecture. It largely follows the idea of Inception modules - and grouped convolutions - but also includes residual connections." ;
    skos:prefLabel "Inception-ResNet-v2-C" .

:Inception-ResNet-v2Reduction-B a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1602.07261v2> ;
    rdfs:seeAlso <https://github.com/Cadene/pretrained-models.pytorch/blob/8aae3d8f1135b6b13fed79c1d431e3449fdbf6e0/pretrainedmodels/models/inceptionresnetv2.py#L171> ;
    skos:definition "**Inception-ResNet-v2 Reduction-B** is an image model block used in the [Inception-ResNet-v2](https://paperswithcode.com/method/inception-resnet-v2) architecture." ;
    skos:prefLabel "Inception-ResNet-v2 Reduction-B" .

:Inception-v3 a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1512.00567v3> ;
    rdfs:seeAlso <https://github.com/pytorch/vision/blob/6db1569c89094cf23f3bc41f79275c45e9fcb3f3/torchvision/models/inception.py#L64> ;
    skos:definition "**Inception-v3** is a convolutional neural network architecture from the Inception family that makes several improvements including using [Label Smoothing](https://paperswithcode.com/method/label-smoothing), Factorized 7 x 7 convolutions, and the use of an auxiliary classifer to propagate label information lower down the network (along with the use of [batch normalization](https://paperswithcode.com/method/batch-normalization) for layers in the sidehead)." ;
    skos:prefLabel "Inception-v3" .

:Inception-v3Module a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1512.00567v3> ;
    rdfs:seeAlso <https://github.com/pytorch/vision/blob/6db1569c89094cf23f3bc41f79275c45e9fcb3f3/torchvision/models/inception.py#L210> ;
    skos:definition "**Inception-v3 Module** is an image block used in the [Inception-v3](https://paperswithcode.com/method/inception-v3) architecture. This architecture is used on the coarsest (8 × 8) grids to promote high dimensional representations." ;
    skos:prefLabel "Inception-v3 Module" .

:Inception-v4 a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1602.07261v2> ;
    rdfs:seeAlso <https://github.com/kentsommer/keras-inceptionV4/blob/ef1db6f09b6511779c05fab47d374741bc89b5ee/inception_v4.py#L242> ;
    skos:definition "**Inception-v4** is a convolutional neural network architecture that builds on previous iterations of the Inception family by simplifying the architecture and using more inception modules than [Inception-v3](https://paperswithcode.com/method/inception-v3)." ;
    skos:prefLabel "Inception-v4" .

:InceptionModule a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1409.4842v1> ;
    rdfs:seeAlso <https://github.com/hskang9/Googlenet/blob/654d126b6a2cd3ac944cf5613419deb73da5311e/keras/googlenet.py#L39> ;
    skos:definition "An **Inception Module** is an image model block that aims to approximate an optimal local sparse structure in a CNN. Put simply, it allows for us to use multiple types of filter size, instead of being restricted to a single filter size, in a single image block, which we then concatenate and pass onto the next layer." ;
    skos:prefLabel "Inception Module" .

:InceptionTime a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1909.04939v3> ;
    skos:definition "" ;
    skos:prefLabel "InceptionTime" .

:Inceptionv2 a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1502.03167v3> ;
    skos:definition "**Inception v2** is the second generation of Inception convolutional neural network architectures which notably uses [batch normalization](https://paperswithcode.com/method/batch-normalization). Other changes include dropping [dropout](https://paperswithcode.com/method/dropout) and removing [local response normalization](https://paperswithcode.com/method/local-response-normalization), due to the benefits of batch normalization." ;
    skos:prefLabel "Inception v2" .

:InfoGAN a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1606.03657v1> ;
    rdfs:seeAlso <https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/infogan/infogan.py> ;
    skos:definition """**InfoGAN** is a type of generative adversarial network that modifies the [GAN](https://paperswithcode.com/method/gan) objective to\r
encourage it to learn interpretable and meaningful representations. This is done by maximizing the\r
mutual information between a fixed small subset of the GAN’s noise variables and the observations.\r
\r
Formally, InfoGAN is defined as a minimax game with a variational regularization of mutual information and the hyperparameter $\\lambda$:\r
\r
$$ \\min\\_{G, Q}\\max\\_{D}V\\_{INFOGAN}\\left(D, G, Q\\right) = V\\left(D, G\\right) - \\lambda{L}\\_{I}\\left(G, Q\\right) $$\r
\r
Where $Q$ is an auxiliary distribution that approximates the posterior $P\\left(c\\mid{x}\\right)$ - the probability of the latent code $c$ given the data $x$ - and $L\\_{I}$ is the variational lower bound of the mutual information between the latent code and the observations.\r
\r
In the practical implementation, there is another fully-connected layer to output parameters for the conditional distribution $Q$ (negligible computation ontop of regular GAN structures). Q is represented with a [softmax](https://paperswithcode.com/method/softmax) non-linearity for a categorical latent code. For a continuous latent code, the authors assume a factored Gaussian.""" ;
    skos:prefLabel "InfoGAN" .

:InfoNCE a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1807.03748v2> ;
    rdfs:seeAlso <https://github.com/jefflai108/Contrastive-Predictive-Coding-PyTorch/blob/dfe687cf463668b16b0c2e205a166dbfbc9db227/src/model/model.py#L98> ;
    skos:definition """**InfoNCE**, where NCE stands for Noise-Contrastive Estimation, is a type of contrastive loss function used for [self-supervised learning](https://paperswithcode.com/methods/category/self-supervised-learning).\r
\r
Given a set $X = ${$x\\_{1}, \\dots, x\\_{N}$} of $N$ random samples containing one positive sample from $p\\left(x\\_{t+k}|c\\_{t}\\right)$ and $N − 1$ negative samples from the 'proposal' distribution $p\\left(x\\_{t+k}\\right)$, we optimize:\r
\r
$$ \\mathcal{L}\\_{N} = - \\mathbb{E}\\_{X}\\left[\\log\\frac{f\\_{k}\\left(x\\_{t+k}, c\\_{t}\\right)}{\\sum\\_{x\\_{j}\\in{X}}f\\_{k}\\left(x\\_{j}, c\\_{t}\\right)}\\right] $$\r
\r
Optimizing this loss will result in $f\\_{k}\\left(x\\_{t+k}, c\\_{t}\\right)$ estimating the density ratio, which is:\r
\r
$$ f\\_{k}\\left(x\\_{t+k}, c\\_{t}\\right) \\propto \\frac{p\\left(x\\_{t+k}|c\\_{t}\\right)}{p\\left(x\\_{t+k}\\right)} $$""" ;
    skos:prefLabel "InfoNCE" .

:InformativeSampleMiningNetwork a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2001.01173v4> ;
    skos:definition "**Informative Sample Mining Network** is a multi-stage sample training scheme for GANs to reduce sample hardness while preserving sample informativeness. Adversarial Importance Weighting is proposed to select informative samples and assign them greater weight. The authors also propose Multi-hop Sample Training to avoid the potential problems in model training caused by sample mining. Based on the principle of divide-and-conquer, the authors produce target images by multiple hops, which means the image translation is decomposed into several separated steps." ;
    skos:prefLabel "Informative Sample Mining Network" .

:Inpainting a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1604.07379v2> ;
    skos:definition "Train a convolutional neural network to generate the contents of an arbitrary image region conditioned on its surroundings." ;
    skos:prefLabel "Inpainting" .

:InstaBoost a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1908.07801v1> ;
    rdfs:seeAlso <https://github.com/GothicAi/InstaBoost> ;
    skos:definition """**InstaBoost** is a data augmentation technique for instance segmentation that utilises existing instance mask annotations.\r
\r
Intuitively in a small neighbor area of $(x_0, y_0, 1, 0)$, the probability map $P(x, y, s, r)$ should be high-valued since images are usually continuous and redundant in pixel level. Based on this, InstaBoost is a form of augmentation where we apply object jittering that randomly samples transformation tuples from the neighboring space of identity transform $(x_0, y_0, 1, 0)$ and paste the cropped object following affine transform $\\mathbf{H}$.""" ;
    skos:prefLabel "InstaBoost" .

:Instance-LevelMetaNormalization a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1904.03516v1> ;
    rdfs:seeAlso <https://github.com/Gasoonjia/ILM-Norm/blob/efb189fadc045d5f80ef50192634d78a3443846b/lib/nn/ilm_ln.py#L6> ;
    skos:definition "**Instance-Level Meta Normalization** is a normalization method that addresses a learning-to-normalize problem. ILM-Norm learns to predict the normalization parameters via both the feature feed-forward and the gradient back-propagation paths. It uses an auto-encoder to predict the weights $\\omega$ and bias $\\beta$ as the rescaling parameters for recovering the distribution of the tensor $x$ of feature maps. Instead of using the entire feature tensor $x$ as the input for the auto-encoder, it uses the mean $\\mu$ and variance $\\gamma$ of $x$ for characterizing its statistics." ;
    skos:prefLabel "Instance-Level Meta Normalization" .

:InstanceNormalization a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1607.08022v3> ;
    rdfs:seeAlso <https://github.com/pytorch/pytorch/blob/1c5c289b6218eb1026dcb5fd9738231401cfccea/torch/nn/modules/instancenorm.py#L141> ;
    skos:definition """**Instance Normalization** (also known as contrast normalization) is a normalization layer where:\r
\r
$$\r
    y_{tijk} =  \\frac{x_{tijk} - \\mu_{ti}}{\\sqrt{\\sigma_{ti}^2 + \\epsilon}},\r
    \\quad\r
    \\mu_{ti} = \\frac{1}{HW}\\sum_{l=1}^W \\sum_{m=1}^H x_{tilm},\r
    \\quad\r
    \\sigma_{ti}^2 = \\frac{1}{HW}\\sum_{l=1}^W \\sum_{m=1}^H (x_{tilm} - \\mu_{ti})^2.\r
$$\r
\r
This prevents instance-specific mean and covariance shift simplifying the learning process. Intuitively, the normalization process allows to remove instance-specific contrast information from the content image in a task like image stylization, which simplifies generation.""" ;
    skos:prefLabel "Instance Normalization" .

:InterBERT a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2003.13198v4> ;
    skos:definition "InterBERT aims to model interaction between information flows pertaining to different modalities. This new architecture builds multi-modal interaction and preserves the independence of single modal representation. InterBERT is built with an image embedding layer, a text embedding layer, a single-stream interaction module, and a two stream extraction module. The model is pre-trained with three tasks: 1) masked segment modeling, 2) masked region modeling, and 3) image-text matching." ;
    skos:prefLabel "InterBERT" .

:InternVideo a skos:Concept ;
    skos:altLabel "InternVideo: General Video Foundation Models via Generative and Discriminative Learning" ;
    skos:definition "The foundation models have recently shown excellent performance on a variety of downstream tasks in computer vision. However, most existing vision foundation models simply focus on image-level pretraining and adpation, which are limited for dynamic and complex video-level understanding tasks. To fill the gap, we present general video foundation models, InternVideo, by taking advantage of both generative and discriminative self-supervised video learning. Specifically, InternVideo efficiently explores masked video modeling and video-language contrastive learning as the pretraining objectives, and selectively coordinates video representations of these two complementary frameworks in a learnable manner to boost various video applications. Without bells and whistles, InternVideo achieves state-of-the-art performance on 39 video datasets from extensive tasks including video action recognition/detection, video-language alignment, and open-world video applications. Especially, our methods can obtain 91.1% and 77.2% top-1 accuracy on the challenging Kinetics-400 and Something-Something V2 benchmarks, respectively. All of these results effectively show the generality of our InternVideo for video understanding. The code will be released at https://github.com/OpenGVLab/InternVideo." ;
    skos:prefLabel "InternVideo" .

:InternetExplorer a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2302.14051v2> ;
    skos:definition "Internet Explorer explores the web in a self-supervised manner to progressively find relevant examples that improve performance on a desired target dataset. It cycles between searching for images on the Internet with text queries, self-supervised training on downloaded images, determining which images were useful, and prioritizing what to search for next." ;
    skos:prefLabel "Internet Explorer" .

:InverseSquareRootSchedule a skos:Concept ;
    skos:definition """**Inverse Square Root** is a learning rate schedule 1 / $\\sqrt{\\max\\left(n, k\\right)}$ where\r
$n$ is the current training iteration and $k$ is the number of warm-up steps. This sets a constant learning rate for the first $k$ steps, then exponentially decays the learning rate until pre-training is over.""" ;
    skos:prefLabel "Inverse Square Root Schedule" .

:InvertedResidualBlock a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1801.04381v4> ;
    rdfs:seeAlso <https://github.com/pytorch/vision/blob/1aef87d01eec2c0989458387fa04baebcc86ea7b/torchvision/models/mobilenet.py#L45> ;
    skos:definition """An **Inverted Residual Block**, sometimes called an **MBConv Block**, is a type of residual block used for image models that uses an inverted structure for efficiency reasons. It was originally proposed for the [MobileNetV2](https://paperswithcode.com/method/mobilenetv2) CNN architecture. It has since been reused for several mobile-optimized CNNs.\r
\r
A traditional [Residual Block](https://paperswithcode.com/method/residual-block) has a wide -> narrow -> wide structure with the number of channels. The input has a high number of channels, which are compressed with a [1x1 convolution](https://paperswithcode.com/method/1x1-convolution). The number of channels is then increased again with a 1x1 [convolution](https://paperswithcode.com/method/convolution) so input and output can be added. \r
\r
In contrast, an Inverted Residual Block follows a narrow -> wide -> narrow approach, hence the inversion. We first widen with a 1x1 convolution, then use a 3x3 [depthwise convolution](https://paperswithcode.com/method/depthwise-convolution) (which greatly reduces the number of parameters), then we use a 1x1 convolution to reduce the number of channels so input and output can be added.""" ;
    skos:prefLabel "Inverted Residual Block" .

:Invertible1x1Convolution a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1807.03039v2> ;
    rdfs:seeAlso <https://github.com/openai/glow/blob/d3906b2f61183e2178feeb1d63dd5e5c9b78d04e/model.py#L437> ;
    skos:definition """The **Invertible 1x1 Convolution** is a type of [convolution](https://paperswithcode.com/method/convolution) used in flow-based generative models that reverses the ordering of channels. The weight matrix is initialized as a random rotation matrix. The log-determinant of an invertible 1 × 1 convolution of a $h \\times w \\times c$ tensor $h$ with $c \\times c$ weight matrix $\\mathbf{W}$ is straightforward to compute:\r
\r
$$ \\log | \\text{det}\\left(\\frac{d\\text{conv2D}\\left(\\mathbf{h};\\mathbf{W}\\right)}{d\\mathbf{h}}\\right) | = h \\cdot w \\cdot \\log | \\text{det}\\left(\\mathbf{W}\\right) | $$""" ;
    skos:prefLabel "Invertible 1x1 Convolution" .

:InvertibleNxNConvolution a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2107.01358v1> ;
    skos:definition "" ;
    skos:prefLabel "Invertible NxN Convolution" .

:Involution a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2103.06255v2> ;
    skos:definition """**Involution** is an atomic operation for deep neural networks that inverts the design principles of convolution. Involution kernels are distinct in the spatial extent but shared across channels. If involution kernels are parameterized as fixed-sized matrices like convolution kernels and updated using the back-propagation algorithm, the learned involution kernels are impeded from transferring between input images with variable resolutions. \r
\r
The authors argue for two benefits of involution over convolution: (i) involution can summarize the context in a wider spatial arrangement, thus overcome the difficulty of modeling long-range interactions well; (ii) involution can adaptively allocate the weights over different positions, so as to prioritize the most informative visual elements in the spatial domain.""" ;
    skos:prefLabel "Involution" .

:IoU-BalancedSampling a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1904.02701v1> ;
    rdfs:seeAlso <https://github.com/OceanPang/Libra_R-CNN/blob/c798fdd9b97b7b44bfdcd6b50b9f08600c6d5aca/mmdet/core/bbox/samplers/iou_balanced_neg_sampler.py#L7> ;
    skos:definition """**IoU-Balanced Sampling** is hard mining method for object detection. Suppose we need to sample $N$ negative samples from $M$ corresponding candidates. The selected probability for each sample under random sampling is:\r
\r
$$ p = \\frac{N}{M} $$\r
\r
To raise the selected probability of hard negatives, we evenly split the sampling interval into $K$ bins according to IoU. $N$ demanded negative samples are equally distributed to each bin. Then we select samples from them uniformly. Therefore, we get the selected probability under IoU-balanced sampling:\r
\r
$$ p\\_{k} = \\frac{N}{K}*\\frac{1}{M\\_{k}}\\text{ , } k\\in\\left[0, K\\right)$$\r
\r
where $M\\_{k}$ is the number of sampling candidates in the corresponding interval denoted by $k$. $K$ is set to 3 by default in our experiments.\r
\r
The sampled histogram with IoU-balanced sampling is shown by green color in the Figure to the right. The IoU-balanced sampling can guide the distribution of training samples close to the one of hard negatives.""" ;
    skos:prefLabel "IoU-Balanced Sampling" .

:IoU-Net a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1807.11590v1> ;
    skos:definition "**IoU-Net** is an object detection architecture that introduces localization confidence. IoU-Net learns to predict the IoU between each detected bounding box and the matched ground-truth. The network acquires this confidence of localization, which improves the NMS procedure by preserving accurately localized bounding boxes. Furthermore, an optimization-based bounding box refinement method is proposed, where the predicted IoU is formulated as the objective." ;
    skos:prefLabel "IoU-Net" .

:IoU-guidedNMS a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1807.11590v1> ;
    skos:definition "**IoU-guided NMS** is a type of non-maximum suppression that help to eliminate the suppression failure caused by the misleading classification confidences. This is achieved through using the predicted IoU instead of the classification confidence as the ranking keyword for bounding boxes. " ;
    skos:prefLabel "IoU-guided NMS" .

:IterInpaint a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2304.06671v2> ;
    skos:altLabel "Iterative Inpainting" ;
    skos:definition "" ;
    skos:prefLabel "IterInpaint" .

:JLA a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2108.10543v1> ;
    skos:altLabel "Joint Learning Architecture" ;
    skos:definition "**JLA**, or **Joint Learning Architecture**, is an approach for multiple object tracking and trajectory forecasting. It jointly trains a tracking and trajectory forecasting model, and the trajectory forecasts are used for short-term motion estimates in lieu of linear motion prediction methods such as the Kalman filter. It uses a [FairMOT](https://paperswithcode.com/method/fairmot) model as the base model because this architecture already performs detection and tracking. A forecasting branch is added to the network and is trained end-to-end. [FairMOT](https://paperswithcode.com/method/fairmot) consist of a backbone network utilizing [Deep Layer Aggregation](https://www.paperswithcode.com/method/dla), an object detection head, and a reID head." ;
    skos:prefLabel "JLA" .

:Jigsaw a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1603.09246v3> ;
    skos:definition "**Jigsaw** is a self-supervision approach that relies on jigsaw-like puzzles as the pretext task in order to learn image representations." ;
    skos:prefLabel "Jigsaw" .

:Jukebox a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2005.00341v1> ;
    skos:definition """**Jukebox** is a model that generates music with singing in the raw audio domain. It tackles the long context of raw audio using a multi-scale [VQ-VAE](https://paperswithcode.com/method/vq-vae) to compress it to discrete codes, and modeling those using [autoregressive Transformers](https://paperswithcode.com/methods/category/autoregressive-transformers). It can condition on artist and genre to steer the musical and vocal style, and on unaligned lyrics to make the singing more controllable.\r
\r
Three separate VQ-VAE models are trained with different temporal resolutions. At each level, the input audio is segmented and encoded into latent vectors $\\mathbf{h}\\_{t}$, which are then quantized to the closest codebook vectors $\\mathbf{e}\\_{z\\_{t}}$. The code $z\\_{t}$ is a discrete representation of the audio that we later train our prior on. The decoder takes the sequence of codebook vectors and reconstructs the audio. The top level learns the highest degree of abstraction, since it is encoding longer audio per token while keeping the codebook size the same. Audio can be reconstructed using the codes at any one of the abstraction levels, where the least abstract bottom-level codes result in the highest-quality audio.""" ;
    skos:prefLabel "Jukebox" .

:K-MaximalWordAllocation a skos:Concept ;
    dcterms:source <https://aclanthology.org/2022.fnp-1.9> ;
    skos:definition "" ;
    skos:prefLabel "K-Maximal Word Allocation" .

:K-Net a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2106.14855v2> ;
    rdfs:seeAlso <https://github.com/ZwwWayne/K-Net> ;
    skos:definition """**K-Net** is a framework for unified semantic and instance segmentation that segments both instances and semantic categories consistently by a group of learnable kernels, where each kernel is responsible for generating a mask for either a potential instance or a stuff class. It begins with a set of kernels that are randomly initialized, and learns the kernels in accordance to the segmentation targets at hand, namely, semantic kernels for semantic categories and instance kernels for instance identities. A simple combination of semantic kernels and instance kernels allows panoptic segmentation naturally. In the forward pass, the kernels perform [convolution](https://paperswithcode.com/method/convolution) on the image features to obtain the corresponding segmentation predictions.\r
\r
K-Net is formulated so that it dynamically updates the kernels to make them conditional to their activations on the image. Such a content-aware mechanism is crucial to ensure that each kernel, especially an instance kernel, responds accurately to varying objects in an image. Through applying this adaptive kernel update strategy iteratively, K-Net significantly improves the discriminative ability of the kernels and boosts the final segmentation performance. It is noteworthy that this strategy universally applies to kernels for all the segmentation tasks.\r
\r
It also utilises a bipartite matching strategy to assign learning targets for each kernel. This training approach is advantageous to conventional training strategies as it builds a one-to-one mapping between kernels and instances in an image. It thus resolves the problem of dealing with a varying number of instances in an image. In addition, it is purely mask-driven without involving boxes. Hence, K-Net is naturally [NMS](https://paperswithcode.com/method/non-maximum-suppression)-free and box-free, which is appealing to real-time applications.""" ;
    skos:prefLabel "K-Net" .

:K3M a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2109.00895v1> ;
    skos:definition "**K3M** is a multi-modal pretraining method for e-commerce product data that introduces knowledge modality to correct the noise and supplement the missing of image and text modalities. The modal-encoding layer extracts the features of each modality. The modal-interaction layer is capable of effectively modeling the interaction of multiple modalities, where an initial-interactive feature fusion model is designed to maintain the independence of image modality and text modality, and a structure aggregation module is designed to fuse the information of image, text, and knowledge modalities. K3M is pre-trained with three pretraining tasks, including masked object modeling (MOM), masked language modeling (MLM), and link prediction modeling ([LPM](https://paperswithcode.com/method/local-prior-matching))." ;
    skos:prefLabel "K3M" .

:KAF a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1707.04035v2> ;
    rdfs:seeAlso <https://github.com/ispamm/kernel-activation-functions/blob/e525f7e82b54508ac262f82c9557ad66be2732f3/pytorch/kafnets.py#L11> ;
    skos:altLabel "Kernel Activation Function" ;
    skos:definition """A **Kernel Activation Function** is a non-parametric activation function defined as a one-dimensional kernel approximator:\r
\r
$$ f(s) = \\sum_{i=1}^D \\alpha_i \\kappa( s, d_i) $$\r
\r
where:\r
\r
1. The dictionary of the kernel elements $d_0, \\ldots, d_D$ is fixed by sampling the $x$-axis with a uniform step around 0.\r
2. The user selects the kernel function (e.g., Gaussian, [ReLU](https://paperswithcode.com/method/relu), [Softplus](https://paperswithcode.com/method/softplus)) and the number of kernel elements $D$ as a hyper-parameter. A larger dictionary leads to more expressive activation functions and a larger number of trainable parameters.\r
3. The linear coefficients are adapted independently at every neuron via standard back-propagation.\r
\r
In addition, the linear coefficients can be initialized using kernel ridge regression to behave similarly to a known function in the beginning of the optimization process.""" ;
    skos:prefLabel "KAF" .

:KE-MLM a skos:Concept ;
    dcterms:source <https://aclanthology.org/2021.naacl-main.376> ;
    skos:altLabel "Knowledge Enhanced Masked Language Model" ;
    skos:definition "" ;
    skos:prefLabel "KE-MLM" .

:KGRefiner a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2106.14233v2> ;
    skos:altLabel "Knowledge Graph Refiner" ;
    skos:definition "" ;
    skos:prefLabel "KGRefiner" .

:KIP a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2011.00050v3> ;
    skos:altLabel "Kernel Inducing Points" ;
    skos:definition "**Kernel Inducing Points**, or **KIP**, is a meta-learning algorithm for learning datasets that can mitigate the challenges which occur for naturally occurring datasets without a significant sacrifice in performance. KIP uses kernel-ridge regression to learn $\\epsilon$-approximate datasets. It can be regarded as an adaption of the inducing point method for Gaussian processes to the case of Kernel Ridge Regression." ;
    skos:prefLabel "KIP" .

:KNNandIOUbasedverification a skos:Concept ;
    dcterms:source <https://ieeexplore.ieee.org/abstract/document/8822896> ;
    skos:definition "**KNN and IoU-based Verification** is used to verify detections and choose between multiple detections of the same underlying object. It was originally used within the context of blood cell counting in medical images. To avoid this double counting problem, the KNN algorithm is applied in each platelet to determine its closest platelet and then using the intersection of union (IOU) between two platelets we calculate their extent of overlap. The authors allow 10% of the overlap between platelet and its closest platelet based on empirical observations. If the overlap is larger than that, they ignore that cell as a double count to get rid of spurious counting." ;
    skos:prefLabel "KNN and IOU based verification" .

:KOVA a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2002.07171v1> ;
    skos:altLabel "Kalman Optimization for Value Approximation" ;
    skos:definition "**Kalman Optimization for Value Approximation**, or **KOVA** is a general framework for addressing uncertainties while approximating value-based functions in deep RL domains. KOVA minimizes a regularized objective function that concerns both parameter and noisy return uncertainties. It is feasible when using non-linear approximation functions as DNNs and can estimate the value in both on-policy and off-policy settings. It can be incorporated as a policy evaluation component in policy optimization algorithms." ;
    skos:prefLabel "KOVA" .

:KP a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1904.05391v5> ;
    skos:altLabel "Kollen-Pollack Learning" ;
    skos:definition "" ;
    skos:prefLabel "KP" .

:KPE a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2203.04907v2> ;
    skos:altLabel "Keypoint Pose Encoding" ;
    skos:definition "" ;
    skos:prefLabel "KPE" .

:KaimingInitialization a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1502.01852v1> ;
    rdfs:seeAlso <https://github.com/pytorch/pytorch/blob/0adb5843766092fba584791af76383125fd0d01c/torch/nn/init.py#L389> ;
    skos:definition """**Kaiming Initialization**, or **He Initialization**, is an initialization method for neural networks that takes into account the non-linearity of activation functions, such as [ReLU](https://paperswithcode.com/method/relu) activations.\r
\r
A proper initialization method should avoid reducing or magnifying the magnitudes of input signals exponentially. Using a derivation they work out that the condition to stop this happening is:\r
\r
$$\\frac{1}{2}n\\_{l}\\text{Var}\\left[w\\_{l}\\right] = 1 $$\r
\r
This implies an initialization scheme of:\r
\r
$$ w\\_{l} \\sim \\mathcal{N}\\left(0,  2/n\\_{l}\\right)$$\r
\r
That is, a zero-centered Gaussian with standard deviation of $\\sqrt{2/{n}\\_{l}}$ (variance shown in equation above). Biases are initialized at $0$.""" ;
    skos:prefLabel "Kaiming Initialization" .

:Kaleido-BERT a skos:Concept ;
    rdfs:seeAlso <https://github.com/mczhuge/Kaleido-BERT> ;
    skos:definition "**Kaleido-BERT**(CVPR2021) is the pioneering work that focus on solving PTM in e-commerce field. It achieves SOTA performances compared with many models published in general domain." ;
    skos:prefLabel "Kaleido-BERT" .

:KnowPrompt a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2104.07650v7> ;
    skos:definition "**KnowPrompt** is a prompt-tuning approach for relational understanding. It injects entity and relation knowledge into prompt construction with learnable virtual template words as well as answer words and synergistically optimize their representation with knowledge constraints. To be specific, TYPED MARKER is utilized around entities initialized with aggregated entity-type embeddings as learnable virtual template words to inject entity type knowledge. The average embeddings of each token are leveraged in relation labels as virtual answer words to inject relation knowledge. Since there exist implicit structural constraints among entities and relations, and virtual words should be consistent with the surrounding contexts, synergistic optimization is introduced to obtain optimized virtual templates and answer words. Concretely, a context-aware prompt calibration method is used with implicit structural constraints to inject structural knowledge implications among relational triples and associate prompt embeddings with each other." ;
    skos:prefLabel "KnowPrompt" .

:KnowledgeDistillation a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1503.02531v1> ;
    skos:definition """A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.\r
Source: [Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531)""" ;
    skos:prefLabel "Knowledge Distillation" .

:KungFu a skos:Concept ;
    skos:definition """**KungFu** is a distributed ML library for TensorFlow that is designed to enable adaptive training. KungFu allows users to express high-level Adaptation Policies (APs) that describe how to change hyper- and system parameters during training. APs take real-time monitored metrics (e.g. signal-to-noise ratios and noise scale) as input and trigger control actions (e.g. cluster rescaling or synchronisation strategy updates). For execution, APs are translated into monitoring and control operators, which are embedded in the dataflow graph. APs exploit an efficient asynchronous collective communication layer, which ensures concurrency and consistency\r
of monitoring and adaptation operations.""" ;
    skos:prefLabel "KungFu" .

:L-GCN a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2104.05089v2> ;
    skos:altLabel "Learnable adjacency matrix GCN" ;
    skos:definition "Graph structure is learnable" ;
    skos:prefLabel "L-GCN" .

:L1Regularization a skos:Concept ;
    skos:definition """**$L_{1}$ Regularization** is a regularization technique applied to the weights of a neural network. We minimize a loss function compromising both the primary loss function and a penalty on the $L\\_{1}$ Norm of the weights:\r
\r
$$L\\_{new}\\left(w\\right) = L\\_{original}\\left(w\\right) + \\lambda{||w||}\\_{1}$$\r
\r
where $\\lambda$ is a value determining the strength of the penalty. In contrast to [weight decay](https://paperswithcode.com/method/weight-decay), $L_{1}$ regularization promotes sparsity; i.e. some parameters have an optimal value of zero.\r
\r
Image Source: [Wikipedia](https://en.wikipedia.org/wiki/Regularization_(mathematics)#/media/File:Sparsityl1.png)""" ;
    skos:prefLabel "L1 Regularization" .

:L2M a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2007.10791v3> ;
    skos:altLabel "Learning to Match" ;
    skos:definition "**L2M** is a learning algorithm that can work for most cross-domain distribution matching tasks. It automatically learns the cross-domain distribution matching without relying on hand-crafted priors on the matching loss. Instead, L2M reduces the inductive bias by using a meta-network to learn the distribution matching loss in a data-driven way." ;
    skos:prefLabel "L2M" .

:LAMA a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1912.00835v2> ;
    rdfs:seeAlso <https://github.com/JohnGiorgi/compact-multi-head-self-attention-pytorch/blob/56358eaddde176d539916896d6836c00d1dc5f0c/modules/lama.py#L10> ;
    skos:altLabel "Low-Rank Factorization-based Multi-Head Attention" ;
    skos:definition "**Low-Rank Factorization-based Multi-head Attention Mechanism**, or **LAMA**, is a type of attention module that uses low-rank factorization to reduce computational complexity. It uses low-rank bilinear pooling to construct a structured sentence representation that attends to multiple aspects of a sentence." ;
    skos:prefLabel "LAMA" .

:LAMB a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1904.00962v5> ;
    rdfs:seeAlso <https://github.com/cybertronai/pytorch-lamb/blob/5ef3ebd5e32f7a7bdcddbb2ce55879bfa88f6a5f/pytorch_lamb/lamb.py#L24> ;
    skos:definition """**LAMB** is a a layerwise adaptive large batch optimization technique. It provides a strategy for adapting the learning rate in large batch settings. LAMB uses [Adam](https://paperswithcode.com/method/adam) as the base algorithm and then forms an update as:\r
\r
$$r\\_{t} = \\frac{m\\_{t}}{\\sqrt{v\\_{t}} + \\epsilon}$$\r
$$x\\_{t+1}^{\\left(i\\right)} = x\\_{t}^{\\left(i\\right)}  - \\eta\\_{t}\\frac{\\phi\\left(|| x\\_{t}^{\\left(i\\right)} ||\\right)}{|| m\\_{t}^{\\left(i\\right)} || }\\left(r\\_{t}^{\\left(i\\right)}+\\lambda{x\\_{t}^{\\left(i\\right)}}\\right) $$\r
\r
Unlike [LARS](https://paperswithcode.com/method/lars), the adaptivity of LAMB is two-fold: (i) per dimension normalization with respect to the square root of the second moment used in Adam and (ii) layerwise normalization obtained due to layerwise adaptivity.""" ;
    skos:prefLabel "LAMB" .

:LAPGAN a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1506.05751v1> ;
    rdfs:seeAlso <https://github.com/AaronYALai/Generative_Adversarial_Networks_PyTorch/blob/master/LAPGAN/LAPGAN.py> ;
    skos:definition """A **LAPGAN**, or **Laplacian Generative Adversarial Network**, is a type of generative adversarial network that has a [Laplacian pyramid](https://paperswithcode.com/method/laplacian-pyramid) representation. In the sampling procedure following training, we have a set of generative convnet models {$G\\_{0}, \\dots , G\\_{K}$}, each of which captures the distribution of coefficients $h\\_{k}$ for natural images at a different level of the Laplacian pyramid. Sampling an image is akin to a reconstruction procedure, except that the generative\r
models are used to produce the $h\\_{k}$’s:\r
\r
$$ \\tilde{I}\\_{k} = u\\left(\\tilde{I}\\_{k+1}\\right) + \\tilde{h}\\_{k} = u\\left(\\tilde{I}\\_{k+1}\\right) + G\\_{k}\\left(z\\_{k}, u\\left(\\tilde{I}\\_{k+1}\\right)\\right)$$\r
\r
The recurrence starts by setting $\\tilde{I}\\_{K+1} = 0$ and using the model at the final level $G\\_{K}$ to generate a residual image $\\tilde{I}\\_{K}$ using noise vector $z\\_{K}$: $\\tilde{I}\\_{K} = G\\_{K}\\left(z\\_{K}\\right)$. Models at all levels except the final are conditional generative models that take an upsampled version of the current image $\\tilde{I}\\_{k+1}$ as a conditioning variable, in addition to the noise vector $z\\_{k}$.\r
\r
The generative models {$G\\_{0}, \\dots, G\\_{K}$} are trained using the CGAN approach at each level of the pyramid. Specifically, we construct a Laplacian pyramid from each training image $I$. At each level we make a stochastic choice (with equal probability) to either (i) construct the coefficients $h\\_{k}$ either using the standard Laplacian pyramid coefficient generation procedure or (ii) generate them using $G\\_{k}:\r
\r
$$ \\tilde{h}\\_{k} = G\\_{k}\\left(z\\_{k}, u\\left(I\\_{k+1}\\right)\\right) $$\r
\r
Here $G\\_{k}$ is a convnet which uses a coarse scale version of the image $l\\_{k} = u\\left(I\\_{k+1}\\right)$ as an input, as well as noise vector $z\\_{k}$. $D\\_{k}$ takes as input $h\\_{k}$ or $\\tilde{h}\\_{k}$, along with the low-pass image $l\\_{k}$ (which is explicitly added to $h\\_{k}$ or $\\tilde{h}\\_{k}$ before the first [convolution](https://paperswithcode.com/method/convolution) layer), and predicts if the image was real or\r
generated. At the final scale of the pyramid, the low frequency residual is sufficiently small that it\r
can be directly modeled with a standard [GAN](https://paperswithcode.com/method/gan): $\\tilde{h}\\_{K} = G\\_{K}\\left(z\\_{K}\\right)$ and $D\\_{K}$ only has $h\\_{K}$ or $\\tilde{h}\\_{K}$ as input.\r
\r
Breaking the generation into successive refinements is the key idea. We give up any “global” notion of fidelity; an attempt is never made to train a network to discriminate between the output of a cascade and a real image and instead the focus is on making each step plausible.""" ;
    skos:prefLabel "LAPGAN" .

:LARS a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1708.03888v3> ;
    rdfs:seeAlso <https://github.com/kakaobrain/torchlars/blob/3b3d7e9c7bd35a31b2c2fa6f213beb0bf6881892/torchlars/lars.py#L11> ;
    skos:definition """**Layer-wise Adaptive Rate Scaling**, or **LARS**, is a large batch optimization technique.  There are two notable differences between LARS and other adaptive algorithms such as [Adam](https://paperswithcode.com/method/adam) or [RMSProp](https://paperswithcode.com/method/rmsprop): first, LARS uses a separate learning rate for each layer and not for each weight. And second, the magnitude of the update is controlled with respect to the weight norm for better control of training speed.\r
\r
$$m\\_{t} = \\beta\\_{1}m\\_{t-1} + \\left(1-\\beta\\_{1}\\right)\\left(g\\_{t} + \\lambda{x\\_{t}}\\right)$$\r
$$x\\_{t+1}^{\\left(i\\right)} = x\\_{t}^{\\left(i\\right)}  - \\eta\\_{t}\\frac{\\phi\\left(|| x\\_{t}^{\\left(i\\right)} ||\\right)}{|| m\\_{t}^{\\left(i\\right)} || }m\\_{t}^{\\left(i\\right)} $$""" ;
    skos:prefLabel "LARS" .

:LCC a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1804.04368v3> ;
    skos:altLabel "Lipschitz Constant Constraint" ;
    skos:definition "Please enter a description about the method here" ;
    skos:prefLabel "LCC" .

:LFME a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2001.01536v3> ;
    skos:altLabel "Learning From Multiple Experts" ;
    skos:definition "**Learning From Multiple Experts** is a self-paced knowledge distillation framework that aggregates the knowledge from multiple 'Experts' to learn a unified student model. Specifically, the proposed framework involves two levels of adaptive learning schedules: Self-paced Expert Selection and Curriculum Instance Selection, so that the knowledge is adaptively transferred to the 'Student'. The self-paced expert selection automatically controls the impact of knowledge distillation from each expert, so that the learned student model will gradually acquire the knowledge from the experts, and finally exceed the expert. The curriculum instance selection, on the other hand, designs a curriculum for the unified model where the training samples are organized from easy to hard, so that the unified student model will receive a less challenging learning schedule, and gradually learns from easy to hard samples." ;
    skos:prefLabel "LFME" .

:LFPNet\(TTA\) a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2003.07711v1> ;
    skos:altLabel "LFPNet with test time augmentation" ;
    skos:definition "" ;
    skos:prefLabel "LFPNet (TTA)" .

:LGCL a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1808.03965v1> ;
    skos:altLabel "Learnable graph convolutional layer" ;
    skos:definition """Learnable graph convolutional layer (LGCL) automatically selects a fixed number of neighboring nodes for each feature based on value ranking in order to transform graph data into grid-like structures in 1-D format, thereby enabling the use of regular convolutional operations on generic graphs.\r
\r
Description and image from: [Large-Scale Learnable Graph Convolutional Networks](https://arxiv.org/pdf/1808.03965.pdf)""" ;
    skos:prefLabel "LGCL" .

:LIME a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1602.04938v3> ;
    rdfs:seeAlso <https://github.com/marcotcr/lime> ;
    skos:altLabel "Local Interpretable Model-Agnostic Explanations" ;
    skos:definition """**LIME**, or **Local Interpretable Model-Agnostic Explanations**, is an algorithm that can explain the predictions of any classifier or regressor in a faithful way, by approximating it locally with an interpretable model. It modifies a single data sample by tweaking the feature values and observes the resulting impact on the output. It performs the role of an "explainer" to explain predictions from each data sample. The output of LIME is a set of explanations representing the contribution of each feature to a prediction for a single sample, which is a form of local interpretability.\r
\r
Interpretable models in LIME can be, for instance, [linear regression](https://paperswithcode.com/method/linear-regression) or decision trees, which are trained on small perturbations (e.g. adding noise, removing words, hiding parts of the image) of the original model to provide a good local approximation.""" ;
    skos:prefLabel "LIME" .

:LIMix a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2108.12278v1> ;
    skos:altLabel "Lifelong Infinite Mixture" ;
    skos:definition "**LIMix**, or **Lifelong Infinite Mixture**, is a lifelong learning model which grows a mixture of models to adapt to an increasing number of tasks.  LIMix can automatically expand its network architectures or choose an appropriate component to adapt its parameters for learning a new task, while preserving its previously learnt information. Knowledge is incorporated by means of Dirichlet processes by using a gating mechanism which computes the dependence between the knowledge learnt previously and stored in each component, and a new set of data. Besides, a Student model is trained which can accumulate cross-domain representations over time and make quick inferences." ;
    skos:prefLabel "LIMix" .

:LINE a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1503.03578v1> ;
    skos:altLabel "Large-scale Information Network Embedding" ;
    skos:definition """LINE is a novel network embedding method which is suitable for arbitrary types of information networks: undirected, directed, and/or weighted. The method optimizes a carefully designed objective function that preserves both the local and global network structures.\r
\r
Source: [Tang et al.](https://arxiv.org/pdf/1503.03578v1.pdf)\r
\r
Image source: [Tang et al.](https://arxiv.org/pdf/1503.03578v1.pdf)""" ;
    skos:prefLabel "LINE" .

:LLaMA a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2302.13971v1> ;
    skos:definition """**LLaMA** is a collection of foundation language models ranging from 7B to 65B parameters. It is based on the transformer architecture with various improvements that were subsequently proposed. The main difference with the original architecture are listed below.\r
\r
- RMSNorm normalizing function is used to improve the training stability, by normalizing the input of each transformer sub-layer, instead of normalizing the output.\r
- The ReLU non-linearity is replaced by the SwiGLU activation function to improve performance.\r
- Absolute positional embeddings are removed and instead rotary positional embeddings (RoPE) are added at each layer of the network.""" ;
    skos:prefLabel "LLaMA" .

:LMOT a skos:Concept ;
    rdfs:seeAlso <https://github.com/RanaMostafaAbdElMohsen/LMOT> ;
    skos:altLabel "LMOT: Efficient Light-Weight Detection and Tracking in Crowds" ;
    skos:definition """Rana Mostafa, Hoda Baraka and AbdelMoniem Bayoumi\r
\r
**LMOT**, i.e., Light-weight Multi-Object Tracker,  performs joint pedestrian detection and tracking. LMOT introduces a simplified DLA-34 encoder network to extract detection features for the current image that are computationally efficient. Furthermore, we generate efficient tracking features using a linear transformer for the prior image frame and its corresponding detection heatmap. After that, LMOT fuses both detection and tracking feature maps in a multi-layer scheme and performs a two-stage online data association relying on the Kalman filter to generate tracklets. We evaluated our model on the challenging real-world MOT16/17/20 datasets, showing LMOT significantly outperforms the state-of-the-art trackers concerning runtime while maintaining high robustness. LMOT is approximately ten times faster than state-of-the-art trackers while being only 3.8% behind in performance accuracy on average leading to a much computationally lighter model.\r
\r
Code: https://github.com/RanaMostafaAbdElMohsen/LMOT\r
Paper: https://doi.org/10.1109/ACCESS.2022.3197157""" ;
    skos:prefLabel "LMOT" .

:LMU a skos:Concept ;
    dcterms:source <http://papers.nips.cc/paper/9689-legendre-memory-units-continuous-time-representation-in-recurrent-neural-networks> ;
    skos:altLabel "Legendre Memory Unit" ;
    skos:definition """The Legendre Memory Unit (LMU) is mathematically derived to orthogonalize\r
its continuous-time history – doing so by solving d coupled ordinary differential\r
equations (ODEs), whose phase space linearly maps onto sliding windows of\r
time via the Legendre polynomials up to degree d-1.  It is optimal for compressing temporal information.\r
\r
See paper for equations (markdown isn't working).\r
\r
Official github repo: [https://github.com/abr/lmu](https://github.com/abr/lmu)""" ;
    skos:prefLabel "LMU" .

:LOGAN a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1912.00953v2> ;
    rdfs:seeAlso <https://github.com/rafaelbidese/LOGAN> ;
    skos:definition """**LOGAN** is a generative adversarial network that uses a latent optimization approach using [natural gradient descent](https://paperswithcode.com/method/natural-gradient-descent) (NGD). For the Fisher matrix in NGD, the authors use the empirical Fisher $F'$ with Tikhonov damping:\r
\r
$$ F' = g \\cdot g^{T} + \\beta{I} $$\r
\r
They also use Euclidian Norm regularization for the optimization step.\r
\r
For LOGAN's base architecture, [BigGAN-deep](https://paperswithcode.com/method/biggan-deep) is used with a few modifications: increasing the size of the latent source from $186$ to $256$, to compensate the randomness of the source lost\r
when optimising $z$. 2, using the uniform distribution $U\\left(−1, 1\\right)$ instead of the standard normal distribution $N\\left(0, 1\\right)$ for $p\\left(z\\right)$ to be consistent with the clipping operation, using  leaky [ReLU](https://paperswithcode.com/method/relu) (with the slope of 0.2 for the negative part) instead of ReLU as the non-linearity for smoother gradient flow for $\\frac{\\delta{f}\\left(z\\right)}{\\delta{z}}$ .""" ;
    skos:prefLabel "LOGAN" .

:LPM a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2002.10336v1> ;
    skos:altLabel "Local Prior Matching" ;
    skos:definition "**Local Prior Matching** is a semi-supervised objective for speech recognition that distills knowledge from a strong prior (e.g. a language model) to provide learning signal to a discriminative model trained on unlabeled speech. The LPM objective minimizes the cross entropy between the local prior and the model distribution, and is minimized when $q\\_{y\\mid{x}} = \\bar{p}\\_{y\\mid{x}}$. Intuitively, LPM encourages the ASR model to assign posterior probabilities proportional to the linguistic probabilities of the proposed hypotheses." ;
    skos:prefLabel "LPM" .

:LR-Net a skos:Concept ;
    rdfs:seeAlso <https://github.com/gan3sh500/local-relational-nets/blob/master/lrnet.ipynb> ;
    skos:definition "An **LR-Net** is a type of non-convolutional neural network that utilises local relation layers instead of convolutions for image feature extraction. Otherwise, the architecture follows the same design as a [ResNet](https://paperswithcode.com/method/resnet)." ;
    skos:prefLabel "LR-Net" .

:LRNet a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1904.11491v1> ;
    skos:altLabel "Local Relation Network" ;
    skos:definition "The **Local Relation Network** (**LR-Net**) is a network built with local relation layers which represent a feature image extractor. This feature extractor adaptively determines aggregation weights based on the compositional relationship of local pixel pairs." ;
    skos:prefLabel "LRNet" .

:LSGAN a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1611.04076v3> ;
    rdfs:seeAlso <https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/lsgan/lsgan.py> ;
    skos:definition """**LSGAN**, or **Least Squares GAN**, is a type of generative adversarial network that adopts the least squares loss function for the discriminator. Minimizing the objective function of LSGAN yields minimizing the Pearson $\\chi^{2}$ divergence. The objective function can be defined as:\r
\r
$$ \\min\\_{D}V\\_{LSGAN}\\left(D\\right) = \\frac{1}{2}\\mathbb{E}\\_{\\mathbf{x} \\sim p\\_{data}\\left(\\mathbf{x}\\right)}\\left[\\left(D\\left(\\mathbf{x}\\right) - b\\right)^{2}\\right] + \\frac{1}{2}\\mathbb{E}\\_{\\mathbf{z}\\sim p\\_{\\mathbf{z}}\\left(\\mathbf{z}\\right)}\\left[\\left(D\\left(G\\left(\\mathbf{z}\\right)\\right) - a\\right)^{2}\\right] $$\r
\r
$$ \\min\\_{G}V\\_{LSGAN}\\left(G\\right) = \\frac{1}{2}\\mathbb{E}\\_{\\mathbf{z} \\sim p\\_{\\mathbf{z}}\\left(\\mathbf{z}\\right)}\\left[\\left(D\\left(G\\left(\\mathbf{z}\\right)\\right) - c\\right)^{2}\\right] $$\r
\r
where $a$ and $b$ are the labels for fake data and real data and $c$ denotes the value that $G$ wants $D$ to believe for fake data.""" ;
    skos:prefLabel "LSGAN" .

:LSHAttention a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2001.04451v2> ;
    rdfs:seeAlso <https://github.com/google/trax/blob/af3a38917bd1bc69cf5d25ce007e16185f22f050/trax/layers/research/efficient_attention.py#L1084> ;
    skos:altLabel "Locality Sensitive Hashing Attention" ;
    skos:definition "**LSH Attention**, or **Locality Sensitive Hashing Attention** is a replacement for [dot-product attention](https://paperswithcode.com/method/scaled) with one that uses locality-sensitive hashing, changing its complexity from O($L^2$) to O($L\\log L$), where $L$ is the length of the sequence. LSH refers to a family of functions (known as LSH families) to hash data points into buckets so that data points near each other are located in the same buckets with high probability, while data points far from each other are likely to be in different buckets. It was proposed as part of the [Reformer](https://paperswithcode.com/method/reformer) architecture." ;
    skos:prefLabel "LSH Attention" .

:LSUVInitialization a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1511.06422v7> ;
    skos:altLabel "Layer-Sequential Unit-Variance Initialization" ;
    skos:definition """**Layer-Sequential Unit-Variance Initialization** (**LSUV**) is a simple method for weight initialization for deep net learning. The initialization strategy involves the following two step:\r
\r
1) First, pre-initialize weights of each [convolution](https://paperswithcode.com/method/convolution) or inner-product layer with\r
orthonormal matrices. \r
\r
2) Second, proceed from the first to the final layer, normalizing the variance of the output of each layer to be equal to one.""" ;
    skos:prefLabel "LSUV Initialization" .

:LTLS a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1611.01964v1> ;
    rdfs:seeAlso <https://github.com/kjasinska/ltls/blob/bf542bf2791352bc9e75a70f50b7156f94bf6ecc/ltls.py#L15> ;
    skos:altLabel "Log-time and Log-space Extreme Classification" ;
    skos:definition "**LTLS** is a technique for multiclass and multilabel prediction that can perform training and inference in logarithmic time and space. LTLS embeds large classification problems into simple structured prediction problems and relies on efficient dynamic programming algorithms for inference. It tackles extreme multi-class and multi-label classification problems where the size $C$ of the output space is extremely large." ;
    skos:prefLabel "LTLS" .

:LV-ViT a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2104.10858v3> ;
    skos:definition """**LV-ViT** is a type of [vision transformer](https://paperswithcode.com/method/vision-transformer) that uses token labelling as a training objective. Different from the standard training\r
objective of ViTs that computes the classification loss on an additional trainable class token, token labelling takes advantage of all the image patch tokens to compute the training loss in a dense manner. Specifically, token labeling reformulates the image classification problem into multiple token-level recognition problems and assigns each patch token with an individual location-specific supervision generated by a machine annotator.""" ;
    skos:prefLabel "LV-ViT" .

:LXMERT a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1908.07490v3> ;
    skos:altLabel "Learning Cross-Modality Encoder Representations from Transformers" ;
    skos:definition "LXMERT is a model for learning vision-and-language cross-modality representations. It consists of a Transformer model that consists three encoders: object relationship encoder, a language encoder, and a cross-modality encoder. The model takes two inputs: image with its related sentence. The images are represented as a sequence of objects, whereas each sentence is represented as sequence of words. By combining the self-attention and cross-attention layers the model is able to generated language representation, image representations, and cross-modality representations from the input. The model is pre-trained with image-sentence pairs via five pre-training tasks: masked language modeling, masked object prediction, cross-modality matching, and image questions answering. These tasks help the model to learn both intra-modality and cross-modality relationships." ;
    skos:prefLabel "LXMERT" .

:LabelQualityModel a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2107.11413v4> ;
    skos:definition "**Label Quality Model** is an intermediate supervised task aimed at predicting the clean labels from noisy labels by leveraging rater features and a paired subset for supervision. The LQM technique assumes the existence of rater features and a subset of training data with both noisy and clean labels, which we call paired-subset. In real world scenarios, some level of label noise may be unavoidable. The LQM approach still works as long as the clean(er) label is less noisy than a label from a rater that is randomly selected from the pool, e.g., clean labels can be from either expert raters or aggregation of multiple raters. LQM is trained on the paired-subset using rater features and noisy label as input, and inferred on the entire training corpus. The output of LQM is used during model training as a more accurate alternative to the noisy labels." ;
    skos:prefLabel "Label Quality Model" .

:LabelSmoothing a skos:Concept ;
    skos:definition """**Label Smoothing** is a regularization technique that introduces noise for the labels. This accounts for the fact that datasets may have mistakes in them, so maximizing the likelihood of $\\log{p}\\left(y\\mid{x}\\right)$ directly can be harmful. Assume for a small constant $\\epsilon$, the training set label $y$ is correct with probability $1-\\epsilon$ and incorrect otherwise. Label Smoothing regularizes a model based on a [softmax](https://paperswithcode.com/method/softmax) with $k$ output values by replacing the hard $0$ and $1$ classification targets with targets of $\\frac{\\epsilon}{k-1}$ and $1-\\epsilon$ respectively.\r
\r
Source: Deep Learning, Goodfellow et al\r
\r
Image Source: [When Does Label Smoothing Help?](https://arxiv.org/abs/1906.02629)""" ;
    skos:prefLabel "Label Smoothing" .

:LambdaLayer a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2102.08602v1> ;
    rdfs:seeAlso <https://github.com/lucidrains/lambda-networks/blob/8116ff3685c679efaaca4137fc0f5d7cc88b5e4b/lambda_networks/lambda_networks.py#L22> ;
    skos:definition "**Lambda layers** are a building block for modeling long-range dependencies in data. They consist of long-range interactions between a query and a structured set of context elements at a reduced memory cost. Lambda layers transform each available context into a linear function, termed a lambda, which is then directly applied to the corresponding query. Whereas self-attention defines a similarity kernel between the query and the context elements, a lambda layer instead summarizes contextual information into a fixed-size linear function (i.e. a matrix), thus bypassing the need for memory-intensive attention maps." ;
    skos:prefLabel "Lambda Layer" .

:LapEigen a skos:Concept ;
    dcterms:source <https://ieeexplore.ieee.org/abstract/document/6789755> ;
    skos:altLabel "Laplacian EigenMap" ;
    skos:definition "" ;
    skos:prefLabel "LapEigen" .

:LapStyle a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2104.05376v2> ;
    skos:altLabel "Laplacian Pyramid Network" ;
    skos:definition """**LapStyle**, or **Laplacian Pyramid Network**, is a feed-forward style transfer method. It uses a [Drafting Network](https://paperswithcode.com/method/drafting-network) to transfer global style patterns in low-resolution, and adopts higher resolution [Revision Networks](https://paperswithcode.com/method/revision-network) to revise local styles in a pyramid manner according to outputs of multi-level Laplacian filtering of the content image. Higher resolution details can be generated by stacking Revision Networks with multiple Laplacian pyramid levels. The final stylized image is obtained by aggregating outputs of all pyramid levels.\r
\r
Specifically, we first generate image pyramid $\\left\\(\\bar{x}\\_{c}, r\\_{c}\\right\\)$ from content image $x\\_{c}$ with the help of Laplacian filter. Rough low-resolution stylized image are then generated by the Drafting Network. Then the Revision Network generates stylized detail image in high resolution. Then the final stylized image is generated by aggregating the outputs pyramid. $L, C$ and $A$ in an image represent Laplacian, concatenate and aggregation operation separately.""" ;
    skos:prefLabel "LapStyle" .

:LaplacianPE a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2003.00982v5> ;
    rdfs:seeAlso <https://github.com/graphdeeplearning/benchmarking-gnns/blob/ef8bd8c7d2c87948bc1bdd44099a52036e715cd0/data/molecules.py#L147-L168> ;
    skos:altLabel "Laplacian Positional Encodings" ;
    skos:definition """[Laplacian eigenvectors](https://paperswithcode.com/paper/laplacian-eigenmaps-and-spectral-techniques) represent a natural generalization of the [Transformer](https://paperswithcode.com/method/transformer) positional encodings (PE) for graphs as the eigenvectors of a discrete line (NLP graph) are the cosine and sinusoidal functions. They help encode distance-aware information (i.e., nearby nodes have similar positional features and farther nodes have dissimilar positional features).\r
\r
Hence, Laplacian Positional Encoding (PE) is a general method to encode node positions in a graph. For each node, its Laplacian PE is the k smallest non-trivial eigenvectors.""" ;
    skos:prefLabel "Laplacian PE" .

:LaplacianPyramid a skos:Concept ;
    skos:definition """A **Laplacian Pyramid** is a linear invertible image representation consisting of a set of band-pass\r
images spaced an octave apart, plus a low-frequency residual. Formally, let $d\\left(.\\right)$ be a downsampling operation that blurs and decimates a $j \\times j$ image $I$ so that $d\\left(I\\right)$ is a new image of size $\\frac{j}{2} \\times \\frac{j}{2}$. Also, let $u\\left(.\\right)$ be an upsampling operator which smooths and expands $I$ to be twice the size, so $u\\left(I\\right)$ is a new image of size $2j \\times 2j$. We first build a Gaussian pyramid $G\\left(I\\right) = \\left[I\\_{0}, I\\_{1}, \\dots, I\\_{K}\\right]$, where\r
$I\\_{0} = I$ and $I\\_{k}$ is $k$ repeated application of $d\\left(.\\right)$ to $I$. $K$ is the number of levels in the pyramid selected so that the final level has a minimal spatial extent ($\\leq 8 \\times 8$ pixels).\r
\r
The coefficients $h\\_{k}$ at each level $k$ of the Laplacian pyramid $L\\left(I\\right)$ are constructed by taking the difference between adjacent levels in the Gaussian pyramid, upsampling the smaller one with $u\\left(.\\right)$ so that the sizes are compatible:\r
\r
$$ h\\_{k} = \\mathcal{L}\\_{k}\\left(I\\right) = G\\_{k}\\left(I\\right) − u\\left(G\\_{k+1}\\left(I\\right)\\right) = I\\_{k} − u\\left(I\\_{k+1}\\right) $$\r
\r
Intuitively, each level captures the image structure present at a particular scale. The final level of the\r
Laplacian pyramid $h\\_{K}$ is not a difference image, but a low-frequency residual equal to the final\r
Gaussian pyramid level, i.e. $h\\_{K} = I\\_{K}$. Reconstruction from a Laplacian pyramid coefficients\r
$\\left[h\\_{1}, \\dots, h\\_{K}\\right]$ is performed using the backward recurrence:\r
\r
$$ I\\_{k} = u\\left(I\\_{k+1}\\right) + h\\_{k} $$\r
\r
which is started with $I\\_{K} = h\\_{K}$ and the reconstructed image being $I = I\\_{o}$. In other words, starting at the coarsest level, we repeatedly upsample and add the difference image h at the next finer level until we return to the full-resolution image.\r
Source: [LAPGAN](https://paperswithcode.com/method/lapgan)\r
\r
Image : [Design of FIR Filters for Fast Multiscale Directional Filter Banks](https://www.researchgate.net/figure/Relationship-between-Gaussian-and-Laplacian-Pyramids_fig2_275038450)""" ;
    skos:prefLabel "Laplacian Pyramid" .

:Large-scalespectralclustering a skos:Concept ;
    dcterms:source <https://www.researchgate.net/publication/351270623_Divide-and-conquer_based_Large-Scale_Spectral_Clustering> ;
    skos:definition """# [Spectral Clustering](https://paperswithcode.com/method/spectral-clustering)\r
\r
Spectral clustering aims to partition the data points into $k$ clusters using the spectrum of the graph Laplacians \r
Given a dataset $X$ with $N$ data points, spectral clustering algorithm first constructs similarity matrix ${W}$, where ${w_{ij}}$ indicates the similarity between data points $x_i$ and $x_j$ via a similarity measure metric.\r
\r
Let $L=D-W$, where $L$ is called graph Laplacian and ${D}$ is a diagonal matrix with $d_{ii} = \\sum_ {j=1}^n w_{ij}$.\r
The objective function of spectral clustering can be formulated based on the graph Laplacian as follow:\r
\\begin{equation}\r
  \\label{eq:SC_obj}\r
  {\\max_{{U}}  \\operatorname{tr}\\left({U}^{T} {L} {U}\\right)}, \\\\ {\\text { s.t. } \\quad {U}^{T} {{U}={I}}},\r
\\end{equation}\r
where $\\operatorname{tr(\\cdot)}$ denotes the trace norm of a matrix.\r
The rows of matrix ${U}$ are the low dimensional embedding of the original data points.\r
Generally, spectral clustering computes ${U}$ as the bottom $k$ eigenvectors of ${L}$, and finally applies $k$-means on ${U}$ to obtain the clustering results.\r
\r
\r
# Large-scale Spectral Clustering\r
\r
To capture the relationship between all data points in $X$, an $N\\times N$ similarity matrix is needed to be constructed in conventional spectral clustering, which costs $O(N^2d)$ time and $O(N^2)$ memory and is not feasible for large-scale clustering tasks.\r
Instead of a full similarity matrix, many accelerated spectral clustering methods are using a similarity sub-matrix to represent each data points by the cross-similarity between data points and a set of representative data points (i.e., landmarks) via some similarity measures, as\r
\\begin{equation}\r
    \\label{eq: cross-similarity}\r
    B = \\Phi(X,R),\r
\\end{equation}\r
where $R = \\{r_1,r_2,\\dots, r_p \\}$ ($p \\ll N$) is a set of landmarks with the same dimension to $X$, $\\Phi(\\cdot)$ indicate a similarity measure metric, and $B\\in \\mathbb{R}^{N\\times p}$ is the similarity sub-matrix to represent the $X \\in \\mathbb{R}^{N\\times d}$ with respect to the $R\\in \\mathbb{R}^{p\\times d}$.\r
\r
For large-scale spectral clustering using such similarity matrix,\r
a symmetric similarity matrix $W$ can be designed as \r
\\begin{equation}\r
  \\label{eq: WusedB }\r
  W=\\left[\\begin{array}{ll}\r
      \\mathbf{0} & B         ; \\\\\r
      B^{T}      & \\mathbf{0}\r
    \\end{array}\\right].\r
\\end{equation}\r
The size of matrix $W$ is $(N+p)\\times (N+p)$. \r
Taking the advantage of the bipartite structure, some fast eigen-decomposition methods can then  be used to obtain the spectral embedding.\r
Finally, $k$-means is conducted on the embedding to obtain clustering results.\r
\r
The clustering result is directly related to the quality of $B$ that consists of the similarities between data points and landmarks.\r
Thus, the performance of landmark selection is crucial to the clustering result.""" ;
    skos:prefLabel "Large-scale spectral clustering" .

:LatentDiffusionModel a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2112.10752v2> ;
    skos:definition "Diffusion models applied to latent spaces, which are normally built with (Variational) Autoencoders." ;
    skos:prefLabel "Latent Diffusion Model" .

:LatentOptimisation a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1905.06723v2> ;
    rdfs:seeAlso <https://github.com/deepmind/deepmind-research/blob/2160fc7f174a2d0ffe79aef5ca1b59419e4fafa1/cs_gan/utils.py#L89> ;
    skos:definition """**Latent Optimisation** is a technique used for generative adversarial networks to refine the sample quality of $z$. Specifically, it exploits knowledge from the discriminator $D$ to refine the latent source $z$. Intuitively, the gradient $\\nabla\\_{z}f\\left(z\\right) = \\delta{f}\\left(z\\right)\\delta{z}$ points in the direction that better satisfies the discriminator $D$, which implies better samples. Therefore, instead of using the randomly sampled $z \\sim p\\left(z\\right)$, we uses the optimised latent:\r
\r
$$ \\Delta{z} = \\alpha\\frac{\\delta{f}\\left(z\\right)}{\\delta{z}} $$\r
\r
$$ z' = z + \\Delta{z} $$\r
\r
Source: [LOGAN](https://paperswithcode.com/method/logan)\r
.""" ;
    skos:prefLabel "Latent Optimisation" .

:LayerDrop a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1909.11556v1> ;
    rdfs:seeAlso <https://github.com/pytorch/fairseq/blob/9ebcd6554daab7bac4948de49aeb85bfff81d876/fairseq/checkpoint_utils.py#L365-L446> ;
    skos:definition "**LayerDrop** is a form of structured [dropout](https://paperswithcode.com/method/dropout) for [Transformer](https://paperswithcode.com/method/transformer) models which has a regularization effect during training and allows for efficient pruning at inference time. It randomly drops layers from the Transformer according to an \"every other\" strategy where pruning with a rate $p$ means dropping the layers at depth $d$ such that $d = 0\\left\\(\\text{mod}\\left(\\text{floor}\\left(\\frac{1}{p}\\right)\\right)\\right)$." ;
    skos:prefLabel "LayerDrop" .

:LayerNormalization a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1607.06450v1> ;
    rdfs:seeAlso <https://github.com/CyberZHG/torch-layer-normalization/blob/89f405b60f53f85da6f03fe685c190ef394ce50c/torch_layer_normalization/layer_normalization.py#L8> ;
    skos:definition """Unlike [batch normalization](https://paperswithcode.com/method/batch-normalization), **Layer Normalization** directly estimates the normalization statistics from the summed inputs to the neurons within a hidden layer so the normalization does not introduce any new dependencies between training cases. It works well for [RNNs](https://paperswithcode.com/methods/category/recurrent-neural-networks) and improves both the training time and the generalization performance of several existing RNN models. More recently, it has been used with [Transformer](https://paperswithcode.com/methods/category/transformers) models.\r
\r
We compute the layer normalization statistics over all the hidden units in the same layer as follows:\r
\r
$$ \\mu^{l} = \\frac{1}{H}\\sum^{H}\\_{i=1}a\\_{i}^{l} $$\r
\r
$$ \\sigma^{l} = \\sqrt{\\frac{1}{H}\\sum^{H}\\_{i=1}\\left(a\\_{i}^{l}-\\mu^{l}\\right)^{2}}  $$\r
\r
where $H$ denotes the number of hidden units in a layer. Under layer normalization, all the hidden units in a layer share the same normalization terms $\\mu$ and $\\sigma$, but different training cases have different normalization terms. Unlike batch normalization, layer normalization does not impose any constraint on the size of the mini-batch and it can be used in the pure online regime with batch size 1.""" ;
    skos:prefLabel "Layer Normalization" .

:LayerScale a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2103.17239v2> ;
    skos:definition """**LayerScale** is a method used for [vision transformer](https://paperswithcode.com/methods/category/vision-transformer) architectures to help improve training dynamics. It adds a learnable diagonal matrix on output of each residual block, initialized close to (but not at) 0. Adding this simple layer after each residual block improves the training dynamic, allowing for the training of deeper high-capacity image transformers that benefit from depth.\r
\r
Specifically, LayerScale is a per-channel multiplication of the vector produced by each residual block, as opposed to a single scalar, see Figure (d). The objective is to group the updates of the weights associated with the same output channel. Formally, LayerScale is a multiplication by a diagonal matrix on output of each residual block. In other words:\r
\r
$$\r
x\\_{l}^{\\prime} =x\\_{l}+\\operatorname{diag}\\left(\\lambda\\_{l, 1}, \\ldots, \\lambda\\_{l, d}\\right) \\times \\operatorname{SA}\\left(\\eta\\left(x\\_{l}\\right)\\right) \r
$$\r
\r
$$\r
x\\_{l+1} =x\\_{l}^{\\prime}+\\operatorname{diag}\\left(\\lambda\\_{l, 1}^{\\prime}, \\ldots, \\lambda\\_{l, d}^{\\prime}\\right) \\times \\operatorname{FFN}\\left(\\eta\\left(x\\_{l}^{\\prime}\\right)\\right)\r
$$\r
\r
where the parameters $\\lambda\\_{l, i}$ and $\\lambda\\_{l, i}^{\\prime}$ are learnable weights. The diagonal values are all initialized to a fixed small value $\\varepsilon:$ we set it to $\\varepsilon=0.1$ until depth 18 , $\\varepsilon=10^{-5}$ for depth 24 and $\\varepsilon=10^{-6}$ for deeper networks. \r
\r
This formula is akin to other [normalization](https://paperswithcode.com/methods/category/normalization) strategies [ActNorm](https://paperswithcode.com/method/activation-normalization) or [LayerNorm](https://paperswithcode.com/method/layer-normalization) but executed on output of the residual block. Yet LayerScale seeks a different effect: [ActNorm](https://paperswithcode.com/method/activation-normalization) is a data-dependent initialization that calibrates activations so that they have zero-mean and unit variance, like [BatchNorm](https://paperswithcode.com/method/batch-normalization). In contrast, in LayerScale, we initialize the diagonal with small values so that the initial contribution of the residual branches to the function implemented by the transformer is small. In that respect the motivation is therefore closer to that of [ReZero](https://paperswithcode.com/method/rezero), [SkipInit](https://paperswithcode.com/method/skipinit), [Fixup](https://paperswithcode.com/method/fixup-initialization) and [T-Fixup](https://paperswithcode.com/method/t-fixup): to train closer to the identity function and let the network integrate the additional parameters progressively during the training. LayerScale offers more diversity in the optimization than just adjusting the whole layer by a single learnable scalar as in [ReZero](https://paperswithcode.com/method/rezero)/[SkipInit](https://paperswithcode.com/method/skipinit), [Fixup](https://paperswithcode.com/method/fixup-initialization) and [T-Fixup](https://paperswithcode.com/method/t-fixup).""" ;
    skos:prefLabel "LayerScale" .

:LayoutLMv2 a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2012.14740v4> ;
    skos:definition """**LayoutLMv2** is an architecture and pre-training method for document understanding. The model is pre-trained with a great number of unlabeled scanned document images from the IIT-CDIP dataset, where some images in the text-image pairs are randomly replaced with another document image to make the model learn whether the image and OCR texts are correlated or not. Meanwhile, it also integrates a spatial-aware self-attention mechanism into the Transformer architecture, so that the model can fully understand the relative positional relationship among different text blocks.\r
\r
Specifically, an enhanced Transformer architecture is used, i.e. a multi-modal Transformer asisthe backbone of LayoutLMv2. The multi-modal Transformer accepts inputs of three modalities: text, image, and layout. The input of each modality is converted to an embedding sequence and fused by the encoder. The model establishes deep interactions within and between modalities by leveraging the powerful Transformer layers.""" ;
    skos:prefLabel "LayoutLMv2" .

:LayoutReader a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2108.11591v2> ;
    skos:definition """** LayoutReader** is a sequence-to-sequence model for reading order detection that uses both textual and layout information, where the layout-aware language model [LayoutLM](https://paperswithcode.com/method/layoutlmv2) is leveraged as an encoder. The generation step in the encoder-decoder structure tis modified to generate the reading order sequence.\r
\r
In the encoding stage, LayoutReader packs the pair of source and target segments into a contiguous input sequence of LayoutLM and carefully designs the [self-attention mask](https://paperswithcode.com/methods/category/factorized-attention) to control the visibility between tokens. As shown in the Figure, LayoutReader allows the tokens in the source segment to attend to each other while preventing the tokens in the target segment from attending to the rightward context. If 1 means allowing and 0 means preventing, the detail of the mask $M$ is as follows:\r
\r
$$ M\\_{i, j}= \\begin{cases}1, & \\text { if } i<j \\text { or } i, j \\in \\operatorname{src} \\\\ 0, & \\text { otherwise }\\end{cases} $$\r
\r
where $i, j$ are the indices in the packed input sequence, so they may be from source or target segments; $i, j \\in$ src means both tokens are from source segment.\r
\r
In the decoding stage, since the source and target are reordered sequences, the prediction candidates can be constrained to the source segment. Therefore, we ask the model to predict the indices in the source sequence. The probability is calculated as follows:\r
\r
$$\r
\\mathcal{P}\\left(x_{k}=i \\mid x_{<k}\\right)=\\frac{\\exp \\left(e_{i}^{T} h\\_{k}+b\\_{k}\\right)}{\\sum_{j} \\exp \\left(e\\_{j}^{T} h_{k}+b\\_{k}\\right)}\r
$$\r
\r
where $i$ is an index in the source segment; $e\\_{i}$ and $e\\_{j}$ are the $\\mathrm{i}$-th and $\\mathrm{j}$-th input embeddings of the source segment; $h\\_{k}$ is the hidden states at the $\\mathrm{k}$-th time step; $b\\_{k}$ is the bias at the $\\mathrm{k}$-th time step.""" ;
    skos:prefLabel "LayoutReader" .

:Lbl2TransformerVec a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2211.16285v2> ;
    skos:definition "" ;
    skos:prefLabel "Lbl2TransformerVec" .

:Lbl2Vec a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2210.06023v1> ;
    skos:definition "" ;
    skos:prefLabel "Lbl2Vec" .

:LeNet a skos:Concept ;
    rdfs:seeAlso <https://github.com/Elman295/Paper_with_code/blob/main/LeNet_5_Pytorch.ipynb> ;
    skos:definition """**LeNet** is a classic convolutional neural network employing the use of convolutions, pooling and fully connected layers. It was used for the handwritten digit recognition task with the MNIST dataset. The architectural design served as inspiration for future networks such as [AlexNet](https://paperswithcode.com/method/alexnet) and [VGG](https://paperswithcode.com/method/vgg)..\r
\r
[code](https://github.com/Elman295/Paper_with_code/blob/main/LeNet_5_Pytorch.ipynb)""" ;
    skos:prefLabel "LeNet" .

:LeVIT a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2104.01136v2> ;
    skos:definition "**LeVIT** is a hybrid neural network for fast inference image classification. LeViT is a stack of [transformer blocks](https://paperswithcode.com/method/transformer), with [pooling steps](https://paperswithcode.com/methods/category/pooling-operation) to reduce the resolution of the activation maps as in classical [convolutional architectures](https://paperswithcode.com/methods/category/convolutional-neural-networks). This replaces the uniform structure of a Transformer by a pyramid with pooling, similar to the [LeNet](https://paperswithcode.com/method/lenet) architecture" ;
    skos:prefLabel "LeVIT" .

:LeViTAttentionBlock a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2104.01136v2> ;
    skos:definition "**LeViT Attention Block** is a module used for [attention](https://paperswithcode.com/methods/category/attention-mechanisms) in the [LeViT](https://paperswithcode.com/method/levit) architecture. Its main feature is providing positional information within each attention block, i.e. where we explicitly inject relative position information in the attention mechanism. This is achieved by adding an attention bias to the attention maps." ;
    skos:prefLabel "LeViT Attention Block" .

:LeakyReLU a skos:Concept ;
    rdfs:seeAlso <https://github.com/pytorch/pytorch/blob/96aaa311c0251d24decb9dc5da4957b7c590af6f/torch/nn/modules/activation.py#L649> ;
    skos:definition "**Leaky Rectified Linear Unit**, or **Leaky ReLU**, is a type of activation function based on a [ReLU](https://paperswithcode.com/method/relu), but it has a small slope for negative values instead of a flat slope. The slope coefficient is determined before training, i.e. it is not learnt during training. This type of activation function is popular in tasks where we may suffer from sparse gradients, for example training generative adversarial networks." ;
    skos:prefLabel "Leaky ReLU" .

<http://w3id.org/mlso/vocab/ml_algorithm/Lecun'sTanh> a skos:Concept ;
    skos:definition "**LeCun's tanh** is an activation function of the form $f\\left(x\\right) = 1.7159\\tanh\\left(\\frac{2}{3}x\\right)$. The constants were chosen to keep the variance of the output close to 1." ;
    skos:prefLabel "Lecun's Tanh" .

:LevenshteinTransformer a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1905.11006v2> ;
    rdfs:seeAlso <https://github.com/pytorch/fairseq/blob/28876638114948711fd4bd4e350fdd6809013f1e/fairseq/models/nat/levenshtein_transformer.py#L34> ;
    skos:definition """The **Levenshtein Transformer** (LevT) is a type of [transformer](https://paperswithcode.com/method/transformer) that aims to address the lack of flexibility of previous decoding models. Notably, in previous frameworks, the length of generated sequences is either fixed or monotonically increased as the decoding proceeds. The authors argue this is incompatible with human-level intelligence where humans can revise, replace, revoke or delete any part of their generated text. Hence, LevT is proposed to bridge this gap by breaking the in-so-far standardized decoding mechanism and replacing it with two basic operations — insertion and deletion.\r
\r
LevT is trained using imitation learning. The resulted model contains two policies and they are executed in an alternate manner. The authors argue that with this model decoding becomes more flexible. For example, when the decoder is given an empty token, it falls back to a normal sequence generation model. On the other hand, the decoder acts as a refinement model when the initial state is a low-quality generated sequence.\r
\r
One crucial component in LevT framework is the learning algorithm. The authors leverage the characteristics of insertion and deletion — they are complementary but also adversarial. The algorithm they propose is called “dual policy learning”. The idea is that when training one policy (insertion or deletion), we use the output from its adversary at the previous iteration as input. An expert policy, on the other hand, is drawn to provide a correction signal.""" ;
    skos:prefLabel "Levenshtein Transformer" .

:LibraR-CNN a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1904.02701v1> ;
    rdfs:seeAlso <https://github.com/OceanPang/Libra_R-CNN> ;
    skos:definition """**Libra R-CNN** is an object detection model that seeks to achieve a balanced training procedure. The authors motivation is that training in past detectors has suffered from imbalance during the training process, which generally consists in three levels – sample level, feature level, and objective level. To mitigate the adverse effects, Libra R-CNN integrates three novel components: IoU-balanced\r
sampling, [balanced feature pyramid](https://paperswithcode.com/method/balanced-feature-pyramid), and [balanced L1 loss](https://paperswithcode.com/method/balanced-l1-loss), respectively for reducing the imbalance at sample, feature, and objective level.""" ;
    skos:prefLabel "Libra R-CNN" .

:LightAutoML a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2109.01528v2> ;
    skos:definition """**LightAutoML** is an AutoML solution targeted for financial services companies. A typical LightAutoML pipeline scheme is presented in the Figure, each pipeline containing:\r
\r
- Reader: object that receives raw data and task as input, calculates some useful metadata, performs initial data cleaning and decides about data manipulations that should be done before fitting different model types.\r
\r
- LightAutoML inner datasets that contains metadata and CV iterators that implements validation scheme for the datasets.\r
\r
- Multiple ML Pipelines that are stacked and/or blended to get a single prediction.\r
\r
An ML pipeline in LightAutoML is one or multiple ML models that share a single data preprocessing and validation scheme. The preprocessing step may have up to two feature selection steps, a feature engineering step or even just be empty if no preprocessing is needed. The ML pipelines can be computed independently on the same datasets and then blended together using averaging (or weighted averaging). Alternatively, a stacking ensemble scheme can be used to build multi level ensemble architectures.""" ;
    skos:prefLabel "LightAutoML" .

:LightConv a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1901.10430v2> ;
    rdfs:seeAlso <https://github.com/pytorch/fairseq/blob/28876638114948711fd4bd4e350fdd6809013f1e/fairseq/modules/lightweight_convolution.py#L33> ;
    skos:altLabel "Lightweight Convolution" ;
    skos:definition """**LightConv** is a type of [depthwise convolution](https://paperswithcode.com/method/depthwise-convolution) for sequential modelling which shares certain output channels and whose weights are normalized across the temporal dimension using a [softmax](https://paperswithcode.com/method/softmax). Compared to self-attention, LightConv has a fixed context window and it determines the importance of context elements with a set of weights that do not change over time steps. LightConv computes the following for the $i$-th element in the sequence and output channel $c$:\r
\r
$$ \\text{LightConv}\\left(X, W\\_{\\text{ceil}\\left(\\frac{cH}{d}\\right),:}, i, c\\right) = \\text{DepthwiseConv}\\left(X,\\text{softmax}\\left(W\\_{\\text{ceil}\\left(\\frac{cH}{d}\\right),:}\\right), i, c\\right) $$""" ;
    skos:prefLabel "LightConv" .

:LightGCN a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2002.02126v4> ;
    skos:definition "**LightGCN** is a type of [graph convolutional neural network](https://paperswithcode.com/method/gcn) (GCN), including only the most essential component in GCN (neighborhood aggregation) for collaborative filtering. Specifically, LightGCN learns user and item embeddings by linearly propagating them on the user-item interaction graph, and uses the weighted sum of the embeddings learned at all layers as the final embedding." ;
    skos:prefLabel "LightGCN" .

:LinComb a skos:Concept ;
    dcterms:source <https://doi.org/10.20944/preprints202301.0463.v1> ;
    skos:altLabel "Linear Combination of Activations" ;
    skos:definition """The **Linear Combination of Activations**, or **LinComb**, is a type of activation function that has trainable parameters and uses the linear combination of other activation functions.\r
\r
$$LinComb(x) = \\sum\\limits_{i=0}^{n} w_i \\mathcal{F}_i(x)$$""" ;
    skos:prefLabel "LinComb" .

:LinearLayer a skos:Concept ;
    skos:definition "A **Linear Layer** is a projection $\\mathbf{XW + b}$." ;
    skos:prefLabel "Linear Layer" .

:LinearRegression a skos:Concept ;
    skos:definition """**Linear Regression** is a method for modelling a relationship between a dependent variable and independent variables. These models can be fit with numerous approaches. The most common is *least squares*, where we minimize the mean square error between the predicted values $\\hat{y} = \\textbf{X}\\hat{\\beta}$ and actual values $y$: $\\left(y-\\textbf{X}\\beta\\right)^{2}$.\r
\r
We can also define the problem in probabilistic terms as a generalized linear model (GLM) where the pdf is a Gaussian distribution, and then perform maximum likelihood estimation to estimate $\\hat{\\beta}$.\r
\r
Image Source: [Wikipedia](https://en.wikipedia.org/wiki/Linear_regression)""" ;
    skos:prefLabel "Linear Regression" .

:LinearWarmup a skos:Concept ;
    skos:definition """**Linear Warmup** is a learning rate schedule where we linearly increase the learning rate from a low rate to a constant rate thereafter. This reduces volatility in the early stages of training.\r
\r
Image Credit: [Chengwei Zhang](https://www.dlology.com/about-me/)""" ;
    skos:prefLabel "Linear Warmup" .

:LinearWarmupWithCosineAnnealing a skos:Concept ;
    skos:definition "**Linear Warmup With Cosine Annealing** is a learning rate schedule where we increase the learning rate linearly for $n$ updates and then anneal according to a cosine schedule afterwards." ;
    skos:prefLabel "Linear Warmup With Cosine Annealing" .

:LinearWarmupWithLinearDecay a skos:Concept ;
    skos:definition "**Linear Warmup With Linear Decay** is a learning rate schedule in which we increase the learning rate linearly for $n$ updates and then linearly decay afterwards." ;
    skos:prefLabel "Linear Warmup With Linear Decay" .

:Linformer a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2006.04768v3> ;
    skos:definition "**Linformer** is a linear [Transformer](https://paperswithcode.com/method/transformer) that utilises a linear self-attention mechanism to tackle the self-attention bottleneck with [Transformer models](https://paperswithcode.com/methods/category/transformers). The original [scaled dot-product attention](https://paperswithcode.com/method/scaled) is decomposed into multiple smaller attentions through linear projections, such that the combination of these operations forms a low-rank factorization of the original attention." ;
    skos:prefLabel "Linformer" .

:Lion a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2302.06675v4> ;
    rdfs:seeAlso <https://github.com/google/automl/blob/ecbc7a3b5ab1944d5f39a80a21fd07b7606583f0/lion/lion_pytorch.py> ;
    skos:altLabel "Evolved Sign Momentum" ;
    skos:definition "The Lion optimizer is discovered by symbolic program search. It is more memory-efficient than most adaptive optimizers as it only needs to momentum. The update of Lion is produced by the sign function." ;
    skos:prefLabel "Lion" .

:LipGAN a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2003.00418v1> ;
    skos:definition "**LipGAN** is a generative adversarial network for generating realistic talking faces conditioned on translated speech. It employs an adversary that measures the extent of lip synchronization in the frames generated by the generator. The system is capable of handling faces in random poses without the need for realignment to a template pose. LipGAN is a fully self-supervised approach that learns a phoneme-viseme mapping, making it language independent." ;
    skos:prefLabel "LipGAN" .

:LiteSeg a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1912.06683v1> ;
    rdfs:seeAlso <https://github.com/tahaemara/LiteSeg> ;
    skos:definition "**LiteSeg** is a lightweight architecture for semantic segmentation that uses a deeper version of Atrous [Spatial Pyramid Pooling](https://paperswithcode.com/method/spatial-pyramid-pooling) module ([ASPP](https://paperswithcode.com/method/aspp)) and applies short and long residual connections, and [depthwise separable convolution](https://paperswithcode.com/method/depthwise-separable-convolution), resulting in a faster, more efficient model." ;
    skos:prefLabel "LiteSeg" .

:LocalAugmentation a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2109.03856v4> ;
    skos:definition "**Local Augmentation for Graph Neural Networks**, or **LA-GNN**, is a data augmentation technique that enhances node features by its local subgraph structures. Specifically, it learns the conditional distribution of the connected neighbors’ representations given the representation of the central node, which has an analogy with the [Skip-gram of word2vec](https://paperswithcode.com/method/skip-gram-word2vec) model that predicts the probability of the context given the central word. After augmenting the neighborhood, we concat the initial and the generated feature matrix as input for GNNs." ;
    skos:prefLabel "Local Augmentation" .

:LocalContrastNormalization a skos:Concept ;
    rdfs:seeAlso <https://github.com/Northengard/dl_pipe/blob/ffbccda1227d8abdbac0fa72af05593dedd3e0b5/data/transformations/normalization.py#L24> ;
    skos:definition "**Local Contrast Normalization** is a type of normalization that performs local subtraction and division normalizations, enforcing a sort of local competition between adjacent features in a feature map, and between features at the same spatial location in different feature maps." ;
    skos:prefLabel "Local Contrast Normalization" .

:LocalImportance-basedPooling a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1908.04156v3> ;
    rdfs:seeAlso <https://github.com/sebgao/LIP/blob/5e85b9e55b9212fdf6abccb1fa8783d23620f53a/imagenet/lip_resnet.py#L19> ;
    skos:definition "**Local Importance-based Pooling (LIP)** is a pooling layer that can enhance discriminative features during the downsampling procedure by learning adaptive importance weights based on inputs. By using a learnable network $G$ in $F$, the importance function now is not limited in hand-crafted forms and able to learn the criterion for the discriminativeness of features. Also, the window size of LIP is restricted to be not less than stride to fully utilize the feature map and avoid the issue of fixed interval sampling scheme. More specifically, the importance function in LIP is implemented by a tiny fully convolutional network, which learns to produce the importance map based on inputs in an end-to-end manner." ;
    skos:prefLabel "Local Importance-based Pooling" .

:LocalMixup a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2201.04368v1> ;
    skos:definition "" ;
    skos:prefLabel "Local Mixup" .

:LocalPatchInteraction a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2106.09681v2> ;
    skos:definition "**Local Patch Interaction**, or **LPI**, is a module used for the [XCiT layer](https://paperswithcode.com/method/xcit-layer) to enable explicit communication across patches. LPI consists of two [depth-wise 3×3 convolutional layers](https://paperswithcode.com/method/depthwise-convolution) with [Batch Normalization](https://paperswithcode.com/method/batch-normalization) and [GELU](https://paperswithcode.com/method/gelu) non-linearity in between. Due to its depth-wise structure, the LPI block has a negligible overhead in terms of parameters, as well as a limited overhead in terms of throughput and memory usage during inference." ;
    skos:prefLabel "Local Patch Interaction" .

:LocalRelationLayer a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1904.11491v1> ;
    rdfs:seeAlso <https://github.com/gan3sh500/local-relational-nets/blob/master/lrnet.ipynb> ;
    skos:definition "A **Local Relation Layer** is an image feature extractor that is an alternative to a [convolution](https://paperswithcode.com/method/convolution) operator. The intuition is that aggregation in convolution is basically a pattern matching process that applies fixed filters, which can be inefficient at modeling visual elements with varying spatial distributions. The local relation layer adaptively determines aggregation weights based on the compositional relationship of local pixel pairs. It is argued that, with this relational approach, it can composite visual elements into higher-level entities in a more efficient manner that benefits semantic inference." ;
    skos:prefLabel "Local Relation Layer" .

:LocalResponseNormalization a skos:Concept ;
    dcterms:source <http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks> ;
    rdfs:seeAlso <https://github.com/pytorch/pytorch/blob/1c5c289b6218eb1026dcb5fd9738231401cfccea/torch/nn/modules/normalization.py#L13> ;
    skos:definition """**Local Response Normalization** is a normalization layer that implements the idea of lateral inhibition. Lateral inhibition is a concept in neurobiology that refers to the phenomenon of an excited neuron inhibiting its neighbours: this leads to a peak in the form of a local maximum, creating contrast in that area and increasing sensory perception. In practice, we can either normalize within the same channel or normalize across channels when we apply LRN to convolutional neural networks.\r
\r
$$ b_{c} = a_{c}\\left(k + \\frac{\\alpha}{n}\\sum_{c'=\\max(0, c-n/2)}^{\\min(N-1,c+n/2)}a_{c'}^2\\right)^{-\\beta} $$\r
\r
Where the size is the number of neighbouring channels used for normalization, $\\alpha$ is multiplicative factor, $\\beta$ an exponent and $k$ an additive factor""" ;
    skos:prefLabel "Local Response Normalization" .

:LocalSGD a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1805.09767v3> ;
    skos:definition "**Local SGD** is a distributed training technique that runs [SGD](https://paperswithcode.com/method/sgd) independently in parallel on different workers and averages the sequences only once in a while." ;
    skos:prefLabel "Local SGD" .

:LocalViT a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2104.05707v1> ;
    skos:definition """**LocalViT** aims to introduce depthwise convolutions to enhance local features modeling capability of ViTs. The network, as shown in Figure (c), brings localist mechanism into transformers through the depth-wise convolution (denoted by "DW"). To cope with the convolution operation, the conversation between sequence and image feature map is added by "Seq2Img" and "Img2Seq". The computation is as follows:\r
\r
$$\r
\\mathbf{Y}^{r}=f\\left(f\\left(\\mathbf{Z}^{r} \\circledast \\mathbf{W}_{1}^{r} \\right) \\circledast \\mathbf{W}_d  \\right) \\circledast \\mathbf{W}_2^{r}\r
$$\r
\r
where $\\mathbf{W}_{d} \\in \\mathbb{R}^{\\gamma d \\times 1 \\times k \\times k}$ is the kernel of the depth-wise convolution.\r
\r
The input (sequence of tokens) is first reshaped to a feature map rearranged on a 2D lattice. Two convolutions along with a depth-wise convolution are applied to the feature map. The feature map is reshaped to a sequence of tokens which are used as by the self-attention of the network transformer layer.""" ;
    skos:prefLabel "LocalViT" .

:Locally-GroupedSelf-Attention a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2104.13840v4> ;
    skos:definition """**Locally-Grouped Self-Attention**, or **LSA**, is a local attention mechanism used in the [Twins-SVT](https://paperswithcode.com/method/twins-svt) architecture. Locally-grouped self-attention (LSA). Motivated by the group design in depthwise convolutions for efficient inference, we first equally divide the 2D feature maps into sub-windows, making self-attention communications only happen within each sub-window. This design also resonates with the multi-head design in self-attention, where the communications only occur within the channels of the same head. To be specific, the feature maps are divided into $m \\times n$ sub-windows. Without loss of generality, we assume $H \\% m=0$ and $W \\% n=0$. Each group contains $\\frac{H W}{m n}$ elements, and thus the computation cost of the self-attention in this window is $\\mathcal{O}\\left(\\frac{H^{2} W^{2}}{m^{2} n^{2}} d\\right)$, and the total cost is $\\mathcal{O}\\left(\\frac{H^{2} W^{2}}{m n} d\\right)$. If we let $k\\_{1}=\\frac{H}{n}$ and $k\\_{2}=\\frac{W}{n}$, the cost can be computed as $\\mathcal{O}\\left(k\\_{1} k\\_{2} H W d\\right)$, which is significantly more efficient when $k\\_{1} \\ll H$ and $k\\_{2} \\ll W$ and grows linearly with $H W$ if $k\\_{1}$ and $k\\_{2}$ are fixed.\r
\r
Although the locally-grouped self-attention mechanism is computation friendly, the image is divided into non-overlapping sub-windows. Thus, we need a mechanism to communicate between different sub-windows, as in Swin. Otherwise, the information would be limited to be processed locally, which makes the receptive field small and significantly degrades the performance as shown in our experiments. This resembles the fact that we cannot replace all standard convolutions by depth-wise convolutions in CNNs.""" ;
    skos:prefLabel "Locally-Grouped Self-Attention" .

:Location-basedAttention a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1508.04025v5> ;
    skos:definition """**Location-based Attention** is an attention mechanism in which the alignment scores are computed from solely the target hidden state $\\mathbf{h}\\_{t}$ as follows:\r
\r
$$ \\mathbf{a}\\_{t} = \\text{softmax}(\\mathbf{W}\\_{a}\\mathbf{h}_{t}) $$""" ;
    skos:prefLabel "Location-based Attention" .

:LocationSensitiveAttention a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1506.07503v1> ;
    rdfs:seeAlso <https://github.com/begeekmyfriend/tacotron2/blob/cbafb1dd8c79e9dcc0ce2fd6f756dbfb1c1153da/tacotron2/model.py#L58> ;
    skos:definition """**Location Sensitive Attention** is an attention mechanism that extends the [additive attention mechanism](https://paperswithcode.com/method/additive-attention) to use cumulative attention weights from previous decoder time steps as an additional feature. This encourages the model to move forward consistently through the input, mitigating potential failure modes where some subsequences are repeated or ignored by the decoder.\r
\r
Starting with additive attention where $h$ is a sequential representation from a BiRNN encoder and ${s}\\_{i-1}$ is the $(i − 1)$-th state of a recurrent neural network (e.g. a [LSTM](https://paperswithcode.com/method/lstm) or [GRU](https://paperswithcode.com/method/gru)):\r
\r
$$ e\\_{i, j} = w^{T}\\tanh\\left(W{s}\\_{i-1} + Vh\\_{j} + b\\right) $$\r
\r
where $w$ and $b$ are vectors, $W$ and $V$ are matrices. We extend this to be location-aware by making it take into account the alignment produced at the previous step. First, we extract $k$ vectors\r
$f\\_{i,j} \\in \\mathbb{R}^{k}$ for every position $j$ of the previous alignment $\\alpha\\_{i−1}$ by convolving it with a matrix $F \\in R^{k\\times{r}}$:\r
\r
$$ f\\_{i} = F ∗ \\alpha\\_{i−1} $$\r
\r
These additional vectors $f\\_{i,j}$ are then used by the scoring mechanism $e\\_{i,j}$:\r
\r
$$ e\\_{i,j} = w^{T}\\tanh\\left(Ws\\_{i−1} + Vh\\_{j} + Uf\\_{i,j} + b\\right) $$""" ;
    skos:prefLabel "Location Sensitive Attention" .

:LogisticRegression a skos:Concept ;
    skos:definition """**Logistic Regression**, despite its name, is a linear model for classification rather than regression. Logistic regression is also known in the literature as logit regression, maximum-entropy classification (MaxEnt) or the log-linear classifier. In this model, the probabilities describing the possible outcomes of a single trial are modeled using a logistic function.\r
\r
Source: [scikit-learn](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression)\r
\r
Image: [Michaelg2015](https://commons.wikimedia.org/wiki/User:Michaelg2015)""" ;
    skos:prefLabel "Logistic Regression" .

:Longformer a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2004.05150v2> ;
    skos:definition """**Longformer** is a modified [Transformer](https://paperswithcode.com/method/transformer) architecture. Traditional [Transformer-based models](https://paperswithcode.com/methods/category/transformers) are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this, **Longformer** uses an attention pattern that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. The attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention.\r
\r
The attention patterns utilised include: [sliding window attention](https://paperswithcode.com/method/sliding-window-attention), [dilated sliding window attention](https://paperswithcode.com/method/dilated-sliding-window-attention) and global + sliding window. These can be viewed in the components section of this page.""" ;
    skos:prefLabel "Longformer" .

:Lookahead a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1907.08610v2> ;
    rdfs:seeAlso <https://github.com/jettify/pytorch-optimizer/blob/155246597d66dd774156599be0f07a8c6f7758aa/torch_optimizer/lookahead.py#L13> ;
    skos:definition """**Lookahead** is a type of stochastic optimizer that iteratively updates two sets of weights: "fast" and "slow". Intuitively, the algorithm chooses a search direction by looking ahead at the sequence of *fast weights* generated by another optimizer.\r
\r
\r
\r
**Algorithm 1** Lookahead Optimizer\r
\r
**Require** Initial parameters $\\phi_0$, objective function $L$ \r
\r
**Require** Synchronization period $k$, slow weights step size $\\alpha$, optimizer $A$\r
\r
&nbsp;&nbsp;  **for** $t=1, 2, \\dots$\r
\r
&nbsp;&nbsp;&nbsp;&nbsp; Synchronize parameters $\\theta_{t,0} \\gets \\phi_{t-1}$\r
\r
&nbsp;&nbsp;&nbsp;&nbsp; **for** $i=1, 2, \\dots, k$\r
\r
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; sample minibatch of data $d \\sim \\mathcal{D}$\r
\r
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\theta_{t,i} \\gets \\theta_{t,i-1} + A(L, \\theta_{t,i-1}, d)$\r
\r
&nbsp;&nbsp;&nbsp;&nbsp; **endfor**\r
\r
&nbsp;&nbsp;&nbsp;&nbsp; Perform outer update $\\phi_t \\gets \\phi_{t-1} + \\alpha (\\theta_{t,k} - \\phi_{t-1})$\r
\r
&nbsp;&nbsp; **endfor**\r
\r
&nbsp;&nbsp; **return** parameters $\\phi$""" ;
    skos:prefLabel "Lookahead" .

:Lovasz-Softmax a skos:Concept ;
    rdfs:seeAlso <https://github.com/bermanmaxim/LovaszSoftmax/blob/7d48792d35a04d3167de488dd00daabbccd8334b/pytorch/lovasz_losses.py#L153> ;
    skos:definition "The **Lovasz-Softmax loss** is a loss function for multiclass semantic segmentation that incorporates the [softmax](https://paperswithcode.com/method/softmax) operation in the Lovasz extension. The Lovasz extension is a means by which we can achieve direct optimization of the mean intersection-over-union loss in neural networks." ;
    skos:prefLabel "Lovasz-Softmax" .

:Low-levelbackbone a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2004.12186v2> ;
    skos:definition "" ;
    skos:prefLabel "Low-level backbone" .

:Low-resolutioninput a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2004.12186v2> ;
    skos:definition "" ;
    skos:prefLabel "Low-resolution input" .

:LowRankTensorLearningParadigms a skos:Concept ;
    skos:altLabel "Time-homogenuous Top-K Ranking" ;
    skos:definition "Please enter a description about the method here" ;
    skos:prefLabel "Low Rank Tensor Learning Paradigms" .

:LowerBoundonTransmissionusingNon-LinearBoundingFunctioninSingleImageDehazing a skos:Concept ;
    dcterms:source <https://ieeexplore.ieee.org/document/9018379> ;
    skos:definition "" ;
    skos:prefLabel "Lower Bound on Transmission using Non-Linear Bounding Function in Single Image Dehazing" .

:M2Det a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1811.04533v3> ;
    rdfs:seeAlso <https://github.com/qijiezhao/M2Det> ;
    skos:definition "**M2Det** is a one-stage object detection model that utilises a Multi-Level Feature Pyramid Network ([MLFPN](https://paperswithcode.com/method/mlfpn)) to extract features from the input image, and then similar to [SSD](https://paperswithcode.com/method/ssd), produces dense bounding boxes and category scores based on the learned features, followed by the non-maximum suppression (NMS) operation to produce the final results." ;
    skos:prefLabel "M2Det" .

:M3L a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2304.10756v1> ;
    skos:altLabel "Multi-modal Teacher for Masked Modality Learning" ;
    skos:definition "" ;
    skos:prefLabel "M3L" .

:MACEst a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2109.01531v1> ;
    skos:definition "**Model Agnostic Confidence Estimator**, or **MACEst**, is a model-agnostic confidence estimator. Using a set of nearest neighbours, the algorithm differs from other methods by estimating confidence independently as a local quantity which explicitly accounts for both aleatoric and epistemic uncertainty. This approach differs from standard calibration methods that use a global point prediction model as a starting point for the confidence estimate." ;
    skos:prefLabel "MACEst" .

:MADDPG a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1706.02275v4> ;
    skos:definition "**MADDPG**, or **Multi-agent DDPG**, extends [DDPG](https://paperswithcode.com/method/ddpg) into a multi-agent policy gradient algorithm where decentralized agents learn a centralized critic based on the observations and actions of all agents. It leads to learned policies that only use local information (i.e. their own observations) at execution time, does not assume a differentiable model of the environment dynamics or any particular structure on the communication method between agents, and is applicable not only to cooperative interaction but to competitive or mixed interaction involving both physical and communicative behavior. The critic is augmented with extra information about the policies of other agents, while the actor only has access to local information. After training is completed, only the local actors are used at execution phase, acting in a decentralized manner." ;
    skos:prefLabel "MADDPG" .

:MADGRAD a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2101.11075v3> ;
    skos:altLabel "Momentumized, adaptive, dual averaged gradient" ;
    skos:definition "The MADGRAD method contains a series of modifications to the [AdaGrad](https://paperswithcode.com/method/adagrad)-DA method to improve its performance on deep learning optimization problems. It gives state-of-the-art generalization performance across a diverse set of problems, including those that [Adam](https://paperswithcode.com/method/adam) normally under-performs on." ;
    skos:prefLabel "MADGRAD" .

:MADLearning a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2102.05246v2> ;
    skos:altLabel "Memory-Associated Differential Learning" ;
    skos:definition """**Memory-Associated Differential** (**MAD**) Learning was developed to inference from the memorized facts that we already know to predict what we want to know.\r
\r
Image source: [Luo et al.](https://arxiv.org/pdf/2102.05246v1.pdf)""" ;
    skos:prefLabel "MAD Learning" .

:MAE a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2111.06377v2> ;
    skos:altLabel "Masked autoencoder" ;
    skos:definition "" ;
    skos:prefLabel "MAE" .

:MAML a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1703.03400v3> ;
    rdfs:seeAlso <https://github.com/cbfinn/maml/blob/a7f45f1bcd7457fe97b227a21e89b8a82cc5fa49/maml.py#L17> ;
    skos:altLabel "Model-Agnostic Meta-Learning" ;
    skos:definition """**MAML**, or **Model-Agnostic Meta-Learning**, is a model and task-agnostic algorithm for meta-learning that trains a model’s parameters such that a small number of gradient updates will lead to fast learning on a new task.\r
\r
Consider a model represented by a parametrized function $f\\_{\\theta}$ with parameters $\\theta$. When adapting to a new task $\\mathcal{T}\\_{i}$, the model’s parameters $\\theta$ become $\\theta'\\_{i}$. With MAML, the updated parameter vector $\\theta'\\_{i}$ is computed using one or more gradient descent updates on task $\\mathcal{T}\\_{i}$. For example, when using one gradient update,\r
\r
$$ \\theta'\\_{i} = \\theta - \\alpha\\nabla\\_{\\theta}\\mathcal{L}\\_{\\mathcal{T}\\_{i}}\\left(f\\_{\\theta}\\right) $$\r
\r
The step size $\\alpha$ may be fixed as a hyperparameter or metalearned. The model parameters are trained by optimizing for the performance of $f\\_{\\theta'\\_{i}}$ with respect to $\\theta$ across tasks sampled from $p\\left(\\mathcal{T}\\_{i}\\right)$. More concretely the meta-objective is as follows:\r
\r
$$ \\min\\_{\\theta} \\sum\\_{\\mathcal{T}\\_{i} \\sim p\\left(\\mathcal{T}\\right)} \\mathcal{L}\\_{\\mathcal{T\\_{i}}}\\left(f\\_{\\theta'\\_{i}}\\right) = \\sum\\_{\\mathcal{T}\\_{i} \\sim p\\left(\\mathcal{T}\\right)} \\mathcal{L}\\_{\\mathcal{T\\_{i}}}\\left(f\\_{\\theta - \\alpha\\nabla\\_{\\theta}\\mathcal{L}\\_{\\mathcal{T}\\_{i}}\\left(f\\_{\\theta}\\right)}\\right) $$\r
\r
Note that the meta-optimization is performed over the model parameters $\\theta$, whereas the objective is computed using the updated model parameters $\\theta'$. In effect MAML aims to optimize the model parameters such that one or a small number of gradient steps on a new task will produce maximally effective behavior on that task. The meta-optimization across tasks is performed via stochastic gradient descent ([SGD](https://paperswithcode.com/method/sgd)), such that the model parameters $\\theta$ are updated as follows:\r
\r
$$ \\theta \\leftarrow \\theta - \\beta\\nabla\\_{\\theta} \\sum\\_{\\mathcal{T}\\_{i} \\sim p\\left(\\mathcal{T}\\right)} \\mathcal{L}\\_{\\mathcal{T\\_{i}}}\\left(f\\_{\\theta'\\_{i}}\\right)$$\r
\r
where $\\beta$ is the meta step size.""" ;
    skos:prefLabel "MAML" .

:MARLIN a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2211.06627v3> ;
    skos:definition "" ;
    skos:prefLabel "MARLIN" .

:MAS a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2011.08042v1> ;
    skos:altLabel "Mixing Adam and SGD" ;
    skos:definition "This optimizer mix [ADAM](https://paperswithcode.com/method/adam) and [SGD](https://paperswithcode.com/method/sgd) creating the MAS optimizer." ;
    skos:prefLabel "MAS" .

:MATE a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2109.04312v1> ;
    skos:definition "**MATE** is a [Transformer](https://paperswithcode.com/method/transformer) architecture designed to model the structure of web tables. It uses sparse attention in a way that allows heads to efficiently attend to either rows or columns in a table. Each attention head reorders the tokens by either column or row index and then applies a windowed attention mechanism. Unlike traditional self-attention, Mate scales linearly in the sequence length." ;
    skos:prefLabel "MATE" .

:MAVL a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2111.11430v6> ;
    skos:altLabel "Multiscale Attention ViT with Late fusion" ;
    skos:definition "Multiscale Attention ViT with Late fusion (MAVL) is a multi-modal network, trained with aligned image-text pairs, capable of performing targeted detection using human understandable natural language text queries. It utilizes multi-scale image features and uses deformable convolutions with late multi-modal fusion. The authors demonstrate excellent ability of MAVL as class-agnostic object detector when queried using general human understandable natural language command, such as \"all objects\", \"all entities\", etc." ;
    skos:prefLabel "MAVL" .

:MCKERNEL a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1702.08159v9> ;
    rdfs:seeAlso <https://github.com/curto2/mckernel> ;
    skos:definition """McKernel introduces a framework to use kernel approximates in the mini-batch setting with Stochastic Gradient Descent ([SGD](https://paperswithcode.com/method/sgd)) as an alternative to Deep Learning.\r
\r
The core library was developed in 2014 as integral part of a thesis of Master of Science [1,2] at Carnegie Mellon and City University of Hong Kong. The original intend was to implement a speedup of Random Kitchen Sinks (Rahimi and Recht 2007) by writing a very efficient HADAMARD tranform, which was the main bottleneck of the construction. The code though was later expanded at ETH Zürich (in McKernel by Curtó et al. 2017) to propose a framework that could explain both Kernel Methods and Neural Networks. This manuscript and the corresponding theses, constitute one of the first usages (if not the first) in the literature of FOURIER features and Deep Learning; which later got a lot of research traction and interest in the community.\r
\r
More information can be found in this presentation that the first author gave at ICLR 2020 [iclr2020_DeCurto](https://www.decurto.tw/c/iclr2020_DeCurto.pdf).\r
\r
[1] [https://www.curto.hk/c/decurto.pdf](https://www.curto.hk/c/decurto.pdf)\r
\r
[2] [https://www.zarza.hk/z/dezarza.pdf](https://www.zarza.hk/z/dezarza.pdf)""" ;
    skos:prefLabel "MCKERNEL" .

:MDETR a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2104.12763v2> ;
    skos:definition "**MDETR** is an end-to-end modulated detector that detects objects in an image conditioned on a raw text query, like a caption or a question. It utilizes a [transformer](https://paperswithcode.com/method/transformer)-based architecture to reason jointly over text and image by fusing the two modalities at an early stage of the model. The network is pre-trained on 1.3M text-image pairs, mined from pre-existing multi-modal datasets having explicit alignment between phrases in text and objects in the image. The network is then fine-tuned on several downstream tasks such as phrase grounding, referring expression comprehension and segmentation." ;
    skos:prefLabel "MDETR" .

:MDL a skos:Concept ;
    skos:altLabel "Minimum Description Length" ;
    skos:definition """**Minimum Description Length** provides a criterion for the selection of models, regardless of their complexity, without the restrictive assumption that the data form a sample from a 'true' distribution.\r
\r
Extracted from [scholarpedia](http://scholarpedia.org/article/Minimum_description_length)\r
\r
**Source**:\r
\r
Paper: [J. Rissanen (1978) Modeling by the shortest data description. Automatica 14, 465-471](https://doi.org/10.1016/0005-1098(78)90005-5)\r
\r
Book: [P. D. Grünwald (2007) The Minimum Description Length Principle, MIT Press, June 2007, 570 pages](https://ieeexplore.ieee.org/servlet/opac?bknumber=6267274)""" ;
    skos:prefLabel "MDL" .

:MDPO a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2005.09814v5> ;
    skos:altLabel "Mirror Descent Policy Optimization" ;
    skos:definition """**Mirror Descent Policy Optimization (MDPO)** is a policy gradient algorithm based on the idea of iteratively solving a trust-region problem that minimizes a sum of two terms: a linearization of the standard RL objective function and a proximity term that restricts two consecutive updates to be close to each other. It is based on Mirror Descent, which is a general trust region method that\r
attempts to keep consecutive iterates close to each other.""" ;
    skos:prefLabel "MDPO" .

:MDTVSFA a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2011.04263v2> ;
    skos:definition "" ;
    skos:prefLabel "MDTVSFA" .

:MEI a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2006.16365v2> ;
    rdfs:seeAlso <https://github.com/tranhungnghiep/MEI-KGE> ;
    skos:altLabel "Multi-partition Embedding Interaction" ;
    skos:definition "**MEI** introduces the *multi-partition embedding interaction* technique with block term tensor format to systematically address the efficiency--expressiveness trade-off in knowledge graph embedding. It divides the embedding vector into multiple partitions and learns the local interaction patterns from data instead of using fixed special patterns as in ComplEx or SimplE models. This enables MEI to achieve optimal efficiency--expressiveness trade-off, not just being fully expressive. Previous methods such as TuckER, RESCAL, DistMult, ComplEx, and SimplE are suboptimal restricted special cases of MEI." ;
    skos:prefLabel "MEI" .

:MEUZZ a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2002.08568v2> ;
    skos:definition "**MEUZZ** is a machine learning-based hybrid fuzzer which employs supervised machine learning for adaptive and generalizable seed scheduling -- a prominent factor in determining the yields of hybrid fuzzing. MEUZZ determines which new seeds are expected to produce better fuzzing yields based on the knowledge learned from past seed scheduling decisions made on the same or similar programs. MEUZZ's learning is based on a series of features extracted via code reachability and dynamic analysis, which incurs negligible runtime overhead (in microseconds). Moreover, MEUZZ automatically infers the data labels by evaluating the fuzzing performance of each selected seed." ;
    skos:prefLabel "MEUZZ" .

:MFEC a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1606.04460v1> ;
    skos:altLabel "Model-Free Episodic Control" ;
    skos:definition "Non-parametric approximation of Q-values by storing all visited states and doing inference through k-Nearest Neighbors." ;
    skos:prefLabel "MFEC" .

:MFF a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1611.09926v1> ;
    rdfs:seeAlso <https://github.com/Fuminides/athena> ;
    skos:altLabel "Multimodal Fuzzy Fusion Framework" ;
    skos:definition """BCI MI signal Classification Framework using Fuzzy integrals.\r
\r
Paper: Ko, L. W., Lu, Y. C., Bustince, H., Chang, Y. C., Chang, Y., Ferandez, J., ... & Lin, C. T. (2019). Multimodal fuzzy fusion for enhancing the motor-imagery-based brain computer interface. IEEE Computational Intelligence Magazine, 14(1), 96-106.""" ;
    skos:prefLabel "MFF" .

:MFR a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2003.07733v2> ;
    skos:altLabel "Meta Face Recognition" ;
    skos:definition "**Meta Face Recognition** (MFR) is a meta-learning face recognition method. MFR synthesizes the source/target domain shift with a meta-optimization objective, which requires the model to learn effective representations not only on synthesized source domains but also on synthesized target domains. Specifically, domain-shift batches are built through a domain-level sampling strategy and back-propagated gradients/meta-gradients are obtained on synthesized source/target domains by optimizing multi-domain distributions. The gradients and meta-gradients are further combined to update the model to improve generalization." ;
    skos:prefLabel "MFR" .

:MHMA a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2109.03223v2> ;
    rdfs:seeAlso <https://github.com/CAMMA-public/rendezvous> ;
    skos:altLabel "Multi-Heads of Mixed Attention" ;
    skos:definition "The multi-head of mixed attention combines both self- and cross-attentions, encouraging high-level learning of interactions between entities captured in the various attention features. It is build with several attention heads, each of the head can implement either self or cross attention. A self attention is when the key and query features are the same or come from the same domain features. A cross attention is when the key and query features are generated from different features. Modeling MHMA allows a model to identity the relationship between features of different domains. This is very useful in tasks involving relationship modeling such as human-object interaction, tool-tissue interaction, man-machine interaction, human-computer interface, etc." ;
    skos:prefLabel "MHMA" .

:MIM a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1910.03175v5> ;
    skos:altLabel "Mutual Information Machine/Mask Image Modeling" ;
    skos:definition "" ;
    skos:prefLabel "MIM" .

:MLFPN a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1811.04533v3> ;
    rdfs:seeAlso <https://github.com/qijiezhao/M2Det/blob/ade4f3d12979800c367bf1e46d2e316e73a87514/m2det.py> ;
    skos:definition "**Multi-Level Feature Pyramid Network**, or **MLFPN**, is a feature pyramid block used in object detection models, notably [M2Det](https://paperswithcode.com/method/m2det). We first fuse multi-level features (i.e. multiple layers) extracted by a backbone as a base feature, and then feed it into a block of alternating joint Thinned U-shape Modules ([TUM](https://paperswithcode.com/method/tum)) and Feature Fusion Modules (FFM) to extract more representative, multi-level multi-scale features. Finally, we gather up the feature maps with equivalent scales to construct the final feature pyramid for object detection. Decoder layers that form the final feature pyramid are much deeper than the layers in the backbone, namely, they are more representative. Moreover, each feature map in the final feature pyramid consists of the decoder layers from multiple levels. Hence, the feature pyramid block is called Multi-Level Feature Pyramid Network (MLFPN)." ;
    skos:prefLabel "MLFPN" .

:MLP-Mixer a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2105.01601v4> ;
    skos:definition """The **MLP-Mixer** architecture (or “Mixer” for short) is an image architecture that doesn't use convolutions or self-attention. Instead, Mixer’s architecture is based entirely on multi-layer perceptrons (MLPs) that are repeatedly applied across either spatial locations or feature channels. Mixer relies only on basic matrix multiplication routines, changes to data layout (reshapes and transpositions), and scalar nonlinearities.\r
\r
It accepts a sequence of linearly projected image patches (also referred to as tokens) shaped as a “patches × channels” table as an input, and maintains this dimensionality. Mixer makes use of two types of MLP layers: channel-mixing MLPs and token-mixing MLPs. The channel-mixing MLPs allow communication between different channels; they operate on each token independently and take individual rows of the table as inputs. The token-mixing MLPs allow communication between different spatial locations (tokens); they operate on each channel independently and take individual columns of the table as inputs. These two types of layers are interleaved to enable interaction of both input dimensions.""" ;
    skos:prefLabel "MLP-Mixer" .

:MNMF a skos:Concept ;
    dcterms:source <https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14589> ;
    skos:altLabel "Modularity preserving NMF" ;
    skos:definition "" ;
    skos:prefLabel "MNMF" .

:MODERN a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1707.00683v3> ;
    skos:altLabel "Modulated Residual Network" ;
    skos:definition "**MODERN**, or **Modulated Residual Network**, is an architecture for [visual question answering](https://paperswithcode.com/task/visual-question-answering) (VQA). It employs [conditional batch normalization](https://paperswithcode.com/method/conditional-batch-normalization) to allow a linguistic embedding from an [LSTM](https://paperswithcode.com/method/lstm) to modulate the [batch normalization](https://paperswithcode.com/method/batch-normalization) parameters of a [ResNet](https://paperswithcode.com/method/resnet). This enables the linguistic embedding to manipulate entire feature maps by scaling them up or down, negating them, or shutting them off, etc." ;
    skos:prefLabel "MODERN" .

:MODNet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2011.11961v4> ;
    skos:definition """**MODNet** is a light-weight matting objective decomposition network that can process portrait matting from a single input image in real time. The design of MODNet benefits from optimizing a series of correlated sub-objectives simultaneously via explicit constraints. To overcome the domain shift problem, MODNet introduces a self-supervised strategy based on subobjective consistency (SOC) and  a one-frame delay trick to smooth the results when applying MODNet to portrait video sequence.\r
\r
Given an input image $I$, MODNet predicts human semantics $s\\_{p}$, boundary details $d\\_{p}$, and final alpha matte $\\alpha\\_{p}$ through three interdependent branches, $S, D$, and $F$, which are constrained by specific supervisions generated from the ground truth matte $\\alpha\\_{g}$. Since the decomposed sub-objectives are correlated and help strengthen each other, we can optimize MODNet end-to-end.""" ;
    skos:prefLabel "MODNet" .

:MPN a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1703.08050v3> ;
    skos:altLabel "Matrix-power Normalization" ;
    skos:definition "" ;
    skos:prefLabel "MPN" .

:MPNN a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1704.01212v2> ;
    skos:altLabel "Message Passing Neural Network" ;
    skos:definition """There are at least eight notable examples of models from the literature that can be described using the **Message Passing Neural Networks** (**MPNN**) framework. For simplicity we describe MPNNs which operate on undirected graphs $G$ with node features $x_{v}$ and edge features $e_{vw}$. It is trivial to extend the formalism to directed multigraphs. The forward pass has two phases, a message passing phase and a readout phase. The message passing phase runs for $T$ time steps and is defined in terms of message functions $M_{t}$ and vertex update functions $U_{t}$. During the message passing phase, hidden states $h_{v}^{t}$ at each node in the graph are updated based on messages $m_{v}^{t+1}$ according to\r
$$\r
m_{v}^{t+1} = \\sum_{w \\in N(v)} M_{t}(h_{v}^{t}, h_{w}^{t}, e_{vw})\r
$$\r
$$\r
h_{v}^{t+1} = U_{t}(h_{v}^{t}, m_{v}^{t+1})\r
$$\r
where in the sum, $N(v)$ denotes the neighbors of $v$ in graph $G$. The readout phase computes a feature vector for the whole graph using some readout function $R$ according to\r
$$\r
\\hat{y} = R(\\\\{ h_{v}^{T} | v \\in G \\\\})\r
$$\r
The message functions $M_{t}$, vertex update functions $U_{t}$, and readout function $R$ are all learned differentiable functions. $R$ operates on the set of node states and must be invariant to permutations of the node states in order for the MPNN to be invariant to graph isomorphism.""" ;
    skos:prefLabel "MPNN" .

:MPNet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2004.09297v2> ;
    skos:definition """**MPNet** is a pre-training method for language models that combines masked language modeling (MLM) and permuted language modeling (PLM) in one view. It takes the dependency among the predicted tokens into consideration through permuted language modeling and thus avoids the issue of [BERT](https://paperswithcode.com/method/bert). On the other hand, it takes position information of all tokens as input to make the model see the position information of all the tokens and thus alleviates the position discrepancy of [XLNet](https://paperswithcode.com/method/xlnet).\r
\r
The training objective of MPNet is:\r
\r
$$ \\mathbb{E}\\_{z\\in{\\mathcal{Z}\\_{n}}} \\sum^{n}\\_{t=c+1}\\log{P}\\left(x\\_{z\\_{t}}\\mid{x\\_{z\\_{<t}}}, M\\_{z\\_{{>}{c}}}; \\theta\\right) $$\r
\r
As can be seen, MPNet conditions on ${x\\_{z\\_{<t}}}$ (the tokens preceding the current predicted token $x\\_{z\\_{t}}$) rather than only the non-predicted tokens ${x\\_{z\\_{<=c}}}$ in MLM; comparing with PLM, MPNet takes more information (i.e., the mask symbol $[M]$ in position $z\\_{>c}$) as inputs. Although the objective seems simple, it is challenging to implement the model efficiently. For details, see the paper.""" ;
    skos:prefLabel "MPNet" .

:MPRNet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2102.02808v2> ;
    skos:definition "**MPRNet** is a multi-stage progressive image restoration architecture that progressively learns restoration functions for the degraded inputs, thereby breaking down the overall recovery process into more manageable steps. Specifically, the model first learns the contextualized features using encoder-decoder architectures and later combines them with a high-resolution branch that retains local information. At each stage, a per-pixel adaptive design is introduced that leverages in-situ supervised attention to reweight the local features." ;
    skos:prefLabel "MPRNet" .

:MPSO a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2010.02039v1> ;
    skos:altLabel "Motion-Encoded Particle Swarm Optimization" ;
    skos:definition "" ;
    skos:prefLabel "MPSO" .

:MSGAN a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2001.03886v1> ;
    skos:altLabel "Multi-source Sentiment Generative Adversarial Network" ;
    skos:definition "**Multi-source Sentiment Generative Adversarial Network** is a multi-source domain adaptation (MDA) method for visual sentiment classification. It is composed of three pipelines, i.e., image reconstruction, image translation, and cycle-reconstruction. To handle data from multiple source domains, it learns to find a unified sentiment latent space where data from both the source and target domains share a similar distribution. This is achieved via cycle consistent adversarial learning in an end-to-end manner. Notably, thanks to the unified sentiment latent space, MSGAN requires a single classification network to handle data from different source domains." ;
    skos:prefLabel "MSGAN" .

:MSPFN a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2003.10985v2> ;
    skos:altLabel "Multi-scale Progressive Fusion Network" ;
    skos:definition """**Multi-scale Progressive Fusion Network** (MSFPN) is a neural network representation for single image deraining. It aims to exploit the correlated information of rain streaks across scales for single image deraining. \r
\r
Specifically, we first generate the Gaussian pyramid rain images using Gaussian kernels to down-sample the original rain image in sequence. A coarse-fusion module (CFM) is designed to capture the global texture information from these multi-scale rain images through recurrent calculation (Conv-[LSTM](https://paperswithcode.com/method/lstm)), thus enabling the network to cooperatively represent the target rain streak using similar counterparts from global feature space. Meanwhile, the representation of the high-resolution pyramid layer is guided by previous outputs as well as all low-resolution pyramid layers. A finefusion module (FFM) is followed to further integrate these correlated information from different scales. By using the channel attention mechanism, the network not only discriminatively learns the scale-specific knowledge from all preceding pyramid layers, but also reduces the feature redundancy effectively. Moreover, multiple FFMs can be cascaded to form a progressive multi-scale fusion. Finally, a reconstruction module (RM) is appended to aggregate the coarse and fine rain information extracted respectively from CFM and FFM for learning the residual rain image, which is the approximation of real rain streak distribution.""" ;
    skos:prefLabel "MSPFN" .

:MT-PET a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2108.13493v1> ;
    skos:definition """**MT-PET** is a multi-task version of [Pattern Exploiting Training](https://arxiv.org/abs/2001.07676) (PET) for exaggeration detection, which leverages knowledge from complementary cloze-style QA tasks to improve few-shot learning. It defines pairs of complementary pattern-verbalizer pairs for a main task and auxiliary task. These PVPs are then used to train PET on data from both tasks.\r
\r
PET uses the masked language modeling objective of pretrained language models to transform a task into one or more cloze-style question answering tasks.  In the original PET implementation, PVPs are defined for a single target task. MT-PET extends this by allowing for auxiliary PVPs from related tasks, adding complementary cloze-style QA tasks during training. The motivation for the multi-task approach is two-fold: 1) complementary cloze-style tasks can potentially help the model to learn different aspects of the main task, i.e. the similar tasks of exaggeration detection and claim strength prediction; 2) data on related tasks can be utilized during training, which is important in situations where data for the main task is limited.""" ;
    skos:prefLabel "MT-PET" .

:MTS a skos:Concept ;
    dcterms:source <https://aclanthology.org/2021.argmining-1.17> ;
    skos:altLabel "Matching The Statements" ;
    skos:definition "" ;
    skos:prefLabel "MTS" .

:MUSIQ a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2108.05997v1> ;
    skos:definition "**MUSIQ**, or **Multi-scale Image Quality Transformer**, is a [Transformer](https://paperswithcode.com/method/transformer)-based model for multi-scale image quality assessment. It processes native resolution images with varying sizes and aspect ratios. In MUSIQ, we construct a multi-scale image representation as input, including the native resolution image and its ARP resized variants.  Each image is split into fixed-size patches which are embedded by a patch encoding module (blue boxes). To capture 2D structure of the image and handle images of varying aspect ratios, the spatial embedding is encoded by hashing the patch position $(i,j)$ to $(t_{i},t_{j})$ within a grid of learnable embeddings (red boxes). Scale Embedding (green boxes) is introduced to capture scale information. The Transformer encoder takes the input tokens and performs multi-head self-attention. To predict the image quality, MUSIQ follows a common strategy in Transformers to add an [CLS] token to the sequence to represent the whole multi-scale input and the corresponding Transformer output is used as the final representation." ;
    skos:prefLabel "MUSIQ" .

:MViT a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2104.11227v1> ;
    skos:altLabel "Multiscale Vision Transformer" ;
    skos:definition "**Multiscale Vision Transformer**, or **MViT**, is a [transformer](https://paperswithcode.com/method/transformer) architecture for modeling visual data such as images and videos. Unlike conventional transformers, which maintain a constant channel capacity and resolution throughout the network, Multiscale Transformers have several channel-resolution scale stages. Starting from the input resolution and a small channel dimension, the stages hierarchically expand the channel capacity while reducing the spatial resolution. This creates a multiscale pyramid of features with early layers operating at high spatial resolution to model simple low-level visual information, and deeper layers at spatially coarse, but complex, high-dimensional features." ;
    skos:prefLabel "MViT" .

:MXMNet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2011.07457v1> ;
    skos:altLabel "Multiplex Molecular Graph Neural Network" ;
    skos:definition "The **Multiplex Molecular Graph Neural Network (MXMNet)** is an approach for the representation learning of molecules. The molecular interactions are divided into two categories: local and global. Then a two-layer multiplex graph $G = \\\\{ G_{l}, G_{g} \\\\}$ is constructed for a molecule. In $G$, the local layer $G_{l}$ only contains the local connections that mainly capture covalent interactions, and the global layer $G_{g}$ contains the global connections that cover non-covalent interactions. MXMNet uses the Multiplex Molecular (MXM) module that contains a novel angle-aware message passing operated on $G_{l}$ and an efficient message passing operated on $G_{g}$." ;
    skos:prefLabel "MXMNet" .

:MacBERT a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2004.13922v2> ;
    skos:definition """**MacBERT** is a [Transformer](https://paperswithcode.com/methods/category/transformers)-based model for Chinese NLP that alters [RoBERTa](https://paperswithcode.com/method/roberta) in several ways, including a modified masking strategy. Instead of masking with [MASK] token, which never appears in the fine-tuning stage, MacBERT masks the word with its similar word. Specifically MacBERT shares the same pre-training tasks as [BERT](https://paperswithcode.com/method/bert) with several modifications. For the MLM task, the following modifications are performed:\r
\r
- Whole word masking is used as well as Ngram masking strategies for selecting candidate tokens for masking, with a percentage of\r
40%, 30%, 20%, 10% for word-level unigram to 4-gram.\r
- Instead of masking with [MASK] token, which never appears in the fine-tuning stage, similar words are used for the masking purpose. A similar word is obtained by using Synonyms toolkit which is based on word2vec similarity calculations. If an N-gram is selected to mask, we will find similar words individually. In rare cases, when there is no similar word, we will degrade to use random word replacement.\r
- A percentage of 15% input words is used for masking, where 80% will replace with similar words, 10% replace with a random word, and keep with original words for the rest of 10%.""" ;
    skos:prefLabel "MacBERT" .

:Macaw a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2109.02593v1> ;
    skos:definition "**Macaw** is a generative question-answering (QA) system that is built on UnifiedQA, itself built on [T5](https://paperswithcode.com/method/t5). Macaw has three interesting features. First, it often produces high-quality answers to questions far outside the domain it was trained on, sometimes surprisingly so. Second, Macaw allows different permutations (“an gles”) of inputs and outputs to be used. For example, we can give it a question and get an answer; or give it an answer and get a question; or give it a question and answer and get a set of multiple-choice (MC) options for that question. This multi-angle QA capability allows versatility in the way Macaw can be used, include recursively using outputs as new inputs to the system. Finally, Macaw also generates explanations as an optional output (or even input) element." ;
    skos:prefLabel "Macaw" .

:MachineLearningAlgorithm a owl:NamedIndividual,
        skos:Collection ;
    skos:member :ArtificialNeuralNetwork,
        :AssociationRuleLearningAlgorithm,
        :Bayesian,
        :ClusteringAlgorithm,
        :DecisionTree,
        :DeepLearningAlgorithm,
        :DimensionalityReductionAlgorithm,
        :EnsembleAlgorithm,
        :InstanceBasedAlgorithm,
        :RegressionAlgorithm,
        :RegularizationAlgorithm,
        :ReinforcementLearningAlgorithm,
        :RuleBased ;
    skos:prefLabel "Machine Learning Algorithm" .

:MagFace a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2103.06627v4> ;
    skos:definition "**MagFace** is a category of losses for face recognition that learn a universal feature embedding whose magnitude can measure the quality of a given face. Under the new loss, it can be proven that the magnitude of the feature embedding monotonically increases if the subject is more likely to be recognized. In addition, MagFace introduces an adaptive mechanism to learn a well-structured within-class feature distributions by pulling easy samples to class centers while pushing hard samples away. For face recognition, MagFace helps prevent model overfitting on noisy and low-quality samples by an adaptive mechanism to learn well-structured within-class feature distributions -- by pulling easy samples to class centers while pushing hard samples away." ;
    skos:prefLabel "MagFace" .

:MagnificationPriorContrastiveSimilarity a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2203.07707v2> ;
    skos:definition "Self-supervised pre-training method to learn efficient representations without labels on histopathology medical images utilizing magnification factors." ;
    skos:prefLabel "Magnification Prior Contrastive Similarity" .

:Make-A-Scene a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2203.13131v1> ;
    skos:definition "Make-A-Scene is a text-to-image method that (i) enables a simple control mechanism complementary to text in the form of a scene, (ii) introduces elements that improve the tokenization process by employing domain-specific knowledge over key image regions (faces and salient objects), and (iii) adapts classifier-free guidance for the transformer use case." ;
    skos:prefLabel "Make-A-Scene" .

:ManifoldMixup a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1806.05236v7> ;
    rdfs:seeAlso <https://github.com/vikasverma1077/manifold_mixup/blob/118ec709808b79dd336b10f4cf7deeacf541dfc7/supervised/models/resnet.py#L98> ;
    skos:definition """**Manifold Mixup** is a regularization method that encourages neural networks to predict less confidently on interpolations of hidden representations. It leverages semantic interpolations as an additional training signal, obtaining neural networks with smoother decision boundaries at multiple levels of representation. As a result, neural networks trained with Manifold Mixup learn class-representations with fewer directions of variance.\r
\r
Consider training a deep neural network $f\\left(x\\right) = f\\_{k}\\left(g\\_{k}\\left(x\\right)\\right)$, where $g\\_{k}$ denotes the part of the neural network mapping the input data to the hidden representation at layer $k$, and $f\\_{k}$ denotes the\r
part mapping such hidden representation to the output $f\\left(x\\right)$. Training $f$ using Manifold Mixup is performed in five steps:\r
\r
(1) Select a random layer $k$ from a set of eligible layers $S$ in the neural network. This set may include the input layer $g\\_{0}\\left(x\\right)$.\r
\r
(2) Process two random data minibatches $\\left(x, y\\right)$ and $\\left(x', y'\\right)$ as usual, until reaching layer $k$. This provides us with two intermediate minibatches $\\left(g\\_{k}\\left(x\\right), y\\right)$ and $\\left(g\\_{k}\\left(x'\\right), y'\\right)$.\r
\r
(3) Perform Input [Mixup](https://paperswithcode.com/method/mixup) on these intermediate minibatches. This produces the mixed minibatch:\r
\r
$$\r
\\left(\\tilde{g}\\_{k}, \\tilde{y}\\right) = \\left(\\text{Mix}\\_{\\lambda}\\left(g\\_{k}\\left(x\\right), g\\_{k}\\left(x'\\right)\\right), \\text{Mix}\\_{\\lambda}\\left(y, y'\\right\r
)\\right),\r
$$\r
\r
where $\\text{Mix}\\_{\\lambda}\\left(a, b\\right) = \\lambda \\cdot a + \\left(1 − \\lambda\\right) \\cdot b$. Here, $\\left(y, y'\r
\\right)$ are one-hot labels, and the mixing coefficient\r
$\\lambda \\sim \\text{Beta}\\left(\\alpha, \\alpha\\right)$ as in mixup. For instance, $\\alpha = 1.0$ is equivalent to sampling $\\lambda \\sim U\\left(0, 1\\right)$.\r
\r
(4) Continue the forward pass in the network from layer $k$ until the output using the mixed minibatch $\\left(\\tilde{g}\\_{k}, \\tilde{y}\\right)$.\r
\r
(5) This output is used to compute the loss value and\r
gradients that update all the parameters of the neural network.""" ;
    skos:prefLabel "Manifold Mixup" .

:ManifoldPlus a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/2005.11621v1> ;
    skos:definition "**ManifoldPlus** is a method for robust and scalable conversion of triangle soups to watertight manifolds. It extracts exterior faces between occupied voxels and empty voxels, and uses a projection based optimization method to accurately recover a watertight manifold that resembles the reference mesh. It does not rely on face normals of the input triangle soups and can accurately recover zero-volume structures. For scalability, it employs an adaptive Gauss-Seidel method for shape optimization, in which each step is an easy-to-solve convex problem." ;
    skos:prefLabel "ManifoldPlus" .

:MaskFlownet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2003.10955v2> ;
    skos:definition "**MaskFlownet** is an asymmetric occlusion-aware feature matching module, which can learn a rough occlusion mask that filters useless (occluded) areas immediately after feature warping without any explicit supervision. The learned occlusion mask can be further fed into a subsequent network cascade with dual feature pyramids." ;
    skos:prefLabel "MaskFlownet" .

:MaskR-CNN a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1703.06870v3> ;
    rdfs:seeAlso <https://github.com/facebookresearch/detectron2/blob/601d7666faaf7eb0ba64c9f9ce5811b13861fe12/detectron2/modeling/roi_heads/mask_head.py#L154> ;
    skos:definition """**Mask R-CNN** extends [Faster R-CNN](http://paperswithcode.com/method/faster-r-cnn) to solve instance segmentation tasks. It achieves this by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. In principle, Mask R-CNN is an intuitive extension of Faster [R-CNN](https://paperswithcode.com/method/r-cnn), but constructing the mask branch properly is critical for good results. \r
\r
Most importantly, Faster R-CNN was not designed for pixel-to-pixel alignment between network inputs and outputs. This is evident in how [RoIPool](http://paperswithcode.com/method/roi-pooling), the *de facto* core operation for attending to instances, performs coarse spatial quantization for feature extraction. To fix the misalignment, Mask R-CNN utilises a simple, quantization-free layer, called [RoIAlign](http://paperswithcode.com/method/roi-align), that faithfully preserves exact spatial locations. \r
\r
Secondly, Mask R-CNN *decouples* mask and class prediction: it predicts a binary mask for each class independently, without competition among classes, and relies on the network's RoI classification branch to predict the category. In contrast, an [FCN](http://paperswithcode.com/method/fcn) usually perform per-pixel multi-class categorization, which couples segmentation and classification.""" ;
    skos:prefLabel "Mask R-CNN" .

:MaskScoringR-CNN a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1903.00241v1> ;
    skos:definition "**Mask Scoring R-CNN** is a Mask RCNN with MaskIoU Head, which takes the instance feature and the predicted mask together as input, and predicts the IoU between input mask and ground truth mask." ;
    skos:prefLabel "Mask Scoring R-CNN" .

:MaskedConvolution a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1601.06759v3> ;
    rdfs:seeAlso <https://github.com/jzbontar/pixelcnn-pytorch/blob/14c9414602e0694692c77a5e0d87188adcded118/main.py#L17> ;
    skos:definition "A **Masked Convolution** is a type of [convolution](https://paperswithcode.com/method/convolution) which masks certain pixels so that the model can only predict based on pixels already seen. This type of convolution was introduced with [PixelRNN](https://paperswithcode.com/method/pixelrnn) generative models, where an image is generated pixel by pixel, to ensure that the model was conditional only on pixels already visited." ;
    skos:prefLabel "Masked Convolution" .

:MatrixNMS a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2003.10152v3> ;
    skos:altLabel "Matrix Non-Maximum Suppression" ;
    skos:definition """**Matrix NMS**, or **Matrix Non-Maximum Suppression**,  performs [non-maximum suppression](https://paperswithcode.com/method/non-maximum-suppression) with parallel matrix operations in one shot. It is motivated by [Soft-NMS](https://paperswithcode.com/method/soft-nms). Soft-NMS decays the other detection scores as a monotonic decreasing function $f(iou)$ of their overlaps. By decaying the scores according to IoUs recursively, higher IoU detections will be eliminated with a minimum score threshold. However, such process is sequential like traditional Greedy NMS and can not be implemented in parallel.\r
\r
Matrix NMS views this process from another perspective by considering how a predicted mask $m\\_{j}$ being suppressed. For $m\\_{j}$, its decay factor is affected by: (a) The penalty of each prediction $m\\_{i}$ on $m\\_{j}$ $\\left(s\\_{i}>s\\_{j}\\right)$, where $s\\_{i}$ and $s\\_{j}$ are the confidence scores; and (b) the probability of $m\\_{i}$ being suppressed. For (a), the penalty of each prediction $m\\_{i}$ on $m\\_{j}$ could be easily computed by $f\\left(\\right.$ iou $\\left.\\_{i, j}\\right)$. For (b), the probability of $m\\_{i}$ being suppressed is not so elegant to be computed. However, the probability usually has positive correlation with the IoUs. So here we directly approximate the probability by the most overlapped prediction on $m\\_{i}$ as\r
\r
$$\r
f\\left(\\text { iou. }\\_{, i}\\right)=\\min\\_{\\forall s\\_{k}>s\\_{i}} f\\left(\\text { iou }\\_{k, i}\\right)\r
$$\r
\r
To this end, the final decay factor becomes\r
\r
$$\r
\\operatorname{decay}\\_{j}=\\min\\_{\\forall s\\_{i}>s\\_{j}} \\frac{f\\left(\\text { iou }\\_{i, j}\\right)}{f\\left(\\text { iou }\\_{\\cdot, i}\\right)}\r
$$\r
\r
and the updated score is computed by $s\\_{j}=s\\_{j} \\cdot$ decay $\\_{j} .$ The authors consider the two most simple decremented functions, denoted as linear $f\\left(\\right.$ iou $\\left.\\_{i, j}\\right)=1-$ iou $\\_{i, j}$, and Gaussian $f\\left(\\right.$ iou $\\left.\\_{i, j}\\right)=\\exp \\left(-\\frac{i o u\\_{i, j}^{2}}{\\sigma}\\right)$.""" ;
    skos:prefLabel "Matrix NMS" .

:MatrixNet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2001.03194v1> ;
    rdfs:seeAlso <https://github.com/arashwan/matrixnet/blob/ac6a0613535053ccd82ed3f6b9d55e39bece723c/models/matrixnet.py#L15> ;
    skos:definition """**MatrixNet** is a scale and aspect ratio aware building block for object detection that seek to handle objects of different sizes and aspect ratios. They have several matrix layers, each layer handles an object of specific size and aspect ratio. They can be seen as an alternative to [FPNs](https://paperswithcode.com/method/fpn). While FPNs are capable of handling objects of different sizes, they do not have a solution for objects of different aspect ratios. Objects such as a high tower, a giraffe, or a knife introduce a design difficulty for FPNs: does one map these objects to layers according to their width or height? Assigning the object to a layer according to its larger dimension would result in loss of information along the smaller dimension due to aggressive downsampling, and vice versa. \r
\r
MatrixNets assign objects of different sizes and aspect ratios to layers such that object sizes within their assigned layers are close to uniform. This assignment allows a square output [convolution](https://paperswithcode.com/method/convolution) kernel to equally gather information about objects of all aspect ratios and scales. MatrixNets can be applied to any backbone, similar to FPNs. We denote this by appending a "-X" to the backbone, i.e. ResNet50-X.""" ;
    skos:prefLabel "MatrixNet" .

:MaxPooling a skos:Concept ;
    skos:definition """**Max Pooling** is a pooling operation that calculates the maximum value for patches of a feature map, and uses it to create a downsampled (pooled) feature map.  It is usually used after a convolutional layer. It adds a small amount of translation invariance - meaning translating the image by a small amount does not significantly affect the values of most pooled outputs.\r
\r
Image Source: [here](https://computersciencewiki.org/index.php/File:MaxpoolSample2.png)""" ;
    skos:prefLabel "Max Pooling" .

:MaxUp a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2002.09024v1> ;
    skos:definition "**MaxUp** is an adversarial data augmentation technique for improving the generalization performance of machine learning models. The idea is to generate a set of augmented data with some random perturbations or transforms, and minimize the maximum, or worst case loss over the augmented data.  By doing so, we implicitly introduce a smoothness or robustness regularization against the random perturbations, and hence improve the generation performance.  For example, in the case of Gaussian perturbation, MaxUp is asymptotically equivalent to using the gradient norm of the loss as a penalty to encourage smoothness." ;
    skos:prefLabel "MaxUp" .

:Maxout a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1302.4389v4> ;
    rdfs:seeAlso <https://gist.github.com/daskol/05439f018465c8fb42ae547b8cc8a77b> ;
    skos:definition """The **Maxout Unit** is a generalization of the [ReLU](https://paperswithcode.com/method/relu) and the [leaky ReLU](https://paperswithcode.com/method/leaky-relu) functions. It is a piecewise linear function that returns the maximum of the inputs, designed to be used in conjunction with [dropout](https://paperswithcode.com/method/dropout). Both ReLU and leaky ReLU are special cases of Maxout. \r
\r
$$f\\left(x\\right) = \\max\\left(w^{T}\\_{1}x + b\\_{1}, w^{T}\\_{2}x + b\\_{2}\\right)$$\r
\r
The main drawback of Maxout is that it is computationally expensive as it doubles the number of parameters for each neuron.""" ;
    skos:prefLabel "Maxout" .

:MeRL a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1902.07198v4> ;
    rdfs:seeAlso <https://github.com/google-research/google-research/tree/master/meta_reward_learning> ;
    skos:altLabel "Meta Reward Learning" ;
    skos:definition "**Meta Reward Learning (MeRL)** is a meta-learning method for the problem of learning from sparse and underspecified rewards. For example, an agent receives a complex input, such as a natural language instruction, and needs to generate a complex response, such as an action sequence, while only receiving binary success-failure feedback. The key insight of MeRL in dealing with underspecified rewards is that spurious trajectories and programs that achieve accidental success are detrimental to the agent's generalization performance. For example, an agent might be able to solve a specific instance of the maze problem above. However, if it learns to perform spurious actions during training, it is likely to fail when provided with unseen instructions. To mitigate this issue, MeRL optimizes a more refined auxiliary reward function, which can differentiate between accidental and purposeful success based on features of action trajectories. The auxiliary reward is optimized by maximizing the trained agent's performance on a hold-out validation set via meta learning." ;
    skos:prefLabel "MeRL" .

:MechanismTransfer a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2002.03497v2> ;
    skos:definition "**Mechanism Transfer** is a meta-distributional scenario for few-shot domain adaptation in which a data generating mechanism is invariant across domains. This transfer assumption can accommodate nonparametric shifts resulting in apparently different distributions while providing a solid statistical basis for domain adaptation." ;
    skos:prefLabel "Mechanism Transfer" .

:Meena a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2001.09977v3> ;
    skos:definition "**Meena** is a multi-turn open-domain chatbot trained end-to-end on data mined and filtered from public domain social media conversations. This 2.6B parameter neural network is simply trained to minimize perplexity of the next token. A seq2seq model is used with the Evolved [Transformer](https://paperswithcode.com/method/transformer) as the main architecture. The model is trained on multi-turn conversations where the input sequence is all turns of the context and the output sequence is the response." ;
    skos:prefLabel "Meena" .

:MelGAN a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1910.06711v3> ;
    skos:definition """**MelGAN** is a non-autoregressive feed-forward convolutional architecture to perform audio waveform generation in a [GAN](https://paperswithcode.com/method/gan) setup. The architecture is a fully convolutional feed-forward network with mel-spectrogram $s$ as input and raw waveform $x$ as output. Since the mel-spectrogram is at\r
a 256× lower temporal resolution, the authors use a stack of transposed convolutional layers to upsample the input sequence. Each transposed convolutional layer is followed by a stack of residual blocks with dilated convolutions. Unlike traditional GANs, the MelGAN generator does not use a global noise vector as input.\r
\r
To deal with 'checkerboard artifacts' in audio, instead of using [PhaseShuffle](https://paperswithcode.com/method/phase-shuffle), MelGAN uses kernel-size as a multiple of stride.\r
\r
[Weight normalization](https://paperswithcode.com/method/weight-normalization) is used for normalization. A [window-based discriminator](https://paperswithcode.com/method/window-based-discriminator), similar to a [PatchGAN](https://paperswithcode.com/method/patchgan) is used for the discriminator.""" ;
    skos:prefLabel "MelGAN" .

:MelGANResidualBlock a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1910.06711v3> ;
    rdfs:seeAlso <https://github.com/descriptinc/melgan-neurips/blob/6488045bfba1975602288de07a58570c7b4d66ea/mel2wav/modules.py#L72> ;
    skos:definition "The **MelGAN Residual Block** is a convolutional [residual block](https://paperswithcode.com/method/residual-block) used in the [MelGAN](https://paperswithcode.com/method/melgan) generative audio architecture. It employs residual connections with dilated convolutions. Dilations are used so that temporally far output activations of each subsequent layer has significant overlapping inputs. Receptive field of a stack of [dilated convolution](https://paperswithcode.com/method/dilated-convolution) layers increases exponentially with the number of layers. Incorporating these into the MelGAN generator allows us to efficiently increase the induced receptive fields of each output time-step. This effectively implies larger overlap in the induced receptive field of far apart time-steps, leading to better long range correlation." ;
    skos:prefLabel "MelGAN Residual Block" .

:MemoryNetwork a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1410.3916v11> ;
    rdfs:seeAlso <https://github.com/aykutaaykut/Memory-Networks> ;
    skos:definition """A **Memory Network** provides a memory component that can be read from and written to with the inference capabilities of a neural network model. The motivation is that many neural networks lack a long-term memory component, and their existing memory component encoded by states and weights is too small and not compartmentalized enough to accurately remember facts from the past (RNNs for example, have difficult memorizing and doing tasks like copying). \r
\r
A memory network consists of a memory $\\textbf{m}$ (an array of objects indexed by $\\textbf{m}\\_{i}$ and four potentially learned components:\r
\r
- Input feature map $I$ - feature representation of the data input.\r
- Generalization $G$ - updates old memories given the new input.\r
- Output feature map $O$ - produces new feature map given $I$ and $G$.\r
- Response $R$ - converts output into the desired response. \r
\r
Given an input $x$ (e.g., an input character, word or sentence depending on the granularity chosen, an image or an audio signal) the flow of the model is as follows:\r
\r
1. Convert $x$ to an internal feature representation $I\\left(x\\right)$.\r
2. Update memories $m\\_{i}$ given the new input: $m\\_{i} = G\\left(m\\_{i}, I\\left(x\\right), m\\right)$, $\\forall{i}$.\r
3. Compute output features $o$ given the new input and the memory: $o = O\\left(I\\left(x\\right), m\\right)$.\r
4. Finally, decode output features $o$ to give the final response: $r = R\\left(o\\right)$.\r
\r
This process is applied at both train and test time, if there is a distinction between such phases, that\r
is, memories are also stored at test time, but the model parameters of $I$, $G$, $O$ and $R$ are not updated. Memory networks cover a wide class of possible implementations. The components $I$, $G$, $O$ and $R$ can potentially use any existing ideas from the machine learning literature.\r
\r
Image Source: [Adrian Colyer](https://blog.acolyer.org/2016/03/10/memory-networks/)""" ;
    skos:prefLabel "Memory Network" .

:Mesh-TensorFlow a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1811.02084v1> ;
    skos:definition "**Mesh-TensorFlow** is a language for specifying a general class of distributed tensor computations. Where data-parallelism can be viewed as splitting tensors and operations along the \"batch\" dimension, in Mesh-TensorFlow, the user can specify any tensor dimensions to be split across any dimensions of a multi-dimensional mesh of processors. A MeshTensorFlow graph compiles into a SPMD program consisting of parallel operations coupled with collective communication primitives such as Allreduce." ;
    skos:prefLabel "Mesh-TensorFlow" .

:MeshGraphNet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2010.03409v4> ;
    skos:definition "**MeshGraphNet** is a framework for learning mesh-based simulations using [graph neural networks](https://paperswithcode.com/methods/category/graph-models). The model can be trained to pass messages on a mesh graph and to adapt the mesh discretization during forward simulation. The model uses an Encode-Process-Decode architecture trained with one-step supervision, and can be applied iteratively to generate long trajectories at inference time. The encoder transforms the input mesh $M^{t}$ into a graph, adding extra world-space edges. The processor performs several rounds of message passing along mesh edges and world edges, updating all node and edge embeddings. The decoder extracts the acceleration for each node, which is used to update the mesh to produce $M^{t+1}$." ;
    skos:prefLabel "MeshGraphNet" .

:Meta-augmentation a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2007.05549v2> ;
    skos:definition """**Meta-augmentation** helps generate more varied tasks for a single example in meta-learning. It can be distinguished from data augmentation in classic machine learning as follows. For data augmentation in classical machine learning, the aim is to generate more varied examples, within a single task. Meta-augmentation has the exact opposite aim: we wish to generate more varied tasks,\r
for a single example, to force the learner to quickly learn a new task from feedback. In meta-augmentation, adding randomness discourages the base learner and model from learning trivial solutions that do not generalize to new tasks.""" ;
    skos:prefLabel "Meta-augmentation" .

:MetaFormer a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2111.11418v3> ;
    skos:definition "MetaFormer is a general architecture abstracted from Transformers by not specifying the token mixer." ;
    skos:prefLabel "MetaFormer" .

:MetaPseudoLabels a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2003.10580v4> ;
    skos:definition "**Meta Pseudo Labels** is a semi-supervised learning method that uses a teacher network to generate pseudo labels on unlabeled data to teach a student network. The teacher receives feedback from the student to inform the teacher to generate better pseudo labels. This feedback signal is used as a reward to train the teacher throughout the course of the student’s learning." ;
    skos:prefLabel "Meta Pseudo Labels" .

:Metrix a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2106.04990v2> ;
    skos:altLabel "Metric mixup" ;
    skos:definition "A generic way of representing and interpolating labels, which allows straightforward extension of any kind of [mixup](https://paperswithcode.com/method/mixup) to deep metric learning for a large class of loss functions." ;
    skos:prefLabel "Metrix" .

:MetropolisHastings a skos:Concept ;
    skos:definition """**Metropolis-Hastings** is a Markov Chain Monte Carlo (MCMC) algorithm for approximate inference. It allows for sampling from a probability distribution where direct sampling is difficult - usually owing to the presence of an intractable integral.\r
\r
M-H consists of a proposal distribution $q\\left(\\theta^{'}\\mid\\theta\\right)$ to draw a parameter value. To decide whether $\\theta^{'}$ is accepted or rejected, we then calculate a ratio:\r
\r
$$ \\frac{p\\left(\\theta^{'}\\mid{D}\\right)}{p\\left(\\theta\\mid{D}\\right)} $$\r
\r
We then draw a random number $r \\in \\left[0, 1\\right]$ and accept if it is under the ratio, reject otherwise. If we accept, we set $\\theta_{i} = \\theta^{'}$ and repeat.\r
\r
By the end we have a sample of $\\theta$ values that we can use to form quantities over an approximate posterior, such as the expectation and uncertainty bounds. In practice, we typically have a period of tuning to achieve an acceptable acceptance ratio for the algorithm, as well as a warmup period to reduce bias towards initialization values.\r
\r
Image: [Samuel Hudec](https://static1.squarespace.com/static/52e69d46e4b05a145935f24d/t/5a7dbadcf9619a745c5b2513/1518189289690/Stan.pdf)""" ;
    skos:prefLabel "Metropolis Hastings" .

:MiVOS a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2103.07941v3> ;
    skos:altLabel "Modular Interactive VOS" ;
    skos:definition "**MiVOS** is a video object segmentation model which decouples interaction-to-mask and mask propagation. By decoupling interaction from propagation, MiVOS is versatile and not limited by the type of interactions. It uses three modules: Interaction-to-Mask, Propagation and Difference-Aware Fusion. Trained separately, the interaction module converts user interactions to an object mask, which is then temporally propagated by our propagation module using a novel top-filtering strategy in reading the space-time memory. To effectively take the user's intent into account, a novel difference-aware module is proposed to learn how to properly fuse the masks before and after each interaction, which are aligned with the target frames by employing the space-time memory." ;
    skos:prefLabel "MiVOS" .

:MinCutPool a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1907.00481v6> ;
    skos:altLabel "MinCut Pooling" ;
    skos:definition """MinCutPool is a trainable pooling operator for graphs that learns to map nodes into clusters.\r
The method is trained to approximate the minimum K-cut of the graph to ensure that the clusters are balanced, while also jointly optimizing the objective of the task at hand.""" ;
    skos:prefLabel "MinCutPool" .

:MinibatchDiscrimination a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1606.03498v1> ;
    skos:definition "**Minibatch Discrimination** is a discriminative technique for generative adversarial networks where we discriminate between whole minibatches of samples rather than between individual samples. This is intended to avoid collapse of the generator." ;
    skos:prefLabel "Minibatch Discrimination" .

:Mirror-BERT a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2104.08027v2> ;
    rdfs:seeAlso <https://github.com/cambridgeltl/mirror-bert> ;
    skos:definition "Mirror-BERT converts pretrained language models into effective universal text encoders without any supervision, in 20-30 seconds. It is an extremely simple, fast, and effective contrastive learning technique. It relies on fully identical *or* slightly modified string pairs as positive (i.e., synonymous) fine-tuning examples, and aims to maximise their similarity during identity fine-tuning." ;
    skos:prefLabel "Mirror-BERT" .

:Mish a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1908.08681v3> ;
    rdfs:seeAlso <https://github.com/digantamisra98/Mish/blob/39af18c5265d578a7a8a99308c50337aa8f95ff4/Mish/Torch/mish.py#L12> ;
    skos:definition """**Mish** is an activation function for neural networks which can be defined as:\r
\r
$$ f\\left(x\\right) = x\\cdot\\tanh{\\text{softplus}\\left(x\\right)}$$\r
\r
where\r
\r
$$\\text{softplus}\\left(x\\right) = \\ln\\left(1+e^{x}\\right)$$\r
\r
(Compare with functionally similar previously proposed activation functions such as the [GELU](https://paperswithcode.com/method/silu) $x\\Phi(x)$ and the [SiLU](https://paperswithcode.com/method/silu) $x\\sigma(x)$.)""" ;
    skos:prefLabel "Mish" .

:Mix-FFN a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2105.15203v3> ;
    skos:definition """**Mix-FFN** is a feedforward layer used in the [SegFormer](https://paperswithcode.com/method/segformer) architecture. [ViT](https://www.paperswithcode.com/method/vision-transformer) uses [positional encoding](https://paperswithcode.com/methods/category/position-embeddings) (PE) to introduce the location information. However, the resolution of $\\mathrm{PE}$ is fixed. Therefore, when the test resolution is different from the training one, the positional code needs to be interpolated and this often leads to dropped accuracy. To alleviate this problem, [CPVT](https://www.paperswithcode.com/method/cpvt) uses $3 \\times 3$ Conv together with the PE to implement a data-driven PE. The authors of Mix-FFN argue that positional encoding is actually not necessary for semantic segmentation. Instead, they use Mix-FFN which considers the effect of zero padding to leak location information, by directly using a $3 \\times 3$ Conv in the feed-forward network (FFN). Mix-FFN can be formulated as:\r
\r
$$\r
\\mathbf{x}\\_{\\text {out }}=\\operatorname{MLP}\\left(\\operatorname{GELU}\\left(\\operatorname{Conv}\\_{3 \\times 3}\\left(\\operatorname{MLP}\\left(\\mathbf{x}\\_{i n}\\right)\\right)\\right)\\right)+\\mathbf{x}\\_{i n}\r
$$\r
\r
where $\\mathbf{x}\\_{i n}$ is the feature from a self-attention module. Mix-FFN mixes a $3 \\times 3$ convolution and an MLP into each FFN.""" ;
    skos:prefLabel "Mix-FFN" .

:MixConv a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1907.09595v3> ;
    rdfs:seeAlso <https://github.com/rwightman/pytorch-image-models/blob/e0685dd41585a75385c2f62352669cf5e1df278f/timm/models/layers/mixed_conv2d.py#L20> ;
    skos:altLabel "Mixed Depthwise Convolution" ;
    skos:definition "**MixConv**, or **Mixed Depthwise Convolution**, is a type of [depthwise convolution](https://paperswithcode.com/method/depthwise-convolution) that naturally mixes up multiple kernel sizes in a single [convolution](https://paperswithcode.com/method/convolution). It is based on the insight that depthwise convolution applies a single kernel size to all channels, which MixConv overcomes by combining the benefits of multiple kernel sizes. It does this by partitioning channels into groups and applying a different kernel size to each group." ;
    skos:prefLabel "MixConv" .

:MixNet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1907.09595v3> ;
    rdfs:seeAlso <https://github.com/osmr/imgclsmob/blob/c03fa67de3c9e454e9b6d35fe9cbb6b15c28fda7/pytorch/pytorchcv/models/mixnet.py#L344> ;
    skos:definition "**MixNet** is a type of convolutional neural network discovered via AutoML that utilises MixConvs instead of regular depthwise convolutions." ;
    skos:prefLabel "MixNet" .

:MixText a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2004.12239v1> ;
    skos:definition "**MixText** is a semi-supervised learning method for text classification, which uses a new data augmentation method called TMix. TMix creates a large amount of augmented training samples by interpolating text in hidden space. The technique leverages advances in data augmentation to guess low-entropy labels for unlabeled data, making them as easy to use as labeled data." ;
    skos:prefLabel "MixText" .

:MixedAttentionBlock a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2008.02496v3> ;
    skos:definition "**Mixed Attention Block** is an attention module used in the [ConvBERT](https://paperswithcode.com/method/convbert) architecture. It is a mixture of [self-attention](https://paperswithcode.com/method/scaled) and [span-based dynamic convolution](https://paperswithcode.com/method/span-based-dynamic-convolution) (highlighted in pink). They share the same Query but use different Key to generate the attention map and [convolution](https://paperswithcode.com/method/convolution) kernel respectively. The number of attention heads is reducing by directly projecting the input to a smaller embedding space to form a bottleneck structure for self-attention and span-based dynamic convolution. Dimensions of the input and output of some blocks are labeled on the left top corner to illustrate the overall framework, where $d$ is the embedding size of the input and $\\gamma$ is the reduction ratio." ;
    skos:prefLabel "Mixed Attention Block" .

:MixerLayer a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2105.01601v4> ;
    rdfs:seeAlso <https://github.com/google-research/vision_transformer> ;
    skos:altLabel "MLP-Mixer Layer" ;
    skos:definition """A Mixer layer is a layer used in the MLP-Mixer architecture proposed by Tolstikhin et. al (2021) for computer vision. Mixer layers consist purely of MLPs, without convolutions or attention. It takes an input of embedded image patches (tokens), with its output having the same shape as its input, similar to that of a Vision Transformer encoder. As suggested by its name, Mixer layers "mix" tokens and channels through its "token mixing" and "channel mixing" MLPs contained the layer. It utilizes previous techniques by other architectures, such as layer normalization, skip-connections, and regularization methods.\r
\r
Image credit: Tolstikhin, I. O., Houlsby, N., Kolesnikov, A., Beyer, L., Zhai, X., Unterthiner, T., ... & Dosovitskiy, A. (2021). Mlp-mixer: An all-mlp architecture for vision. Advances in Neural Information Processing Systems, 34, 24261-24272.""" ;
    skos:prefLabel "Mixer Layer" .

:MixtureNormalization a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1806.02892v2> ;
    skos:definition """**Mixture Normalization** is normalization technique that relies on an approximation of the probability density function of the internal representations. Any continuous distribution can be approximated with arbitrary precision using a Gaussian Mixture Model (GMM). Hence, instead of computing one set of statistical measures from the entire population (of instances in the mini-batch) as [Batch Normalization](https://paperswithcode.com/method/batch-normalization) does, Mixture Normalization works on sub-populations which can be identified by disentangling modes of the distribution, estimated via GMM. \r
\r
While BN can only scale and/or shift the whole underlying probability density function, mixture normalization operates like a soft piecewise normalizing transform, capable of completely re-structuring the data distribution by independently scaling and/or shifting individual modes of distribution.""" ;
    skos:prefLabel "Mixture Normalization" .

:MixtureofLogisticDistributions a skos:Concept ;
    skos:definition """**Mixture of Logistic Distributions (MoL)** is a type of output function, and an alternative to a [softmax](https://paperswithcode.com/method/softmax) layer. Discretized logistic mixture likelihood is used in [PixelCNN](https://paperswithcode.com/method/pixelcnn)++ and [WaveNet](https://paperswithcode.com/method/wavenet) to predict discrete values.\r
\r
Image Credit: [Hao Gao](https://medium.com/@smallfishbigsea/an-explanation-of-discretized-logistic-mixture-likelihood-bdfe531751f0)""" ;
    skos:prefLabel "Mixture of Logistic Distributions" .

:MixtureofSoftmaxes a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1711.03953v4> ;
    rdfs:seeAlso <https://github.com/zihangdai/mos/blob/6f89b283878bf17541672391d098c9e6c27b09cf/model.py#L103> ;
    skos:definition "**Mixture of Softmaxes** performs $K$ different softmaxes and mixes them. The motivation is that the traditional [softmax](https://paperswithcode.com/method/softmax) suffers from a softmax bottleneck, i.e. the expressiveness of the conditional probability we can model is constrained by the combination of a dot product and the softmax. By using a mixture of softmaxes, we can model the conditional probability more expressively." ;
    skos:prefLabel "Mixture of Softmaxes" .

:Mixup a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1710.09412v2> ;
    rdfs:seeAlso <https://github.com/facebookresearch/mixup-cifar10> ;
    skos:definition """**Mixup** is a data augmentation technique that generates a weighted combination of random image pairs from the training data. Given two images and their ground truth labels: $\\left(x\\_{i}, y\\_{i}\\right), \\left(x\\_{j}, y\\_{j}\\right)$, a synthetic training example $\\left(\\hat{x}, \\hat{y}\\right)$ is generated as:\r
\r
$$ \\hat{x} = \\lambda{x\\_{i}} + \\left(1 − \\lambda\\right){x\\_{j}} $$\r
$$ \\hat{y} = \\lambda{y\\_{i}} + \\left(1 − \\lambda\\right){y\\_{j}} $$\r
\r
where $\\lambda \\sim \\text{Beta}\\left(\\alpha = 0.2\\right)$ is independently sampled for each augmented example.""" ;
    skos:prefLabel "Mixup" .

:MnasNet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1807.11626v3> ;
    rdfs:seeAlso <https://github.com/osmr/imgclsmob/blob/c03fa67de3c9e454e9b6d35fe9cbb6b15c28fda7/pytorch/pytorchcv/models/mnasnet.py#L161> ;
    skos:definition "**MnasNet** is a type of convolutional neural network optimized for mobile devices that is discovered through mobile [neural architecture search](https://paperswithcode.com/method/neural-architecture-search), which explicitly incorporates model latency into the main objective so that the search can identify a model that achieves a good trade-off between accuracy and latency. The main building block is an [inverted residual block](https://paperswithcode.com/method/inverted-residual-block) (from [MobileNetV2](https://paperswithcode.com/method/mobilenetv2))." ;
    skos:prefLabel "MnasNet" .

:MoBY a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2105.04553v2> ;
    skos:definition """**MoBY** is a self-supervised learning approach for [Vision Transformers](methods/category/vision-transformer). The approach is basically a combination of [MoCo v2](https://paperswithcode.com/method/moco-v2) and [BYOL](https://paperswithcode.com/method/byol). It inherits the momentum design, the key queue, and the contrastive loss used in MoCo v2, and inherits the asymmetric encoders, asymmetric data augmentations and the momentum scheduler in BYOL. It is named MoBY by picking the first two letters of each method.\r
\r
The MoBY approach is illustrated in the Figure. There are two encoders: an online encoder and a target encoder. Both two encoders consist of a backbone and a projector head ([2-layer MLP](https://paperswithcode.com/method/feedforward-network)), and the online encoder introduces an additional prediction head (2-layer MLP), which makes the two encoders asymmetric. The online encoder is updated by gradients, and the target encoder is a moving average of the online encoder by momentum updating in each training iteration. A gradually increasing momentum updating strategy is applied for on the target encoder: the value of momentum term is gradually increased to 1 during the course of training. The default starting value is $0.99$.\r
\r
A contrastive loss is applied to learn the representations. Specifically, for an online view $q$, its contrastive loss is computed as\r
\r
$$\r
\\mathcal{L}\\_{q}=-\\log \\frac{\\exp \\left(q \\cdot k\\_{+} / \\tau\\right)}{\\sum\\_{i=0}^{K} \\exp \\left(q \\cdot k\\_{i} / \\tau\\right)}\r
$$\r
\r
where $k\\_{+}$is the target feature for the other view of the same image; $k\\_{i}$ is a target feature in the key queue; $\\tau$ is a temperature term; $K$ is the size of the key queue (4096 by default).\r
\r
In training, like most [Transformer-based methods](https://paperswithcode.com/methods/category/transformers), the [AdamW](https://paperswithcode.com/method/adamw) optimizer is used, in contrast to previous [self-supervised learning approaches](https://paperswithcode.com/methods/category/self-supervised-learning) built on [ResNet](https://paperswithcode.com/method/resnet) backbone where usually [SGD](https://paperswithcode.com/method/sgd-with-momentum) or [LARS](https://paperswithcode.com/method/lars) $[4,8,19]$ is used. The authors also use a regularization method of asymmetric [drop path](https://paperswithcode.com/method/droppath) which proves important for the final performance.\r
\r
In the experiments, the authors adopt a fixed learning rate of $0.001$ and a fixed weight decay of $0.05$, which performs stably well. Hyper-parameters are tuned of the key queue size $K$, the starting momentum value of the target branch, the temperature $\\tau$, and the drop path rates.""" ;
    skos:prefLabel "MoBY" .

:MoCo a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1911.05722v3> ;
    rdfs:seeAlso <https://github.com/facebookresearch/moco/blob/3631be074a0a14ab85c206631729fe035e54b525/moco/builder.py#L6> ;
    skos:altLabel "Momentum Contrast" ;
    skos:definition """**MoCo**, or **Momentum Contrast**, is a self-supervised learning algorithm with a contrastive loss. \r
\r
Contrastive loss methods can be thought of as building dynamic dictionaries. The "keys" (tokens) in the dictionary are sampled from data (e.g., images or patches) and are represented by an encoder network. Unsupervised learning trains encoders to perform dictionary look-up: an encoded “query” should be similar to its matching key and dissimilar to others. Learning is formulated as minimizing a contrastive loss. \r
\r
MoCo can be viewed as a way to build large and consistent dictionaries for unsupervised learning with a contrastive loss. In MoCo, we maintain the dictionary as a queue of data samples: the encoded representations of the current mini-batch are enqueued, and the oldest are dequeued. The queue decouples the dictionary size from the mini-batch size, allowing it to be large. Moreover, as the dictionary keys come from the preceding several mini-batches, a slowly progressing key encoder, implemented as a momentum-based moving average of the query encoder, is proposed to maintain consistency.""" ;
    skos:prefLabel "MoCo" .

:MoCov2 a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2003.04297v1> ;
    rdfs:seeAlso <https://github.com/facebookresearch/moco> ;
    skos:definition """**MoCo v2** is an improved version of the [Momentum Contrast](https://paperswithcode.com/method/moco) self-supervised learning algorithm. Motivated by the findings presented in the [SimCLR](https://paperswithcode.com/method/simclr) paper, authors:\r
\r
- Replace the 1-layer fully connected layer with a 2-layer MLP head with [ReLU](https://paperswithcode.com/method/relu) for the unsupervised training stage.\r
- Include blur augmentation.\r
- Use cosine learning rate schedule.\r
\r
These modifications enable MoCo to outperform the state-of-the-art SimCLR with a smaller batch size and fewer epochs.""" ;
    skos:prefLabel "MoCo v2" .

:MoCov3 a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2104.02057v4> ;
    skos:definition """**MoCo v3** aims to stabilize training of self-supervised ViTs. MoCo v3 is an incremental improvement of MoCo v1/2. Two crops are used for each image under random data augmentation. They are encoded by two encoders $f_q$ and $f_k$ with output vectors $q$ and $k$. $q$ behaves like a "query", where the goal of learning is to retrieve the corresponding "key". The objective is to minimize a contrastive loss function of the following form: \r
\r
$$\r
\\mathcal{L_q}=-\\log \\frac{\\exp \\left(q \\cdot k^{+} / \\tau\\right)}{\\exp \\left(q \\cdot k^{+} / \\tau\\right)+\\sum_{k^{-}} \\exp \\left(q \\cdot k^{-} / \\tau\\right)}\r
$$\r
\r
This approach aims to train the Transformer in the contrastive/Siamese paradigm. The encoder $f_q$ consists of a backbone (e.g., ResNet and ViT), a projection head, and an extra prediction head. The encoder $f_k$ has the back the backbone and projection head but not the prediction head. $f_k$ is updated by the moving average of $f_q$, excluding the prediction head.""" ;
    skos:prefLabel "MoCo v3" .

:MoGA-A a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1908.01314v4> ;
    rdfs:seeAlso <https://github.com/xiaomi-automl/MoGA/blob/5ddc4bb9a1be845953947f6ec056cfce5ef965b9/models/MoGA_A.py#L122> ;
    skos:definition "**MoGA-A** is a convolutional neural network optimized for mobile latency and discovered via Mobile GPU-Aware (MoGA) [neural architecture search](https://paperswithcode.com/method/neural-architecture-search). The basic building block is MBConvs (inverted residual blocks) from [MobileNetV2](https://paperswithcode.com/method/mobilenetv2). Squeeze-and-excitation layers are also experimented with." ;
    skos:prefLabel "MoGA-A" .

:MoGA-B a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1908.01314v4> ;
    rdfs:seeAlso <https://github.com/xiaomi-automl/MoGA/blob/5ddc4bb9a1be845953947f6ec056cfce5ef965b9/models/MoGA_B.py#L122> ;
    skos:definition "**MoGA-B** is a convolutional neural network optimized for mobile latency and discovered via Mobile GPU-Aware (MoGA) [neural architecture search](https://paperswithcode.com/method/neural-architecture-search). The basic building block is MBConvs (inverted residual blocks) from [MobileNetV2](https://paperswithcode.com/method/mobilenetv2). Squeeze-and-excitation layers are also experimented with." ;
    skos:prefLabel "MoGA-B" .

:MoGA-C a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1908.01314v4> ;
    rdfs:seeAlso <https://github.com/xiaomi-automl/MoGA/blob/5ddc4bb9a1be845953947f6ec056cfce5ef965b9/models/MoGA_C.py#L122> ;
    skos:definition "**MoGA-C** is a convolutional neural network optimized for mobile latency and discovered via Mobile GPU-Aware (MoGA) [neural architecture search](https://paperswithcode.com/method/neural-architecture-search). The basic building block is MBConvs (inverted residual blocks) from [MobileNetV2](https://paperswithcode.com/method/mobilenetv2). Squeeze-and-excitation layers are also experimented with." ;
    skos:prefLabel "MoGA-C" .

:MoNet a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1611.08402v3> ;
    skos:altLabel "Mixture model network" ;
    skos:definition """Mixture model network (MoNet) is a general framework allowing to design convolutional deep architectures on non-Euclidean domains such as graphs and manifolds.\r
\r
Image and description from: [Geometric deep learning on graphs and manifolds using mixture model CNNs](https://arxiv.org/pdf/1611.08402.pdf)""" ;
    skos:prefLabel "MoNet" .

:MoViNet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2103.11511v2> ;
    skos:definition "**Mobile Video Network**, or **MoViNet**, is a type of computation and memory efficient video network that can operate on streaming video for online inference. Three techniques are used to improve efficiency while reducing the peak memory usage of 3D CNNs. First, a video network search space is designed and [neural architecture search](https://paperswithcode.com/method/neural-architecture-search) employed to generate efficient and diverse 3D CNN architectures. Second, a Stream Buffer technique is introduced that decouples memory from video clip duration, allowing 3D CNNs to embed arbitrary-length streaming video sequences for both training and inference with a small constant memory footprint. Third, a simple ensembling technique is used to improve accuracy further without sacrificing efficiency." ;
    skos:prefLabel "MoViNet" .

:MobileBERT a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2004.02984v2> ;
    skos:definition "**MobileBERT** is a type of inverted-bottleneck [BERT](https://paperswithcode.com/method/bert) that compresses and accelerates the popular BERT model. MobileBERT is a thin version of BERT_LARGE, while equipped with bottleneck structures and a carefully designed balance between self-attentions and feed-forward networks. To train MobileBERT, we first train a specially designed teacher model, an inverted-bottleneck incorporated BERT_LARGE model. Then, we conduct knowledge transfer from this teacher to MobileBERT. Like the original BERT, MobileBERT is task-agnostic, that is, it can be generically applied to various downstream NLP tasks via simple fine-tuning. It is trained by layer-to-layer imitating the inverted bottleneck BERT." ;
    skos:prefLabel "MobileBERT" .

:MobileDenseNet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2004.12186v2> ;
    skos:definition "" ;
    skos:prefLabel "Mobile DenseNet" .

:MobileDet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2004.14525v3> ;
    skos:definition "**MobileDet** is an object detection model developed for mobile accelerators. MobileDets uses regular convolutions extensively on EdgeTPUs and DSPs, especially in the early stage of the network where depthwise convolutions tend to be less efficient.  This helps boost the latency-accuracy trade-off for object detection on accelerators, provided that they are placed strategically in the network via [neural architecture search](https://paperswithcode.com/method/neural-architecture-search). By incorporating regular convolutions in the search space and directly optimizing the network architectures for object detection, an efficient family of object detection models is obtained." ;
    skos:prefLabel "MobileDet" .

:MobileNetV1 a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1704.04861v1> ;
    rdfs:seeAlso <https://github.com/osmr/imgclsmob/blob/956b4ebab0bbf98de4e1548287df5197a3c7154e/pytorch/pytorchcv/models/mobilenet.py#L14> ;
    skos:definition "**MobileNet** is a type of convolutional neural network designed for mobile and embedded vision applications. They are based on a streamlined architecture that uses depthwise separable convolutions to build lightweight deep neural networks that can have low latency for mobile and embedded devices." ;
    skos:prefLabel "MobileNetV1" .

:MobileNetV2 a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1801.04381v4> ;
    rdfs:seeAlso <https://github.com/pytorch/vision/blob/6db1569c89094cf23f3bc41f79275c45e9fcb3f3/torchvision/models/mobilenet.py#L77> ;
    skos:definition "**MobileNetV2** is a convolutional neural network architecture that seeks to perform well on mobile devices. It is based on an inverted residual structure where the residual connections are between the bottleneck layers.  The intermediate expansion layer uses lightweight depthwise convolutions to filter features as a source of non-linearity. As a whole, the architecture of MobileNetV2 contains the initial fully [convolution](https://paperswithcode.com/method/convolution) layer with 32 filters, followed by 19 residual bottleneck layers." ;
    skos:prefLabel "MobileNetV2" .

:MobileNetV3 a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1905.02244v5> ;
    rdfs:seeAlso <https://github.com/xiaolai-sqlai/mobilenetv3/blob/adc0ca87e1dd8136cd000ae81869934060171689/mobilenetv3.py#L75> ;
    skos:definition """**MobileNetV3** is a convolutional neural network that is tuned to mobile phone CPUs through a combination of hardware-aware network architecture search (NAS) complemented by the [NetAdapt](https://paperswithcode.com/method/netadapt) algorithm, and then subsequently improved through novel architecture advances. Advances include (1) complementary search techniques, (2) new efficient versions of nonlinearities practical for the mobile setting, (3) new efficient network design.\r
\r
The network design includes the use of a [hard swish](https://paperswithcode.com/method/hard-swish) activation and squeeze-and-excitation modules in the MBConv blocks.""" ;
    skos:prefLabel "MobileNetV3" .

:MobileNeuralNetwork a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2002.12418v1> ;
    skos:altLabel "MNN" ;
    skos:definition "**Mobile Neural Network (MNN)** is a mobile inference engine tailored to mobile applications. The contributions of MNN include: (1) presenting a mechanism called pre-inference that manages to conduct runtime optimization; (2) delivering thorough kernel optimization on operators to achieve optimal computation performance; (3) introducing backend abstraction module which enables hybrid scheduling and keeps the engine lightweight." ;
    skos:prefLabel "Mobile Neural Network" .

:ModeNormalization a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1810.05466v1> ;
    rdfs:seeAlso <https://github.com/ldeecke/mn-torch/blob/8000e582e4809093f3179656e390150ceec048c1/nn/ops.py#L18> ;
    skos:definition "**Mode Normalization** extends normalization to more than a single mean and variance, allowing for detection of modes of data on-the-fly, jointly normalizing samples that share common features. It first assigns samples in a mini-batch to different modes via a gating network, and then normalizes each sample with estimators for its corresponding mode." ;
    skos:prefLabel "Mode Normalization" .

:ModelsGenesis a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2004.07882v4> ;
    skos:definition "**Models Genesis**, or **Generic Autodidactic Models**, is a self-supervised approach for learning 3D image representations. The objective of Models Genesis is to learn a common image representation that is transferable and generalizable across diseases, organs, and modalities.  It consists of an encoder-decoder architecture with skip connections in between, and is trained to learn a common image representation by restoring the original sub-volume $x\\_{i}$ (as ground truth) from the transformed one $\\bar{x}\\_{i}$ (as input), in which the reconstruction loss (MSE) is computed between the model prediction $x'\\_{0}$ and ground truth $x\\_{i}$. Once trained, the encoder alone can be fine-tuned for target classification tasks; while the encoder and decoder together can be fine-tuned for target segmentation tasks." ;
    skos:prefLabel "Models Genesis" .

:MogrifierLSTM a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1909.01792v2> ;
    skos:definition """The **Mogrifier LSTM** is an extension to the [LSTM](https://paperswithcode.com/method/lstm) where the LSTM’s input $\\mathbf{x}$ is gated conditioned on the output of the previous step $\\mathbf{h}\\_{prev}$. Next, the gated input is used in a similar manner to gate the output of the\r
previous time step. After a couple of rounds of this mutual gating, the last updated $\\mathbf{x}$ and $\\mathbf{h}\\_{prev}$ are fed to an LSTM.  \r
\r
In detail, the Mogrifier is an LSTM where two inputs $\\mathbf{x}$ and $\\mathbf{h}\\_{prev}$ modulate one another in an alternating fashion before the usual LSTM computation takes place. That is: $ \\text{Mogrify}\\left(\\mathbf{x}, \\mathbf{c}\\_{prev}, \\mathbf{h}\\_{prev}\\right) = \\text{LSTM}\\left(\\mathbf{x}^{↑}, \\mathbf{c}\\_{prev}, \\mathbf{h}^{↑}\\_{prev}\\right)$ where the modulated inputs $\\mathbf{x}^{↑}$ and $\\mathbf{h}^{↑}\\_{prev}$ are defined as the highest indexed $\\mathbf{x}^{i}$ and $\\mathbf{h}^{i}\\_{prev}$, respectively, from the interleaved sequences:\r
\r
$$ \\mathbf{x}^{i} = 2\\sigma\\left(\\mathbf{Q}^{i}\\mathbf{h}^{i−1}\\_{prev}\\right) \\odot x^{i-2} \\text{ for odd } i \\in \\left[1 \\dots r\\right] $$\r
\r
$$ \\mathbf{h}^{i}\\_{prev}  = 2\\sigma\\left(\\mathbf{R}^{i}\\mathbf{x}^{i-1}\\right) \\odot \\mathbf{h}^{i-2}\\_{prev} \\text{ for even } i \\in \\left[1 \\dots r\\right] $$\r
\r
with $\\mathbf{x}^{-1} = \\mathbf{x}$ and $\\mathbf{h}^{0}\\_{prev} = \\mathbf{h}\\_{prev}$. The number of "rounds", $r \\in \\mathbb{N}$, is a hyperparameter; $r = 0$ recovers the LSTM. Multiplication with the constant 2 ensures that randomly initialized $\\mathbf{Q}^{i}$, $\\mathbf{R}^{i}$ matrices result in transformations close to identity. To reduce the number of additional model parameters, we typically factorize the $\\mathbf{Q}^{i}$, $\\mathbf{R}^{i}$ matrices as products of low-rank matrices: $\\mathbf{Q}^{i}$ =\r
$\\mathbf{Q}^{i}\\_{left}\\mathbf{Q}^{i}\\_{right}$ with $\\mathbf{Q}^{i} \\in \\mathbb{R}^{m\\times{n}}$, $\\mathbf{Q}^{i}\\_{left} \\in \\mathbb{R}^{m\\times{k}}$, $\\mathbf{Q}^{i}\\_{right} \\in \\mathbb{R}^{k\\times{n}}$, where $k < \\min\\left(m, n\\right)$ is the rank.""" ;
    skos:prefLabel "Mogrifier LSTM" .

:MonoPort a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2007.13988v1> ;
    skos:altLabel "Monocular Real-Time Volumetric Performance Capture" ;
    skos:definition "" ;
    skos:prefLabel "MonoPort" .

:Monte-CarloTreeSearch a skos:Concept ;
    skos:definition """**Monte-Carlo Tree Search** is a planning algorithm that accumulates value estimates obtained from Monte Carlo simulations in order to successively direct simulations towards more highly-rewarded trajectories. We execute MCTS after encountering each new state to select an agent's action for that state: it is executed again to select the action for the next state. Each execution is an iterative process that simulates many trajectories starting from the current state to the terminal state. The core idea is to successively focus multiple simulations starting at the current state by extending the initial portions of trajectories that have received high evaluations from earlier simulations.\r
\r
Source: Sutton and Barto, Reinforcement Learning (2nd Edition)\r
\r
Image Credit: [Chaslot et al](https://www.aaai.org/Papers/AIIDE/2008/AIIDE08-036.pdf)""" ;
    skos:prefLabel "Monte-Carlo Tree Search" .

:MonteCarloDropout a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1506.02142v6> ;
    skos:definition "" ;
    skos:prefLabel "Monte Carlo Dropout" .

:Morphence a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2108.13952v4> ;
    skos:definition "**Morphence** is an approach for adversarial defense that shifts the defense landscape by making a model a moving target against adversarial examples. By regularly moving the decision function of a model, Morphence makes it significantly challenging for repeated or correlated attacks to succeed. Morphence deploys a pool of models generated from a base model in a manner that introduces sufficient randomness when it responds to prediction queries. To ensure repeated or correlated attacks fail, the deployed pool of models automatically expires after a query budget is reached and the model pool is replaced by a new model pool generated in advance." ;
    skos:prefLabel "Morphence" .

:MotionDisentanglement a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2202.14019v2> ;
    skos:altLabel "Self-Supervised Motion Disentanglement" ;
    skos:definition "A self-supervised learning method to disentangle irregular (anomalous) motion from regular motion in unlabeled videos." ;
    skos:prefLabel "Motion Disentanglement" .

:MotionNet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2003.06754v1> ;
    skos:definition "**MotionNet** is a system for joint perception and motion prediction based on a bird's eye view (BEV) map, which encodes the object category and motion information from 3D point clouds in each grid cell. MotionNet takes a sequence of LiDAR sweeps as input and outputs the bird's eye view (BEV) map. The backbone of MotionNet is a spatio-temporal pyramid network, which extracts deep spatial and temporal features in a hierarchical fashion. To enforce the smoothness of predictions over both space and time, the training of MotionNet is further regularized with novel spatial and temporal consistency losses." ;
    skos:prefLabel "MotionNet" .

:MovementPruning a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2005.07683v2> ;
    skos:definition "**Movement Pruning** is a simple, deterministic first-order weight pruning method that is more adaptive to pretrained model fine-tuning. Magnitude pruning can be seen as utilizing zeroth-order information (absolute value) of the running model. In contrast, movement pruning methods are where importance is derived from first-order information. Intuitively, instead of selecting weights that are far from zero, we retain connections that are moving away from zero during the training process." ;
    skos:prefLabel "Movement Pruning" .

:MuVER a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2109.05716v1> ;
    skos:definition "**Multi-View Entity Representations**, or **MuVER**, is an approach for entity retrieval that constructs multi-view representations for entity descriptions and approximates the optimal view for mentions via a heuristic searching method. It matches a mention to the appropriate entity by comparing it with entity descriptions. Motivated by the fact that mentions with different contexts correspond to different parts in descriptions, multi-view representations are constructed for each description. Specifically, we segment a description into several sentences. We refer to each sentence as a view $v$, which contains partial information, to form a view set $\\mathcal{V}$ of the entity $e$. The Figure illustrates an example that constructs a view set $\\mathcal{V}$ for “Kobe Bryant”." ;
    skos:prefLabel "MuVER" .

:MuZero a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1911.08265v2> ;
    skos:definition """**MuZero** is a model-based reinforcement learning algorithm. It builds upon [AlphaZero](https://paperswithcode.com/method/alphazero)'s search and search-based policy iteration algorithms, but incorporates a learned model into the training procedure. \r
\r
The main idea of the algorithm is to predict those aspects of the future that are directly relevant for planning. The model receives the observation (e.g. an image of the Go board or the Atari screen) as an\r
input and transforms it into a hidden state. The hidden state is then updated iteratively by a recurrent process that receives the previous hidden state and a hypothetical next action. At every one of these steps the model predicts the policy (e.g. the move to play), value function (e.g. the predicted winner), and immediate reward (e.g. the points scored by playing a move). The model is trained end-to-end, with the sole objective of accurately estimating these three important quantities, so as to match the improved estimates of policy and value generated by search as well as the observed reward. \r
\r
There is no direct constraint or requirement for the hidden state to capture all information necessary to reconstruct the original observation, drastically reducing the amount of information the model has to maintain and predict; nor is there any requirement for the hidden state to match the unknown, true state of the environment; nor any other constraints on the semantics of state. Instead, the hidden states are free to represent state in whatever way is relevant to predicting current and future values and policies. Intuitively, the agent can invent, internally, the rules or dynamics that lead to most accurate planning.""" ;
    skos:prefLabel "MuZero" .

:Multi-AttentionNetwork a skos:Concept ;
    dcterms:source <http://openaccess.thecvf.com/content_cvpr_2017/html/Wang_Multi-Attention_Network_for_CVPR_2017_paper.html> ;
    skos:definition "" ;
    skos:prefLabel "Multi-Attention Network" .

:Multi-DConv-HeadAttention a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2109.08668v2> ;
    skos:definition """**Multi-DConv-Head Attention**, or **MDHA**, is a type of [Multi-Head Attention](https://paperswithcode.com/method/multi-head-attention) that utilizes [depthwise convolutions](https://paperswithcode.com/method/depthwise-convolution) after the multi-head projections. It is used in the [Primer](https://paperswithcode.com/method/primer) [Transformer](https://paperswithcode.com/method/transformer) architecture.\r
\r
Specifically, 3x1 depthwise convolutions are added after each of the multi-head projections for query $Q$, key $K$ and value $V$ in self-attention. These depthwise convolutions are performed over the spatial dimension of each dense projection’s output. Interestingly, this ordering of pointwise followed by depthwise convolution is the reverse of typical [separable convolution](https://paperswithcode.com/method/depthwise-separable-convolution), which the authors find to be less effective. They also find that wider depthwise convolution and [standard convolution](https://paperswithcode.com/method/convolution) not only do not improve performance, but in several cases hurt it. \r
\r
MDHA is similar to [Convolutional Attention](https://paperswithcode.com/method/cvt), which uses [separable convolution](https://paperswithcode.com/method/depthwise-separable-convolution) instead of depthwise convolution and does not apply convolution operations per attention head as in MDHA.""" ;
    skos:prefLabel "Multi-DConv-Head Attention" .

:Multi-HeadAttention a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1706.03762v7> ;
    rdfs:seeAlso <https://github.com/jadore801120/attention-is-all-you-need-pytorch/blob/fec78a687210851f055f792d45300d27cc60ae41/transformer/SubLayers.py#L9> ;
    skos:definition """**Multi-head Attention** is a module for attention mechanisms which runs through an attention mechanism several times in parallel. The independent attention outputs are then concatenated and linearly transformed into the expected dimension. Intuitively, multiple attention heads allows for attending to parts of the sequence differently (e.g. longer-term dependencies versus shorter-term dependencies). \r
\r
$$ \\text{MultiHead}\\left(\\textbf{Q}, \\textbf{K}, \\textbf{V}\\right) = \\left[\\text{head}\\_{1},\\dots,\\text{head}\\_{h}\\right]\\textbf{W}_{0}$$\r
\r
$$\\text{where} \\text{ head}\\_{i} = \\text{Attention} \\left(\\textbf{Q}\\textbf{W}\\_{i}^{Q}, \\textbf{K}\\textbf{W}\\_{i}^{K}, \\textbf{V}\\textbf{W}\\_{i}^{V} \\right) $$\r
\r
Above $\\textbf{W}$ are all learnable parameter matrices.\r
\r
Note that [scaled dot-product attention](https://paperswithcode.com/method/scaled) is most commonly used in this module, although in principle it can be swapped out for other types of attention mechanism.\r
\r
Source: [Lilian Weng](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html#a-family-of-attention-mechanisms)""" ;
    skos:prefLabel "Multi-Head Attention" .

:Multi-HeadLinearAttention a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2006.04768v3> ;
    rdfs:seeAlso <https://github.com/tatp22/linformer-pytorch/blob/3f71fe757ea8c19135880414e707701b24de27f1/linformer_pytorch/linformer_pytorch.py#L173> ;
    skos:definition """**Multi-Head Linear Attention** is a type of linear multi-head self-attention module, proposed with the [Linformer](https://paperswithcode.com/method/linformer) architecture. The main idea is to add two linear projection matrices $E\\_{i}, F\\_{i} \\in \\mathbb{R}^{n\\times{k}}$ when computing key and value. We first project the original $\\left(n \\times d\\right)$-dimensional key and value layers $KW\\_{i}^{K}$ and $VW\\_{i}^{V}$ into $\\left(k\\times{d}\\right)$-dimensional projected key and value layers. We then compute a $\\left(n\\times{k}\\right)$ dimensional context mapping $\\bar{P}$ using scaled-dot product attention:\r
\r
$$ \\bar{\\text{head}\\_{i}} = \\text{Attention}\\left(QW^{Q}\\_{i}, E\\_{i}KW\\_{i}^{K}, F\\_{i}VW\\_{i}^{V}\\right) $$\r
\r
$$ \\bar{\\text{head}\\_{i}} = \\text{softmax}\\left(\\frac{QW^{Q}\\_{i}\\left(E\\_{i}KW\\_{i}^{K}\\right)^{T}}{\\sqrt{d\\_{k}}}\\right) \\cdot F\\_{i}VW\\_{i}^{V} $$\r
\r
Finally, we compute context embeddings for each head using $\\bar{P} \\cdot \\left(F\\_{i}{V}W\\_{i}^{V}\\right)$.""" ;
    skos:prefLabel "Multi-Head Linear Attention" .

:Multi-QueryAttention a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1911.02150v1> ;
    skos:definition """Multi-head attention consists of multiple attention layers (heads) in parallel with different linear\r
transformations on the queries, keys, values and outputs. **Multi-query attention** is identical except that the\r
different heads share a single set of keys and values.""" ;
    skos:prefLabel "Multi-Query Attention" .

:Multi-bandMelGAN a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2005.05106v1> ;
    skos:definition "**Multi-band MelGAN**, or **MB-MelGAN**, is a waveform generation model focusing on high-quality text-to-speech. It improves the original [MelGAN](https://paperswithcode.com/method/melgan) in several ways. First, it increases the receptive field of the generator, which is proven to be beneficial to speech generation. Second, it substitutes the feature matching loss with the multi-resolution STFT loss to better measure the difference between fake and real speech. Lastly, [MelGAN](https://paperswithcode.com/method/melgan) is extended with multi-band processing: the generator takes mel-spectrograms as input and produces sub-band signals which are subsequently summed back to full-band signals as discriminator input." ;
    skos:prefLabel "Multi-band MelGAN" .

:MultiGrain a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1902.05509v2> ;
    rdfs:seeAlso <https://github.com/facebookresearch/multigrain/blob/172bb2a2ef8a7f69f02fd6a4eb20c9c3d5d7a909/multigrain/lib/multigrain.py#L24> ;
    skos:definition "**MultiGrain** is a type of image model that learns a single embedding for classes, instances and copies.  In other words, it is a convolutional neural network that is suitable for both image classification and instance retrieval. We learn MultiGrain by jointly training an image embedding for multiple tasks. The resulting representation is compact and can outperform narrowly-trained embeddings. The learned embedding output incorporates different levels of granularity." ;
    skos:prefLabel "MultiGrain" .

<http://w3id.org/mlso/vocab/ml_algorithm/MultiLoss(BCELoss+FocalLoss)+DiceLoss> a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2209.00729v1> ;
    skos:definition """Our proposed loss function is a combination of BCE Loss, Focal Loss, and Dice loss. Each one of them contributes individually to improve performance further details of loss functions are mentioned below,\r
\r
(1) BCE Loss calculates probabilities and compares each actual class output with predicted probabilities which can be either 0 or 1, it is based on Bernoulli distribution loss, it is mostly used when there are only two classes are available in our case there are exactly two classes are available one is background and other is foreground. In a proposed method it is used for pixel-level classification.\r
\r
(2) Focal Loss is a variant of BCE, it enables the model to focus on learning hard examples by decreasing the wights of easy examples it works well when the data is highly imbalanced.\r
\r
(3) Dice Loss is inspired by the Dice Coefficient Score which is an evaluation metric used to evaluate the results of image segmentation tasks. Dice Coefficient is convex in nature so it has been changed, so it can be more traceable. It is used to calculate the similarity between two images, Dice Loss represent as\r
\r
\r
We proposed a Loss function which is a combination of all three above mention loss functions to benefit from all, BCE is used for pixel-wise classification, Focal Loss is used for learning hard examples, we use 0.25 as the value for alpha and 2.0 as the value of gamma. Dice Loss is used for learning better boundary representation, our proposed loss function represent as\r
\\begin{equation}\r
Loss = \\left( BCE Loss + Focal Loss \\right)  + Dice Loss\r
\\end{equation}""" ;
    skos:prefLabel "Multi Loss ( BCE Loss + Focal Loss )  + Dice Loss" .

:MultipleRandomWindowDiscriminator a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1909.11646v2> ;
    rdfs:seeAlso <https://github.com/yanggeng1995/GAN-TTS/blob/75d70dec97ba11dbe5ee3e2e9ebfbdd10bd52389/models/discriminator.py#L5> ;
    skos:definition """**Multiple Random Window Discriminator** is a discriminator used for the [GAN-TTS](https://paperswithcode.com/method/gan-tts) text-to-speech architecture. These discriminators operate on randomly sub-sampled fragments of the real or generated samples. The ensemble allows for the evaluation of audio in different complementary ways, and is obtained by taking\r
a Cartesian product of two parameter spaces: (i) the size of the random windows fed into the discriminator; (ii) whether a discriminator is conditioned on linguistic and pitch features. For example,\r
in the authors' best-performing model, they consider five window sizes (240, 480, 960, 1920, 3600 samples), which yields 10 discriminators in total. \r
\r
Using random windows of different size, rather than the full generated sample, has a data augmentation effect and also reduces the computational complexity of RWDs. In the first layer of each discriminator, the MRWD reshapes (downsamples) the input raw waveform to a constant\r
temporal dimension $\\omega = 240$ by moving consecutive blocks of samples into the channel dimension, i.e. from $\\left[\\omega\\_{k}, 1\\right]$ to $\\left[\\omega, k\\right]$, where $k$ is the downsampling factor (e.g. $k = 8$ for input window size $1920$). This way, all the RWDs have the same architecture and similar computational complexity despite different window sizes. \r
\r
The conditional discriminators have access to linguistic and pitch features, and can measure whether\r
the generated audio matches the input conditioning. This means that random windows in conditional\r
discriminators need to be aligned with the conditioning frequency to preserve the correspondence\r
between the waveform and linguistic features within the sampled window. This limits the valid sampling to that of the frequency of the conditioning signal (200Hz, or every 5ms). The unconditional\r
discriminators, on the contrary, only evaluate whether the generated audio sounds realistic regardless\r
of the conditioning. The random windows for these discriminators are sampled without constraints\r
at full 24kHz frequency, which further increases the amount of training data. \r
\r
For the architecture, the discriminators consists of blocks (DBlocks) that are similar to the [GBlocks](https://paperswithcode.com/method/gblock) used in the generator, but without batch normalisation. Unconditional RWDs are composed entirely of DBlocks. In conditional RWDs, the input waveform is gradually downsampled by DBlocks, until the temporal dimension of the activation is equal to that of the conditioning, at which point a conditional [DBlock](https://paperswithcode.com/method/dblock) is used. This joint information is then passed to the remaining DBlocks, whose final output is average-pooled to obtain a scalar. The dilation factors in the DBlocks’ convolutions follow the pattern 1, 2, 1, 2 – unlike the generator, the discriminator operates on a relatively small window, and the authors did not observe any benefit from using larger dilation factors.""" ;
    skos:prefLabel "Multiple Random Window Discriminator" .

:MultiplicativeAttention a skos:Concept ;
    dcterms:source <https://ruder.io/deep-learning-nlp-best-practices/> ;
    rdfs:seeAlso <https://github.com/LukasMut/ATNLP/blob/0aa097062dd5c520cfd163c0bfd0cb4a945c3430/models/Attention.py#L37> ;
    skos:definition """**Multiplicative Attention** is an attention mechanism where the alignment score function is calculated as:\r
\r
$$f_{att}\\left(\\textbf{h}_{i}, \\textbf{s}\\_{j}\\right) = \\mathbf{h}\\_{i}^{T}\\textbf{W}\\_{a}\\mathbf{s}\\_{j}$$\r
\r
Here $\\mathbf{h}$ refers to the hidden states for the encoder/source, and $\\mathbf{s}$ is the hidden states for the decoder/target. The function above is thus a type of alignment score function. We can use a matrix of alignment scores to show the correlation between source and target words, as the Figure to the right shows. Within a neural network, once we have the alignment scores, we calculate the final scores using a [softmax](https://paperswithcode.com/method/softmax) function of these alignment scores (ensuring it sums to 1).\r
\r
Additive and multiplicative attention are similar in complexity, although multiplicative attention is faster and more space-efficient in practice as it can be implemented more efficiently using matrix multiplication. Both variants perform similar for small dimensionality $d_{h}$ of the decoder states, but [additive attention](https://paperswithcode.com/method/additive-attention) performs better for larger dimensions. One way to mitigate this is to scale $f_{att}\\left(\\textbf{h}_{i}, \\textbf{s}\\_{j}\\right)$ by $1/\\sqrt{d\\_{h}}$ as with [scaled dot-product attention](https://paperswithcode.com/method/scaled).""" ;
    skos:prefLabel "Multiplicative Attention" .

:MultiscaleDilatedConvolutionBlock a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1609.07093v3> ;
    rdfs:seeAlso <https://github.com/ajbrock/Neural-Photo-Editor/blob/d234cf1f80cf8c8f621f871dc704dc43e212201f/layers.py#L207> ;
    skos:definition "A **Multiscale Dilated Convolution Block** is an Inception-style convolutional block motivated by the ideas that image features naturally occur at multiple scales, that a network’s expressivity is proportional to the range of functions it can represent divided by its total number of parameters, and by the desire to efficiently expand a network’s receptive field. The Multiscale [Dilated Convolution](https://paperswithcode.com/method/dilated-convolution) (MDC) block applies a single $F\\times{F}$ filter at multiple dilation factors, then performs a weighted elementwise sum of each dilated filter’s output, allowing the network to simultaneously learn a set of features and the relevant scales at which those features occur with a minimal increase in parameters. This also rapidly expands the network’s receptive field without requiring an increase in depth or the number of parameters." ;
    skos:prefLabel "Multiscale Dilated Convolution Block" .

:MushroomRL a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2001.01102v2> ;
    skos:definition "**MushroomRL** is an open-source Python library developed to simplify the process of implementing and running Reinforcement Learning (RL) experiments. The architecture of MushroomRL is built in such a way that every component of an RL problem is already provided, and most of the time users can only focus on the implementation of their own algorithms and experiments. MushroomRL comes with a strongly modular architecture that makes it easy to understand how each component is structured and how it interacts with other ones; moreover it provides an exhaustive list of RL methodologies, such as:" ;
    skos:prefLabel "MushroomRL" .

:MutualGuide a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2009.14085v1> ;
    skos:altLabel "Mutual Guidance" ;
    skos:definition "" ;
    skos:prefLabel "MutualGuide" .

:N-stepReturns a skos:Concept ;
    skos:definition """**$n$-step Returns** are used for value function estimation in reinforcement learning. Specifically, for $n$ steps we can write the complete return as:\r
\r
$$ R\\_{t}^{(n)} = r\\_{t+1} + \\gamma{r}\\_{t+2} + \\cdots + \\gamma^{n-1}\\_{t+n} + \\gamma^{n}V\\_{t}\\left(s\\_{t+n}\\right) $$\r
\r
We can then write an $n$-step backup, in the style of TD learning, as:\r
\r
$$ \\Delta{V}\\_{r}\\left(s\\_{t}\\right) = \\alpha\\left[R\\_{t}^{(n)} - V\\_{t}\\left(s\\_{t}\\right)\\right] $$\r
\r
Multi-step returns often lead to faster learning with suitably tuned $n$.\r
\r
Image Credit: Sutton and Barto, Reinforcement Learning""" ;
    skos:prefLabel "N-step Returns" .

:NADAM a skos:Concept ;
    rdfs:seeAlso <https://github.com/Dawn-Of-Eve/nadir> ;
    skos:definition """**NADAM**, or **Nesterov-accelerated Adaptive Moment Estimation**, combines [Adam](https://paperswithcode.com/method/adam) and [Nesterov Momentum](https://paperswithcode.com/method/nesterov-accelerated-gradient). The update rule is of the form:\r
\r
$$ \\theta\\_{t+1} = \\theta\\_{t} - \\frac{\\eta}{\\sqrt{\\hat{v}\\_{t}}+\\epsilon}\\left(\\beta\\_{1}\\hat{m}\\_{t} + \\frac{(1-\\beta\\_{t})g\\_{t}}{1-\\beta^{t}\\_{1}}\\right)$$\r
\r
Image Source: [Incorporating Nesterov Momentum into Adam](http://cs229.stanford.edu/proj2015/054_report.pdf)""" ;
    skos:prefLabel "NADAM" .

:NAFNet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2204.04676v4> ;
    skos:altLabel "Nonlinear Activation Free Network" ;
    skos:definition "" ;
    skos:prefLabel "NAFNet" .

:NAM a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2004.13912v2> ;
    rdfs:seeAlso <https://github.com/google-research/google-research/tree/master/neural_additive_models> ;
    skos:altLabel "Neural Additive Model" ;
    skos:definition """**Neural Additive Models (NAMs)** make restrictions on the structure of neural networks, which yields a family of models that are inherently interpretable while suffering little loss in prediction accuracy when applied to tabular data. Methodologically, NAMs belong to a larger model family called Generalized Additive Models (GAMs). \r
\r
NAMs learn a linear combination of networks that each attend to a single input feature: each $f\\_{i}$ in the traditional GAM formulationis parametrized by a neural network. These networks are trained jointly using backpropagation and can learn arbitrarily complex shape functions. Interpreting NAMs is easy as the impact of a feature on the prediction does not rely on the other features and can be understood by visualizing its corresponding shape function (e.g., plotting $f\\_{i}\\left(x\\_{i}\\right)$ vs. $x\\_{i}$).""" ;
    skos:prefLabel "NAM" .

:NAS-FCOS a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1906.04423v4> ;
    skos:definition "**NAS-FCOS** consists of two sub networks, an [FPN](https://paperswithcode.com/method/fpn) $f$ and a set of prediction heads $h$ which have shared structures. One notable difference with other FPN-based one-stage detectors is that our heads have partially shared weights. Only the last several layers of the predictions heads (marked as yellow) are tied by their weights. The number of layers to share is decided automatically by the search algorithm. Note that both FPN and head are in our actual search space; and have more layers than shown in this figure." ;
    skos:prefLabel "NAS-FCOS" .

:NAS-FPN a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1904.07392v1> ;
    rdfs:seeAlso <https://github.com/tensorflow/tpu/blob/f3bafc6a96197b770178cc6c373dc071387b8cfe/models/official/detection/modeling/architecture/nasfpn.py#L165> ;
    skos:definition "**NAS-FPN** is a Feature Pyramid Network that is discovered via [Neural Architecture Search](https://paperswithcode.com/method/neural-architecture-search) in a novel scalable search space covering all cross-scale connections. The discovered architecture consists of a combination of top-down and bottom-up connections to fuse features across scales" ;
    skos:prefLabel "NAS-FPN" .

:NCL a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2106.05142v1> ;
    skos:altLabel "Neighborhood Contrastive Learning" ;
    skos:definition "" ;
    skos:prefLabel "NCL" .

:NEAT a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2109.04456v1> ;
    skos:altLabel "Neural Attention Fields" ;
    skos:definition "**NEAT**, or **Neural Attention Fields**, is a feature representation for end-to-end imitation learning models. NEAT is a continuous function which maps locations in Bird's Eye View (BEV) scene coordinates to waypoints and semantics, using intermediate attention maps to iteratively compress high-dimensional 2D image features into a compact representation. This allows the model to selectively attend to relevant regions in the input while ignoring information irrelevant to the driving task, effectively associating the images with the BEV representation. Furthermore, visualizing the attention maps for models with NEAT intermediate representations provides improved interpretability." ;
    skos:prefLabel "NEAT" .

:NFN a skos:Concept ;
    dcterms:source <https://doi.org/10.1109/TAI.1994.346442> ;
    skos:altLabel "Neo-fuzzy-neuron" ;
    skos:definition "**Neo-fuzzy-neuron** is a type of artificial neural network that combines the characteristics of both fuzzy logic and neural networks. It uses a fuzzy inference system to model non-linear relationships between inputs and outputs, and a feedforward neural network to learn the parameters of the fuzzy system. The combination of these two approaches provides a flexible and powerful tool for solving a wide range of problems in areas such as pattern recognition, control, and prediction." ;
    skos:prefLabel "NFN" .

:NFR a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2002.09181v1> ;
    skos:altLabel "Negative Face Recognition" ;
    skos:definition "**Negative Face Recognition**, or **NFR**, is a face recognition approach that enhances the soft-biometric privacy on the template-level by representing face templates in a complementary (negative) domain. While ordinary templates characterize facial properties of an individual, negative templates describe facial properties that does not exist for this individual. This suppresses privacy-sensitive information from stored templates. Experiments are conducted on two publicly available datasets captured under controlled and uncontrolled scenarios on three privacy-sensitive attributes." ;
    skos:prefLabel "NFR" .

:NICE a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1410.8516v6> ;
    rdfs:seeAlso <https://github.com/paultsw/nice_pytorch/blob/15cfc543fc3dc81ee70398b8dfc37b67269ede95/nice/models.py#L20> ;
    skos:altLabel "Non-linear Independent Component Estimation" ;
    skos:definition """**NICE**, or **Non-Linear Independent Components Estimation** is a framework for modeling complex high-dimensional densities. It is based on the idea that a good representation is one in which the data has a distribution that is easy to model. For this purpose, a non-linear deterministic transformation of the data is learned that maps it to a latent space so as to make the transformed data conform to a factorized distribution, i.e., resulting in independent latent variables.  The transformation is parameterised so that computing the determinant of the Jacobian and inverse Jacobian is trivial, yet it maintains the ability to learn complex non-linear transformations, via a composition of simple building blocks, each based on a deep neural network. The training criterion is simply the exact log-likelihood. The transformation used in NICE is the [affine coupling](https://paperswithcode.com/method/affine-coupling) layer without the scale term, known as additive coupling layer:\r
\r
$$ y\\_{I\\_{2}} = x\\_{I\\_{2}} + m\\left(x\\_{I\\_{1}}\\right) $$\r
\r
$$ x\\_{I\\_{2}} = y\\_{I\\_{2}} + m\\left(y\\_{I\\_{1}}\\right) $$""" ;
    skos:prefLabel "NICE" .

:NICE-SLAM a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2112.12130v2> ;
    rdfs:seeAlso <https://pengsongyou.github.io/nice-slam> ;
    skos:altLabel "NICE-SLAM: Neural Implicit Scalable Encoding for SLAM" ;
    skos:definition """NICE-SLAM, a dense RGB-D SLAM system that combines neural implicit decoders with hierarchical grid-based representations, which can be applied to large-scale scenes.\r
\r
Neural implicit representations have recently shown encouraging results in various domains, including promising progress in simultaneous localization and mapping (SLAM). Nevertheless, existing methods produce over-smoothed scene reconstructions and have difficulty scaling up to large scenes. These limitations are mainly due to their simple fully-connected network architecture that does not incorporate local information in the observations. In this paper, we present NICE-SLAM, a dense SLAM system that incorporates multi-level local information by introducing a hierarchical scene representation. Optimizing this representation with pre-trained geometric priors enables detailed reconstruction on large indoor scenes. Compared to recent neural implicit SLAM systems, our approach is more scalable, efficient, and robust. Experiments on five challenging datasets demonstrate competitive results of NICE-SLAM in both mapping and tracking quality.""" ;
    skos:prefLabel "NICE-SLAM" .

:NIMA a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1709.05424v2> ;
    skos:altLabel "Neural Image Assessment" ;
    skos:definition "In the context of image enhancement, maximizing NIMA score as a prior can increase the likelihood of enhancing perceptual quality of an image." ;
    skos:prefLabel "NIMA" .

:NLSIG a skos:Concept ;
    rdfs:seeAlso <https://github.com/somefunAgba/NLSIG_COVID19Lab> ;
    skos:altLabel "nlogistic-sigmoid function" ;
    skos:definition "Nlogistic-sigmoid function (NLSIG) is a modern logistic-sigmoid function definition for modelling growth (or decay) processes. It features two logistic metrics (YIR and XIR) for monitoring growth from a two-dimensional (x-y axis) perspective." ;
    skos:prefLabel "NLSIG" .

:NN4G a skos:Concept ;
    skos:altLabel "Neural network for graphs" ;
    skos:definition """NN4G is based on a constructive feedforward architecture with state variables that uses neurons with no feedback connections. The neurons are applied to the input graphs by a general traversal process that relaxes the constraints of previous approaches derived by the causality assumption over hierarchical input data.\r
\r
Description from: [Neural network for graphs: a contextual constructive approach](https://www.meta.org/papers/neural-network-for-graphs-a-contextual/19193509)""" ;
    skos:prefLabel "NN4G" .

:NNCF a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2002.08679v4> ;
    skos:altLabel "Neural Network Compression Framework" ;
    skos:definition "**Neural Network Compression Framework**, or **NNCF**, is a Python-based framework for neural network compression with fine-tuning. It leverages recent advances of various network compression methods and implements some of them, namely quantization, sparsity, filter pruning and binarization. These methods allow producing more hardware-friendly models that can be efficiently run on general-purpose hardware computation units (CPU, GPU) or specialized deep learning accelerators." ;
    skos:prefLabel "NNCF" .

:NNCLR a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2104.14548v2> ;
    skos:altLabel "Nearest-Neighbor Contrastive Learning of Visual Representations" ;
    skos:definition "" ;
    skos:prefLabel "NNCLR" .

:NODE a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1909.06312v2> ;
    skos:altLabel "Neural Oblivious Decision Ensembles" ;
    skos:definition """**Neural Oblivious Decision Ensembles (NODE)** is a tabular data architecture that consists of differentiable\r
oblivious decision trees (ODT) that are trained end-to-end by backpropagation. \r
\r
The core building block is a Neural Oblivious Decision Ensemble (NODE) layer. The layer is composed of $m$ differentiable oblivious decision trees (ODTs) of equal depth $d$. As an input, all $m$ trees get a common vector $x \\in \\mathbb{R}^{n}$, containing $n$ numeric features. Below we describe a design of a single differentiable ODT.\r
\r
In its essence, an ODT is a decision table that splits the data along $d$ splitting features and compares each feature to a learned threshold. Then, the tree returns one of the $2^{d}$ possible responses, corresponding to the comparisons result. Therefore, each ODT is completely determined by its splitting features $f \\in \\mathbb{R}^{d}$, splitting thresholds $b \\in \\mathbb{R}^{d}$ and a $d$-dimensional tensor of responses $R \\in \\mathbb{R} \\underbrace{2 \\times 2 \\times 2}_{d}$. In this notation, the tree output is defined as:\r
\r
$$\r
h(x)=R\\left[\\mathbb{1}\\left(f\\_{1}(x)-b_{1}\\right), \\ldots, \\mathbb{1}\\left(f\\_{d}(x)-b\\_{d}\\right)\\right]\r
$$\r
where $\\mathbb{1}(\\cdot)$ denotes the Heaviside function.""" ;
    skos:prefLabel "NODE" .

:NON a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2005.10114v2> ;
    skos:altLabel "Network On Network" ;
    skos:definition "Network On Network (NON) is practical tabular data classification model based on deep neural network to provide accurate predictions. Various deep methods have been proposed and promising progress has been made. However, most of them use operations like neural network and factorization machines to fuse the embeddings of different features directly, and linearly combine the outputs of those operations to get the final prediction. As a result, the intra-field information and the non-linear interactions between those operations (e.g. neural network and factorization machines) are ignored. Intra-field information is the information that features inside each field belong to the same field. NON is proposed to take full advantage of intra-field information and non-linear interactions. It consists of three components: field-wise network at the bottom to capture the intra-field information, across field network in the middle to choose suitable operations data-drivenly, and operation fusion network on the top to fuse outputs of the chosen operations deeply" ;
    skos:prefLabel "NON" .

:NPID a skos:Concept ;
    dcterms:source <http://openaccess.thecvf.com/content_cvpr_2018/html/Wu_Unsupervised_Feature_Learning_CVPR_2018_paper.html> ;
    skos:definition "**NPID** (Non-Parametric Instance Discrimination) is a self-supervision approach that takes a non-parametric classification approach. Noise contrastive estimation is used to learn representations. Specifically, distances (similarity) between instances are calculated directly from the features in a non-parametric way." ;
    skos:prefLabel "NPID" .

<http://w3id.org/mlso/vocab/ml_algorithm/NPID++> a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1912.01991v1> ;
    skos:definition "**NPID++** (Non-Parametric Instance Discrimination) is a self-supervision approach that takes a non-parametric classification approach. It approves upon [NPID](https://paperswithcode.com/method/npid) by using more negative samples and training for more epochs." ;
    skos:prefLabel "NPID++" .

:NT-ASGD a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1708.02182v1> ;
    rdfs:seeAlso <https://github.com/salesforce/awd-lstm-lm/blob/32fcb42562aeb5c7e6c9dec3f2a3baaaf68a5cb5/main.py#L275> ;
    skos:altLabel "Non-monotonically Triggered ASGD" ;
    skos:definition """**NT-ASGD**, or **Non-monotonically Triggered ASGD**, is an averaged stochastic gradient descent technique. \r
\r
In regular ASGD, we take steps identical to [regular SGD](https://paperswithcode.com/method/sgd) but instead of returning the last iterate as the solution, we return $\\frac{1}{\\left(K-T+1\\right)}\\sum^{T}\\_{i=T}w\\_{i}$, where $K$ is the total number of iterations and $T < K$ is a user-specified averaging trigger.\r
\r
NT-ASGD has a non-monotonic criterion that conservatively triggers the averaging when the validation metric fails to improve for multiple cycles. Given that the choice of triggering is irreversible, this conservatism ensures that the randomness of training does not play a major role in the decision.""" ;
    skos:prefLabel "NT-ASGD" .

:NT-Xent a skos:Concept ;
    dcterms:source <http://papers.nips.cc/paper/6200-improved-deep-metric-learning-with-multi-class-n-pair-loss-objective> ;
    rdfs:seeAlso <https://github.com/google-research/simclr/blob/bfe07eed7f101ab51f3360100a28690e1bfbf6ec/objective.py#L38> ;
    skos:altLabel "Normalized Temperature-scaled Cross Entropy Loss" ;
    skos:definition """**NT-Xent**, or **Normalized Temperature-scaled Cross Entropy Loss**, is a loss function. Let $\\text{sim}\\left(\\mathbf{u}, \\mathbf{v}\\right) = \\mathbf{u}^{T}\\mathbf{v}/||\\mathbf{u}|| ||\\mathbf{v}||$ denote the cosine similarity between two vectors $\\mathbf{u}$ and $\\mathbf{v}$. Then the loss function for a positive pair of examples $\\left(i, j\\right)$ is :\r
\r
$$ \\mathbb{l}\\_{i,j} = -\\log\\frac{\\exp\\left(\\text{sim}\\left(\\mathbf{z}\\_{i}, \\mathbf{z}\\_{j}\\right)/\\tau\\right)}{\\sum^{2N}\\_{k=1}\\mathcal{1}\\_{[k\\neq{i}]}\\exp\\left(\\text{sim}\\left(\\mathbf{z}\\_{i}, \\mathbf{z}\\_{k}\\right)/\\tau\\right)}$$\r
\r
where $\\mathcal{1}\\_{[k\\neq{i}]} \\in ${$0, 1$} is an indicator function evaluating to $1$ iff $k\\neq{i}$ and $\\tau$ denotes a temperature parameter. The final loss is computed across all positive pairs, both $\\left(i, j\\right)$ and $\\left(j, i\\right)$, in a mini-batch.\r
\r
Source: [SimCLR](https://paperswithcode.com/method/simclr)""" ;
    skos:prefLabel "NT-Xent" .

:NTK a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1806.07572v4> ;
    skos:altLabel "Neural Tangent Kernel" ;
    skos:definition "" ;
    skos:prefLabel "NTK" .

:NUQSGD a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1908.06077> ;
    rdfs:seeAlso <https://github.com/fartashf/nuqsgd> ;
    skos:altLabel "Nonuniform Quantization for Stochastic Gradient Descent" ;
    skos:definition "As the size and complexity of models and datasets grow, so does the need for communication-efficient variants of stochastic gradient descent that can be deployed to perform parallel model training. One popular communication-compression method for data-parallel [SGD](https://paperswithcode.com/method/sgd) is QSGD (Alistarh et al., 2017), which quantizes and encodes gradients to reduce communication costs. The baseline variant of QSGD provides strong theoretical guarantees, however, for practical purposes, the authors proposed a heuristic variant which we call QSGDinf, which demonstrated impressive empirical gains for distributed training of large neural networks. In this paper, we build on this work to propose a new gradient quantization scheme, and show that it has both stronger theoretical guarantees than QSGD, and matches and exceeds the empirical performance of the QSGDinf heuristic and of other compression methods." ;
    skos:prefLabel "NUQSGD" .

:NVAE a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2007.03898v3> ;
    skos:altLabel "Nouveau VAE" ;
    skos:definition """**NVAE**, or **Nouveau VAE**, is deep, hierarchical variational autoencoder. It can be trained with the original [VAE](https://paperswithcode.com/method/vae) objective, unlike alternatives such as [VQ-VAE-2](https://paperswithcode.com/method/vq-vae-2). NVAE’s design focuses on tackling two main challenges: (i) designing expressive neural\r
networks specifically for VAEs, and (ii) scaling up the training to a large number of hierarchical\r
groups and image sizes while maintaining training stability.\r
\r
To tackle long-range correlations in the data, the model employs hierarchical multi-scale modelling. The generative model starts from a small spatially arranged latent variables as $\\mathbf{z}\\_{1}$ and samples from the hierarchy group-by-group while gradually doubling the spatial dimensions. This multi-scale approach enables NVAE to capture global long-range correlations at the top of the hierarchy and local fine-grained dependencies at the lower groups.\r
\r
Additional design choices include the use of residual cells for the generative models and the encoder, which employ a number of tricks and modules to achieve good performance, and the use of residual normal distributions to smooth optimization. See the components section for more details.""" ;
    skos:prefLabel "NVAE" .

:NVAEEncoderResidualCell a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2007.03898v3> ;
    skos:definition "The **NVAE Encoder Residual Cell** is a [residual connection](https://paperswithcode.com/method/residual-connection) block used in the [NVAE](https://paperswithcode.com/method/nvae) architecture for the encoder. It applies two series of BN-[Swish](https://paperswithcode.com/method/swish)-Conv layers without changing the number of channels." ;
    skos:prefLabel "NVAE Encoder Residual Cell" .

:NVAEGenerativeResidualCell a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2007.03898v3> ;
    skos:definition "The **NVAE Generative Residual Cell** is a skip connection block used as part of the [NVAE](https://paperswithcode.com/method/nvae) architecture for the generator. The residual cell expands the number of channels $E$ times before applying the [depthwise separable convolution](https://paperswithcode.com/method/depthwise-separable-convolution), and then maps it back to $C$ channels. The design motivation was to help model long-range correlations in the data by increasing the receptive field of the network, which explains the expanding path but also the use of depthwise convolutions to keep a handle on parameter count." ;
    skos:prefLabel "NVAE Generative Residual Cell" .

:NaturalGradientDescent a skos:Concept ;
    skos:definition """**Natural Gradient Descent** is an approximate second-order optimisation method. It has an interpretation as optimizing over a Riemannian manifold using an intrinsic distance metric, which implies the updates are invariant to transformations such as whitening. By using the positive semi-definite (PSD) Gauss-Newton matrix to approximate the (possibly negative definite) Hessian, NGD can often work better than exact second-order methods.\r
\r
Given the gradient of $z$, $g = \\frac{\\delta{f}\\left(z\\right)}{\\delta{z}}$, NGD computes the update as:\r
\r
$$\\Delta{z} = \\alpha{F}^{−1}g$$\r
\r
where the Fisher information matrix $F$ is defined as:\r
\r
$$ F = \\mathbb{E}\\_{p\\left(t\\mid{z}\\right)}\\left[\\nabla\\ln{p}\\left(t\\mid{z}\\right)\\nabla\\ln{p}\\left(t\\mid{z}\\right)^{T}\\right] $$\r
\r
The log-likelihood function $\\ln{p}\\left(t\\mid{z}\\right)$ typically corresponds to commonly used error functions such as the cross entropy loss.\r
\r
Source: [LOGAN](https://paperswithcode.com/method/logan)\r
\r
Image: [Fast Convergence of Natural Gradient Descent for Overparameterized Neural Networks\r
](https://arxiv.org/abs/1905.10961)""" ;
    skos:prefLabel "Natural Gradient Descent" .

:NeRF a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2003.08934v2> ;
    skos:altLabel "Neural Radiance Field" ;
    skos:definition "NeRF represents a scene with learned, continuous volumetric radiance field $F_\\theta$ defined over a bounded 3D volume. In a NeRF, $F_\\theta$ is a multilayer perceptron (MLP) that takes as input a 3D position $x = (x, y, z)$ and unit-norm viewing direction $d = (dx, dy, dz)$, and produces as output a density $\\sigma$ and color $c = (r, g, b)$. The weights of the multilayer perceptron that parameterize $F_\\theta$ are optimized so as to encode the radiance field of the scene. Volume rendering is used to compute the color of a single pixel." ;
    skos:prefLabel "NeRF" .

:NeighborhoodAttention a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2204.07143v5> ;
    skos:definition """Neighborhood Attention is a restricted self attention pattern in which each token's receptive field is limited to its nearest neighboring pixels. It was proposed in [Neighborhood Attention Transformer](https://paperswithcode.com/paper/neighborhood-attention-transformer) as an alternative to other local attention mechanisms used in Hierarchical Vision Transformers.\r
\r
NA is in concept similar to [stand alone self attention (SASA)](https://paperswithcode.com/method/sasa), in that both can be implemented with a raster scan sliding window operation over the key value pair. However, NA would require a modification to handle corner pixels, which helps maintain a fixed receptive field size and an increased number of relative positions.\r
\r
The primary challenge in experimenting with both NA and SASA has been computation. Simply extracting key values for each query is slow, takes up a large amount of memory, and is eventually intractable at scale. NA was therefore implemented through a new CUDA extension to PyTorch, [NATTEN](https://github.com/SHI-Labs/NATTEN).""" ;
    skos:prefLabel "Neighborhood Attention" .

:NesT a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2105.12723v4> ;
    skos:definition "**NesT** stacks canonical transformer layers to conduct local self-attention on every image block independently, and then \"nests\" them hierarchically. Coupling of processed information between spatially adjacent blocks is achieved through a proposed block aggregation between every two hierarchies. The overall hierarchical structure can be determined by two key hyper-parameters: patch size $S × S$ and number of block hierarchies $T_d$. All blocks inside each hierarchy share one set of parameters. Given input of image, each image is linearly projected to an embedding. All embeddings are partitioned to blocks and flattened to generate final input. Each transformer layers is composed of a multi-head self attention (MSA) layer followed by a feed-forward fully-connected network (FFN) with skip-connection and Layer normalization. Positional embeddings are added to encode spatial information before feeding into the block. Lastly, a nested hierarchy with block aggregation is built -- every four spatially connected blocks are merged into one." ;
    skos:prefLabel "NesT" .

:NesterovAcceleratedGradient a skos:Concept ;
    skos:definition """**Nesterov Accelerated Gradient** is a momentum-based [SGD](https://paperswithcode.com/method/sgd) optimizer that "looks ahead" to where the parameters will be to calculate the gradient **ex post** rather than **ex ante**:\r
\r
$$ v\\_{t} = \\gamma{v}\\_{t-1} + \\eta\\nabla\\_{\\theta}J\\left(\\theta-\\gamma{v\\_{t-1}}\\right) $$\r
$$\\theta\\_{t} = \\theta\\_{t-1} + v\\_{t}$$\r
\r
Like SGD with momentum $\\gamma$ is usually set to $0.9$.\r
\r
The intuition is that the [standard momentum](https://paperswithcode.com/method/sgd-with-momentum) method first computes the gradient at the current location and then takes a big jump in the direction of the updated accumulated gradient. In contrast Nesterov momentum first makes a big jump in the direction of the previous accumulated gradient and then measures the gradient where it ends up and makes a correction. The idea being that it is better to correct a mistake after you have made it. \r
\r
Image Source: [Geoff Hinton lecture notes](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)""" ;
    skos:prefLabel "Nesterov Accelerated Gradient" .

:NetAdapt a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1804.03230v2> ;
    rdfs:seeAlso <https://github.com/denru01/netadapt> ;
    skos:definition """**NetAdapt** is a network shrinking algorithm to adapt a pretrained network to a mobile platform given a real resource budget. NetAdapt can incorporate direct metrics, such as latency and energy, into the optimization to maximize the adaptation performance based on the characteristics of the platform. By using empirical measurements, NetAdapt can be applied to any platform as long as we can measure the desired metrics, without any knowledge of the underlying implementation of the platform. \r
\r
While many existing algorithms simplify networks based on the number of MACs or weights, optimizing those indirect metrics may not necessarily reduce the direct metrics, such as latency and energy consumption. To solve this problem, NetAdapt incorporates direct metrics into its adaptation algorithm. These direct metrics are evaluated using *empirical measurements*, so that detailed knowledge of the platform and toolchain is not required. NetAdapt automatically and progressively simplifies a pre-trained network until the resource budget is met while maximizing the accuracy.""" ;
    skos:prefLabel "NetAdapt" .

:NetMF a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1710.02971v4> ;
    skos:altLabel "Network Embedding as Matrix Factorization:" ;
    skos:definition "" ;
    skos:prefLabel "NetMF" .

:NetworkDissection a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1711.05611v2> ;
    skos:definition """**Network Dissection** is an interpretability method for [CNNs](https://paperswithcode.com/methods/category/convolutional-neural-networks) that evaluates the alignment between individual hidden units and a set of visual semantic concepts. By identifying the best alignments, units are given human interpretable labels across a range of objects, parts, scenes, textures, materials, and colors. \r
\r
The measurement of interpretability proceeds in three steps:\r
\r
- Identify a broad set of human-labeled visual concepts.\r
- Gather the response of the hidden variables to known concepts.\r
- Quantify alignment of hidden variable−concept pairs.""" ;
    skos:prefLabel "Network Dissection" .

:NeuralArchitectureSearch a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1707.07012v4> ;
    skos:definition """**Neural Architecture Search (NAS)** learns a modular architecture which can be transferred from a small dataset to a large dataset. The method does this by reducing the problem of learning best convolutional architectures to the problem of learning a small convolutional cell. The cell can then be stacked in series to handle larger images and more complex datasets.\r
\r
Note that this refers to the original method referred to as NAS - there is also a broader category of methods called "neural architecture search".""" ;
    skos:prefLabel "Neural Architecture Search" .

:NeuralCache a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1612.04426v1> ;
    skos:definition """A **Neural Cache**, or a **Continuous Cache**, is a module for language modelling which stores previous hidden states in memory cells. They are then used as keys to retrieve their corresponding word, that is the next word. There is no transformation applied to the storage during writing and reading.\r
\r
More formally it exploits the hidden representations $h\\_{t}$ to define a probability distribution over the words in the cache. As\r
illustrated in the Figure, the cache stores pairs $\\left(h\\_{i}, x\\_{i+1}\\right)$ of a hidden representation, and the word which was generated based on this representation (the vector $h\\_{i}$ encodes the history $x\\_{i}, \\dots, x\\_{1}$). At time $t$, we then define a probability distribution over words stored in the cache based on the stored hidden representations and the current one $h\\_{t}$ as:\r
\r
$$ p\\_{cache}\\left(w | h\\_{1\\dots{t}}, x\\_{1\\dots{t}}\\right) \\propto \\sum^{t-1}\\_{i=1}\\mathcal{1}\\_{\\text{set}\\left(w=x\\_{i+1}\\right)} \\exp\\left(θ\\_{h}>h\\_{t}^{T}h\\_{i}\\right) $$\r
\r
where the scalar $\\theta$ is a parameter which controls the flatness of the distribution. When $\\theta$ is equal to zero, the probability distribution over the history is uniform, and the model is equivalent to a unigram cache model.""" ;
    skos:prefLabel "Neural Cache" .

:NeuralProbabilisticLanguageModel a skos:Concept ;
    skos:definition """A **Neural Probablistic Language Model** is an early language modelling architecture. It involves a feedforward architecture that takes in input vector representations (i.e. word embeddings) of the previous $n$ words, which are looked up in a table $C$.\r
\r
The word embeddings are concatenated and fed into a hidden layer which then feeds into a [softmax](https://paperswithcode.com/method/softmax) layer to estimate the probability of the word given the context.""" ;
    skos:prefLabel "Neural Probabilistic Language Model" .

:NeuralRecon a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2104.00681v1> ;
    rdfs:seeAlso <https://zju3dv.github.io/neuralrecon/> ;
    skos:altLabel "NeuralRecon: Real-Time Coherent 3D Reconstruction from Monocular Video" ;
    skos:definition "**NeuralRecon** is a framework for real-time 3D scene reconstruction from a monocular video. Unlike previous methods that estimate single-view depth maps separately on each key-frame and fuse them later, NeuralRecon proposes to directly reconstruct local surfaces represented as sparse TSDF volumes for each video fragment sequentially by a neural network. A learning-based TSDF fusion module based on gated recurrent units is used to guide the network to fuse features from previous fragments. This design allows the network to capture local smoothness prior and global shape prior of 3D surfaces." ;
    skos:prefLabel "NeuralRecon" .

:NeuralTangentTransfer a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2006.08228v2> ;
    skos:definition "**Neural Tangent Transfer**, or **NTT**, is a method for finding trainable sparse networks in a label-free manner. Specifically, NTT finds sparse networks whose training dynamics, as characterized by the neural tangent kernel, mimic those of dense networks in function space." ;
    skos:prefLabel "Neural Tangent Transfer" .

:NeuralTuringMachine a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1410.5401v2> ;
    rdfs:seeAlso <https://github.com/carpedm20/NTM-tensorflow> ;
    skos:definition """A **Neural Turing Machine** is a working memory neural network model. It couples a neural network architecture with external memory resources. The whole architecture is differentiable end-to-end with gradient descent. The models can infer tasks such as copying, sorting and associative recall.\r
\r
A Neural Turing Machine (NTM) architecture contains two basic components: a neural\r
network controller and a memory bank. The Figure presents a high-level diagram of the NTM\r
architecture. Like most neural networks, the controller interacts with the external world via\r
input and output vectors. Unlike a standard network, it also interacts with a memory matrix\r
using selective read and write operations. By analogy to the Turing machine we refer to the\r
network outputs that parameterise these operations as “heads.”\r
\r
Every component of the architecture is differentiable. This is achieved by defining 'blurry' read and write operations that interact to a greater or lesser degree with all the elements in memory (rather\r
than addressing a single element, as in a normal Turing machine or digital computer). The\r
degree of blurriness is determined by an attentional “focus” mechanism that constrains each\r
read and write operation to interact with a small portion of the memory, while ignoring the\r
rest. Because interaction with the memory is highly sparse, the NTM is biased towards\r
storing data without interference. The memory location brought into attentional focus is\r
determined by specialised outputs emitted by the heads. These outputs define a normalised\r
weighting over the rows in the memory matrix (referred to as memory “locations”). Each\r
weighting, one per read or write head, defines the degree to which the head reads or writes\r
at each location. A head can thereby attend sharply to the memory at a single location or\r
weakly to the memory at many locations""" ;
    skos:prefLabel "Neural Turing Machine" .

:Neuraladjoint a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2009.12919v4> ;
    skos:altLabel "Neural adjoint method" ;
    skos:definition """The NA method can be divided into two steps: (i) Training a neural network approximation of f , and (ii) inference of xˆ. Step (i) is conventional and involves training a generic neural network on a dataset\r
ˆ\r
of input/output pairs from the simulator, denoted D, resulting in f, an approximation of the forward ˆ\r
model. This is illustrated in the left inset of Fig 1. In step (ii), our goal is to use ∂f/∂x to help us gradually adjust x so that we achieve a desired output of the forward model, y. This is similar to many classical inverse modeling approaches, such as the popular Adjoint method [8, 9]. For many practical\r
ˆ\r
expression for the simulator, from which it is trivial to compute ∂f/∂x, and furthermore, we can use modern deep learning software packages to efficiently estimate gradients, given a loss function L.\r
More formally, let y be our target output, and let xˆi be our current estimate of the solution, where i indexes each solution we obtain in an iterative gradient-based estimation procedure. Then we compute xˆi+1 with\r
inverse problems, however, obtaining ∂f/∂x requires significant expertise and/or effort, making these approaches challenging. Crucially, fˆ from step (i) provides us with a closed-form differentiable""" ;
    skos:prefLabel "Neural adjoint" .

:NeuroTactic a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2108.10821v1> ;
    skos:definition "**NeuroTactic** is a model for theorem proving which leverages [graph neural networks](https://paperswithcode.com/methods/category/graph-models) to represent the theorem and premises, and applies graph contrastive learning for pre-training. Specifically, premise selection is designed as a pretext task for the graph contrastive learning approach. The learned representations are then used for the downstream task, tactic prediction" ;
    skos:prefLabel "NeuroTactic" .

:Nipuna a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2206.09379v2> ;
    skos:altLabel "Optimizer Activation Function" ;
    skos:definition "A new activation function named NIPUNA : f(x)=max⁡〖(g(x),x)〗 where g(x)=x/(〖(1+e〗^(-βx)))" ;
    skos:prefLabel "Nipuna" .

:Noise2Fast a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2108.10209v1> ;
    skos:definition "**Noise2Fast** is a model for single image blind denoising. It is similar to masking based methods -- filling in the pixel gaps -- in that the network is blind to many of the input pixels during training. The method is inspired by Neighbor2Neighbor, where the neural network learns a mapping between adjacent pixels. Noise2Fast is tuned to speed by using a discrete four image training set obtained by a form of downsampling called “checkerboard downsampling." ;
    skos:prefLabel "Noise2Fast" .

:NoisyLinearLayer a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1706.10295v3> ;
    skos:definition """A **Noisy Linear Layer** is a [linear layer](https://paperswithcode.com/method/linear-layer) with parametric noise added to the weights. This induced stochasticity can be used in reinforcement learning networks for the agent's policy to aid efficient exploration. The parameters of the noise are learned with gradient descent along with any other remaining network weights. Factorized Gaussian noise is the type of noise usually employed.\r
\r
The noisy linear layer takes the form:\r
\r
$$y = \\left(b + Wx\\right) + \\left(b\\_{noisy}\\odot\\epsilon^{b}+\\left(W\\_{noisy}\\odot\\epsilon^{w}\\right)x\\right) $$\r
\r
where $\\epsilon^{b}$ and $\\epsilon^{w}$ are random variables.""" ;
    skos:prefLabel "Noisy Linear Layer" .

:NoisyNet-A3C a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1706.10295v3> ;
    skos:definition """**NoisyNet-A3C** is a modification of [A3C](https://paperswithcode.com/method/a3c) that utilises noisy linear layers for exploration instead of \r
$\\epsilon$-greedy exploration as in the original [DQN](https://paperswithcode.com/method/dqn) formulation.""" ;
    skos:prefLabel "NoisyNet-A3C" .

:NoisyNet-DQN a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1706.10295v3> ;
    skos:definition "**NoisyNet-DQN** is a modification of a [DQN](https://paperswithcode.com/method/dqn) that utilises noisy linear layers for exploration instead of $\\epsilon$-greedy exploration as in the original DQN formulation." ;
    skos:prefLabel "NoisyNet-DQN" .

:NoisyNet-Dueling a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1706.10295v3> ;
    skos:definition "**NoisyNet-Dueling** is a modification of a [Dueling Network](https://paperswithcode.com/method/dueling-network) that utilises noisy linear layers for exploration instead of $\\epsilon$-greedy exploration as in the original Dueling formulation." ;
    skos:prefLabel "NoisyNet-Dueling" .

:NoisyStudent a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1911.04252v4> ;
    rdfs:seeAlso <https://github.com/google-research/noisystudent> ;
    skos:definition """**Noisy Student Training** is a semi-supervised learning approach. It extends the idea of self-training\r
and distillation with the use of equal-or-larger student models and noise added to the student during learning. It has three main steps: \r
\r
1. train a teacher model on labeled images\r
2. use the teacher to generate pseudo labels on unlabeled images\r
3. train a student model on the combination of labeled images and pseudo labeled images. \r
\r
The algorithm is iterated a few times by treating the student as a teacher to relabel the unlabeled data and training a new student.\r
\r
Noisy Student Training seeks to improve on self-training and distillation in two ways. First, it makes the student larger than, or at least equal to, the teacher so the student can better learn from a larger dataset. Second, it adds noise to the student so the noised student is forced to learn harder from the pseudo labels. To noise the student, it uses input noise such as [RandAugment](https://paperswithcode.com/method/randaugment) data augmentation, and model noise such as [dropout](https://paperswithcode.com/method/dropout) and [stochastic depth](https://paperswithcode.com/method/stochastic-depth) during training.""" ;
    skos:prefLabel "Noisy Student" .

:Non-Linear-Bounding-Function a skos:Concept ;
    dcterms:source <https://ieeexplore.ieee.org/document/9018379> ;
    skos:altLabel "Lower Bound on Transmission Using Non-Linear Bounding Function in Single Image Dehazing" ;
    skos:definition "" ;
    skos:prefLabel "Non-Linear-Bounding-Function" .

:Non-LocalBlock a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1711.07971v3> ;
    rdfs:seeAlso <https://github.com/tea1528/Non-Local-NN-Pytorch/blob/986937674eb3b85d3d3fbaaa8f384c0a26624121/models/non_local.py#L6> ;
    skos:definition """A **Non-Local Block** is an image block module used in neural networks that wraps a [non-local operation](https://paperswithcode.com/method/non-local-operation). We can define a non-local block as:\r
\r
$$ \\mathbb{z}\\_{i} = W\\_{z}\\mathbb{y\\_{i}} + \\mathbb{x}\\_{i} $$\r
\r
where $y\\_{i}$ is the output from the non-local operation and $+ \\mathbb{x}\\_{i}$ is a [residual connection](https://paperswithcode.com/method/residual-connection).""" ;
    skos:prefLabel "Non-Local Block" .

:Non-LocalOperation a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1711.07971v3> ;
    rdfs:seeAlso <https://github.com/tea1528/Non-Local-NN-Pytorch/blob/986937674eb3b85d3d3fbaaa8f384c0a26624121/models/non_local.py#L124> ;
    skos:definition """A **Non-Local Operation** is a component for capturing long-range dependencies with deep neural networks. It is a generalization of the classical non-local mean operation in computer vision. Intuitively a non-local operation computes the response at a position as a weighted sum of the features at all positions in the input feature maps. The set of positions can be in space, time, or spacetime, implying that these operations are applicable for image, sequence, and video problems.\r
\r
Following the non-local mean operation, a generic non-local operation for deep neural networks is defined as:\r
\r
$$ \\mathbb{y}\\_{i} = \\frac{1}{\\mathcal{C}\\left(\\mathbb{x}\\right)}\\sum\\_{\\forall{j}}f\\left(\\mathbb{x}\\_{i}, \\mathbb{x}\\_{j}\\right)g\\left(\\mathbb{x}\\_{j}\\right) $$\r
\r
Here $i$ is the index of an output position (in space, time, or spacetime) whose response is to be computed and $j$ is the index that enumerates all possible positions. x is the input signal (image, sequence, video; often their features) and $y$ is the output signal of the same size as $x$. A pairwise function $f$ computes a scalar (representing relationship such as affinity) between $i$ and all $j$. The unary function $g$ computes a representation of the input signal at the position $j$. The\r
response is normalized by a factor $C\\left(x\\right)$.\r
\r
The non-local behavior is due to the fact that all positions ($\\forall{j}$) are considered in the operation. As a comparison, a convolutional operation sums up the weighted input in a local neighborhood (e.g., $i − 1 \\leq j \\leq i + 1$ in a 1D case with kernel size 3), and a recurrent operation at time $i$ is often based only on the current and the latest time steps (e.g., $j = i$ or $i − 1$).\r
\r
The non-local operation is also different from a fully-connected (fc) layer. The equation above computes responses based on relationships between different locations, whereas fc uses learned weights. In other words, the relationship between $x\\_{j}$ and $x\\_{i}$ is not a function of the input data in fc, unlike in nonlocal layers. Furthermore, the formulation in the equation above supports inputs of variable sizes, and maintains the corresponding size in the output. On the contrary, an fc layer requires a fixed-size input/output and loses positional correspondence (e.g., that from $x\\_{i}$ to $y\\_{i}$ at the position $i$).\r
\r
A non-local operation is a flexible building block and can be easily used together with convolutional/recurrent layers. It can be added into the earlier part of deep neural networks, unlike fc layers that are often used in the end. This allows us to build a richer hierarchy that combines both non-local and local information.\r
\r
In terms of parameterisation, we usually parameterise $g$ as a linear embedding of the form $g\\left(x\\_{j}\\right) = W\\_{g}\\mathbb{x}\\_{j}$ , where $W\\_{g}$ is a weight matrix to be learned. This is implemented as, e.g., 1×1 [convolution](https://paperswithcode.com/method/convolution) in space or 1×1×1 convolution in spacetime. For $f$ we use an affinity function, a list of which can be found [here](https://paperswithcode.com/methods/category/affinity-functions).""" ;
    skos:prefLabel "Non-Local Operation" .

:NonMaximumSuppression a skos:Concept ;
    skos:definition """**Non Maximum Suppression** is a computer vision method that selects a single entity out of many overlapping entities (for example bounding boxes in object detection). The criteria is usually discarding entities that are below a given probability bound. With remaining entities we repeatedly pick the entity with the highest probability, output that as the prediction, and discard any remaining box where a $\\text{IoU} \\geq 0.5$ with the box output in the previous step.\r
\r
Image Credit: [Martin Kersner](https://github.com/martinkersner/non-maximum-suppression-cpp)""" ;
    skos:prefLabel "Non Maximum Suppression" .

:NormFormer a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2110.09456v2> ;
    skos:definition "**NormFormer** is a type of [Pre-LN](https://paperswithcode.com/method/layer-normalization) transformer that adds three normalization operations to each layer: a Layer Norm after self attention, head-wise scaling of self-attention outputs, and a Layer Norm after the first [fully connected layer](https://paperswithcode.com/method/position-wise-feed-forward-layer). The modifications introduce a small number of additional learnable parameters, which provide a cost-effective way for each layer to change the magnitude of its features, and therefore the magnitude of the gradients to subsequent components." ;
    skos:prefLabel "NormFormer" .

:NormLinComb a skos:Concept ;
    dcterms:source <https://doi.org/10.20944/preprints202301.0463.v1> ;
    skos:altLabel "Normalized Linear Combination of Activations" ;
    skos:definition """The **Normalized Linear Combination of Activations**, or **NormLinComb**, is a type of activation function that has trainable parameters and uses the normalized linear combination of other activation functions.\r
\r
$$NormLinComb(x) = \\frac{\\sum\\limits_{i=0}^{n} w_i \\mathcal{F}_i(x)}{\\mid \\mid W \\mid \\mid}$$""" ;
    skos:prefLabel "NormLinComb" .

:NormalizingFlows a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1505.05770v6> ;
    rdfs:seeAlso <https://github.com/ex4sperans/variational-inference-with-normalizing-flows/blob/922b569f851e02fa74700cd0754fe2ef5c1f3180/flow.py#L9> ;
    skos:definition """**Normalizing Flows** are a method for constructing complex distributions by transforming a\r
probability density through a series of invertible mappings. By repeatedly applying the rule for change of variables, the initial density ‘flows’ through the sequence of invertible mappings. At the end of this sequence we obtain a valid probability distribution and hence this type of flow is referred to as a normalizing flow.\r
\r
In the case of finite flows, the basic rule for the transformation of densities considers an invertible, smooth mapping $f : \\mathbb{R}^{d} \\rightarrow \\mathbb{R}^{d}$ with inverse $f^{-1} = g$, i.e. the composition $g \\cdot f\\left(z\\right) = z$. If we use this mapping to transform a random variable $z$ with distribution $q\\left(z\\right)$, the resulting random variable $z' = f\\left(z\\right)$ has a distribution:\r
\r
$$ q\\left(\\mathbf{z}'\\right) = q\\left(\\mathbf{z}\\right)\\bigl\\vert{\\text{det}}\\frac{\\delta{f}^{-1}}{\\delta{\\mathbf{z'}}}\\bigr\\vert = q\\left(\\mathbf{z}\\right)\\bigl\\vert{\\text{det}}\\frac{\\delta{f}}{\\delta{\\mathbf{z}}}\\bigr\\vert ^{-1} $$\r
\r
where the last equality can be seen by applying the chain rule (inverse function theorem) and is a property of Jacobians of invertible functions. We can construct arbitrarily complex densities by composing several simple maps and successively applying the above equation. The density $q\\_{K}\\left(\\mathbf{z}\\right)$ obtained by successively transforming a random variable $z\\_{0}$ with distribution $q\\_{0}$ through a chain of $K$ transformations $f\\_{k}$ is:\r
\r
$$ z\\_{K} = f\\_{K} \\cdot \\dots \\cdot f\\_{2} \\cdot f\\_{1}\\left(z\\_{0}\\right) $$\r
\r
$$ \\ln{q}\\_{K}\\left(z\\_{K}\\right) = \\ln{q}\\_{0}\\left(z\\_{0}\\right) − \\sum^{K}\\_{k=1}\\ln\\vert\\det\\frac{\\delta{f\\_{k}}}{\\delta{\\mathbf{z\\_{k-1}}}}\\vert $$\r
\r
The path traversed by the random variables $z\\_{k} = f\\_{k}\\left(z\\_{k-1}\\right)$ with initial distribution $q\\_{0}\\left(z\\_{0}\\right)$ is called the flow and the path formed by the successive distributions $q\\_{k}$ is a normalizing flow.""" ;
    skos:prefLabel "Normalizing Flows" .

:Nyströmformer a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2102.03902v3> ;
    rdfs:seeAlso <https://github.com/mlpen/Nystromformer/blob/4429fb3ea385dcaeeb76d5e547a03a8a5f40a5bf/code/attention_nystrom.py#L6> ;
    skos:definition "Nyströmformer replaces the self-attention in [BERT](https://paperswithcode.com/method/bert)-small and BERT-base using the proposed Nyström approximation. This reduces self-attention complexity to $O(n)$ and allows the [Transformer](https://paperswithcode.com/method/transformer) to support longer sequences." ;
    skos:prefLabel "Nyströmformer" .

:O-Net a skos:Concept ;
    dcterms:source <https://www.sciencedirect.com/science/article/pii/S0097849322000358> ;
    skos:definition "" ;
    skos:prefLabel "O-Net" .

:OASIS a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2012.04781v3> ;
    rdfs:seeAlso <https://github.com/boschresearch/OASIS> ;
    skos:definition """OASIS is a [GAN](https://paperswithcode.com/method/gan)-based model to translate semantic label maps into realistic-looking images. The model builds on preceding work such as [Pix2Pix](https://paperswithcode.com/method/pix2pix) and SPADE. OASIS introduces the following innovations:  \r
\r
1. The method is not dependent on the perceptual loss, which is commonly used for the semantic image synthesis task. A [VGG](https://paperswithcode.com/method/vgg) network trained on ImageNet is routinely employed as the perceptual loss to strongly improve the synthesis quality. The authors show that this perceptual loss also has negative effects: First, it reduces the diversity of the generated images. Second, it negatively influences the color distribution to be more biased towards ImageNet. OASIS eliminates the dependence on the perceptual loss by changing the common discriminator design: The OASIS discriminator segments an image into one of the real classes or an additional fake class. In doing so, it makes more efficient use of the label maps that the discriminator normally receives. This distinguishes the discriminator from the commonly used encoder-shaped discriminators, which concatenate the label maps to the input image and predict a single score per image. With the more fine-grained supervision through the loss of the OASIS discriminator, the perceptual loss is shown to become unnecessary.\r
\r
2. A user can generate a diverse set of images per label map by simply resampling noise. This is achieved by conditioning the [spatially-adaptive denormalization](https://arxiv.org/abs/1903.07291) module in each layer of the GAN generator directly on spatially replicated input noise. A side effect of this conditioning is that at inference time an image can be resampled either globally or locally (either the complete image changes or a restricted region in the image).""" ;
    skos:prefLabel "OASIS" .

:OCD a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2210.00471v5> ;
    skos:altLabel "Overfitting Conditional Diffusion Model" ;
    skos:definition "" ;
    skos:prefLabel "OCD" .

:ODL a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1711.03705v1> ;
    skos:altLabel "online deep learning" ;
    skos:definition "Deep Neural Networks (DNNs) are typically trained by backpropagation in a batch learning setting, which requires the entire training data to be made available prior to the learning task. This is not scalable for many real-world scenarios where new data arrives sequentially in a stream form. We aim to address an open challenge of \"Online Deep Learning\" (ODL) for learning DNNs on the fly in an online setting. Unlike traditional online learning that often optimizes some convex objective function with respect to a shallow model (e.g., a linear/kernel-based hypothesis), ODL is significantly more challenging since the optimization of the DNN objective function is non-convex, and regular backpropagation does not work well in practice, especially for online learning settings." ;
    skos:prefLabel "ODL" .

:OFA a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2202.03052v2> ;
    skos:definition "In this work, we pursue a unified paradigm for multimodal pretraining to break the scaffolds of complex task/modality-specific customization. We propose OFA, a Task-Agnostic and Modality-Agnostic framework that supports Task Comprehensiveness. OFA unifies a diverse set of cross-modal and unimodal tasks, including image generation, visual grounding, image captioning, image classification, language modeling, etc., in a simple sequence-to-sequence learning framework. OFA follows the instruction-based learning in both pretraining and finetuning stages, requiring no extra task-specific layers for downstream tasks. In comparison with the recent state-of-the-art vision & language models that rely on extremely large cross-modal datasets, OFA is pretrained on only 20M publicly available image-text pairs. Despite its simplicity and relatively small-scale training data, OFA achieves new SOTAs in a series of cross-modal tasks while attaining highly competitive performances on uni-modal tasks. Our further analysis indicates that OFA can also effectively transfer to unseen tasks and unseen domains. Our code and models are publicly available at https://github.com/OFA-Sys/OFA." ;
    skos:prefLabel "OFA" .

:OHEM a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1604.03540v1> ;
    rdfs:seeAlso <https://github.com/abhi2610/ohem/blob/1f07dd09b50c8c21716ae36aede92125fe437579/lib/roi_data_layer/minibatch.py#L146> ;
    skos:altLabel "Online Hard Example Mining" ;
    skos:definition """Some object detection datasets contain an overwhelming number of easy examples and a small number of hard examples. Automatic selection of these hard examples can make training more\r
effective and efficient. **OHEM**, or **Online Hard Example Mining**, is a bootstrapping technique that modifies [SGD](https://paperswithcode.com/method/sgd) to sample from examples in a non-uniform way depending on the current loss of each example under consideration. The method takes advantage of detection-specific problem structure in which each SGD mini-batch consists of only one or two images, but thousands of candidate examples. The candidate examples are subsampled according to a distribution\r
that favors diverse, high loss instances.""" ;
    skos:prefLabel "OHEM" .

:OMGD a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2108.06908v2> ;
    skos:altLabel "Online Multi-granularity Distillation" ;
    skos:definition """**OMGD**, or **Online Multi-Granularity Distillation** is a framework for learning efficient [GANs](https://paperswithcode.com/methods/category/generative-adversarial-networks). The student generator is optimized in a discriminator-free and ground-truth-free setting. The scheme trains the teacher and student alternatively, promoting these two generators iteratively and progressively. The progressively optimized teacher generator helps to warm up the student and guide the optimization direction step by step.\r
\r
Specifically, the student generator $G\\_{S}$ only leverages the complementary teacher generators $G^{W}\\_{T}$ and $G^{D}\\_{T}$ for optimization and can be trained in the discriminator-free and ground-truth-free setting. This framework transfers different levels concepts from the intermediate layers and output layer to perform the knowledge distillation. The whole optimization is conducted on an online distillation scheme. Namely,  $G^{W}\\_{T}$, $G^{D}\\_{T}$ and $G\\_{S}$ are optimized simultaneously and progressively.""" ;
    skos:prefLabel "OMGD" .

:OODformer a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2107.08976v2> ;
    skos:definition """OODformer is a [transformer](https://paperswithcode.com/method/transformer)-based OOD detection architecture that leverages the contextualization capabilities of the transformer. Incorporating the transformer as the principal feature extractor allows to exploit the object concepts and their discriminate attributes along with their co-occurrence via [visual attention](https://paperswithcode.com/method/visual-attention). \r
\r
OODformer employs [ViT](method/vision-transformer) and its data efficient variant [DeiT](/method/deit). Each encoder layer consist of multi-head self attention and a multi-layer perception block. The combination of MSA and MLP layers in the encoder jointly encode the attributes' importance, associated correlation, and co-occurrence. The [class] token (a representative of an image $x$) consolidated multiple attributes and their related features via the global context. The [class] token from the final layer is used for OOD detection in two ways; first, it is passed to $\r
F_{\\text {classifier }}\\left(x_{\\text {feat }}\\right)$  for softmax confidence score, and second it is used for latent space distance calculation.""" ;
    skos:prefLabel "OODformer" .

:OPT a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2205.01068v4> ;
    skos:definition "**OPT** is a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters. The model uses an AdamW optimizer and weight decay of 0.1. It follows a linear learning rate schedule, warming up from 0 to the maximum learning rate over the first 2000 steps in OPT-175B, or over 375M tokens in the smaller models, and decaying down to 10% of the maximum LR over 300B tokens. The batch sizes range from 0.5M to 4M depending on the model size and is kept constant throughout the course of training." ;
    skos:prefLabel "OPT" .

:OPT-IML a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2212.12017v3> ;
    skos:definition "**OPT-IML** is a version of OPT fine-tuned on a large collection of 1500+ NLP tasks divided into various task categories." ;
    skos:prefLabel "OPT-IML" .

:ORB-SLAM2 a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1610.06475v2> ;
    skos:altLabel "ORB-Simultaneous localization and mapping" ;
    skos:definition """ORB-SLAM2 is a complete SLAM system for monocular, stereo and RGB-D cameras, including map reuse, loop closing and relocalization capabilities. The system works in real-time on standard CPUs in a wide variety of environments from small hand-held indoors sequences, to drones flying in industrial environments and cars driving around a city.\r
\r
Source: [Mur-Artal and Tardos](https://arxiv.org/pdf/1610.06475v2.pdf)\r
\r
Image source: [Mur-Artal and Tardos](https://arxiv.org/pdf/1610.06475v2.pdf)""" ;
    skos:prefLabel "ORB-SLAM2" .

:ORN a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2003.11163v2> ;
    skos:altLabel "Orientation Regularized Network" ;
    skos:definition "**Orientation Regularized Network** (ORN) is a multi-view image fusion technique for pose estimation. It uses IMU orientations as a structural prior to mutually fuse the image features of each pair of joints linked by IMUs. For example, it uses the features of the elbow to reinforce those of the wrist based on the IMU at the lower-arm." ;
    skos:prefLabel "ORN" .

<http://w3id.org/mlso/vocab/ml_algorithm/OSA(identitymapping+eSE)> a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1911.06667v6> ;
    rdfs:seeAlso <https://github.com/youngwanLEE/CenterMask/blob/72147e8aae673fcaf4103ee90a6a6b73863e7fa1/maskrcnn_benchmark/modeling/backbone/vovnet.py#L124> ;
    skos:definition """**One-Shot Aggregation with an Identity Mapping and eSE** is an image model block that extends [one-shot aggregation](https://paperswithcode.com/method/one-shot-aggregation) with a [residual connection](https://paperswithcode.com/method/residual-connection) and [effective squeeze-and-excitation block](https://paperswithcode.com/method/effective-squeeze-and-excitation-block). It is proposed as part of the [VoVNetV2](https://paperswithcode.com/method/vovnetv2) CNN architecture.\r
\r
The module adds an identity mapping to the OSA module - the input path is connected to the end of an OSA module that is able to backpropagate the gradients of every OSA module in an end-to-end manner on each stage like a [ResNet](https://paperswithcode.com/method/resnet). Additionally, a [channel attention module](https://paperswithcode.com/method/channel-attention-module) - effective Squeeze-Excitation - is used which is like regular [squeeze-and-excitation](https://paperswithcode.com/method/squeeze-and-excitation-block) but uses only one FC layer with $C$ channels instead of two FCs without a channel dimension reduction, which maintains channel information.""" ;
    skos:prefLabel "OSA (identity mapping + eSE)" .

:OSCAR a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2004.06165v5> ;
    skos:definition "OSCAR is a new learning method that uses object tags detected in images as anchor points to ease the learning of image-text alignment. The model take a triple as input (word-tag-region) and pre-trained with two losses (masked token loss over words and tags, and a contrastive loss between tags and others). OSCAR represents an image-text pair into semantic space via dictionary lookup. Object tags are used as anchor points to align image regions with word embeddings of pre-trained language models. The model is then fine-tuned for understanding and generation tasks." ;
    skos:prefLabel "OSCAR" .

:OTM a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2110.02999v2> ;
    skos:altLabel "Optimal Transport Modeling" ;
    skos:definition "" ;
    skos:prefLabel "OTM" .

:ObjectDropout a skos:Concept ;
    dcterms:source <https://www.ijcai.org/proceedings/2021/105> ;
    skos:definition "Object Dropout is a technique that perturbs object features in an image for [noisy student](https://paperswithcode.com/method/noisy-student) training. It performs at par with standard data augmentation techniques while being significantly faster than the latter to implement." ;
    skos:prefLabel "Object Dropout" .

:OctaveConvolution a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1904.05049v3> ;
    rdfs:seeAlso <https://github.com/lxtGH/OctaveConv_pytorch/blob/079f7da29d55c2eeed8985d33f0b2f765d7a469e/libs/nn/OctaveConv2.py#L11> ;
    skos:definition """An **Octave Convolution (OctConv)** stores and process feature maps that vary spatially “slower” at a lower spatial resolution reducing both memory and computation cost. It takes in feature maps containing tensors of two frequencies one octave apart, and extracts information directly from the\r
low-frequency maps without the need of decoding it back to the high-frequency. The motivation is that in natural images, information is conveyed at different frequencies where higher frequencies are usually encoded with fine details and lower frequencies are usually encoded with global structures.""" ;
    skos:prefLabel "Octave Convolution" .

:Off-DiagonalOrthogonalRegularization a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1809.11096v2> ;
    rdfs:seeAlso <https://github.com/ajbrock/BigGAN-PyTorch/blob/b70f16c4a879b2d5d5d7bcb73794424aef5eec1f/train_fns.py#L51> ;
    skos:definition """**Off-Diagonal Orthogonal Regularization** is a modified form of [orthogonal regularization](https://paperswithcode.com/method/orthogonal-regularization) originally used in [BigGAN](https://paperswithcode.com/method/biggan). The original orthogonal regularization is known to be limiting so the authors explore several variants designed to relax the constraint while still imparting the desired smoothness to the models. They opt for a modification where they remove diagonal terms from the regularization, and aim to minimize the pairwise cosine similarity between filters but does not constrain their norm:\r
\r
$$ R\\_{\\beta}\\left(W\\right) = \\beta|| W^{T}W \\odot \\left(\\mathbf{1}-I\\right) ||^{2}\\_{F} $$\r
\r
where $\\mathbf{1}$ denotes a matrix with all elements set to 1. The authors sweep $\\beta$ values and select $10^{−4}$.""" ;
    skos:prefLabel "Off-Diagonal Orthogonal Regularization" .

:One-ShotAggregation a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1904.09730v1> ;
    rdfs:seeAlso <https://github.com/osmr/imgclsmob/blob/3197ca90e0270c01e553e4091fc37104718ad822/pytorch/pytorchcv/models/vovnet.py#L14> ;
    skos:definition "**One-Shot Aggregation** is an image model block that is an alternative to [Dense Blocks](https://paperswithcode.com/method/dense-block), by aggregating intermediate features. It is proposed as part of the [VoVNet](https://paperswithcode.com/method/vovnet) architecture. Each [convolution](https://paperswithcode.com/method/convolution) layer is connected by two-way connection. One way is connected to the subsequent layer to produce the feature with a larger receptive field while the other way is aggregated only once into the final output feature map. The difference with [DenseNet](https://paperswithcode.com/method/densenet) is that the output of each layer is not routed to all subsequent intermediate layers which makes the input size of intermediate layers constant." ;
    skos:prefLabel "One-Shot Aggregation" .

:OneR a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2211.11153v1> ;
    skos:altLabel "One Representation" ;
    skos:definition "In the OneR method, model input can be one of image, text or image+text, and CMC objective is combined with the traditional image-text contrastive (ITC) loss. Masked modeling is also carried out for all three input types (i.e., image, text and multi-modal). This framework employs no modality-specific architectural component except for the initial token embedding layer, making our model generic and modality-agnostic with minimal inductive bias." ;
    skos:prefLabel "OneR" .

:OnlineNormalization a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1905.05894v3> ;
    skos:definition """**Online Normalization** is a normalization technique for training deep neural networks. To define Online Normalization. we replace arithmetic averages over the full dataset in with exponentially decaying averages of online samples. The decay factors $\\alpha\\_{f}$ and $\\alpha\\_{b}$ for forward and backward passes respectively are hyperparameters for the technique.\r
\r
We allow incoming samples $x\\_{t}$, such as images, to have multiple scalar components and denote\r
feature-wide mean and variance by $\\mu\\left(x\\_{t}\\right)$ and $\\sigma^{2}\\left(x\\_{t}\\right)$. The algorithm also applies to outputs of fully connected layers with only one scalar output per feature. In fact, this case simplifies to $\\mu\\left(x\\_{t}\\right) = x\\_{t}$ and $\\sigma\\left(x\\_{t}\\right) = 0$. Denote scalars $\\mu\\_{t}$ and $\\sigma\\_{t}$ to denote running estimates of mean and variance across\r
all samples. The subscript $t$ denotes time steps corresponding to processing new incoming samples.\r
\r
Online Normalization uses an ongoing process during the forward pass to estimate activation means\r
and variances. It implements the standard online computation of mean and variance generalized to processing multi-value samples and exponential averaging of sample statistics. The\r
resulting estimates directly lead to an affine normalization transform.\r
\r
$$ y\\_{t} = \\frac{x\\_{t} - \\mu\\_{t-1}}{\\sigma\\_{t-1}} $$ \r
\r
$$ \\mu\\_{t} = \\alpha\\_{f}\\mu\\_{t-1} + \\left(1-\\alpha\\_{f}\\right)\\mu\\left(x\\_{t}\\right) $$\r
\r
$$ \\sigma^{2}\\_{t} = \\alpha\\_{f}\\sigma^{2}\\_{t-1} + \\left(1-\\alpha\\_{f}\\right)\\sigma^{2}\\left(x\\_{t}\\right) + \\alpha\\_{f}\\left(1-\\alpha\\_{f}\\right)\\left(\\mu\\left(x\\_{t}\\right) - \\mu\\_{t-1}\\right)^{2} $$""" ;
    skos:prefLabel "Online Normalization" .

:Ontology a skos:Concept ;
    dcterms:source <https://www.mdpi.com/2076-3417/12/3/1608> ;
    skos:definition "" ;
    skos:prefLabel "Ontology" .

:OpenPose a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1812.08008v2> ;
    skos:definition "" ;
    skos:prefLabel "OpenPose" .

:OrthogonalRegularization a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1609.07093v3> ;
    rdfs:seeAlso <https://github.com/pytorch/pytorch/blob/b7bda236d18815052378c88081f64935427d7716/torch/nn/init.py#L423> ;
    skos:definition """**Orthogonal Regularization** is a regularization technique for convolutional neural networks, introduced with generative modelling as the task in mind. Orthogonality is argued to be a desirable quality in ConvNet filters, partially because multiplication by an orthogonal matrix leaves the norm of the original matrix unchanged. This property is valuable in deep or recurrent networks, where repeated matrix multiplication can result in signals vanishing or exploding. To try to maintain orthogonality throughout training, Orthogonal Regularization encourages weights to be orthogonal by pushing them towards the nearest orthogonal manifold. The objective function is augmented with the cost:\r
\r
$$ \\mathcal{L}\\_{ortho} = \\sum\\left(|WW^{T} − I|\\right) $$\r
\r
Where $\\sum$ indicates a sum across all filter banks, $W$ is a filter bank, and $I$ is the identity matrix""" ;
    skos:prefLabel "Orthogonal Regularization" .

:OverFeat a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1312.6229v4> ;
    rdfs:seeAlso <https://github.com/tensorflow/models/blob/24501315d2a559876215b62b080397cea3a66b97/research/slim/nets/overfeat.py#L52> ;
    skos:definition "**OverFeat** is a classic type of convolutional neural network architecture, employing [convolution](https://paperswithcode.com/method/convolution), pooling and fully connected layers. The Figure to the right shows the architectural details." ;
    skos:prefLabel "OverFeat" .

:PAA a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2103.11099v2> ;
    skos:altLabel "Patch AutoAugment" ;
    skos:definition "**Patch AutoAugment** is a patch-level automatic data augmentation algorithm that automatically searches for the optimal augmentation policies for the patches of an image. Specifically, PAA allows each patch DA operation to be controlled by an agent and models it as a Multi-Agent Reinforcement Learning (MARL) problem. At each step, PAA samples the most effective operation for each patch based on its content and the semantics of the whole image. The agents cooperate as a team and share a unified team reward for achieving the joint optimal DA policy of the whole image. PAA is co-trained with a target network through adversarial training. At each step, the policy network samples the most effective operation for each patch based on its content and the semantics of the image." ;
    skos:prefLabel "PAA" .

:PAFNet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2104.13534v1> ;
    skos:altLabel "Paddle Anchor Free Network" ;
    skos:definition "**PAFNet** is an anchor-free detector for object detection that removes pre-defined anchors and regresses the locations directly, which can achieve higher efficiency. The overall network is composed of a backbone, an up-sampling module, an AGS module, a localization branch and a regression branch. Specifically,  ResNet50-vd is chosen as the backbone for server side, and [MobileNetV3](https://paperswithcode.com/method/mobilenetv3) for mobile side. Besides, for mobile side, we replace traditional [convolution](https://paperswithcode.com/method/convolution) layers with lite convolution operators." ;
    skos:prefLabel "PAFNet" .

:PAFPN a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1803.01534v4> ;
    rdfs:seeAlso <https://github.com/ShuLiu1993/PANet/blob/2644d5ad6ae98c2bf58df45c8792c019b1d7b2b9/lib/modeling/FPN.py#L79> ;
    skos:definition "**PAFPN** is a feature pyramid module used in Path Aggregation networks ([PANet](https://paperswithcode.com/method/panet)) that combines FPNs with [bottom-up path augmentation](https://paperswithcode.com/method/bottom-up-path-augmentation), which shortens the information path between lower layers and topmost feature." ;
    skos:prefLabel "PAFPN" .

:PAFs a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1611.08050v2> ;
    skos:altLabel "Part Affinity Fields" ;
    skos:definition "" ;
    skos:prefLabel "PAFs" .

:PALED a skos:Concept ;
    dcterms:source <https://www.sciencedirect.com/science/article/abs/pii/S1746809422006437> ;
    skos:altLabel "Segmentation of patchy areas in biomedical images based on local edge density estimation" ;
    skos:definition "An effective approach to the quantification of patchiness in biomedical images according to their local edge densities." ;
    skos:prefLabel "PALED" .

:PANet a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1803.01534v4> ;
    rdfs:seeAlso <https://github.com/ShuLiu1993/PANet/blob/2644d5ad6ae98c2bf58df45c8792c019b1d7b2b9/lib/modeling/FPN.py#L135> ;
    skos:definition "**Path Aggregation Network**, or **PANet**, aims to boost information flow in a proposal-based instance segmentation framework. Specifically, the feature hierarchy is enhanced with accurate localization signals in lower layers by [bottom-up path augmentation](https://paperswithcode.com/method/bottom-up-path-augmentation), which shortens the information path between lower layers and topmost feature. Additionally, [adaptive feature pooling](https://paperswithcode.com/method/adaptive-feature-pooling) is employed, which links feature grid and all feature levels to make useful information in each feature level propagate directly to following proposal subnetworks. A complementary branch capturing different views for each proposal is created to further improve mask prediction." ;
    skos:prefLabel "PANet" .

:PARTransformer a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2009.04534v3> ;
    skos:definition "**PAR Transformer** is a [Transformer](https://paperswithcode.com/methods/category/transformers) model that uses 63% fewer [self-attention blocks](https://paperswithcode.com/method/scaled), replacing them with [feed-forward blocks](https://paperswithcode.com/method/position-wise-feed-forward-layer), while retaining test accuracies. It is based on the [Transformer-XL](https://paperswithcode.com/method/transformer-xl) architecture and uses [neural architecture search](https://paperswithcode.com/task/architecture-search) to find an an efficient pattern of blocks in the transformer architecture." ;
    skos:prefLabel "PAR Transformer" .

<http://w3id.org/mlso/vocab/ml_algorithm/PASE+> a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2001.09239v2> ;
    skos:altLabel "Problem Agnostic Speech Encoder +" ;
    skos:definition "**PASE+** is a problem-agnostic speech encoder that combines a convolutional encoder followed by multiple neural networks, called workers, tasked to solve self-supervised problems (i.e., ones that do not require manual annotations as ground truth). An online speech distortion module is employed, that contaminates the input signals with a variety of random disturbances. A revised encoder is also proposed that better learns short- and long-term speech dynamics with an efficient combination of recurrent and convolutional networks. Finally, the authors refine the set of workers used in self-supervision to encourage better cooperation." ;
    skos:prefLabel "PASE+" .

:PAU a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1907.06732v3> ;
    skos:altLabel "Padé Activation Units" ;
    skos:definition "Parametrized learnable activation function, based on the Padé approximant." ;
    skos:prefLabel "PAU" .

:PAUSE a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2109.03155v1> ;
    skos:definition "**PAUSE**, or **Positive and Annealed Unlabeled Sentence Embedding**, is an approach for learning sentence embeddings from a partially labeled dataset. It is based on a dual encoder schema that is widely adopted in supervised sentence embedding training. Each individual sample $\\mathbf{x}$ contains a pair of hypothesis and premise sentences $(x\\_{i},x^{\\prime}_{i})$, each of which is fed into a pretrained encoder (e.g. [BERT](https://paperswithcode.com/method/bert)). As shown in Figure, the two encoders are identical during the training by sharing their weights." ;
    skos:prefLabel "PAUSE" .

:PCAWhitening a skos:Concept ;
    skos:definition """**PCA Whitening** is a processing step for image based data that makes input less redundant. Adjacent pixel or feature values can be highly correlated, and whitening through the use of [PCA](https://paperswithcode.com/method/pca) reduces this degree of correlation.\r
\r
Image Source: [Wikipedia](https://en.wikipedia.org/wiki/Principal_component_analysis#/media/File:GaussianScatterPCA.svg)""" ;
    skos:prefLabel "PCA Whitening" .

:PCB a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1711.09349v3> ;
    skos:altLabel "Part-based Convolutional Baseline" ;
    skos:definition "" ;
    skos:prefLabel "PCB" .

:PCIDA a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2007.01807v2> ;
    skos:altLabel "Probabilistic Continuously Indexed Domain Adaptation" ;
    skos:definition "**Probabilistic Continuously Indexed Domain Adaptation** (**PCIDA**) enjoys better theoretical guarantees to match both the mean and variance of the distribution $p(u|z)$. PCIDA can be extended to match higher-order moments." ;
    skos:prefLabel "PCIDA" .

:PDC a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2205.03043v2> ;
    skos:altLabel "Prime Dilated Convolution" ;
    skos:definition "" ;
    skos:prefLabel "PDC" .

:PELU a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1605.09332v4> ;
    rdfs:seeAlso <https://github.com/InzamamRahaman/PELU/blob/ee2598c32f3596f18d957417c97c03e8862086bf/pelu.py#L6> ;
    skos:altLabel "Parametric Exponential Linear Unit" ;
    skos:definition """**Parameterized Exponential Linear Units**, or **PELU**, is an activation function for neural networks. It involves learning a parameterization of [ELU](https://paperswithcode.com/method/elu) in order to learn the proper activation shape at each layer in a CNN. \r
\r
The PELU has two additional parameters over the ELU:\r
\r
$$ f\\left(x\\right) = cx \\text{ if } x > 0 $$\r
$$ f\\left(x\\right) = \\alpha\\exp^{\\frac{x}{b}} - 1 \\text{ if } x \\leq 0 $$\r
\r
Where $a$, $b$, and $c > 0$. Here $c$ causes a change in the slope in the positive quadrant, $b$ controls the scale of the [exponential decay](https://paperswithcode.com/method/exponential-decay), and $\\alpha$ controls the saturation in the negative quadrant.\r
\r
Source: [Activation Functions](https://arxiv.org/pdf/1811.03378.pdf)""" ;
    skos:prefLabel "PELU" .

:PFGM a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2209.11178v4> ;
    skos:altLabel "Poisson Flow Generative Models" ;
    skos:definition "" ;
    skos:prefLabel "PFGM" .

:PFPNet a skos:Concept ;
    dcterms:source <http://openaccess.thecvf.com/content_ECCV_2018/html/Seung-Wook_Kim_Parallel_Feature_Pyramid_ECCV_2018_paper.html> ;
    skos:altLabel "Parallel Feature Pyramid Network" ;
    skos:definition "" ;
    skos:prefLabel "PFPNet" .

:PGC-DGCNN a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1811.10435v1> ;
    skos:definition """PGC-DGCNN provides a new definition of graph convolutional filter. It generalizes the most commonly adopted filter, adding an hyper-parameter controlling the distance of the considered neighborhood. The model extends graph convolutions, following an intuition derived from the well-known convolutional filters over multi-dimensional tensors. The methods involves a simple, efficient and effective way to introduce a hyper-parameter on graph convolutions that influences the filter size, i.e. its receptive field over the considered graph.\r
\r
Description and image from: [On Filter Size in Graph Convolutional Networks](https://arxiv.org/pdf/1811.10435.pdf)""" ;
    skos:prefLabel "PGC-DGCNN" .

:PGHI a skos:Concept ;
    rdfs:seeAlso <https://github.com/tifgan/phase-recovery> ;
    skos:altLabel "Phase Gradient Heap Integration" ;
    skos:definition """Z. Průša, P. Balazs and P. L. Søndergaard, "A Noniterative Method for Reconstruction of Phase From STFT Magnitude," in IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 25, no. 5, pp. 1154-1164, May 2017, doi: 10.1109/TASLP.2017.2678166.\r
Abstract: A noniterative method for the reconstruction of the short-time fourier transform (STFT) phase from the magnitude is presented. The method is based on the direct relationship between the partial derivatives of the phase and the logarithm of the magnitude of the un-sampled STFT with respect to the Gaussian window. Although the theory holds in the continuous setting only, the experiments show that the algorithm performs well even in the discretized setting (discrete Gabor transform) with low redundancy using the sampled Gaussian window, the truncated Gaussian window and even other compactly supported windows such as the Hann window. Due to the noniterative nature, the algorithm is very fast and it is suitable for long audio signals. Moreover, solutions of iterative phase reconstruction algorithms can be improved considerably by initializing them with the phase estimate provided by the present algorithm. We present an extensive comparison with the state-of-the-art algorithms in a reproducible manner.\r
URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7890450&isnumber=7895265""" ;
    skos:prefLabel "PGHI" .

:PGM a skos:Concept ;
    rdfs:seeAlso <https://github.com/clferrari/probability-guided-maxout> ;
    skos:altLabel "Probability Guided Maxout" ;
    skos:definition "A regularization criterion that, differently from [dropout](https://paperswithcode.com/method/dropout) and its variants, is deterministic rather than random. It grounds on the empirical evidence that feature descriptors with larger L2-norm and highly-active nodes are strongly correlated to confident class predictions. Thus, the criterion guides towards dropping a percentage of the most active nodes of the descriptors, proportionally to the estimated class probability" ;
    skos:prefLabel "PGM" .

:PGNet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2104.05458v1> ;
    skos:altLabel "Point Gathering Network" ;
    skos:definition "**PGNet** is a point-gathering network for reading arbitrarily-shaped text in real-time. It is a single-shot text spotter, where the pixel-level character classification map is learned with proposed PG-CTC loss avoiding the usage of character-level annotations. With a PG-CTC decoder, we gather high-level character classification vectors from two-dimensional space and decode them into text symbols without NMS and RoI operations involved, which guarantees high efficiency. Additionally, reasoning the relations between each character and its neighbors, a graph refinement module (GRM) is proposed to optimize the coarse recognition and improve the end-to-end performance." ;
    skos:prefLabel "PGNet" .

:PICARD a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2109.05093v1> ;
    skos:altLabel "Parsing Incrementally for Constrained Auto-Regressive Decoding" ;
    skos:definition "" ;
    skos:prefLabel "PICARD" .

:PIRL a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1912.01991v1> ;
    skos:definition "**Pretext-Invariant Representation Learning (PIRL, pronounced as “pearl”)** learns invariant representations based on pretext tasks. PIRL is used with a commonly used pretext task that involves solving [jigsaw](https://paperswithcode.com/method/jigsaw) puzzles. Specifically, PIRL constructs image representations that are similar to the representation of transformed versions of the same image and different from the representations of other images." ;
    skos:prefLabel "PIRL" .

:PISA a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1904.04821v2> ;
    skos:altLabel "PrIme Sample Attention" ;
    skos:definition "**PrIme Sample Attention (PISA)** directs the training of object detection frameworks towards prime samples. These are samples that play a key role in driving the detection performance. The authors define Hierarchical Local Rank (HLR) as a metric of importance. Specifically, they use IoU-HLR to rank positive samples and ScoreHLR to rank negative samples in each mini-batch. This ranking strategy places the positive samples with highest IoUs around each object and the negative samples with highest scores in each cluster to the top of the ranked list and directs the focus of the training process to them via a simple re-weighting scheme. The authors also devise a classification-aware regression loss to jointly optimize the classification and regression branches. Particularly, this loss would suppress those samples with large regression loss, thus reinforcing the attention to prime samples." ;
    skos:prefLabel "PISA" .

:PIoULoss a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2007.09584v1> ;
    skos:definition "**PIoU Loss** is a loss function for oriented object detection which is formulated to exploit both the angle and IoU for accurate oriented bounding box regression. The PIoU loss is derived from IoU metric with a pixel-wise form." ;
    skos:prefLabel "PIoU Loss" .

:PLATO-2 a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2006.16779v4> ;
    skos:definition "" ;
    skos:prefLabel "PLATO-2" .

:PLIP a skos:Concept ;
    dcterms:source <https://www.biorxiv.org/content/10.1101/2023.03.29.534834v1> ;
    skos:altLabel "Pathology Language and Image Pre-Training" ;
    skos:definition "Pathology Language and Image Pre-Training (PLIP) is a vision-and-language foundation model created by fine-tuning CLIP on pathology images." ;
    skos:prefLabel "PLIP" .

:PMLM a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2004.11579v1> ;
    skos:altLabel "Probabilistically Masked Language Model" ;
    skos:definition "**Probabilistically Masked Language Model**, or **PMLM**, is a type of language model that utilizes a probabilistic masking scheme, aiming to bridge the gap between masked and autoregressive language models. The basic idea behind the connection of two categories of models is similar to MADE by Germain et al (2015). PMLM is a masked language model with a probabilistic masking scheme, which defines the way sequences are masked by following a probabilistic distribution. The authors employ a simple uniform distribution of the masking ratio and name the model as u-PMLM." ;
    skos:prefLabel "PMLM" .

:PNA a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2004.05718v5> ;
    skos:altLabel "Principal Neighbourhood Aggregation" ;
    skos:definition "**Principal Neighbourhood Aggregation** (PNA) is a general and flexible architecture for graphs combining multiple aggregators with degree-scalers (which generalize the sum aggregator)." ;
    skos:prefLabel "PNA" .

:PNAS a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1712.00559v3> ;
    skos:altLabel "Progressive Neural Architecture Search" ;
    skos:definition """**Progressive Neural Architecture Search**, or **PNAS**, is a method for learning the structure of convolutional neural networks (CNNs). It uses a sequential model-based optimization (SMBO) strategy, where we search the space of cell structures, starting with simple (shallow) models and progressing to complex ones, pruning out unpromising structures as we go. \r
\r
At iteration $b$ of the algorithm, we have a set of $K$ candidate cells (each of size $b$ blocks), which we train and evaluate on a dataset of interest. Since this process is expensive, PNAS also learns a model or surrogate function which can predict the performance of a structure without needing to train it. We then expand the $K$ candidates of size $b$ into $K' \\gg K$ children, each of size $b+1$. The surrogate function is used to rank all of the $K'$ children, pick the top $K$, and then train and evaluate them. We continue in this way until $b=B$, which is the maximum number of blocks we want to use in a cell.""" ;
    skos:prefLabel "PNAS" .

:POMO a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2010.16011v3> ;
    skos:definition "" ;
    skos:prefLabel "POMO" .

:POTO a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2012.03544v3> ;
    skos:altLabel "Prediction-aware One-To-One" ;
    skos:definition "**Prediction-aware One-To-One**, or **POTO**, is an assignment rule for object detection which dynamically assigns the foreground samples according to the quality of classification and regression simultaneously." ;
    skos:prefLabel "POTO" .

:PP-OCR a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2009.09941v3> ;
    skos:definition "**PP-OCR** is an OCR system that consists of three parts, text detection, detected boxes rectification and text recognition. The purpose of text detection is to locate the text area in the image. In PP-OCR, Differentiable Binarization (DB) is used as text detector which is based on a simple segmentation network. It integrates feature extraction and sequence modeling. It adopts the Connectionist Temporal Classification (CTC) loss to avoid the inconsistency between prediction and label." ;
    skos:prefLabel "PP-OCR" .

:PP-YOLO a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2007.12099v3> ;
    skos:definition """**PP-YOLO** is an object detector based on [YOLOv3](https://paperswithcode.com/method/yolov3). It mainly tries to combine various existing tricks that almost not increase the number of model parameters and FLOPs, to achieve the goal of improving the accuracy of detector as much as possible while ensuring that the speed is almost unchanged. Some of these changes include:\r
\r
- Changing the [DarkNet-53](https://paperswithcode.com/method/darknet-53) backbone with ResNet50-vd. Some of the convolutional layers in ResNet50-vd are also replaced with [deformable convolutional layers](https://paperswithcode.com/method/deformable-convolution).\r
- A larger batch size is used - changing from 64 to 192.\r
- An exponentially moving average is used for the parameters.\r
- [DropBlock](https://paperswithcode.com/method/dropblock) is applied to the [FPN](https://paperswithcode.com/method/fpn).\r
- An IoU loss is used.\r
- An IoU prediction branch is added to measure the accuracy of localization.\r
- [Grid Sensitive](https://paperswithcode.com/method/grid-sensitive) is used, similar to [YOLOv4](https://paperswithcode.com/method/yolov4).\r
- [Matrix NMS](https://paperswithcode.com/method/matrix-nms) is used.\r
- [CoordConv](https://paperswithcode.com/method/coordconv) is used for the [FPN](https://paperswithcode.com/method/fpn), replacing the 1x1 convolution layer, and also the first convolution layer in the detection head.\r
- [Spatial Pyramid Pooling](https://paperswithcode.com/method/spatial-pyramid-pooling) is used for the top feature map.""" ;
    skos:prefLabel "PP-YOLO" .

:PP-YOLOv2 a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2104.10419v1> ;
    skos:definition """**PP-YOLOv2** is an object detector that extends upon [PP-YOLO](https://www.paperswithcode.com/method/pp-yolo) with several refinements:\r
\r
- A [Path Aggregation Network](https://paperswithcode.com/method/pafpn) is included for the FPN to compose bottom-up paths.\r
- [Mish Activation functions](https://paperswithcode.com/method/mish) are used.\r
- The input size is expanded.\r
- An IoU aware branch is calculated with a soft label format.""" ;
    skos:prefLabel "PP-YOLOv2" .

:PPMC a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2003.02655v2> ;
    skos:altLabel "Path Planning and Motion Control" ;
    skos:definition "**Path Planning and Motion Control**, or **PPMC RL**, is a training algorithm that teaches path planning and motion control to robots using reinforcement learning in a simulated environment. The focus is on promoting generalization where there are environmental uncertainties such as rough environments like lunar services. The algorithm is coupled with any generic reinforcement learning algorithm to teach robots how to respond to user commands and to travel to designated locations on a single neural network. The algorithm works independently of the robot structure, demonstrating that it works on a wheeled rover in addition to the past results on a quadruped walking robot." ;
    skos:prefLabel "PPMC" .

:PPO a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1707.06347v2> ;
    skos:altLabel "Proximal Policy Optimization" ;
    skos:definition """**Proximal Policy Optimization**, or **PPO**, is a policy gradient method for reinforcement learning. The motivation was to have an algorithm with the data efficiency and reliable performance of [TRPO](https://paperswithcode.com/method/trpo), while using only first-order optimization. \r
\r
Let $r\\_{t}\\left(\\theta\\right)$ denote the probability ratio $r\\_{t}\\left(\\theta\\right) = \\frac{\\pi\\_{\\theta}\\left(a\\_{t}\\mid{s\\_{t}}\\right)}{\\pi\\_{\\theta\\_{old}}\\left(a\\_{t}\\mid{s\\_{t}}\\right)}$, so $r\\left(\\theta\\_{old}\\right) = 1$. TRPO maximizes a “surrogate” objective:\r
\r
$$ L^{\\text{CPI}}\\left({\\theta}\\right) = \\hat{\\mathbb{E}}\\_{t}\\left[\\frac{\\pi\\_{\\theta}\\left(a\\_{t}\\mid{s\\_{t}}\\right)}{\\pi\\_{\\theta\\_{old}}\\left(a\\_{t}\\mid{s\\_{t}}\\right)})\\hat{A}\\_{t}\\right] = \\hat{\\mathbb{E}}\\_{t}\\left[r\\_{t}\\left(\\theta\\right)\\hat{A}\\_{t}\\right] $$\r
\r
Where $CPI$ refers to a conservative policy iteration. Without a constraint, maximization of $L^{CPI}$ would lead to an excessively large policy update; hence, we PPO modifies the objective, to penalize changes to the policy that move $r\\_{t}\\left(\\theta\\right)$ away from 1:\r
\r
$$ J^{\\text{CLIP}}\\left({\\theta}\\right) = \\hat{\\mathbb{E}}\\_{t}\\left[\\min\\left(r\\_{t}\\left(\\theta\\right)\\hat{A}\\_{t}, \\text{clip}\\left(r\\_{t}\\left(\\theta\\right), 1-\\epsilon, 1+\\epsilon\\right)\\hat{A}\\_{t}\\right)\\right] $$\r
\r
where $\\epsilon$ is a hyperparameter, say, $\\epsilon = 0.2$. The motivation for this objective is as follows. The first term inside the min is $L^{CPI}$. The second term, $\\text{clip}\\left(r\\_{t}\\left(\\theta\\right), 1-\\epsilon, 1+\\epsilon\\right)\\hat{A}\\_{t}$ modifies the surrogate\r
objective by clipping the probability ratio, which removes the incentive for moving $r\\_{t}$ outside of the interval $\\left[1 − \\epsilon, 1 + \\epsilon\\right]$. Finally, we take the minimum of the clipped and unclipped objective, so the final objective is a lower bound (i.e., a pessimistic bound) on the unclipped objective. With this scheme, we only ignore the change in probability ratio when it would make the objective improve, and we include it when it makes the objective worse. \r
\r
One detail to note is that when we apply PPO for a network where we have shared parameters for actor and critic functions, we typically add to the objective function an error term on value estimation and an entropy term to encourage exploration.""" ;
    skos:prefLabel "PPO" .

:PQ-Transformer a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2109.05566v2> ;
    skos:altLabel "PointQuad-Transformer" ;
    skos:definition """**PQ-Transformer**, or **PointQuad-Transformer**, is a [Transformer](https://paperswithcode.com/method/transformer)-based architecture that predicts 3D objects and layouts simultaneously, using point cloud inputs. Unlike existing methods that either estimate layout keypoints or edges, room layouts are directly parameterized as a set of quads. Along with the quad representation, a physical constraint loss function is used that discourages object-layout interference.\r
\r
Given an input 3D point cloud of $N$ points, the point cloud feature learning backbone extracts $M$ context-aware point features of $\\left(3+C\\right)$ dimensions, through sampling and grouping. A voting module and a farthest point sampling (FPS) module are used to generate $K\\_{1}$ object proposals and $K\\_{2}$ quad proposals respectively. Then the proposals are processed by a transformer decoder to further refine proposal features. Through several feedforward layers and non-maximum suppression (NMS), the proposals become the final object bounding boxes and layout quads.""" ;
    skos:prefLabel "PQ-Transformer" .

:PREDATOR a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2011.13005v3> ;
    skos:definition "**PREDATOR** is a model for pairwise point-cloud registration with deep attention to the overlap region. Its key novelty is an overlap-attention block for early information exchange between the latent encodings of the two point clouds. In this way the subsequent decoding of the latent representations into per-point features is conditioned on the respective other point cloud, and thus can predict which points are not only salient, but also lie in the overlap region between the two point clouds." ;
    skos:prefLabel "PREDATOR" .

<http://w3id.org/mlso/vocab/ml_algorithm/PRNet+> a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2108.10613v1> ;
    skos:definition "**PRNet+** is a multi-task neural network for outdoor position recovery from measurement record (MR) data. PRNet+ develops a feature extraction module to learn common local-, short- and long-term spatio-temporal locality from heterogeneous MR samples, with a convolutional neural network (CNN), long short-term memory cells ([LSTM](https://paperswithcode.com/method/lstm)), and attention mechanisms. Specifically, PRNet+ 1) allows the various-length sequences of MR samples, such that the two components (CNN and LSTM) are able to capture spatial locality from the samples within each MR sequence, 2) exploits two attention mechanisms for the time-interval between neighbouring MR samples, together with the one between neighbouring MR sequences, to capture temporal locality, and 3) incorporates the detected transportation modes and predicted locations of heterogeneous MR data into a joint loss for better result." ;
    skos:prefLabel "PRNet+" .

:PReLU a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1502.01852v1> ;
    rdfs:seeAlso <https://github.com/pytorch/pytorch/blob/96aaa311c0251d24decb9dc5da4957b7c590af6f/torch/nn/modules/activation.py#L968> ;
    skos:altLabel "Parameterized ReLU" ;
    skos:definition """A **Parametric Rectified Linear Unit**, or **PReLU**, is an activation function that generalizes the traditional rectified unit with a slope for negative values. Formally:\r
\r
$$f\\left(y\\_{i}\\right) = y\\_{i} \\text{ if } y\\_{i} \\ge 0$$\r
$$f\\left(y\\_{i}\\right) = a\\_{i}y\\_{i} \\text{ if } y\\_{i} \\leq 0$$\r
\r
The intuition is that different layers may require different types of nonlinearity. Indeed the authors find in experiments with convolutional neural networks that PReLus for the initial layer have more positive slopes, i.e. closer to linear. Since the filters of the first layers are Gabor-like filters such as edge or texture detectors, this shows a circumstance where positive and negative responses of filters are respected. In contrast the authors find deeper layers have smaller coefficients, suggesting the model becomes more discriminative at later layers (while it wants to retain more information at earlier layers).""" ;
    skos:prefLabel "PReLU" .

:PReLU-Net a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1502.01852v1> ;
    rdfs:seeAlso <https://github.com/nutszebra/prelu_net/blob/6f301215fa0ecc3b62a45628c3f0f412e5420399/prelu.py#L84> ;
    skos:definition "**PReLU-Net** is a type of convolutional neural network that utilises parameterized ReLUs for its activation function. It also uses a robust initialization scheme - afterwards known as [Kaiming Initialization](https://paperswithcode.com/method/he-initialization) - that accounts for non-linear activation functions." ;
    skos:prefLabel "PReLU-Net" .

:PSANet a skos:Concept ;
    dcterms:source <http://openaccess.thecvf.com/content_ECCV_2018/html/Hengshuang_Zhao_PSANet_Point-wise_Spatial_ECCV_2018_paper.html> ;
    rdfs:seeAlso <https://github.com/hszhao/semseg/blob/7192f922b99468969cfd4535e3e35a838994b115/model/psanet.py#L101> ;
    skos:definition """**PSANet** is a semantic segmentation architecture that utilizes a [Point-wise Spatial Attention](https://paperswithcode.com/method/point-wise-spatial-attention) (PSA) module to aggregate long-range contextual information in a flexible and adaptive manner. Each position in the feature map is connected with all other ones through self-adaptively predicted attention maps, thus harvesting various information nearby and far away. Furthermore, the authors design the bi-directional information propagation path for a comprehensive understanding of complex scenes. Each position collects information from all others to help the prediction of itself and vice versa, the information at each position can be distributed globally, assisting the prediction of all other positions. Finally, the bi-directionally aggregated contextual information is fused with local features to form the final representation of complex scenes.\r
\r
The authors use [ResNet](https://paperswithcode.com/method/resnet) as an [FCN](https://paperswithcode.com/method/fcn) backbone for PSANet, as the Figure to the right illustrates. The proposed PSA module is then used to aggregate long-range contextual information from the local representation. It follows stage-5 in ResNet, which is the final stage of the FCN backbone. Features in stage-5 are semantically stronger. Aggregating them together leads to a more comprehensive representation of long-range context. Moreover, the spatial size of the feature map at stage-5 is smaller and can reduce computation overhead and memory consumption. An auxiliary loss branch is applied apart from the main loss.""" ;
    skos:prefLabel "PSANet" .

:PSFR-GAN a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2009.08709v2> ;
    skos:definition "**PSFR-GAN** is a semantic-aware style transformation framework for face restoration. Given a pair of LQ face image and its corresponding parsing map, we first generate a multi-scale pyramid of the inputs, and then progressively modulate different scale features from coarse-to-fine in a semantic-aware style transfer way. Compared with previous networks, the proposed PSFR-GAN makes full use of the semantic (parsing maps) and pixel (LQ images) space information from different scales of inputs." ;
    skos:prefLabel "PSFR-GAN" .

:PSPNet a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1612.01105v2> ;
    rdfs:seeAlso <https://github.com/xitongpu/PSPNet> ;
    skos:definition """**PSPNet**, or **Pyramid Scene Parsing Network**, is a semantic segmentation model that utilises a pyramid parsing module that exploits global context information by different-region based context aggregation. The local and global clues together make the final prediction more reliable. We also propose an optimization\r
\r
Given an input image, PSPNet use a pretrained CNN with the dilated network strategy to extract the feature map. The final feature map size is $1/8$ of the input image. On top of the map, we use the [pyramid pooling module](https://paperswithcode.com/method/pyramid-pooling-module) to gather context information. Using our 4-level pyramid, the pooling kernels cover the whole, half of, and small portions of the image. They are fused as the global prior.\r
Then we concatenate the prior with the original feature map in the final part of. It is followed by a [convolution](https://paperswithcode.com/method/convolution) layer to generate the final prediction map.""" ;
    skos:prefLabel "PSPNet" .

:PULSE a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2003.03808v3> ;
    skos:definition "**PULSE** is a self-supervised photo upsampling algorithm. Instead of starting with the LR image and slowly adding detail, PULSE traverses the high-resolution natural image manifold, searching for images that downscale to the original LR image. This is formalized through the downscaling loss, which guides exploration through the latent space of a generative model. By leveraging properties of high-dimensional Gaussians, the authors aim to restrict the search space to guarantee realistic outputs." ;
    skos:prefLabel "PULSE" .

:PVT a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2102.12122v2> ;
    skos:altLabel "Pyramid Vision Transformer" ;
    skos:definition """**PVT**, or **Pyramid Vision Transformer**, is a type of [vision transformer](https://paperswithcode.com/methods/category/vision-transformer) that utilizes a pyramid structure to make it an effective backbone for dense prediction tasks. Specifically it allows for more fine-grained inputs (4 x 4 pixels per patch) to be used, while simultaneously shrinking the sequence length of the Transformer as it deepens - reducing the computational cost. Additionally, a [spatial-reduction attention](https://paperswithcode.com/method/spatial-reduction-attention) (SRA) layer is used to further reduce the resource consumption when learning high-resolution features.\r
\r
The entire model is divided into four stages, each of which is comprised of a patch embedding layer and a $\\mathcal{L}\\_{i}$-layer Transformer encoder. Following a pyramid structure, the output resolution of the four stages progressively shrinks from high (4-stride) to low (32-stride).""" ;
    skos:prefLabel "PVT" .

:PVTv2 a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2106.13797v7> ;
    rdfs:seeAlso <https://github.com/whai362/PVT/blob/v2/detection/pvt_v2.py> ;
    skos:altLabel "Pyramid Vision Transformer v2" ;
    skos:definition "**Pyramid Vision Transformer v2** (PVTv2) is a type of [Vision Transformer](https://paperswithcode.com/method/vision-transformer) for detection and segmentation tasks. It improves on [PVTv1](https://paperswithcode.com/method/pvt) through several design improvements: (1) overlapping patch embedding, (2) convolutional feed-forward networks, and (3) linear complexity attention layers that are orthogonal to the PVTv1 framework." ;
    skos:prefLabel "PVTv2" .

:PWIL a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2006.04678v2> ;
    skos:altLabel "Primal Wasserstein Imitation Learning" ;
    skos:definition "**Primal Wasserstein Imitation Learning**, or **PWIL**, is a method for imitation learning which ties to the primal form of the Wasserstein distance between the expert and the agent state-action distributions. The reward function is derived offline, as opposed to recent adversarial IL algorithms that learn a reward function through interactions with the environment, and requires little fine-tuning." ;
    skos:prefLabel "PWIL" .

:PaLM a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2204.02311v5> ;
    skos:altLabel "Pathways Language Model" ;
    skos:definition """**PaLM** (**Pathways Language Model**) uses a standard Transformer model architecture (Vaswani et al., 2017) in a decoder-only setup (i.e., each timestep can only attend to itself and past timesteps), with several modifications. PaLM is trained as a 540 billion parameter, densely activated, autoregressive Transformer on 780 billion tokens. PaLM leverages Pathways (Barham et al., 2022), which enables highly efficient training of very large neural networks across thousands of accelerator chips.\r
\r
Image credit: [PaLM: Scaling Language Modeling with Pathways](https://paperswithcode.com/paper/palm-scaling-language-modeling-with-pathways-1)""" ;
    skos:prefLabel "PaLM" .

:PackedLevitatedMarkers a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2109.06067v5> ;
    skos:definition "**Packed Levitated Markers**, or **PL-Marker**, is a span representation approach for [named entity recognition](https://paperswithcode.com/task/named-entity-recognition-ner) that considers the dependencies between spans (pairs) by strategically packing the markers in the encoder. A pair of Levitated Markers, emphasizing a span, consists of a start marker and an end marker which share the same position embeddings with span’s start and end tokens respectively. In addition, both levitated markers adopt a restricted attention, that is, they are visible to each other, but not to the text token and other pairs of markers. sBased on the above features, the levitated marker would not affect the attended context of the original text tokens, which allows us to flexibly pack a series of related spans with their levitated markers in the encoding phase and thus model their dependencies." ;
    skos:prefLabel "Packed Levitated Markers" .

<http://w3id.org/mlso/vocab/ml_algorithm/PanGu-$α$> a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2104.12369v1> ;
    skos:definition "**PanGu-$α$** is an autoregressive language model (ALM) with up to 200 billion parameters pretrained on a large corpus of text, mostly in Chinese language. The architecture of PanGu-$α$ is based on Transformer, which has been extensively used as the backbone of a variety of pretrained language models such as [BERT](https://paperswithcode.com/method/bert) and [GPT](https://paperswithcode.com/method/gpt). Different from them, there's an additional query layer developed on top of Transformer layers which aims to explicitly induce the expected output." ;
    skos:prefLabel "PanGu-$α$" .

:PanNet a skos:Concept ;
    dcterms:source <http://openaccess.thecvf.com/content_iccv_2017/html/Yang_PanNet_A_Deep_ICCV_2017_paper.html> ;
    skos:altLabel "Pansharpening Network" ;
    skos:definition "We propose a deep network architecture for the pansharpening problem called PanNet. We incorporate domain-specific knowledge to design our PanNet architecture by focusing on the two aims of the pan-sharpening problem: spectral and spatial preservation. For spectral preservation, we add up-sampled multispectral images to the network output, which directly propagates the spectral information to the reconstructed image. To preserve the spatial structure, we train our network parameters in the high-pass filtering domain rather than the image domain. We show that the trained network generalizes well to images from different satellites without needing retraining. Experiments show significant improvement over state-of-the-art methods visually and in terms of standard quality metrics." ;
    skos:prefLabel "PanNet" .

:Panoptic-PolarNet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2103.14962v1> ;
    skos:definition "**Panoptic-PolarNet** is a point cloud segmentation framework for LiDAR point clouds. It learns both semantic segmentation and class-agnostic instance clustering in a single inference network using a polar Bird's Eye View (BEV) representation, enabling the authors to circumvent the issue of occlusion among instances in urban street scenes. We first encode the raw point cloud data with $K$ features into a fixed-size representation on the polar BEV map. Next, we use a single backbone encoder-decoder network to generate semantic prediction, center [heatmap](https://paperswithcode.com/method/heatmap) and offset regression. Finally, we merge these outputs via a voting-based fusion to yield the panoptic segmentation result." ;
    skos:prefLabel "Panoptic-PolarNet" .

:PanopticFPN a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1901.02446v2> ;
    rdfs:seeAlso <https://github.com/facebookresearch/detectron2/blob/1b09e42cc87d47a6e0a3892cd86e780d86a9b122/detectron2/modeling/meta_arch/panoptic_fpn.py#L20> ;
    skos:definition """A **Panoptic FPN** is an extension of an [FPN](https://paperswithcode.com/method/fpn) that can generate both instance and semantic segmentations via FPN. The approach starts with an FPN backbone and adds a branch for performing semantic segmentation in parallel with the existing region-based branch for instance segmentation. No changes are made to the FPN backbone when adding the dense-prediction branch, making it compatible with existing instance segmentation methods. \r
\r
The new semantic segmentation branch achieves its goal as follows. Starting from the deepest FPN level (at 1/32 scale), we perform three upsampling stages to yield a feature map at 1/4 scale, where each upsampling stage consists of 3×3 [convolution](https://paperswithcode.com/method/convolution), group norm, [ReLU](https://paperswithcode.com/method/relu), and 2× bilinear upsampling. This strategy is repeated for FPN scales 1/16, 1/8, and 1/4 (with progressively fewer upsampling stages). The result is a set of feature maps at the same 1/4 scale, which are then element-wise summed. A final 1×1 convolution, 4× bilinear upsampling, and [softmax](https://paperswithcode.com/method/softmax) are used to generate the per-pixel class labels at the original image resolution. In addition to stuff classes, this branch also outputs a special ‘other’ class for all pixels belonging to objects (to avoid predicting stuff classes for such pixels).""" ;
    skos:prefLabel "Panoptic FPN" .

:ParaNet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1905.08459v3> ;
    skos:definition """**ParaNet** is a non-autoregressive attention-based architecture for text-to-speech, which is fully convolutional and converts text to mel spectrogram. ParaNet distills the attention from the autoregressive text-to-spectrogram model, and iteratively refines the alignment between text and spectrogram in a layer-by-layer manner. The architecture is otherwise similar to [Deep Voice 3](https://paperswithcode.com/method/deep-voice-3) except these changes to the decoder; whereas the decoder of DV3 has multiple attention-based layers, where each layer consists of a\r
[causal convolution](https://paperswithcode.com/method/causal-convolution) block followed by an attention block, ParaNet has a single attention block in the encoder.""" ;
    skos:prefLabel "ParaNet" .

:ParaNetConvolutionBlock a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1905.08459v3> ;
    rdfs:seeAlso <https://github.com/vardaan123/ParaNet/blob/8e4561e67df070554230bba871f5dd0fcb084210/onmt/modules/Conv2Conv.py#L25> ;
    skos:definition "A **ParaNet Convolution Block** is a convolutional block that appears in the encoder and decoder of the [ParaNet](https://paperswithcode.com/method/paranet) text-to-speech architecture. It consists of a 1-D [convolution](https://paperswithcode.com/method/convolution) with a gated linear unit ([GLU](https://paperswithcode.com/method/glu)) and a [residual connection](https://paperswithcode.com/method/residual-connection). It is similar to the [DV3 Convolution Block](https://paperswithcode.com/method/dv3-convolution-block)." ;
    skos:prefLabel "ParaNet Convolution Block" .

:Parallax a skos:Concept ;
    skos:definition """**Parallax** is a hybrid parallel method for training large neural networks. Parallax is a framework that optimizes data parallel training by utilizing the sparsity of model parameters. Parallax introduces a hybrid approach that combines Parameter Server and AllReduce architectures to optimize the amount of data transfer according to the sparsity.\r
\r
Parallax pursues a hybrid approach that uses the Parameter Server architecture for handling sparse variables and the AllReduce architecture for handling dense variables. Moreover, Parallax partitions large sparse variables by a near-optimal number of partitions to maximize parallelism while maintaining low computation and communication overhead. Parallax further optimizes training with local aggregation and smart operation placement to mitigate communication overhead. Graph transformation in Parallax automatically applies all of these optimizations and the data parallel training itself at the framework level to minimize user efforts for writing and optimizing a distributed program by composing low-level primitives.""" ;
    skos:prefLabel "Parallax" .

:ParallelLayers a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2204.02311v5> ;
    skos:definition """• Parallel Layers – We use a “parallel” formulation in each Transformer block (Wang & Komatsuzaki, 2021), rather than the standard “serialized” formulation. Specifically, the standard formulation can be written as:   \r
    y = x + MLP(LayerNorm(x + Attention(LayerNorm(x)))   \r
\r
Whereas the parallel formulation can be written as:   \r
    y = x + MLP(LayerNorm(x)) + Attention(LayerNorm(x))   \r
\r
The parallel formulation results in roughly 15% faster training speed at large scales, since the MLP and Attention input matrix multiplications can be fused. Ablation experiments showed a small quality degradation at 8B scale but no quality degradation at 62B scale, so we extrapolated that the effect of parallel layers should be quality neutral at the 540B scale.""" ;
    skos:prefLabel "Parallel Layers" .

:ParamCrop a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2108.10501v3> ;
    skos:definition """**ParamCrop** is a parametric cubic cropping for video contrastive learning, where cubic cropping refers to cropping a 3D cube\r
from the input video. The central component of ParamCrop is a differentiable spatio-temporal cropping operation, which enables ParamCrop to be trained simultaneously with the video backbone and adjust the cropping strategy on the fly. The objective of ParamCrop is set to be adversarial to the video backbone, which is to increase the contrastive loss. Hence, initialized with the simplest setting where two cropped views largely overlaps, ParamCrop gradually increases the disparity between two views.""" ;
    skos:prefLabel "ParamCrop" .

:ParametricUMAP a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2009.12981v4> ;
    skos:definition "**Parametric UMAP** is a non-parametric graph-based dimensionality reduction algorithm that extends the second step of [UMAP](https://www.paperswithcode.com/method/umap) to a parametric optimization over neural network weights, learning a parametric relationship between data and embedding." ;
    skos:prefLabel "Parametric UMAP" .

:Parrot a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2006.16239v2> ;
    skos:definition "**Parrot** is an imitation learning approach to automatically learn cache access patterns by leveraging Belady’s optimal policy. Belady’s optimal policy is an oracle policy that computes the theoretically optimal cache eviction decision based on knowledge of future cache accesses, which Parrot approximates with a policy that only conditions on the past accesses." ;
    skos:prefLabel "Parrot" .

:PartitionFilterNetwork a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2108.12202v8> ;
    skos:definition "**Partition Filter Network** is a framework designed specifically for joint entity and relation extraction. The framework consists of three components: partition filter encoder, NER unit and RE unit. In task units, we use table-filling for word pair prediction. Orange, yellow and green represents NER-related, shared and RE-related component or features. (b) Detailed depiction of partition filter encoder in one single time step. We decompose feature encoding into two steps: partition and filter (shown in the gray area). In partition, we first segment neurons into two task partitions and one shared partition. Then in filter, partitions are selected and combined to form task-specific features and shared features, filtering out information irrelevant to each task." ;
    skos:prefLabel "Partition Filter Network" .

:PatchAugment a skos:Concept ;
    rdfs:seeAlso <https://github.com/VimsLab/PatchAugment> ;
    skos:altLabel "PatchAugment: Local Neighborhood Augmentation in Point Cloud Classification" ;
    skos:definition """Recent deep neural network models trained on smaller and less diverse datasets use data augmentation to alleviate limitations such as overfitting, reduced robustness, and lower generalization. Methods using 3D datasets are among the most common to use data augmentation techniques such as random point drop, scaling, translation, rotations, and jittering. However, these data augmentation techniques are fixed and are often applied to the entire object, ignoring the object’s local geometry. Different local neighborhoods on the object surface hold a different amount of geometric complexity. Applying the same data augmentation techniques at the object level is less effective in augmenting local neighborhoods with complex structures. This paper presents PatchAugment, a data augmentation framework to apply different augmentation techniques to the local neighborhoods. Our experimental studies on PointNet++ and DGCNN models demonstrate the effectiveness of PatchAugment on the task of 3D Point Cloud Classification. We evaluated our technique against these models using four benchmark datasets, ModelNet40 (synthetic), ModelNet10 (synthetic), SHREC’16 (synthetic), and ScanObjectNN (real-world).\r
\r
[[ICCVW 2021]](https://openaccess.thecvf.com/content/ICCV2021W/DLGC/papers/Sheshappanavar_PatchAugment_Local_Neighborhood_Augmentation_in_Point_Cloud_Classification_ICCVW_2021_paper.pdf) PatchAugment: Local Neighborhood Augmentation in Point Cloud Classification. [[Code]](https://github.com/VimsLab/PatchAugment)""" ;
    skos:prefLabel "PatchAugment" .

:PatchGAN a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1611.07004v3> ;
    rdfs:seeAlso <https://github.com/znxlwm/pytorch-pix2pix/blob/3059f2af53324e77089bbcfc31279f01a38c40b8/network.py#L104> ;
    skos:definition "**PatchGAN** is a type of discriminator for generative adversarial networks which only penalizes structure at the scale of local image patches. The PatchGAN discriminator tries to classify if each $N \\times N$ patch in an image is real or fake. This discriminator is run convolutionally across the image, averaging all responses to provide the ultimate output of $D$. Such a discriminator effectively models the image as a Markov random field, assuming independence between pixels separated by more than a patch diameter. It can be understood as a type of texture/style loss." ;
    skos:prefLabel "PatchGAN" .

:PatchMerger a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2202.12015v1> ;
    skos:altLabel "Patch Merger Module" ;
    skos:definition """PatchMerger is a module for Vision Transformers that decreases the number of tokens/patches passed onto each individual transformer encoder block whilst maintaining performance and reducing compute. PatchMerger takes linearly transforms an input of shape N patches × D dimensions through a learnable weight matrix of shape M output patches × D. This generates M scores, in which a Softmax function is applied for each score. The resulting output has a shape of M × N, which is multiplied to the original input to get an output of shape M × D.\r
\r
Mathematically, $$Y = \\text{softmax}({W^T}{X^T})X$$\r
\r
Image and formula from: Renggli, C., Pinto, A. S., Houlsby, N., Mustafa, B., Puigcerver, J., & Riquelme, C. (2022). Learning to Merge Tokens in Vision Transformers. arXiv preprint arXiv:2202.12015.""" ;
    skos:prefLabel "Patch Merger" .

:PathLengthRegularization a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1912.04958v2> ;
    rdfs:seeAlso <https://github.com/NVlabs/stylegan2/blob/7d3145d23013607b987db30736f89fb1d3e10fad/training/loss.py#L156> ;
    skos:definition """**Path Length Regularization** is a type of regularization for [generative adversarial networks](https://paperswithcode.com/methods/category/generative-adversarial-networks) that encourages good conditioning in the mapping from latent codes to images. The idea is to encourage that a fixed-size step in the latent space $\\mathcal{W}$ results in a non-zero, fixed-magnitude change in the image.\r
\r
We can measure the deviation from this ideal empirically by stepping into random directions in the image space and observing the corresponding $\\mathbf{w}$ gradients. These gradients should have close to an equal length regardless of $\\mathbf{w}$ or the image-space direction, indicating that the mapping from the latent space to image space is well-conditioned.\r
\r
At a single $\\mathbf{w} \\in \\mathcal{W}$ the local metric scaling properties of the generator mapping $g\\left(\\mathbf{w}\\right) : \\mathcal{W} \\rightarrow \\mathcal{Y}$ are captured by the Jacobian matrix $\\mathbf{J\\_{w}} = \\delta{g}\\left(\\mathbf{w}\\right)/\\delta{\\mathbf{w}}$. Motivated by the desire to preserve the expected lengths of vectors regardless of the direction, we formulate the regularizer as:\r
\r
$$ \\mathbb{E}\\_{\\mathbf{w},\\mathbf{y} \\sim \\mathcal{N}\\left(0, \\mathbf{I}\\right)} \\left(||\\mathbf{J}^{\\mathbf{T}}\\_{\\mathbf{w}}\\mathbf{y}||\\_{2} - a\\right)^{2} $$\r
\r
where $y$ are random images with normally distributed pixel intensities, and $w \\sim f\\left(z\\right)$, where $z$ are normally distributed. \r
\r
To avoid explicit computation of the Jacobian matrix, we use the identity $\\mathbf{J}^{\\mathbf{T}}\\_{\\mathbf{w}}\\mathbf{y} = \\nabla\\_{\\mathbf{w}}\\left(g\\left(\\mathbf{w}\\right)·y\\right)$, which is efficiently computable using standard backpropagation. The constant $a$ is set dynamically during optimization as the long-running exponential moving average of the lengths $||\\mathbf{J}^{\\mathbf{T}}\\_{\\mathbf{w}}\\mathbf{y}||\\_{2}$, allowing the optimization to find a suitable global scale by itself.\r
\r
The authors note that they find that path length regularization leads to more reliable and consistently behaving models, making architecture exploration easier. They also observe that the smoother generator is significantly easier to invert.""" ;
    skos:prefLabel "Path Length Regularization" .

:Pattern-ExploitingTraining a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2001.07676v3> ;
    skos:definition """**Pattern-Exploiting Training** is a semi-supervised training procedure that reformulates input examples as cloze-style phrases to help language models understand a given task. These phrases are then used to assign soft labels to a large set of unlabeled examples. Finally, standard supervised training is performed on the resulting training set. \r
\r
In the case of PET for sentiment classification, first a number of patterns encoding some form of task description are created to convert training examples to cloze questions; for each pattern, a pretrained language model is finetuned. Secondly, the ensemble of trained models annotates unlabeled data. Lastly, a classifier is trained on the resulting soft-labeled dataset.""" ;
    skos:prefLabel "Pattern-Exploiting Training" .

:Peer-attention a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2008.08072v1> ;
    skos:definition "**Peer-attention** is a network component which dynamically learns the attention weights using another block or input modality. This is unlike AssembleNet which partially relies on exponential mutations to explore connections. Once the attention weights are found, we can either prune the connections by only leaving the argmax over $h$ or leave them with [softmax](https://paperswithcode.com/method/softmax)." ;
    skos:prefLabel "Peer-attention" .

:PeleeNet a skos:Concept ;
    dcterms:source <http://papers.nips.cc/paper/7466-pelee-a-real-time-object-detection-system-on-mobile-devices> ;
    rdfs:seeAlso <https://github.com/osmr/imgclsmob/blob/c03fa67de3c9e454e9b6d35fe9cbb6b15c28fda7/pytorch/pytorchcv/models/peleenet.py#L196> ;
    skos:definition "**PeleeNet** is a convolutional neural network  and object detection backbone that is a variation of [DenseNet](https://paperswithcode.com/method/densenet) with optimizations to meet a memory and computational budget. Unlike competing networks, it does not use depthwise convolutions and instead relies on regular convolutions." ;
    skos:prefLabel "PeleeNet" .

:PerceiverIO a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2107.14795v3> ;
    skos:definition "Perceiver IO is a general neural network architecture that performs well for structured input modalities and output tasks. Perceiver IO is built to easily integrate and transform arbitrary information for arbitrary tasks." ;
    skos:prefLabel "Perceiver IO" .

:Performer a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2009.14794v4> ;
    skos:definition "**Performer** is a [Transformer](https://paperswithcode.com/methods/category/transformers) architectures which can estimate regular ([softmax](https://paperswithcode.com/method/softmax)) full-rank-attention Transformers with provable accuracy, but using only linear (as opposed to quadratic) space and time complexity, without relying on any priors such as sparsity or low-rankness. Performers are linear architectures fully compatible with regular Transformers and with strong theoretical guarantees: unbiased or nearly-unbiased estimation of the attention matrix, uniform convergence and low estimation variance. To approximate softmax attention-kernels, Performers use a Fast Attention Via positive Orthogonal Random features approach (FAVOR+), leveraging new methods for approximating softmax and Gaussian kernels." ;
    skos:prefLabel "Performer" .

:PermuteFormer a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2109.02377v2> ;
    skos:definition """**PermuteFormer** is a [Performer](https://paperswithcode.com/method/performer)-based model with relative position encoding that scales linearly on long sequences. PermuteFormer applies position-dependent transformation on queries and keys to encode positional information into the attention module. This transformation is carefully crafted so that the final output of self-attention is not affected by absolute positions of tokens.\r
\r
Each token’s query / key feature is illustrated as a row of blocks in the figure, and its elements are marked with different colors. The position-aware permutation permutes elements of each token’s query / key feature along the head size dimension in each attention head. Depending on the token’s position, the permutation applied to query / key feature is different.""" ;
    skos:prefLabel "PermuteFormer" .

:PhaseShuffle a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1802.04208v3> ;
    skos:definition """**Phase Shuffle** is a technique for removing pitched noise artifacts that come from using transposed convolutions in audio generation models. Phase shuffle is an operation with hyperparameter $n$. It randomly perturbs the phase of each layer’s activations by −$n$ to $n$ samples before input to the next layer.\r
\r
In the original application in [WaveGAN](https://paperswithcode.com/method/wavegan), the authors only apply phase shuffle to the discriminator, as the latent vector already provides the generator a mechanism to manipulate the phase\r
of a resultant waveform. Intuitively speaking, phase shuffle makes the discriminator’s job more challenging by requiring invariance to the phase of the input waveform.""" ;
    skos:prefLabel "Phase Shuffle" .

:Phish a skos:Concept ;
    dcterms:source <https://www.semanticscholar.org/paper/Phish%3A-A-Novel-Hyper-Optimizable-Activation-Naveen/43eb5e22da6092d28f0e842fec53ec1a76e1ba6b> ;
    skos:altLabel "Phish: A Novel Hyper-Optimizable Activation Function" ;
    skos:definition "Deep-learning models estimate values using backpropagation. The activation function within hidden layers is a critical component to minimizing loss in deep neural-networks. Rectified Linear (ReLU) has been the dominant activation function for the past decade. Swish and Mish are newer activation functions that have shown to yield better results than ReLU given specific circumstances. Phish is a novel activation function proposed here. It is a composite function defined as f(x) = xTanH(GELU(x)), where no discontinuities are apparent in the differentiated graph on the domain observed. Generalized networks were constructed using different activation functions. SoftMax was the output function. Using images from MNIST and CIFAR-10 databanks, these networks were trained to minimize sparse categorical crossentropy. A large scale cross-validation was simulated using stochastic Markov chains to account for the law of large numbers for the probability values. Statistical tests support the research hypothesis stating Phish could outperform other activation functions in classification. Future experiments would involve testing Phish in unsupervised learning algorithms and comparing it to more activation functions." ;
    skos:prefLabel "Phish" .

:PinvGCN a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2008.00720v2> ;
    rdfs:seeAlso <https://github.com/dominikalfke/PinvGCN> ;
    skos:altLabel "Pseudoinverse Graph Convolutional Network" ;
    skos:definition "A [GCN](https://paperswithcode.com/method/gcn) method targeted at the unique spectral properties of dense graphs and hypergraphs, enabled by efficient numerical linear algebra." ;
    skos:prefLabel "PinvGCN" .

:PipeDream a skos:Concept ;
    skos:definition "PipeDream is an asynchronous pipeline parallel strategy for training large neural networks. It adds inter-batch pipelining to intra-batch parallelism to further improve parallel training throughput, helping to better overlap computation with communication and reduce the amount of communication when possible." ;
    skos:prefLabel "PipeDream" .

:PipeDream-2BW a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2006.09503v3> ;
    skos:definition """**PipeDream-2BW** is an asynchronous pipeline parallel method that supports memory-efficient pipeline parallelism, a hybrid form of parallelism that combines data and model parallelism with input pipelining. PipeDream-2BW uses a novel pipelining and weight gradient coalescing strategy, combined with the double buffering of weights, to ensure high throughput, low memory footprint, and weight update semantics similar to data parallelism. In addition, PipeDream2BW automatically partitions the model over the available hardware resources, while respecting hardware constraints such as memory capacities of accelerators, and topologies and bandwidths of interconnects. PipeDream-2BW also determines when to employ existing memory-savings techniques, such as activation recomputation, that trade off extra computation for lower memory footprint.\r
\r
The two main features are a double-buffered weight update (2BW) and flush mechanisms ensure high throughput. PipeDream-2BW\r
splits models into stages over multiple workers, and each stage is replicated an equal number of times (with data-parallel updates across replicas of the same stage).  Such parallel pipelines work well for models where each layer is repeated a fixed number of times (e.g., [transformer](https://paperswithcode.com/method/transformer) models).""" ;
    skos:prefLabel "PipeDream-2BW" .

:PipeMare a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1910.05124v2> ;
    skos:definition "**PipeMare** is an asynchronous (bubble-free) pipeline parallel method for training large neural networks. It involves two main techniques: learning rate rescheduling and discrepancy correction." ;
    skos:prefLabel "PipeMare" .

:PipeTransformer a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2102.03161v2> ;
    skos:definition "**PipeTransformer** is a method for automated elastic pipelining for efficient distributed training of [Transformer](https://paperswithcode.com/method/transformer) models. In PipeTransformer, an adaptive on the fly freeze algorithm is used that can identify and freeze some layers gradually during training, as well as an elastic pipelining system that can dynamically allocate resources to train the remaining active layers. More specifically, PipeTransformer automatically excludes frozen layers from the pipeline, packs active layers into fewer GPUs, and forks more replicas to increase data-parallel width." ;
    skos:prefLabel "PipeTransformer" .

:PipelinedBackpropagation a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2003.11666v3> ;
    skos:definition "**Pipelined Backpropagation** is an asynchronous pipeline parallel training algorithm. It was first introduced by Petrowski et al (1993). It avoids fill and drain overhead by updating the weights without draining the pipeline first. This results in weight inconsistency, the use of different weights on the forward and backward passes for a given micro-batch. The weights used to produce a particular gradient may also have been updated when the gradient is applied, resulting in stale (or delayed) gradients. For these reasons PB resembles Asynchronous [SGD](https://paperswithcode.com/method/sgd) and is not equivalent to standard SGD. Finegrained pipelining increases the number of pipeline stages and hence increases the weight inconsistency and delay." ;
    skos:prefLabel "Pipelined Backpropagation" .

:Pix2Pix a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1611.07004v3> ;
    rdfs:seeAlso <https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/9e6fff7b7d5215a38be3cac074ca7087041bea0d/models/pix2pix_model.py#L6> ;
    skos:definition """**Pix2Pix** is a conditional image-to-image translation architecture that uses a conditional [GAN](https://paperswithcode.com/method/gan) objective combined with a reconstruction loss. The conditional GAN objective for observed images $x$, output images $y$ and the random noise vector $z$ is:\r
\r
$$ \\mathcal{L}\\_{cGAN}\\left(G, D\\right) =\\mathbb{E}\\_{x,y}\\left[\\log D\\left(x, y\\right)\\right]+\r
\\mathbb{E}\\_{x,z}\\left[log(1 − D\\left(x, G\\left(x, z\\right)\\right)\\right] $$\r
\r
We augment this with a reconstruction term:\r
\r
$$ \\mathcal{L}\\_{L1}\\left(G\\right) = \\mathbb{E}\\_{x,y,z}\\left[||y - G\\left(x, z\\right)||\\_{1}\\right] $$\r
\r
and we get the final objective as:\r
\r
$$ G^{*} = \\arg\\min\\_{G}\\max\\_{D}\\mathcal{L}\\_{cGAN}\\left(G, D\\right) + \\lambda\\mathcal{L}\\_{L1}\\left(G\\right) $$\r
\r
The architectures employed for the generator and discriminator closely follow [DCGAN](https://paperswithcode.com/method/dcgan), with a few modifications:\r
\r
- Concatenated skip connections are used to "shuttle" low-level information between the input and output, similar to a [U-Net](https://paperswithcode.com/method/u-net).\r
- The use of a [PatchGAN](https://paperswithcode.com/method/patchgan) discriminator that only penalizes structure at the scale of patches.""" ;
    skos:prefLabel "Pix2Pix" .

:PixLoc a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2103.09213v2> ;
    skos:definition "**PixLoc** is a scene-agnostic neural network that estimates an accurate 6-DoF pose from an image and a 3D model. It is based on the direct alignment of multiscale deep features, casting camera localization as metric learning. PixLoc learns strong data priors by end-to-end training from pixels to pose and exhibits exceptional generalization to new scenes by separating model parameters and scene geometry. As the CNN never sees 3D points, PixLoc can generalize to any 3D structure available. This includes sparse SfM point clouds, dense depth maps from stereo or RGBD sensors, meshes, Lidar scans, but also lines and other primitives." ;
    skos:prefLabel "PixLoc" .

:Pixel-BERT a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2004.00849v2> ;
    skos:definition """Pixel-BERT is a pre-trained model trained to align image pixels with text. The end-to-end framework includes a CNN-based visual encoder and cross-modal transformers for visual and language embedding learning.\r
This model has three parts: one fully convolutional neural network that takes pixels of an image as input, one word-level token embedding based on BERT, and a multimodal transformer for jointly learning visual and language embedding.\r
\r
For language, it uses other pretraining works to use Masked Language Modeling (MLM) to predict masked tokens with surrounding text and images. For vision, it uses the random pixel sampling mechanism that makes up for the challenge of predicting pixel-level features. This mechanism is also suitable for solving overfitting issues and improving the robustness of visual features. \r
\r
It applies Image-Text Matching (ITM) to classify whether an image and a sentence pair match for vision and language interaction. \r
\r
Image captioning is required to understand language and visual semantics for cross-modality tasks like VQA. Region-based visual features extracted from object detection models like Faster RCNN are used for better performance in the newer version of the model.""" ;
    skos:prefLabel "Pixel-BERT" .

:PixelCNN a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1601.06759v3> ;
    rdfs:seeAlso <https://github.com/openai/pixel-cnn> ;
    skos:definition "A **PixelCNN** is a generative model that uses autoregressive connections to model images pixel by pixel, decomposing the joint image distribution as a product of conditionals. PixelCNNs are much faster to train than [PixelRNNs](https://paperswithcode.com/method/pixelrnn) because convolutions are inherently easier to parallelize; given the vast number of pixels present in large image datasets this is an important advantage." ;
    skos:prefLabel "PixelCNN" .

:PixelRNN a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1601.06759v3> ;
    rdfs:seeAlso <https://github.com/carpedm20/pixel-rnn-tensorflow> ;
    skos:altLabel "Pixel Recurrent Neural Network" ;
    skos:definition "**PixelRNNs** are generative neural networks that sequentially predicts the pixels in an image along the two spatial dimensions. They model the discrete probability of the raw pixel values and encode the complete set of dependencies in the image. Variants include the Row [LSTM](https://paperswithcode.com/method/lstm) and the Diagonal [BiLSTM](https://paperswithcode.com/method/bilstm), that scale more easily to larger datasets. Pixel values are treated as discrete random variables by using a [softmax](https://paperswithcode.com/method/softmax) layer in the conditional distributions. Masked convolutions are employed to allow PixelRNNs to model full dependencies between the color channels." ;
    skos:prefLabel "PixelRNN" .

:PixelShuffle a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1609.05158v2> ;
    rdfs:seeAlso <https://github.com/pytorch/pytorch/blob/460970483d51006c61d504fb27985c60a3efbcd3/torch/nn/modules/pixelshuffle.py#L7> ;
    skos:definition """**PixelShuffle** is an operation used in super-resolution models to implement efficient sub-pixel convolutions with a stride of $1/r$. Specifically it rearranges elements in a tensor of shape $(\\*, C \\times r^2, H, W)$ to a tensor of shape $(\\*, C, H \\times r, W \\times r)$.\r
\r
Image Source: [Remote Sensing Single-Image Resolution Improvement Using A Deep Gradient-Aware Network with Image-Specific Enhancement](https://www.researchgate.net/figure/The-pixel-shuffle-layer-transforms-feature-maps-from-the-LR-domain-to-the-HR-image_fig3_339531308)""" ;
    skos:prefLabel "PixelShuffle" .

:PlaystyleDistance a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2110.00950v1> ;
    skos:definition "This method proposes first discretizing observations and calculating the action distribution distance under comparable cases (intersection states)." ;
    skos:prefLabel "Playstyle Distance" .

:PnP a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2109.07036v4> ;
    skos:definition "**PnP**, or **Poll and Pool**, is sampling module extension for [DETR](https://paperswithcode.com/method/detr)-type architectures that adaptively allocates its computation spatially to be more efficient. Concretely, the PnP module abstracts the image feature map into fine foreground object feature vectors and a small number of coarse background contextual feature vectors. The [transformer](https://paperswithcode.com/method/transformer) models information interaction within the fine-coarse feature space and translates the features into the detection result." ;
    skos:prefLabel "PnP" .

:PocketNet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2108.10710v2> ;
    skos:definition "**PocketNet** is a face recognition model family discovered through [neural architecture search](https://paperswithcode.com/methods/category/neural-architecture-search). The training is based on multi-step knowledge distillation." ;
    skos:prefLabel "PocketNet" .

:PoincaréEmbeddings a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1705.08039v2> ;
    skos:definition """**Poincaré Embeddings** learn hierarchical representations of symbolic data by embedding them into hyperbolic space -- or more precisely into an $n$-dimensional Poincaré ball. Due to the underlying hyperbolic geometry, this allows for learning of parsimonious representations of symbolic data by simultaneously capturing hierarchy and similarity. Embeddings are learnt based on\r
Riemannian optimization.""" ;
    skos:prefLabel "Poincaré Embeddings" .

:Point-GNN a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2003.01251v1> ;
    skos:definition "**Point-GNN** is a graph neural network for detecting objects from a LiDAR point cloud. It predicts the category and shape of the object that each vertex in the graph belongs to. In Point-GNN, there is an auto-registration mechanism to reduce translation variance, as well as a box merging and scoring operation to combine detections from multiple vertices accurately." ;
    skos:prefLabel "Point-GNN" .

:Point-wiseSpatialAttention a skos:Concept ;
    dcterms:source <http://openaccess.thecvf.com/content_ECCV_2018/html/Hengshuang_Zhao_PSANet_Point-wise_Spatial_ECCV_2018_paper.html> ;
    rdfs:seeAlso <https://github.com/hszhao/semseg/blob/7192f922b99468969cfd4535e3e35a838994b115/model/psanet.py#L9> ;
    skos:definition """**Point-wise Spatial Attention (PSA)** is a [semantic segmentation](https://paperswithcode.com/task/semantic-segmentation) module. The goal is capture contextual information, especially in the long range, by aggregating information. Through the PSA module, information aggregation is performed as a kind of information flow where we adaptively learn a pixel-wise global attention map for each position from two perspectives to aggregate contextual information over the entire feature map.\r
\r
The PSA module takes a spatial feature map $\\mathbf{X}$ as input. We denote the spatial size of $\\mathbf{X}$ as $H \\times W$. Through the two branches as illustrated, we generate pixel-wise global attention maps for each position in feature map $\\mathbf{X}$ through several convolutional layers.\r
\r
We aggregate input feature maps based on attention maps to generate new feature representations with the long-range contextual information incorporated, i.e., $\\mathbf{Z}\\_{c}$ from the ‘collect’ branch and $\\mathbf{Z}\\_{d}$ from the ‘distribute’ branch.\r
\r
We concatenate the new representations $\\mathbf{Z}\\_{c}$ and $\\mathbf{Z}\\_{d}$ and apply a convolutional layer with [batch normalization](https://paperswithcode.com/method/batch-normalization) and activation layers for dimension reduction and feature fusion. Then we concatenate the new global contextual feature with the local representation feature $\\mathbf{X}$. It is followed by one or several convolutional layers with batch normalization and activation layers to generate the final feature map for following subnetworks.""" ;
    skos:prefLabel "Point-wise Spatial Attention" .

:PointASNL a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2003.00492v3> ;
    skos:definition "**PointASNL** is a non-local neural network for point clouds processing It consists of two general modules: adaptive sampling (AS) module and local-Nonlocal (L-NL) module. The AS module first re-weights the neighbors around the initial sampled points from farthest point sampling (FPS), and then adaptively adjusts the sampled points beyond the entire point cloud. The AS module can not only benefit the feature learning of point clouds, but also ease the biased effect of outliers. The L-NL module capture the neighbor and long-range dependencies of the sampled point, and enables the learning process to be insensitive to noise." ;
    skos:prefLabel "PointASNL" .

:PointAugment a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2002.10876v2> ;
    skos:definition "**PointAugment** is a an auto-augmentation framework that automatically optimizes and augments point cloud samples to enrich the data diversity when we train a classification network. Different from existing auto-augmentation methods for 2D images, PointAugment is sample-aware and takes an adversarial learning strategy to jointly optimize an augmentor network and a classifier network, such that the augmentor can learn to produce augmented samples that best fit the classifier." ;
    skos:prefLabel "PointAugment" .

:PointNet a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1612.00593v2> ;
    skos:definition """**PointNet** provides a unified architecture for applications ranging from object classification, part segmentation, to scene semantic parsing. It directly takes point clouds as input and outputs either class labels for the entire input or per point segment/part labels for each point of the input.\r
\r
Source: [Qi et al.](https://arxiv.org/pdf/1612.00593v2.pdf)\r
\r
Image source: [Qi et al.](https://arxiv.org/pdf/1612.00593v2.pdf)""" ;
    skos:prefLabel "PointNet" .

:PointRend a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1912.08193v2> ;
    skos:definition """**PointRend** is a module for image segmentation tasks, such as instance and semantic segmentation, that attempts to treat segmentation as image rending problem to efficiently "render" high-quality label maps. It uses a subdivision strategy to adaptively select a non-uniform set of points at which to compute labels. PointRend can be incorporated into popular meta-architectures for both instance segmentation (e.g. [Mask R-CNN](https://paperswithcode.com/method/mask-r-cnn)) and semantic segmentation (e.g. [FCN](https://paperswithcode.com/method/fcn)). Its subdivision strategy efficiently computes high-resolution segmentation maps using an order of magnitude fewer floating-point operations than direct, dense computation.\r
\r
PointRend is a general module that admits many possible implementations. Viewed abstractly, a PointRend module accepts one or more typical CNN feature maps $f\\left(x\\_{i}, y\\_{i}\\right)$ that are defined over regular grids, and outputs high-resolution predictions $p\\left(x^{'}\\_{i}, y^{'}\\_{i}\\right)$ over a finer grid. Instead of making excessive predictions over all points on the output grid, PointRend makes predictions only on carefully selected points. To make these predictions, it extracts a point-wise feature representation for the selected points by interpolating $f$, and uses a small point head subnetwork to predict output labels from the point-wise features.""" ;
    skos:prefLabel "PointRend" .

:PointerNetwork a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1506.03134v2> ;
    skos:definition """**Pointer Networks** tackle problems where input and output data are sequential data, but can't be solved by seq2seq type models because discrete categories of output elements depend on the variable input size (and are not decided in advance).\r
\r
A Pointer Network learns the conditional  probability of an output sequence with elements that are discrete tokens corresponding to positions in an input sequence. They solve the problem of variable size output dictionaries using [additive attention](https://paperswithcode.com/method/additive-attention). But instead of using attention to blend hidden units of an encoder to a context vector at each decoder step, Pointer Networks use attention as a pointer to select a member of the input sequence as the output. \r
\r
Pointer-Nets can be used to learn approximate solutions to challenging geometric problems such as finding planar convex hulls, computing Delaunay triangulations, and the planar Travelling Salesman Problem.""" ;
    skos:prefLabel "Pointer Network" .

:PointerSentinel-LSTM a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1609.07843v1> ;
    rdfs:seeAlso <https://github.com/elanmart/psmm/blob/c414b49e1a49ee9ae7f7feb077db9fec77b2fb45/psmm/model.py#L9> ;
    skos:definition "The **Pointer Sentinel-LSTM mixture model** is a type of recurrent neural network that combines the advantages of standard [softmax](https://paperswithcode.com/method/softmax) classifiers with those of a pointer component for effective and efficient language modeling. Rather than relying on the RNN hidden state to decide when to use the pointer, the model allows the pointer component itself to decide when to use the softmax vocabulary through a sentinel." ;
    skos:prefLabel "Pointer Sentinel-LSTM" .

:PointwiseConvolution a skos:Concept ;
    skos:definition """**Pointwise Convolution** is a type of [convolution](https://paperswithcode.com/method/convolution) that uses a 1x1 kernel: a kernel that iterates through every single point. This kernel has a depth of however many channels the input image has. It can be used in conjunction with [depthwise convolutions](https://paperswithcode.com/method/depthwise-convolution) to produce an efficient class of convolutions known as [depthwise-separable convolutions](https://paperswithcode.com/method/depthwise-separable-convolution).\r
\r
Image Credit: [Chi-Feng Wang](https://towardsdatascience.com/a-basic-introduction-to-separable-convolutions-b99ec3102728)""" ;
    skos:prefLabel "Pointwise Convolution" .

:PolarMask a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2105.02184v1> ;
    skos:definition "**PolarMask** is an anchor-box free and single-shot instance segmentation method. Specifically, PolarMask takes an image as input and predicts the distance from a sampled positive location (ie a candidate object's center) with respect to the object's contour at each angle, and then assembles the predicted points to produce the final mask. There are several benefits to the system: (1) The polar representation unifies instance segmentation (masks) and object detection (bounding boxes) into a single framework (2) Two modules are designed (i.e. soft polar centerness and polar IoU loss) to sample high-quality center examples and optimize polar contour regression, making the performance of PolarMask does not depend on the bounding box prediction results and more efficient in training. (3) PolarMask is fully convolutional and can be embedded into most off-the-shelf detection methods." ;
    skos:prefLabel "PolarMask" .

:PolarNet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2003.14032v2> ;
    skos:definition "**PolarNet** is an improved grid representation for online, single-scan LiDAR point clouds. Instead of using common spherical or bird's-eye-view projection, the polar bird's-eye-view representation balances the points across grid cells in a polar coordinate system, indirectly aligning a segmentation network's attention with the long-tailed distribution of the points along the radial axis." ;
    skos:prefLabel "PolarNet" .

:PolicySimilarityMetric a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2101.05265v2> ;
    skos:definition "**Policy Similarity Metric**, or **PSM**, is a similarity metric for measuring behavioral similarity between states in reinforcement learning. It assigns high similarity to states for which the optimal policies in those states as well as in future states are similar. PSM is reward-agnostic, making it more robust for generalization compared to approaches that rely on reward information." ;
    skos:prefLabel "Policy Similarity Metric" .

:Poly a skos:Concept ;
    skos:altLabel "Polynomial" ;
    skos:definition "" ;
    skos:prefLabel "Poly" .

:PolyCAM a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2204.13359v2> ;
    skos:altLabel "Poly-CAM" ;
    skos:definition "" ;
    skos:prefLabel "PolyCAM" .

:PolyConv a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2110.07882v1> ;
    rdfs:seeAlso <https://github.com/myavartanoo/PolyNet_PyTorch/blob/main/src/model/polyconv.py> ;
    skos:altLabel "Polynomial Convolution" ;
    skos:definition "PolyConv learns continuous distributions as the convolutional filters to share the weights across different vertices of graphs or points of point clouds." ;
    skos:prefLabel "PolyConv" .

:Polya-GammaAugmentation a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1205.0310v3> ;
    skos:altLabel "Data augmentation using Polya-Gamma latent variables." ;
    skos:definition "This method applies Polya-Gamma latent variables as a way to obtain closed form expressions for full-conditionals of posterior distributions in sampling algorithms like MCMC." ;
    skos:prefLabel "Polya-Gamma Augmentation" .

:PolyakAveraging a skos:Concept ;
    skos:definition """**Polyak Averaging** is an optimization technique that sets final parameters to an average of (recent) parameters visited in the optimization trajectory. Specifically if in $t$ iterations we have parameters $\\theta\\_{1}, \\theta\\_{2}, \\dots, \\theta\\_{t}$, then Polyak Averaging suggests setting \r
\r
$$ \\theta\\_t =\\frac{1}{t}\\sum\\_{i}\\theta\\_{i} $$\r
\r
Image Credit: [Shubhendu Trivedi & Risi Kondor](https://ttic.uchicago.edu/~shubhendu/Pages/Files/Lecture6_flat.pdf)""" ;
    skos:prefLabel "Polyak Averaging" .

:PolynomialRateDecay a skos:Concept ;
    skos:definition "**Polynomial Rate Decay** is a learning rate schedule where we polynomially decay the learning rate." ;
    skos:prefLabel "Polynomial Rate Decay" .

:PonderNet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2107.05407v2> ;
    skos:definition "**PonderNet** is an adaptive computation method that learns to adapt the amount of computation based on the complexity of the problem at hand. PonderNet learns end-to-end the number of computational steps to achieve an effective compromise between training prediction accuracy, computational cost and generalization." ;
    skos:prefLabel "PonderNet" .

:PoolFormer a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2111.11418v3> ;
    skos:definition "PoolFormer is instantiated from MetaFormer by specifying the token mixer as extremely simple operator, pooling. PoolFormer is utilized as a tool to verify MetaFormer hypothesis \"MetaFormer is actually what you need\" (vs \"Attention is all you need\")." ;
    skos:prefLabel "PoolFormer" .

:PopulationBasedAugmentation a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1905.05393v1> ;
    rdfs:seeAlso <https://github.com/arcelien/pba> ;
    skos:definition """**Population Based Augmentation**, or **PBA**, is a data augmentation strategy (PBA), which generates nonstationary augmentation policy schedules instead of a fixed augmentation policy. In PBA we consider the augmentation policy search problem as a special case of hyperparameter schedule learning. It leverages [Population Based Training](https://paperswithcode.com/method/population-based-training) (PBT), a hyperparameter search algorithm which\r
optimizes the parameters of a network jointly with their hyperparameters to maximize performance. The output of PBT is not an optimal hyperparameter configuration but rather a trained model and schedule of hyperparameters. \r
\r
In PBA, we are only interested in the learned schedule and discard the child model result (similar to [AutoAugment](https://paperswithcode.com/method/autoaugment)). This learned augmentation schedule can then be used to improve the training of different (i.e., larger and costlier to train) models on the same dataset.\r
\r
PBT executes as follows. To start, a fixed population of models are randomly initialized and trained in parallel. At certain intervals, an “exploit-and-explore” procedure is applied to the worse performing population members, where the model clones the weights of a better performing model (i.e., exploitation) and then perturbs the hyperparameters of the cloned model to search in the hyperparameter space (i.e., exploration). Because the weights of the models are cloned and never reinitialized, the total computation required is the computation to train a single model times the population size.""" ;
    skos:prefLabel "Population Based Augmentation" .

:PopulationBasedTraining a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1711.09846v2> ;
    rdfs:seeAlso <https://github.com/elsheikh21/population-based-training-of-NNs/blob/8337041e31e1a1be9b1cc36bdf79fe7921881116/pbt_img_classification/main.py#L163> ;
    skos:definition """**Population Based Training**, or **PBT**, is an optimization method for finding parameters and hyperparameters, and extends upon parallel search methods and sequential optimisation methods.\r
It leverages information sharing across a population of concurrently running optimisation processes, and allows for online propagation/transfer of parameters and hyperparameters between members of the population based on their performance. Furthermore, unlike most other adaptation schemes, the method is capable of performing online adaptation of hyperparameters -- which can be particularly important in problems with highly non-stationary learning dynamics, such as reinforcement learning settings. PBT is decentralised and asynchronous, although it could also be executed semi-serially or with partial synchrony if there is a binding budget constraint.""" ;
    skos:prefLabel "Population Based Training" .

:PoseContrastiveLearning a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2202.14019v2> ;
    skos:altLabel "Self-Supervised Cross View Cross Subject Pose Contrastive Learning" ;
    skos:definition "Please enter a description about the method here" ;
    skos:prefLabel "Pose Contrastive Learning" .

:PoseDisentangling a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2202.14019v2> ;
    skos:altLabel "Pose-Appearance Disentangling" ;
    skos:definition "A method to disentangle pose from other factors in a scene." ;
    skos:prefLabel "Pose Disentangling" .

:Position-SensitiveRoIAlign a skos:Concept ;
    skos:definition "**Position-Sensitive RoIAlign** is a positive sensitive version of [RoIAlign](https://paperswithcode.com/method/roi-align) - i.e. it performs selective alignment, allowing for the learning of position-sensitive region of interest aligning." ;
    skos:prefLabel "Position-Sensitive RoIAlign" .

:Position-SensitiveRoIPooling a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1605.06409v2> ;
    rdfs:seeAlso <https://github.com/pytorch/vision/blob/971c3e45b96bc5aa5868c45cd40e4f3c3d90d126/torchvision/ops/ps_roi_pool.py#L10> ;
    skos:definition "**Position-Sensitive RoI Pooling layer** aggregates the outputs of the last convolutional layer and generates scores for each RoI. Unlike [RoI Pooling](https://paperswithcode.com/method/roi-pooling), PS RoI Pooling conducts selective pooling, and each of the $k$ × $k$ bin aggregates responses from only one score map out of the bank of $k$ × $k$ score maps. With end-to-end training, this RoI layer shepherds the last convolutional layer to learn specialized position-sensitive score maps." ;
    skos:prefLabel "Position-Sensitive RoI Pooling" .

:Position-WiseFeed-ForwardLayer a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1706.03762v7> ;
    skos:definition "**Position-Wise Feed-Forward Layer** is a type of [feedforward layer](https://www.paperswithcode.com/method/category/feedforwad-networks) consisting of two [dense layers](https://www.paperswithcode.com/method/dense-connections) that applies to the last dimension, which means the same dense layers are used for each position item in the sequence, so called position-wise." ;
    skos:prefLabel "Position-Wise Feed-Forward Layer" .

:PositionalEncodingGenerator a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2102.10882v3> ;
    skos:definition "**Positional Encoding Generator**, or **PEG**, is a module used in the [Conditional Position Encoding](https://paperswithcode.com/method/conditional-positional-encoding) position embeddings. It dynamically produce the positional encodings conditioned on the local neighborhood of an input token. To condition on the local neighbors, we first reshape the flattened input sequence $X \\in \\mathbb{R}^{B \\times N \\times C}$ of DeiT back to $X^{\\prime} \\in \\mathbb{R}^{B \\times H \\times W \\times C}$ in the 2 -D image space. Then, a function (denoted by $\\mathcal{F}$ in the Figure) is repeatedly applied to the local patch in $X^{\\prime}$ to produce the conditional positional encodings $E^{B \\times H \\times W \\times C} .$ PEG can be efficiently implemented with a 2-D convolution with kernel $k(k \\geq 3)$ and $\\frac{k-1}{2}$ zero paddings. Note that the zero paddings here are important to make the model be aware of the absolute positions, and $\\mathcal{F}$ can be of various forms such as separable convolutions and many others." ;
    skos:prefLabel "Positional Encoding Generator" .

:PowerSGD a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1905.13727v3> ;
    skos:definition "**PowerSGD** is a distributed optimization technique that computes a low-rank approximation of the gradient using a generalized power iteration (known as subspace iteration). The approximation is computationally light-weight, avoiding any prohibitively expensive Singular Value Decomposition. To improve the quality of the efficient approximation, the authors warm-start the power iteration by reusing the approximation from the previous optimization step." ;
    skos:prefLabel "PowerSGD" .

:Powerpropagation a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2110.00296v2> ;
    skos:definition "**Powerpropagation** is a weight-parameterisation for neural networks that leads to inherently sparse models. Exploiting the behaviour of gradient descent, it gives rise to weight updates exhibiting a “rich get richer” dynamic, leaving low-magnitude parameters largely unaffected by learning.In other words, parameters with larger magnitudes are allowed to adapt faster in order to represent the required features to solve the task, while smaller magnitude parameters are restricted, making it more likely that they will be irrelevant in representing the learned solution.  Models trained in this manner exhibit similar performance, but have a distribution with markedly higher density at zero, allowing more parameters to be pruned safely." ;
    skos:prefLabel "Powerpropagation" .

:PreciseRoIPooling a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1807.11590v1> ;
    rdfs:seeAlso <https://github.com/vacancy/PreciseRoIPooling/blob/070a7950db6a945e30e8e3296204a1e975f131e8/pytorch/prroi_pool/prroi_pool.py#L19> ;
    skos:definition """**Precise RoI Pooling**, or **PrRoI Pooling**, is a region of interest feature extractor that avoids any quantization of coordinates and has a continuous gradient on bounding box coordinates. Given the feature map $\\mathcal{F}$ before RoI/PrRoI Pooling (eg from Conv4 in [ResNet](https://paperswithcode.com/method/resnet)-50), let $w_{i,j}$ be the feature at one discrete location $(i,j)$ on the feature map. Using bilinear interpolation, the discrete feature map can be considered continuous at any continuous coordinates $(x,y)$:\r
\r
$$\r
f(x,y) = \\sum_{i,j}IC(x,y,i,j) \\times w_{i,j},\r
$$\r
\r
where $IC(x,y,i,j) = max(0,1-|x-i|)\\times max(0,1-|y-j|)$ is the interpolation coefficient. Then denote a bin of a RoI as $bin=\\{(x_1,y_1),(x_2,y_2)\\}$, where $(x_1,y_1)$ and $(x_2,y_2)$ are the continuous coordinates of the top-left and bottom-right points, respectively. We perform pooling (e.g. [average pooling](https://paperswithcode.com/method/average-pooling)) given $bin$ and feature map $\\mathcal{F}$ by computing a two-order integral:""" ;
    skos:prefLabel "Precise RoI Pooling" .

:PresGAN a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1910.04302v1> ;
    rdfs:seeAlso <https://github.com/adjidieng/PresGANs> ;
    skos:altLabel "Prescribed Generative Adversarial Network" ;
    skos:definition """**Prescribed GANs** add noise to the output of a density network and optimize an entropy-regularized adversarial loss. The added noise renders tractable approximations of the predictive log-likelihood and stabilizes the training procedure. The entropy regularizer encourages PresGANs to capture all the modes of the data distribution. Fitting PresGANs involves computing the intractable gradients of the [entropy regularization](https://paperswithcode.com/method/entropy-regularization) term; PresGANs sidestep this intractability using\r
unbiased stochastic estimates.""" ;
    skos:prefLabel "PresGAN" .

:Primer a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2109.08668v2> ;
    skos:definition "**Primer** is a [Transformer](https://paperswithcode.com/methods/category/transformers)-based architecture that improves upon the [Transformer](https://paperswithcode.com/method/transformer) architecture with two improvements found through [neural architecture search](https://paperswithcode.com/methods/category/neural-architecture-search): [squared RELU activations](https://paperswithcode.com/method/squared-relu) in the feedforward block, and [depthwise convolutions]() added to the attention multi-head projections: resulting in a new module called [Multi-DConv-Head-Attention](https://paperswithcode.com/method/multi-dconv-head-attention)." ;
    skos:prefLabel "Primer" .

:PrioritizedExperienceReplay a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1511.05952v4> ;
    skos:definition """**Prioritized Experience Replay** is a type of [experience replay](https://paperswithcode.com/method/experience-replay) in reinforcement learning where we more frequently replay transitions with high expected learning progress, as measured by the magnitude of their temporal-difference (TD) error. This prioritization can lead to a loss of diversity, which is alleviated with stochastic prioritization, and introduce bias, which can be corrected with importance sampling.\r
\r
The stochastic sampling method interpolates between pure greedy prioritization and uniform random sampling. The probability of being sampled is ensured to be monotonic in a transition's priority,  while guaranteeing a non-zero probability even for the lowest-priority transition. Concretely, define the probability of sampling transition $i$ as\r
\r
$$P(i) = \\frac{p_i^{\\alpha}}{\\sum_k p_k^{\\alpha}}$$\r
\r
where $p_i > 0$ is the priority of transition $i$. The exponent $\\alpha$ determines how much prioritization is used, with $\\alpha=0$ corresponding to the uniform case.\r
\r
Prioritized replay introduces bias because it changes this distribution in an uncontrolled fashion, and therefore changes the solution that the estimates will converge to. We can correct this bias by using\r
importance-sampling (IS) weights:\r
\r
$$ w\\_{i} = \\left(\\frac{1}{N}\\cdot\\frac{1}{P\\left(i\\right)}\\right)^{\\beta} $$\r
\r
that fully compensates for the non-uniform probabilities $P\\left(i\\right)$ if $\\beta = 1$. These weights can be folded into the [Q-learning](https://paperswithcode.com/method/q-learning) update by using $w\\_{i}\\delta\\_{i}$ instead of $\\delta\\_{i}$ - weighted IS rather than ordinary IS. For stability reasons, we always normalize weights by $1/\\max\\_{i}w\\_{i}$ so\r
that they only scale the update downwards.\r
\r
The two types of prioritization are proportional based, where $p\\_{i} = |\\delta\\_{i}| + \\epsilon$ and rank-based, where $p\\_{i} = \\frac{1}{\\text{rank}\\left(i\\right)}$, the latter where $\\text{rank}\\left(i\\right)$ is the rank of transition $i$ when the replay memory is sorted according to |$\\delta\\_{i}$|, For proportional based, hyperparameters used were $\\alpha = 0.7$, $\\beta\\_{0} = 0.5$. For the rank-based variant, hyperparameters used were $\\alpha = 0.6$, $\\beta\\_{0} = 0.4$.""" ;
    skos:prefLabel "Prioritized Experience Replay" .

:PrioritizedSweeping a skos:Concept ;
    skos:definition """**Prioritized Sweeping** is a reinforcement learning technique for model-based algorithms that prioritizes updates according to a measure of urgency, and performs these updates first. A queue is maintained of every state-action pair whose estimated value would change nontrivially if updated, prioritized by the size of the change. When the top pair in the queue is updated, the effect on each of its predecessor pairs is computed. If the effect is greater than some threshold, then the pair is inserted in the queue with the new priority.\r
\r
Source: Sutton and Barto, Reinforcement Learning, 2nd Edition""" ;
    skos:prefLabel "Prioritized Sweeping" .

:PrivacyNet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2001.00561v3> ;
    skos:definition "**PrivacyNet** is a [GAN](https://paperswithcode.com/method/gan)-based semi-adversarial network (SAN) that modifies an input face image such that it can be used by a face matcher for matching purposes but cannot be reliably used by an attribute classifier. PrivacyNet allows a person to choose specific attributes that have to be obfuscated in the input face images (e.g., age and race), while allowing for other types of attributes to be extracted (e.g., gender)." ;
    skos:prefLabel "PrivacyNet" .

:ProCAN a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2010.15417v3> ;
    skos:altLabel "Progressive Growing Channel Attentive Non-Local Network" ;
    skos:definition "Lung cancer classification in screening computed tomography (CT) scans is one of the most crucial tasks for early detection of this disease. Many lives can be saved if we are able to accurately classify malignant/cancerous lung nodules. Consequently, several deep learning based models have been proposed recently to classify lung nodules as malignant or benign. Nevertheless, the large variation in the size and heterogeneous appearance of the nodules makes this task an extremely challenging one. We propose a new Progressive Growing Channel Attentive Non-Local (ProCAN) network for lung nodule classification. The proposed method addresses this challenge from three different aspects. First, we enrich the Non-Local network by adding channel-wise attention capability to it. Second, we apply Curriculum Learning principles, whereby we first train our model on easy examples before hard ones. Third, as the classification task gets harder during the Curriculum learning, our model is progressively grown to increase its capability of handling the task at hand. We examined our proposed method on two different public datasets and compared its performance with state-of-the-art methods in the literature. The results show that the ProCAN model outperforms state-of-the-art methods and achieves an AUC of 98.05% and an accuracy of 95.28% on the LIDC-IDRI dataset. Moreover, we conducted extensive ablation studies to analyze the contribution and effects of each new component of our proposed method." ;
    skos:prefLabel "ProCAN" .

:ProGAN a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1710.10196v3> ;
    rdfs:seeAlso <https://github.com/tkarras/progressive_growing_of_gans> ;
    skos:altLabel "Progressively Growing GAN" ;
    skos:definition "**ProGAN**, or **Progressively Growing GAN**, is a generative adversarial network that utilises a progressively growing training approach. The idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses." ;
    skos:prefLabel "ProGAN" .

:ProbabilisticAnchorAssignment a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2007.08103v2> ;
    skos:definition """**Probabilistic anchor assignment (PAA)** adaptively separates a set of anchors into positive and negative samples for a GT box according to the learning status of the model associated with it. To do so we first define a score of a detected bounding box that reflects both the classification and localization qualities. We then identify the connection between this score and the training objectives and represent the score as the combination of two loss objectives. Based on this scoring scheme, we calculate the scores of individual anchors that reflect how the model finds useful cues to detect a target object in each anchor. With these anchor scores, we aim to find a probability distribution of two modalities that best represents the scores as positive or negative samples as in the Figure. \r
\r
Under the found probability distribution, anchors with probabilities from the positive component are high are selected as positive samples. This transforms the anchor assignment problem to a maximum likelihood estimation for a probability distribution where the parameters of the distribution is determined by anchor scores. Based on the assumption that anchor scores calculated by the model are samples drawn from a probability distribution, it is expected that the model can infer the sample separation in a probabilistic way, leading to easier training of the model compared to other non-probabilistic assignments. Moreover, since positive samples are adaptively selected based on the anchor score distribution, it does not require a pre-defined number of positive samples nor an IoU threshold.""" ;
    skos:prefLabel "Probabilistic Anchor Assignment" .

:Procrustes a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1911.11431v2> ;
    skos:definition "Procrustes" ;
    skos:prefLabel "Procrustes" .

:ProjectionDiscriminator a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1802.05637v2> ;
    rdfs:seeAlso <https://github.com/pfnet-research/sngan_projection/blob/3ff93ef5f294c3b48c6b2cbc9b0033ee7a103621/dis_models/snresnet_32.py#L8> ;
    skos:definition """A **Projection Discriminator** is a type of discriminator for generative adversarial networks. It is motivated by a probabilistic model in which the distribution of the conditional variable $\\textbf{y}$ given $\\textbf{x}$ is discrete or uni-modal continuous distributions.\r
\r
If we look at the original solution for the loss function $\\mathcal{L}\\_{D}$ in the vanilla GANs, we can decompose it into the sum of two log-likelihood ratios:\r
\r
$$ f^{*}\\left(\\mathbf{x}, \\mathbf{y}\\right) = \\log\\frac{q\\left(\\mathbf{x}\\mid{\\mathbf{y}}\\right)q\\left(\\mathbf{y}\\right)}{p\\left(\\mathbf{x}\\mid{\\mathbf{y}}\\right)p\\left(\\mathbf{y}\\right)} = \\log\\frac{q\\left(\\mathbf{y}\\mid{\\mathbf{x}}\\right)}{p\\left(\\mathbf{y}\\mid{\\mathbf{x}}\\right)} + \\log\\frac{q\\left(\\mathbf{x}\\right)}{p\\left(\\mathbf{x}\\right)}  = r\\left(\\mathbf{y\\mid{x}}\\right) + r\\left(\\mathbf{x}\\right) $$\r
\r
We can model the log likelihood ratio $r\\left(\\mathbf{y\\mid{x}}\\right)$ and  $r\\left(\\mathbf{x}\\right)$ by some parametric functions $f\\_{1}$ and $f\\_{2}$ respectively. If we make a standing assumption that $p\\left(y\\mid{x}\\right)$ and $q\\left(y\\mid{x}\\right)$ are simple distributions like those that are Gaussian or discrete log linear on the feature space, then the parametrization of the following form becomes natural:\r
\r
$$ f\\left(\\mathbf{x}, \\mathbf{y}; \\theta\\right) = f\\_{1}\\left(\\mathbf{x}, \\mathbf{y}; \\theta\\right) + f\\_{2}\\left(\\mathbf{x}; \\theta\\right) = \\mathbf{y}^{T}V\\phi\\left(\\mathbf{x}; \\theta\\_{\\phi}\\right) + \\psi\\left(\\phi(\\mathbf{x}; \\theta\\_{\\phi}); \\theta\\_{\\psi}\\right) $$\r
\r
where $V$ is the embedding matrix of $y$, $\\phi\\left(·, \\theta\\_{\\phi}\\right)$ is a vector output function of $x$, and $\\psi\\left(·, \\theta\\_{\\psi}\\right)$ is a scalar function of the same $\\phi\\left(\\mathbf{x}; \\theta\\_{\\phi}\\right)$ that appears in $f\\_{1}$. The learned parameters $\\theta = ${$V, \\theta\\_{\\phi}, \\theta\\_{\\psi}$} are trained to optimize the adversarial loss. This model of the discriminator is the projection.""" ;
    skos:prefLabel "Projection Discriminator" .

:ProphetNet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2001.04063v3> ;
    skos:definition "**ProphetNet** is a sequence-to-sequence pre-training model that introduces a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. Instead of optimizing one-step-ahead prediction in the traditional sequence-to-sequence model, the ProphetNet is optimized by $n$-step ahead prediction that predicts the next $n$ tokens simultaneously based on previous context tokens at each time step. The future n-gram prediction explicitly encourages the model to plan for the future tokens and further help predict multiple future tokens." ;
    skos:prefLabel "ProphetNet" .

:ProtagonistAntagonistInducedRegretEnvironmentDesign a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2012.02096v2> ;
    skos:definition "**Protagonist Antagonist Induced Regret Environment Design**, or **PAIRED**, is an adversarial method for approximate minimax regret to generate environments for reinforcement learning. It introduces an antagonist which is allied with the environment generating adversary. The primary agent we are trying to train is the protagonist. The environment adversary’s goal is to design environments in which the antagonist achieves high reward and the protagonist receives low reward. If the adversary generates unsolvable environments, the antagonist and protagonist would perform the same and the adversary would get a score of zero, but if the adversary finds environments the antagonist solves and the protagonist does not solve, the adversary achieves a positive score. Thus, the environment adversary is incentivized to create challenging but feasible environments, in which the antagonist can outperform the protagonist. Moreover, as the protagonist learns to solves the simple environments, the antagonist must generate more complex environments to make the protagonist fail, increasing the complexity of the generated tasks and leading to automatic curriculum generation." ;
    skos:prefLabel "Protagonist Antagonist Induced Regret Environment Design" .

:ProximityRegularization a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1812.06127v5> ;
    skos:definition "" ;
    skos:prefLabel "Proximity Regularization" .

:ProxyAnchorLoss a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2003.13911v1> ;
    skos:altLabel "Proxy Anchor Loss for Deep Metric Learning" ;
    skos:definition "" ;
    skos:prefLabel "ProxyAnchorLoss" .

:ProxyOptimization a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2104.11231v1> ;
    skos:altLabel "Proxy Optimization for initial Proxies in Proxy Anchor Loss" ;
    skos:definition "" ;
    skos:prefLabel "ProxyOptimization" .

:ProxylessNAS a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1812.00332v2> ;
    rdfs:seeAlso <https://github.com/mit-han-lab/proxylessnas/tree/master/search> ;
    skos:definition """**ProxylessNAS** directly learns neural network architectures on the target task and target hardware without any proxy task. Additional contributions include:\r
\r
- Using a new path-level pruning perspective for [neural architecture search](https://paperswithcode.com/method/neural-architecture-search), showing a close connection between NAS and model compression. Memory consumption is saved by one order of magnitude by using path-level binarization.\r
- Using a novel gradient-based approach (latency regularization loss) for handling hardware objectives (e.g. latency). Given different hardware platforms: CPU/GPU/Mobile, ProxylessNAS enables hardware-aware neural network specialization that’s exactly optimized for the target hardware.""" ;
    skos:prefLabel "ProxylessNAS" .

:ProxylessNet-CPU a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1812.00332v2> ;
    rdfs:seeAlso <https://github.com/mit-han-lab/proxylessnas/blob/0491e8e8678f5b3dcd3a856f3c842586d3dbf8a3/proxyless_nas/nas_modules.py#L54> ;
    skos:definition "**ProxylessNet-CPU** is an image model learnt with the [ProxylessNAS](https://paperswithcode.com/method/proxylessnas) [neural architecture search](https://paperswithcode.com/method/neural-architecture-search) algorithm that is optimized for CPU devices. It uses inverted residual blocks (MBConvs) from [MobileNetV2](https://paperswithcode.com/method/mobilenetv2) as its basic building block." ;
    skos:prefLabel "ProxylessNet-CPU" .

:ProxylessNet-GPU a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1812.00332v2> ;
    rdfs:seeAlso <https://github.com/mit-han-lab/proxylessnas/blob/0491e8e8678f5b3dcd3a856f3c842586d3dbf8a3/proxyless_nas/nas_modules.py#L54> ;
    skos:definition "**ProxylessNet-GPU** is a convolutional neural network architecture learnt with the [ProxylessNAS](https://paperswithcode.com/method/proxylessnas) [neural architecture search](https://paperswithcode.com/method/neural-architecture-search) algorithm that is optimized for GPU devices. It uses inverted residual blocks (MBConvs) from [MobileNetV2](https://paperswithcode.com/method/mobilenetv2) as its basic building block." ;
    skos:prefLabel "ProxylessNet-GPU" .

:ProxylessNet-Mobile a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1812.00332v2> ;
    rdfs:seeAlso <https://github.com/mit-han-lab/proxylessnas/blob/0491e8e8678f5b3dcd3a856f3c842586d3dbf8a3/proxyless_nas/nas_modules.py#L54> ;
    skos:definition "**ProxylessNet-Mobile** is a convolutional neural architecture learnt with the [ProxylessNAS](https://paperswithcode.com/method/proxylessnas) [neural architecture search](https://paperswithcode.com/method/neural-architecture-search) algorithm that is optimized for mobile devices. It uses inverted residual blocks (MBConvs) from [MobileNetV2](https://paperswithcode.com/method/mobilenetv2) as its basic building block." ;
    skos:prefLabel "ProxylessNet-Mobile" .

:Pruning a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1608.08710v3> ;
    skos:definition "" ;
    skos:prefLabel "Pruning" .

:PyTorchDDP a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2006.15704v1> ;
    skos:definition "**PyTorch DDP** (Distributed Data Parallel) is a distributed data parallel implementation for PyTorch. To guarantee mathematical equivalence, all replicas start from the same initial values for model parameters and synchronize gradients to keep parameters consistent across training iterations. To minimize the intrusiveness, the implementation exposes the same forward API as the user model, allowing applications to seamlessly replace subsequent occurrences of a user model with the distributed data parallel model object with no additional code changes. Several techniques are integrated into the design to deliver high-performance training, including bucketing gradients, overlapping communication with computation, and skipping synchronization." ;
    skos:prefLabel "PyTorch DDP" .

:PyramidNet a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1610.02915v4> ;
    rdfs:seeAlso <https://github.com/osmr/imgclsmob/blob/c03fa67de3c9e454e9b6d35fe9cbb6b15c28fda7/pytorch/pytorchcv/models/pyramidnet.py#L178> ;
    skos:definition "A **PyramidNet** is a type of convolutional network where the key idea is to concentrate on the feature map dimension by increasing it gradually instead of by increasing it sharply at each residual unit with downsampling. In addition, the network architecture works as a mixture of both plain and residual networks by using zero-padded identity-mapping shortcut connections when increasing the feature map dimension." ;
    skos:prefLabel "PyramidNet" .

:PyramidPoolingModule a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1612.01105v2> ;
    skos:definition """A **Pyramid Pooling Module** is a module for semantic segmentation which acts as an effective global contextual prior. The motivation is that the problem of using a convolutional network like a [ResNet](https://paperswithcode.com/method/resnet) is that, while the receptive field is already larger than the input image, the empirical receptive field is much smaller than the theoretical one especially on high-level layers. This makes many networks not sufficiently incorporate the momentous global scenery prior. \r
\r
The PPM is an effective global prior representation that addresses this problem. It contains information with different scales and varying among different sub-regions. Using our 4-level pyramid, the pooling kernels cover the whole, half of, and small portions of the image. They are fused as the global prior. Then we concatenate the prior with the original feature map in the final part.""" ;
    skos:prefLabel "Pyramid Pooling Module" .

:PyramidalBottleneckResidualUnit a skos:Concept ;
    rdfs:seeAlso <https://github.com/dyhan0920/PyramidNet-PyTorch/blob/5a0b32f43d79024a0d9cd2d1851f07e6355daea2/PyramidNet.py#L57> ;
    skos:definition "A **Pyramidal Bottleneck Residual Unit** is a type of residual unit where the number of channels gradually increases as a function of the depth at which the layer occurs, which is similar to a pyramid structure of which the shape gradually widens from the top downwards. It also consists of a bottleneck using 1x1 convolutions. It was introduced as part of the [PyramidNet](https://paperswithcode.com/method/pyramidnet) architecture." ;
    skos:prefLabel "Pyramidal Bottleneck Residual Unit" .

:PyramidalResidualUnit a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1610.02915v4> ;
    rdfs:seeAlso <https://github.com/dyhan0920/PyramidNet-PyTorch/blob/5a0b32f43d79024a0d9cd2d1851f07e6355daea2/PyramidNet.py#L14> ;
    skos:definition "A **Pyramidal Residual Unit** is a type of residual unit where the number of channels gradually increases as a function of the depth at which the layer occurs, which is similar to a pyramid structure of which the shape gradually widens from the top downwards. It was introduced as part of the [PyramidNet](https://paperswithcode.com/method/pyramidnet) architecture." ;
    skos:prefLabel "Pyramidal Residual Unit" .

:Pythia a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2304.01373v2> ;
    skos:definition "**Pythia** is a suite of decoder-only autoregressive language models all trained on public data seen in the exact same order and ranging in size from 70M to 12B parameters. The model architecture and hyperparameters largely follow GPT-3, with a few notable deviations based on recent advances in best practices for large scale language modeling." ;
    skos:prefLabel "Pythia" .

:QHAdam a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1810.06801v4> ;
    rdfs:seeAlso <https://github.com/jettify/pytorch-optimizer/blob/155246597d66dd774156599be0f07a8c6f7758aa/torch_optimizer/qhadam.py#L10> ;
    skos:definition """The **Quasi-Hyperbolic Momentum Algorithm (QHM)** is a simple alteration of [momentum SGD](https://paperswithcode.com/method/sgd-with-momentum), averaging a plain [SGD](https://paperswithcode.com/method/sgd) step with a momentum step. **QHAdam** is a QH augmented version of [Adam](https://paperswithcode.com/method/adam), where we replace both of Adam's moment estimators with quasi-hyperbolic terms. QHAdam decouples the momentum term from the current gradient when updating the weights, and decouples the mean squared gradients term from the current squared gradient when updating the weights. \r
\r
In essence, it is a weighted average of the momentum and plain SGD, weighting the current gradient with an immediate discount factor $v\\_{1}$ divided by a weighted average of the mean squared gradients and the current squared gradient, weighting the current squared gradient with an immediate discount factor $v\\_{2}$. \r
\r
$$ \\theta\\_{t+1, i} = \\theta\\_{t, i} - \\eta\\left[\\frac{\\left(1-v\\_{1}\\right)\\cdot{g\\_{t}} + v\\_{1}\\cdot\\hat{m}\\_{t}}{\\sqrt{\\left(1-v\\_{2}\\right)g^{2}\\_{t} + v\\_{2}\\cdot{\\hat{v}\\_{t}}} + \\epsilon}\\right], \\forall{t} $$\r
\r
It is recommended to set $v\\_{2} = 1$ and $\\beta\\_{2}$ same as in Adam.""" ;
    skos:prefLabel "QHAdam" .

:QHM a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1810.06801v4> ;
    rdfs:seeAlso <https://github.com/jettify/pytorch-optimizer/blob/155246597d66dd774156599be0f07a8c6f7758aa/torch_optimizer/qhm.py#L9> ;
    skos:definition """**Quasi-Hyperbolic Momentum (QHM)** is a stochastic optimization technique that alters [momentum SGD](https://paperswithcode.com/method/sgd-with-momentum) with a momentum step, averaging an [SGD](https://paperswithcode.com/method/sgd) step with a momentum step:\r
\r
$$ g\\_{t+1} = \\beta{g\\_{t}} + \\left(1-\\beta\\right)\\cdot{\\nabla}\\hat{L}\\_{t}\\left(\\theta\\_{t}\\right) $$\r
$$ \\theta\\_{t+1} = \\theta\\_{t} - \\alpha\\left[\\left(1-v\\right)\\cdot\\nabla\\hat{L}\\_{t}\\left(\\theta\\_{t}\\right) + v\\cdot{g\\_{t+1}}\\right]$$\r
\r
The authors suggest a rule of thumb of $v = 0.7$ and $\\beta = 0.999$.""" ;
    skos:prefLabel "QHM" .

:QPT a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2210.11889v3> ;
    skos:altLabel "Quantum Process Tomography" ;
    skos:definition "" ;
    skos:prefLabel "QPT" .

:QRNN a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1611.01576v2> ;
    skos:altLabel "Quasi-Recurrent Neural Network" ;
    skos:definition """A **QRNN**, or **Quasi-Recurrent Neural Network**, is a type of recurrent neural network that alternates convolutional layers, which apply in parallel across timesteps, and a minimalist recurrent pooling function that applies in parallel across channels. Due to their increased parallelism, they can be up to 16 times faster at train and test time than [LSTMs](https://paperswithcode.com/method/lstm).\r
\r
Given an input sequence $\\mathbf{X} \\in \\mathbb{R}^{T\\times{n}}$ of $T$ n-dimensional vectors $\\mathbf{x}\\_{1}, \\dots, \\mathbf{x}\\_{T}$, the convolutional subcomponent of a QRNN performs convolutions in the timestep dimension with a bank of $m$ filters, producing a sequence $\\mathbf{Z} \\in \\mathbb{R}^{T\\times{m}}$ of m-dimensional candidate vectors $\\mathbf{z}\\_{t}$. Masked convolutions are used so filters can not access information from future timesteps (implementing with left padding).\r
\r
Additional convolutions are applied with separate filter banks to obtain sequences of vectors for the\r
elementwise gates that are needed for the pooling function. While the candidate vectors are passed\r
through a $\\tanh$ nonlinearity, the gates use an elementwise sigmoid. If the pooling function requires a\r
forget gate $f\\_{t}$ and an output gate $o\\_{t}$ at each timestep, the full set of computations in the convolutional component is then:\r
\r
$$ \\mathbf{Z} = \\tanh\\left(\\mathbf{W}\\_{z} ∗ \\mathbf{X}\\right) $$\r
$$ \\mathbf{F} = \\sigma\\left(\\mathbf{W}\\_{f} ∗ \\mathbf{X}\\right) $$\r
$$ \\mathbf{O} = \\sigma\\left(\\mathbf{W}\\_{o} ∗ \\mathbf{X}\\right) $$\r
\r
where $\\mathbf{W}\\_{z}$, $\\mathbf{W}\\_{f}$, and $\\mathbf{W}\\_{o}$, each in $\\mathbb{R}^{k×n×m}$, are the convolutional filter banks and ∗ denotes a [masked convolution](https://paperswithcode.com/method/masked-convolution) along the timestep dimension.  Dynamic [average pooling](https://paperswithcode.com/method/average-pooling) by Balduzzi & Ghifary (2016) is used, which uses only a forget gate:\r
\r
$$ \\mathbf{h}\\_{t} = \\mathbf{f}\\_{t} \\odot{\\mathbf{h}\\_{t−1}} + \\left(1 − \\mathbf{f}\\_{t}\\right) \\odot{\\mathbf{z}\\_{t}} $$ \r
\r
Which is denoted f-pooling. The function may also include an output gate:\r
\r
$$ \\mathbf{c}\\_{t} = \\mathbf{f}\\_{t} \\odot{\\mathbf{c}\\_{t−1}} + \\left(1 − \\mathbf{f}\\_{t}\\right) \\odot{\\mathbf{z}\\_{t}} $$ \r
\r
$$ \\mathbf{h}\\_{t} = \\mathbf{o}\\_{t} \\odot{\\mathbf{c}\\_{t}} $$\r
\r
Which is denoted fo-pooling. Or the recurrence relation may include an independent input and forget gate:\r
\r
$$ \\mathbf{c}\\_{t} = \\mathbf{f}\\_{t} \\odot{\\mathbf{c}\\_{t−1}} + \\mathbf{i}\\_{t}\\odot{\\mathbf{z}\\_{t}} $$ \r
\r
$$ \\mathbf{h}\\_{t} = \\mathbf{o}\\_{t} \\odot{\\mathbf{c}\\_{t}} $$\r
\r
Which is denoted ifo-pooling. In each case $h$ or $c$ is initialized to zero. The recurrent part sof these functions must be calculated for each timestep in the sequence, but parallelism along feature dimensions means evaluating them even over long sequences requires a negligible amount of computation time.\r
\r
A single QRNN layer thus performs an input-dependent pooling, followed by a gated linear combination of convolutional features. As with convolutional neural networks, two or more QRNN layers should be stacked to create a model with the capacity to approximate more complex functions.""" ;
    skos:prefLabel "QRNN" .

:QuantTree a skos:Concept ;
    dcterms:source <https://icml.cc/Conferences/2018/Schedule?showEvent=2268> ;
    skos:altLabel "QuantTree histograms" ;
    skos:definition "Given a training set drawn from an unknown $d$-variate probability distribution, QuantTree constructs a histogram by recursively splitting $\\mathbb{R}^d$. The splits are defined by a stochastic process so that each bin contains a certain proportion of the training set. These histograms can be used to define test statistics (e.g., the Pearson statistic) to tell whether a batch of data is drawn from $\\phi_0$ or not. The most crucial property of QuantTree is that the distribution of any statistic based on QuantTree histograms is independent of $\\phi_0$, thus enabling nonparametric statistical testing." ;
    skos:prefLabel "QuantTree" .

:QuickAttention a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2209.00729v1> ;
    skos:definition """\\begin{equation}\r
QA\\left( x \\right) = \\sigma\\left( f\\left( x \\right)^{1x1} \\right) + x \\end{equation}\r
\r
Quick Attention takes in the feature map as an input WxHxC (Width x Height x Channels) and creates two instances of the input feature map then it performs the 1x1xC convolution on the first instance and calculates the sigmoid activations after that it is added with the second instance to generate the final attention map as output which is of same dimensions as of input.""" ;
    skos:prefLabel "Quick Attention" .

<http://w3id.org/mlso/vocab/ml_algorithm/R(2+1)D> a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1711.11248v3> ;
    rdfs:seeAlso <https://github.com/pytorch/vision/blob/9e474c3c46c0871838c021093c67a9c7eb1863ea/torchvision/models/video/resnet.py#L324> ;
    skos:definition "A **R(2+1)D** convolutional neural network is a network for action recognition that employs [R(2+1)D](https://paperswithcode.com/method/2-1-d-convolution) convolutions in a [ResNet](https://paperswithcode.com/method/resnet) inspired architecture. The use of these convolutions over regular [3D Convolutions](https://paperswithcode.com/method/3d-convolution) reduces computational complexity, prevents overfitting, and introduces more non-linearities that allow for a better functional relationship to be modeled." ;
    skos:prefLabel "R(2+1)D" .

:R-CNN a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1311.2524v5> ;
    skos:definition "**R-CNN**, or **Regions with CNN Features**, is an object detection model that uses high-capacity CNNs to bottom-up region proposals in order to localize and segment objects. It uses [selective search](https://paperswithcode.com/method/selective-search) to identify a number of bounding-box object region candidates (“regions of interest”), and then extracts features from each region independently for classification." ;
    skos:prefLabel "R-CNN" .

:R-FCN a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1605.06409v2> ;
    rdfs:seeAlso <https://github.com/facebookresearch/Detectron/blob/8170b25b425967f8f1c7d715bea3c5b8d9536cd8/detectron/modeling/rfcn_heads.py> ;
    skos:altLabel "Region-based Fully Convolutional Network" ;
    skos:definition """**Region-based Fully Convolutional Networks**, or **R-FCNs**, are a type of region-based object detector. In contrast to previous region-based object detectors such as Fast/[Faster R-CNN](https://paperswithcode.com/method/faster-r-cnn) that apply a costly per-region subnetwork hundreds of times, R-FCN is fully convolutional with almost all computation shared on the entire image.\r
\r
To achieve this, R-FCN utilises position-sensitive score maps to address a dilemma between translation-invariance in image classification and translation-variance in object detection.""" ;
    skos:prefLabel "R-FCN" .

:R-Mix a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2212.04875v3> ;
    skos:altLabel "Random Mix-up" ;
    skos:definition "R-Mix (Random Mix-up) is a Mix-up family Data Augmentation method. It combines random Mix-up with Saliency-guided mix-up, producing a procedure that is fast and performant, while reserving good characteristics of Saliency-guided Mix-up such as low Expected Calibration Error and high Weakly-supervised Object Localization accuracy." ;
    skos:prefLabel "R-Mix" .

:R1Regularization a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1801.04406v4> ;
    rdfs:seeAlso <https://github.com/ChristophReich1996/Dirac-GAN/blob/decb8283d919640057c50ff5a1ba01b93ed86332/dirac_gan/loss.py#L292> ;
    skos:definition """**R_INLINE_MATH_1 Regularization** is a regularization technique and gradient penalty for training [generative adversarial networks](https://paperswithcode.com/methods/category/generative-adversarial-networks). It penalizes the discriminator from deviating from the Nash Equilibrium via penalizing the gradient on real data alone: when the generator distribution produces the true data distribution and the discriminator is equal to 0 on the data manifold, the gradient penalty ensures that the discriminator cannot create a non-zero gradient orthogonal to the data manifold without suffering a loss in the [GAN](https://paperswithcode.com/method/gan) game.\r
\r
This leads to the following regularization term:\r
\r
$$ R\\_{1}\\left(\\psi\\right) = \\frac{\\gamma}{2}E\\_{p\\_{D}\\left(x\\right)}\\left[||\\nabla{D\\_{\\psi}\\left(x\\right)}||^{2}\\right] $$""" ;
    skos:prefLabel "R1 Regularization" .

:R2D2 a skos:Concept ;
    dcterms:source <https://openreview.net/forum?id=r1lyTjAqYX> ;
    skos:altLabel "Recurrent Replay Distributed DQN" ;
    skos:definition """Building on the recent successes of distributed training of RL agents, R2D2 is an RL approach that trains a RNN-based RL agents from distributed prioritized experience replay. \r
Using a single network architecture and fixed set of hyperparameters, Recurrent Replay Distributed DQN quadrupled the previous state of the art on Atari-57, and matches the state of the art on DMLab-30. \r
It was the first agent to exceed human-level performance in 52 of the 57 Atari games.""" ;
    skos:prefLabel "R2D2" .

:RAE a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1903.12436v4> ;
    rdfs:seeAlso <https://github.com/clementchadebec/benchmark_VAE/blob/2a782b9e87671f13778c8728d827a46fe5ceea4c/src/pythae/models/rae_l2/rae_l2_model.py#L14> ;
    skos:altLabel "Regularized Autoencoders" ;
    skos:definition "This method introduces several regularization schemes that can be applied to an Autoencoder. To make the model generative *ex-post* density estimation is proposed and consists in fitting a Mixture of Gaussian distribution on the train data embeddings after the model is trained." ;
    skos:prefLabel "RAE" .

:RAG a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2005.11401v4> ;
    skos:definition "**Retriever-Augmented Generation**, or **RAG**, is a type of language generation model that combines pre-trained parametric and non-parametric memory for language generation. Specifically, the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever.  For query $x$, Maximum Inner Product Search (MIPS) is used to find the top-K documents $z\\_{i}$. For final prediction $y$, we treat $z$ as a latent variable and marginalize over seq2seq predictions given different documents." ;
    skos:prefLabel "RAG" .

:RAHP a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2003.06209v1> ;
    skos:altLabel "Review-guided Answer Helpfulness Prediction" ;
    skos:definition "**Review-guided Answer Helpfulness Prediction** (RAHP) is a textual inference model for identifying helpful answers in e-commerce. It not only considers the interactions between QA pairs, but also investigates the opinion coherence between the answer and crowds' opinions reflected in the reviews, which is another important factor to identify helpful answers." ;
    skos:prefLabel "RAHP" .

:RAM a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1406.6247v1> ;
    skos:altLabel "Recurrent models of visual attention" ;
    skos:definition "RAM adopts RNNs and reinforcement learning (RL) to make the network learn where to pay attention." ;
    skos:prefLabel "RAM" .

:RAN a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1704.06904v1> ;
    skos:altLabel "Residual Attention Network" ;
    skos:definition """Inspired by the success of ResNet,\r
Wang et al. proposed\r
the very deep convolutional residual attention network (RAN) by \r
combining an attention mechanism with residual connections. \r
\r
Each attention module stacked in a residual attention network \r
can be divided into a mask branch and a trunk branch. \r
The trunk branch processes features,\r
and can be implemented by any state-of-the-art structure\r
including a pre-activation residual unit and an inception block.\r
The mask branch uses a bottom-up top-down structure\r
to learn a mask of the same size that \r
softly weights output features from the trunk branch. \r
A sigmoid layer normalizes the output to $[0,1]$ after two $1\\times 1$ convolution layers. Overall the residual attention mechanism can be written as\r
\r
\\begin{align}\r
s &= \\sigma(Conv_{2}^{1\\times 1}(Conv_{1}^{1\\times 1}( h_\\text{up}(h_\\text{down}(X))))) \r
\\end{align}\r
\r
\\begin{align}\r
X_{out} &= s f(X) + f(X)\r
\\end{align}\r
where $h_\\text{up}$ is a bottom-up structure, \r
using max-pooling several times after residual units\r
to increase the receptive field, while\r
$h_\\text{down}$ is the top-down part using \r
linear interpolation to keep the output size the \r
same as the input feature map. \r
There are also skip-connections between the two parts,\r
which are omitted from the formulation.\r
$f$ represents the trunk branch\r
which can be any state-of-the-art structure.\r
\r
Inside each attention module, a\r
bottom-up top-down feedforward structure models\r
both spatial and cross-channel dependencies, \r
 leading to a consistent performance improvement. \r
Residual attention can be incorporated into\r
any deep network structure in an end-to-end training fashion.\r
However, the proposed bottom-up top-down structure fails to leverage global spatial information.  \r
Furthermore, directly predicting a 3D attention map  has high computational cost.""" ;
    skos:prefLabel "RAN" .

:RAdam a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1908.03265v4> ;
    rdfs:seeAlso <https://github.com/pytorch/pytorch/blob/d05c1ec007ae75de5f6df7b05de7cca59751dea5/torch/optim/radam.py#L6> ;
    skos:definition """**Rectified Adam**, or **RAdam**, is a variant of the [Adam](https://paperswithcode.com/method/adam) stochastic optimizer that introduces a term to rectify the variance of the adaptive learning rate. It seeks to tackle the bad convergence problem suffered by Adam. The authors argue that the root cause of this behaviour is that the adaptive learning rate has undesirably large variance in the early stage of model training, due to the limited amount of training samples being used. Thus, to reduce such variance, it is better to use smaller learning rates in the first few epochs of training - which justifies the warmup heuristic. This heuristic motivates RAdam which rectifies the variance problem:\r
\r
$$g\\_{t} = \\nabla\\_{\\theta}f\\_{t}\\left(\\theta\\_{t-1}\\right) $$\r
\r
$$v\\_{t} = 1/\\beta\\_{2}v\\_{t-1} + \\left(1-\\beta\\_{2}\\right)g^{2}\\_{t} $$\r
\r
$$m\\_{t} = \\beta\\_{1}m\\_{t-1} + \\left(1-\\beta\\_{1}\\right)g\\_{t} $$\r
\r
$$ \\hat{m\\_{t}} = m\\_{t} / \\left(1-\\beta^{t}\\_{1}\\right) $$\r
\r
$$ \\rho\\_{t} = \\rho\\_{\\infty} - 2t\\beta^{t}\\_{2}/\\left(1-\\beta^{t}\\_{2}\\right) $$\r
\r
$$\\rho_{\\infty} = \\frac{2}{1-\\beta_2} - 1$$ \r
\r
If the variance is tractable - $\\rho\\_{t} > 4$ then:\r
\r
...the adaptive learning rate is computed as:\r
\r
$$ l\\_{t} = \\sqrt{\\left(1-\\beta^{t}\\_{2}\\right)/v\\_{t}}$$\r
\r
...the variance rectification term is calculated as:\r
\r
$$ r\\_{t} = \\sqrt{\\frac{(\\rho\\_{t}-4)(\\rho\\_{t}-2)\\rho\\_{\\infty}}{(\\rho\\_{\\infty}-4)(\\rho\\_{\\infty}-2)\\rho\\_{t}}}$$\r
\r
...and we update parameters with adaptive momentum:\r
\r
$$ \\theta\\_{t} = \\theta\\_{t-1} - \\alpha\\_{t}r\\_{t}\\hat{m}\\_{t}l\\_{t} $$\r
\r
If the variance isn't tractable we update instead with:\r
\r
$$ \\theta\\_{t} = \\theta\\_{t-1} - \\alpha\\_{t}\\hat{m}\\_{t} $$""" ;
    skos:prefLabel "RAdam" .

:RBF a skos:Concept ;
    skos:altLabel "Radial Basis Function" ;
    skos:definition "" ;
    skos:prefLabel "RBF" .

:RBPN a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1903.10128v1> ;
    skos:altLabel "Recurrent Back Projection Network" ;
    skos:definition "" ;
    skos:prefLabel "RBPN" .

:RE-NET a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1904.05530v4> ;
    skos:altLabel "Recurrent Event Network" ;
    skos:definition "Recurrent Event Network (RE-NET) is an autoregressive architecture for predicting future interactions. The occurrence of a fact (event) is modeled as a probability distribution conditioned on temporal sequences of past knowledge graphs. RE-NET employs a recurrent event encoder to encode past facts and uses a neighborhood aggregator to model the connection of facts at the same timestamp. Future facts can then be inferred in a sequential manner based on the two modules." ;
    skos:prefLabel "RE-NET" .

:REM a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1907.04543v4> ;
    skos:altLabel "Random Ensemble Mixture" ;
    skos:definition "Random Ensemble Mixture (REM) is an easy to implement extension of [DQN](https://paperswithcode.com/method/dqn) inspired by [Dropout](https://paperswithcode.com/method/dropout). The key intuition behind REM is that if one has access to multiple estimates of Q-values, then a weighted combination of the Q-value estimates is also an estimate for Q-values. Accordingly, in each training step, REM randomly combines multiple Q-value estimates and uses this random combination for robust training." ;
    skos:prefLabel "REM" .

:RESCAL a skos:Concept ;
    dcterms:source <https://icml.cc/2011/papers/438_icmlpaper.pdf> ;
    skos:definition "RESCAL" ;
    skos:prefLabel "RESCAL" .

:RESCAL-RP a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2110.02834v1> ;
    skos:altLabel "RESCAL with Relation Prediction" ;
    skos:definition "RESCAL model trained with a relation prediction objective on top of the 1vsAll loss" ;
    skos:prefLabel "RESCAL-RP" .

:RESCALRP a skos:Concept ;
    dcterms:source <https://icml.cc/2011/papers/438_icmlpaper.pdf> ;
    skos:altLabel "RESCAL" ;
    skos:definition "" ;
    skos:prefLabel "RESCAL RP" .

:RFB a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1711.07767v3> ;
    rdfs:seeAlso <https://github.com/ruinmessi/RFBNet/blob/0f1d836df9f90e26422c19732088d76bbd8dd489/models/RFB_Net_mobile.py#L89> ;
    skos:altLabel "Receptive Field Block" ;
    skos:definition """**Receptive Field Block (RFB)** is a module for strengthening the deep features learned from lightweight CNN models so that they can contribute to fast and accurate detectors. Specifically, RFB makes use of multi-branch pooling with varying kernels corresponding to RFs of different sizes, applies [dilated convolution](https://paperswithcode.com/method/dilated-convolution) layers to control their eccentricities, and reshapes them to generate\r
final representation.""" ;
    skos:prefLabel "RFB" .

:RFBNet a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1711.07767v3> ;
    rdfs:seeAlso <https://github.com/ruinmessi/RFBNet> ;
    skos:definition "**RFB Net** is a one-stage object detector that utilises a receptive field block module. It utilises a VGG16 backbone, and is otherwise quite similar to the [SSD](https://paperswithcode.com/method/ssd) architecture." ;
    skos:prefLabel "RFB Net" .

:RFE a skos:Concept ;
    skos:altLabel "Rank Flow Embedding" ;
    skos:definition "" ;
    skos:prefLabel "RFE" .

:RFP a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2006.02334v2> ;
    skos:altLabel "Recursive Feature Pyramid" ;
    skos:definition "An **Recursive Feature Pyramid (RFP)** builds on top of the Feature Pyramid Networks ([FPN](https://paperswithcode.com/method/fpn)) by incorporating extra feedback connections from the FPN layers into the bottom-up backbone layers. Unrolling the recursive structure to a sequential implementation, we obtain a backbone for object detector that looks at the images twice or more. Similar to the cascaded detector heads in [Cascade R-CNN](https://paperswithcode.com/method/cascade-r-cnn) trained with more selective examples, an RFP recursively enhances FPN to generate increasingly powerful representations. Resembling Deeply-Supervised Nets, the feedback connections bring the features that directly receive gradients from the detector heads back to the low levels of the bottom-up backbone to speed up training and boost performance." ;
    skos:prefLabel "RFP" .

:RGA a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1904.02998v2> ;
    skos:altLabel "Relation-aware Global Attention" ;
    skos:definition """In relation-aware global attention (RGA) stresses the importance of global structural information provided by pairwise relations, and uses it to produce attention maps. \r
\r
RGA comes in two forms,  spatial RGA (RGA-S) and channel RGA (RGA-C). RGA-S first reshapes the input feature map $X$ to $C\\times (H\\times W)$ and the pairwise relation matrix $R \\in \\mathbb{R}^{(H\\times W)\\times (H\\times W)}$ is computed using \r
\\begin{align}\r
    Q &= \\delta(W^QX) \r
\\end{align}\r
\\begin{align}\r
    K &= \\delta(W^KX) \r
\\end{align}\r
\\begin{align}\r
    R &= Q^TK\r
\\end{align}\r
The relation vector $r_i$ at position $i$ is defined by stacking  pairwise relations at all positions:\r
\\begin{align}\r
    r_i = [R(i, :); R(:,i)]    \r
\\end{align}\r
and the spatial relation-aware feature $y_i$ can be written as\r
\\begin{align}\r
    Y_i = [g^c_\\text{avg}(\\delta(W^\\varphi x_i)); \\delta(W^\\phi r_i)]\r
\\end{align}\r
where $g^c_\\text{avg}$ denotes global average pooling in the channel domain. Finally, the spatial attention score at position $i$ is given by \r
\\begin{align}\r
    a_i = \\sigma(W_2\\delta(W_1y_i))\r
\\end{align}\r
RGA-C has the same form as RGA-S, except for taking the input feature map as a set of $H\\times W$-dimensional features.\r
\r
RGA uses global relations to generate the attention score for each feature node,  so provides valuable structural information and significantly enhances the representational power. RGA-S and RGA-C are flexible enough to be used in any CNN network; Zhang et al. propose  using them jointly in sequence to better capture both spatial and cross-channel relationships.""" ;
    skos:prefLabel "RGA" .

:RGCN a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1703.06103v4> ;
    skos:altLabel "Relational Graph Convolution Network" ;
    skos:definition """An **RGCN**, or **Relational Graph Convolution Network**, is a an application of the [GCN framework](https://paperswithcode.com/method/gcn) to modeling relational data, specifically\r
to link prediction and entity classification tasks.\r
\r
See [here](https://docs.dgl.ai/en/0.4.x/tutorials/models/1_gnn/4_rgcn.html) for an in-depth explanation of RGCNs by DGL.""" ;
    skos:prefLabel "RGCN" .

:RIFE a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2011.06294v11> ;
    skos:definition """**RIFE**, or **Real-time Intermediate Flow Estimation** is an intermediate flow estimation algorithm for Video Frame Interpolation (VFI). Many recent flow-based VFI methods first estimate the bi-directional optical flows, then scale and reverse them to approximate intermediate flows, leading to artifacts on motion boundaries. RIFE uses a neural network named [IFNet](https://paperswithcode.com/method/ifnet) that can directly estimate the intermediate flows from coarse-to-fine with much better speed. It introduces a privileged distillation scheme for training intermediate flow model, which leads to a large performance improvement.\r
\r
In RIFE training, given two input frames $I_{0}, I_{1}$, we directly feed them into the IFNet to approximate intermediate flows $F_{t \\rightarrow 0}, F_{t \\rightarrow 1}$ and the fusion map $M$. During training phase, a privileged teacher refines student's results to get $F_{t \\rightarrow 0}^{T e a}, F_{t \\rightarrow 1}^{T e a}$ and $M^{\\text {Tea }}$ based on ground truth $I_{t}$. The student model and the teacher model are jointly trained from scratch using the reconstruction loss. The teacher's approximations are more accurate so that they can guide the student to learn.""" ;
    skos:prefLabel "RIFE" .

:RMSNorm a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1910.07467v1> ;
    skos:altLabel "Root Mean Square Layer Normalization" ;
    skos:definition "RMSNorm regularizes the summed inputs to a neuron in one layer according to root mean square (RMS), giving the model re-scaling invariance property and implicit learning rate adaptation ability. RMSNorm is computationally simpler and thus more efficient than LayerNorm." ;
    skos:prefLabel "RMSNorm" .

:RMSPooling a skos:Concept ;
    skos:altLabel "Root-of-Mean-Squared Pooling" ;
    skos:definition """**RMS Pooling** is a pooling operation that calculates the square mean root for patches of a feature map, and uses it to create a downsampled (pooled) feature map.  It is usually used after a convolutional layer.\r
\r
$$ z_{j} = \\sqrt{\\frac{1}{M}\\sum^{M}_{i=1}u{ij}^{2}} $$""" ;
    skos:prefLabel "RMS Pooling" .

:RMSProp a skos:Concept ;
    rdfs:seeAlso <https://github.com/pytorch/pytorch/blob/fd8e2064e094f301d910b91a757b860aae3e3116/torch/optim/rmsprop.py#L69-L108> ;
    skos:definition """**RMSProp** is an unpublished adaptive learning rate optimizer [proposed by Geoff Hinton](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf). The motivation is that the magnitude of gradients can differ for different weights, and can change during learning, making it hard to choose a single global learning rate. RMSProp tackles this by keeping a moving average of the squared gradient and adjusting the weight updates by this magnitude. The gradient updates are performed as:\r
\r
$$E\\left[g^{2}\\right]\\_{t} = \\gamma E\\left[g^{2}\\right]\\_{t-1} + \\left(1 - \\gamma\\right) g^{2}\\_{t}$$\r
\r
$$\\theta\\_{t+1} = \\theta\\_{t} - \\frac{\\eta}{\\sqrt{E\\left[g^{2}\\right]\\_{t} + \\epsilon}}g\\_{t}$$\r
\r
Hinton suggests $\\gamma=0.9$, with a good default for $\\eta$ as $0.001$.\r
\r
Image: [Alec Radford](https://twitter.com/alecrad)""" ;
    skos:prefLabel "RMSProp" .

:ROCKET a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1910.13051v1> ;
    skos:altLabel "Random Convolutional Kernel Transform" ;
    skos:definition "Linear classifier using random convolutional kernels applied to time series." ;
    skos:prefLabel "ROCKET" .

:RPDet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1904.11490v2> ;
    rdfs:seeAlso <https://github.com/microsoft/RepPoints> ;
    skos:definition "**RPDet**, or **RepPoints Detector**, is a anchor-free, two-stage object detection model based on deformable convolutions.  [RepPoints](https://paperswithcode.com/method/reppoints) serve as the basic object representation throughout the detection system. Starting from the center points, the first set of RepPoints is obtained via regressing offsets over the center points. The learning of these RepPoints is driven by two objectives: 1) the top-left and bottom-right points distance loss between the induced pseudo box and the ground-truth bounding box; 2) the object recognition loss of the subsequent stage." ;
    skos:prefLabel "RPDet" .

:RPM-Net a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2003.13479v1> ;
    skos:definition "**RPM-Net** is an end-to-end differentiable deep network for robust point matching uses learned features. It preserves robustness of RPM against noisy/outlier points while desensitizing initialization with point correspondences from learned feature distances instead of spatial distances. The network uses the differentiable Sinkhorn layer and annealing to get soft assignments of point correspondences from hybrid features learned from both spatial coordinates and local geometry. To further improve registration performance, the authors introduce a secondary network to predict optimal annealing parameters." ;
    skos:prefLabel "RPM-Net" .

:RPN a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1506.01497v3> ;
    skos:altLabel "Region Proposal Network" ;
    skos:definition """A **Region Proposal Network**, or **RPN**, is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals. RPN and algorithms like [Fast R-CNN](https://paperswithcode.com/method/fast-r-cnn) can be merged into a single network by sharing their convolutional features - using the recently popular terminology of neural networks with attention mechanisms, the RPN component tells the unified network where to look.\r
\r
RPNs are designed to efficiently predict region proposals with a wide range of scales and aspect ratios. RPNs use anchor boxes that serve as references at multiple scales and aspect ratios. The scheme can be thought of as a pyramid of regression references, which avoids enumerating images or filters of multiple scales or aspect ratios.""" ;
    skos:prefLabel "RPN" .

:RReLU a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1505.00853v2> ;
    rdfs:seeAlso <https://github.com/pytorch/pytorch/blob/96aaa311c0251d24decb9dc5da4957b7c590af6f/torch/nn/modules/activation.py#L109> ;
    skos:altLabel "Randomized Leaky Rectified Linear Units" ;
    skos:definition """**Randomized Leaky Rectified Linear Units**, or **RReLU**, are an activation function that randomly samples the negative slope for activation values. It was first proposed and used in the Kaggle NDSB Competition. During training, $a\\_{ji}$ is a random number sampled from a uniform distribution $U\\left(l, u\\right)$. Formally:\r
\r
$$ y\\_{ji} = x\\_{ji} \\text{   if } x\\_{ji} \\geq{0} $$\r
$$ y\\_{ji} = a\\_{ji}x\\_{ji} \\text{   if } x\\_{ji} < 0 $$\r
\r
where\r
\r
$$\\alpha\\_{ji} \\sim U\\left(l, u\\right), l < u \\text{ and } l, u \\in \\left[0,1\\right)$$\r
\r
In the test phase, we take average of all the $a\\_{ji}$ in training similar to [dropout](https://paperswithcode.com/method/dropout), and thus set $a\\_{ji}$ to $\\frac{l+u}{2}$ to get a deterministic result. As suggested by the NDSB competition winner, $a\\_{ji}$ is sampled from $U\\left(3, 8\\right)$. \r
\r
At test time, we use:\r
\r
$$ y\\_{ji} = \\frac{x\\_{ji}}{\\frac{l+u}{2}} $$""" ;
    skos:prefLabel "RReLU" .

:RSE a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2004.04662v4> ;
    skos:altLabel "Residual Shuffle-Exchange Network" ;
    skos:definition "**Residual Shuffle-Exchange Network** is an efficient alternative to models using an attention mechanism that allows the modelling of long-range dependencies in sequences in O(n log n) time. This model achieved state-of-the-art performance on the MusicNet dataset for music transcription while being able to run inference on a single GPU fast enough to be suitable for real-time audio processing." ;
    skos:prefLabel "RSE" .

:RSUBenešBlock a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2004.04662v4> ;
    skos:altLabel "Beneš Block with Residual Switch Units" ;
    skos:definition """The **Beneš block** is a computation-efficient alternative to dense attention, enabling the modelling of long-range dependencies in O(n log n) time. In comparison, dense attention which is commonly used in Transformers has O(n^2) complexity.\r
\r
In music, dependencies occur on several scales, including on a coarse scale which requires processing very long sequences. Beneš blocks have been used in Residual Shuffle-Exchange Networks to achieve state-of-the-art results in music transcription.\r
\r
Beneš blocks have a ‘receptive field’ of the size of the whole sequence, and it has no bottleneck. These properties hold for dense attention but have not been shown for many sparse attention and dilated convolutional architectures.""" ;
    skos:prefLabel "RSU Beneš Block" .

:RTMDet a skos:Concept ;
    rdfs:seeAlso <https://github.com/open-mmlab/mmdetection/tree/3.x/configs/rtmdet> ;
    skos:altLabel "RTMDet: An Empirical Study of Designing Real-Time Object Detectors" ;
    skos:definition "Please enter a description about the method here" ;
    skos:prefLabel "RTMDet" .

:RUN a skos:Concept ;
    rdfs:seeAlso <https://imanahmadianfar.com/codes/> ;
    skos:altLabel "Rung Kutta optimization" ;
    skos:definition "The optimization field suffers from the metaphor-based “pseudo-novel” or “fancy” optimizers. Most of these cliché methods mimic animals' searching trends and possess a small contribution to the optimization process itself. Most of these cliché methods suffer from the locally efficient performance, biased verification methods on easy problems, and high similarity between their components' interactions. This study attempts to go beyond the traps of metaphors and introduce a novel metaphor-free population-based optimization method based on the mathematical foundations and ideas of the Runge Kutta (RK) method widely well-known in mathematics. The proposed RUNge Kutta optimizer (RUN) was developed to deal with various types of optimization problems in the future. The RUN utilizes the logic of slope variations computed by the RK method as a promising and logical searching mechanism for global optimization. This search mechanism benefits from two active exploration and exploitation phases for exploring the promising regions in the feature space and constructive movement toward the global best solution. Furthermore, an enhanced solution quality (ESQ) mechanism is employed to avoid the local optimal solutions and increase convergence speed. The RUN algorithm's efficiency was evaluated by comparing with other metaheuristic algorithms in 50 mathematical test functions and four real-world engineering problems. The RUN provided very promising and competitive results, showing superior exploration and exploitation tendencies, fast convergence rate, and local optima avoidance. In optimizing the constrained engineering problems, the metaphor-free RUN demonstrated its suitable performance as well. The authors invite the community for extensive evaluations of this deep-rooted optimizer as a promising tool for real-world optimization" ;
    skos:prefLabel "RUN" .

:RainbowDQN a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1710.02298v1> ;
    skos:definition """**Rainbow DQN** is an extended [DQN](https://paperswithcode.com/method/dqn) that combines several improvements into a single learner. Specifically:\r
\r
- It uses [Double Q-Learning](https://paperswithcode.com/method/double-q-learning) to tackle overestimation bias.\r
- It uses [Prioritized Experience Replay](https://paperswithcode.com/method/prioritized-experience-replay) to prioritize important transitions.\r
- It uses [dueling networks](https://paperswithcode.com/method/dueling-network).\r
- It uses [multi-step learning](https://paperswithcode.com/method/n-step-returns).\r
- It uses distributional reinforcement learning instead of the expected return.\r
- It uses noisy linear layers for exploration.""" ;
    skos:prefLabel "Rainbow DQN" .

:RandAugment a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1909.13719v2> ;
    rdfs:seeAlso <https://github.com/ildoonet/pytorch-randaugment> ;
    skos:definition """**RandAugment** is an automated data augmentation method. The search space for data augmentation has 2 interpretable hyperparameter $N$ and $M$.  $N$ is the number of augmentation transformations to apply sequentially, and $M$ is the magnitude for all the transformations. To reduce the parameter space but still maintain image diversity, learned policies and probabilities for applying each transformation are replaced with a parameter-free procedure of always selecting a transformation with uniform probability $\\frac{1}{K}$. Here $K$ is the number of transformation options. So given $N$ transformations for a training image, RandAugment may thus express $KN$ potential policies.\r
\r
Transformations applied include identity transformation, autoContrast, equalize, rotation, solarixation, colorjittering, posterizing, changing contrast, changing brightness, changing sharpness, shear-x, shear-y, translate-x, translate-y.""" ;
    skos:prefLabel "RandAugment" .

:RandSol a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2308.12661v1> ;
    skos:altLabel "Randomized Adversarial Solarization" ;
    skos:definition "Attack on image classifiers by a image solarization through greedy random search." ;
    skos:prefLabel "RandSol" .

:RandWire a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1904.01569v2> ;
    rdfs:seeAlso <https://github.com/seungwonpark/RandWireNN/blob/0850008e9204cef5fcb1fe508d4c99576b37f995/model/model.py#L10> ;
    skos:definition """**RandWire** is a type of convolutional neural network that arise from randomly\r
wired neural networks that are sampled from stochastic network generators, in which a human-designed random\r
process defines generation.""" ;
    skos:prefLabel "RandWire" .

:RandomErasing a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1708.04896v2> ;
    rdfs:seeAlso <https://github.com/zhunzhong07/Random-Erasing> ;
    skos:definition "Random Erasing is a data augmentation method for training the convolutional neural network (CNN), which randomly selects a rectangle region in an image and erases its pixels with random values. In this process, training images with various levels of occlusion are generated, which reduces the risk of over-fitting and makes the model robust to occlusion. Random Erasing is parameter learning free, easy to implement, and can be integrated with most of the CNN-based recognition models. Random Erasing is complementary to commonly used data augmentation techniques such as random cropping and flipping, and can be implemented in various vision tasks, such as image classification, object detection, semantic segmentation." ;
    skos:prefLabel "Random Erasing" .

:RandomGaussianBlur a skos:Concept ;
    skos:definition """**Random Gaussian Blur** is an image data augmentation technique where we randomly blur the image using a Gaussian distribution.\r
\r
Image Source: [Wikipedia](https://en.wikipedia.org/wiki/Gaussian_blur)""" ;
    skos:prefLabel "Random Gaussian Blur" .

:RandomGrayscale a skos:Concept ;
    skos:definition "**Random Grayscale**  is an image data augmentation that converts an image to grayscale with probability $p$." ;
    skos:prefLabel "Random Grayscale" .

:RandomHorizontalFlip a skos:Concept ;
    skos:definition """**RandomHorizontalFlip** is a type of image data augmentation which horizontally flips a given image with a given probability.\r
\r
Image Credit: [Apache MXNet](https://mxnet.apache.org/versions/1.5.0/tutorials/gluon/data_augmentation.html)""" ;
    skos:prefLabel "Random Horizontal Flip" .

:RandomMutationSearch a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2103.15692v1> ;
    skos:definition "" ;
    skos:prefLabel "Random Mutation Search" .

:RandomResizedCrop a skos:Concept ;
    skos:definition """**RandomResizedCrop** is a type of image data augmentation where a crop of random size of the original size and a random aspect ratio of the original aspect ratio is made. This crop is finally resized to given size.\r
\r
Image Credit: [Apache MXNet](https://mxnet.apache.org/versions/1.5.0/tutorials/gluon/data_augmentation.html)""" ;
    skos:prefLabel "Random Resized Crop" .

:RandomRotate a skos:Concept ;
    skos:definition "**RandomRotate** is a type of image data augmentation where we randomly rotate the image by a degree." ;
    skos:prefLabel "RandomRotate" .

:RandomScaling a skos:Concept ;
    skos:definition "**Random Scaling** is a type of image data augmentation where we randomly change the scale the image between a specified range." ;
    skos:prefLabel "Random Scaling" .

:RandomSearch a skos:Concept ;
    skos:definition """**Random Search** replaces the exhaustive enumeration of all combinations by selecting them randomly. This can be simply applied to the discrete setting described above, but also generalizes to continuous and mixed spaces. It can outperform Grid search, especially when only a small number of hyperparameters affects the final performance of the machine learning algorithm. In this case, the optimization problem is said to have a low intrinsic dimensionality. Random Search is also embarrassingly parallel, and additionally allows the inclusion of prior knowledge by specifying the distribution from which to sample.\r
\r
\r
Extracted from [Wikipedia](https://en.wikipedia.org/wiki/Hyperparameter_optimization#Random_search)\r
\r
Source [Paper](https://dl.acm.org/doi/10.5555/2188385.2188395)\r
\r
Image Source: [BERGSTRA AND BENGIO](https://dl.acm.org/doi/pdf/10.5555/2188385.2188395)""" ;
    skos:prefLabel "Random Search" .

:RandomSynthesizedAttention a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2005.00743v3> ;
    skos:definition """**Random Synthesized Attention** is a form of synthesized attention where the attention weights are not conditioned on any input tokens. Instead, the attention weights are initialized to random values. It was introduced with the [Synthesizer](https://paperswithcode.com/method/synthesizer) architecture. Random Synthesized Attention contrasts with [Dense Synthesized Attention](https://paperswithcode.com/method/dense-synthesized-attention) which conditions on each token independently, as opposed to pairwise token interactions in the vanilla [Transformer](https://paperswithcode.com/method/transformer) model.\r
\r
Let $R$ be a randomly initialized matrix. Random Synthesized Attention is defined as:\r
\r
$$Y = \\text{Softmax}\\left(R\\right)G\\left(X\\right) $$\r
\r
where $R \\in \\mathbb{R}^{l \\text{ x } l}$. Notably, each head adds 2 parameters to the overall network. The basic idea of the Random Synthesizer is to not rely on pairwise token interactions or any information from individual token but rather to learn a task-specific alignment that works well globally across many samples. This is a direct generalization of the recently proposed fixed self-attention patterns of [Raganato et al (2020)](https://arxiv.org/abs/2002.10260).""" ;
    skos:prefLabel "Random Synthesized Attention" .

:Rank-basedLoss a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2110.05941v2> ;
    skos:altLabel "Rank-based loss" ;
    skos:definition "" ;
    skos:prefLabel "Rank-based Loss" .

:RationalActivationFunction a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1907.06732v3> ;
    skos:definition "Rational Activation Functions, ratio of polynomials as learnable functions" ;
    skos:prefLabel "Rational Activation Function" .

:RationalActivationfunction a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1907.06732v3> ;
    skos:definition "" ;
    skos:prefLabel "Rational Activation function" .

:Re-AttentionModule a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2103.11886v4> ;
    skos:definition """The **Re-Attention Module** is an attention layer used in the [DeepViT](https://paperswithcode.com/method/deepvit) architecture which mixes the attention map with a learnable matrix before multiplying with the values. The motivation is to re-generate the attention maps to increase their diversity at different layers with negligible computation and memory cost. The authors note that traditional self-attention fails to learn effective concepts for representation learning in deeper layers of ViT -- attention maps become more similar and less diverse in deeper layers (attention collapse) - and this hinders the model from getting expected performance gain. Re-attention is implemented by:\r
\r
$$\r
\\operatorname{Re}-\\operatorname{Attention}(Q, K, V)=\\operatorname{Norm}\\left(\\Theta^{\\top}\\left(\\operatorname{Softmax}\\left(\\frac{Q K^{\\top}}{\\sqrt{d}}\\right)\\right)\\right) V\r
$$\r
\r
where transformation matrix $\\Theta$ is multiplied to the self-attention map $\\textbf{A}$ along the head dimension.""" ;
    skos:prefLabel "Re-Attention Module" .

:ReGLU a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2002.05202v1> ;
    skos:definition """**ReGLU** is an activation function which is a variant of [GLU](https://paperswithcode.com/method/glu). The definition is as follows:\r
\r
$$ \\text{ReGLU}\\left(x, W, V, b, c\\right) = \\max\\left(0, xW + b\\right) \\otimes \\left(xV + c\\right) $$""" ;
    skos:prefLabel "ReGLU" .

:ReInfoSelect a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2001.10382v1> ;
    skos:definition "**ReInfoSelect** is a reinforcement weak supervision selection method for information retrieval. It learns to select anchor-document pairs that best weakly supervise the neural ranker (action), using the ranking performance on a handful of relevance labels as the reward. Iteratively, for a batch of anchor-document pairs, ReInfoSelect back propagates the gradients through the neural ranker, gathers its NDCG reward, and optimizes the data selection network using policy gradients, until the neural ranker's performance peaks on target relevance metrics (convergence)." ;
    skos:prefLabel "ReInfoSelect" .

:ReLIC a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2010.07922v1> ;
    skos:definition """**ReLIC**, or **Representation Learning via Invariant Causal Mechanisms**, is a self-supervised learning objective that enforces invariant prediction of proxy targets across augmentations through an invariance regularizer which yields improved generalization guarantees. \r
\r
We can write the objective as:\r
\r
$$\r
\\underset{X}{\\mathbb{E}} \\underset{\\sim\\_{l k}, a\\_{q \\mathcal{A}}}{\\mathbb{E}} \\sum_{b \\in\\left\\(a\\_{l k}, a\\_{q t}\\right\\)} \\mathcal{L}\\_{b}\\left(Y^{R}, f(X)\\right) \\text { s.t. } K L\\left(p^{d o\\left(a\\_{l k}\\right)}\\left(Y^{R} \\mid f(X)\\right), p^{d o\\left(a\\_{q t}\\right)}\\left(Y^{R} \\mid f(X)\\right)\\right) \\leq \\rho\r
$$\r
\r
where $\\mathcal{L}$ is the proxy task loss and $K L$ is the Kullback-Leibler (KL) divergence. Note that any distance measure on distributions can be used in place of the KL divergence.\r
\r
Concretely, as proxy task we associate to every datapoint $x\\_{i}$ the label $y\\_{i}^{R}=i$. This corresponds to the instance discrimination task, commonly used in contrastive learning. We take pairs of points $\\left(x\\_{i}, x\\_{j}\\right)$ to compute similarity scores and use pairs of augmentations $a\\_{l k}=\\left(a\\_{l}, a\\_{k}\\right) \\in$ $\\mathcal{A} \\times \\mathcal{A}$ to perform a style intervention. Given a batch of samples $\\left\\(x\\_{i}\\right\\)\\_{i=1}^{N} \\sim \\mathcal{D}$, we use\r
\r
$$\r
p^{d o\\left(a\\_{l k}\\right)}\\left(Y^{R}=j \\mid f\\left(x\\_{i}\\right)\\right) \\propto \\exp \\left(\\phi\\left(f\\left(x\\_{i}^{a\\_{l}}\\right), h\\left(x\\_{j}^{a\\_{k}}\\right)\\right) / \\tau\\right)\r
$$\r
\r
with $x^{a}$ data augmented with $a$ and $\\tau$ a softmax temperature parameter. We encode $f$ using a neural network and choose $h$ to be related to $f$, e.g. $h=f$ or as a network with an exponential moving average of the weights of $f$ (e.g. target networks). To compare representations we use the function $\\phi\\left(f\\left(x\\_{i}\\right), h\\left(x\\_{j}\\right)\\right)=\\left\\langle g\\left(f\\left(x\\_{i}\\right)\\right), g\\left(h\\left(x\\_{j}\\right)\\right)\\right\\rangle$ where $g$ is a fully-connected neural network often called the critic.\r
\r
Combining these pieces, we learn representations by minimizing the following objective over the full set of data $x\\_{i} \\in \\mathcal{D}$ and augmentations $a_{l k} \\in \\mathcal{A} \\times \\mathcal{A}$\r
\r
$$\r
-\\sum_{i=1}^{N} \\sum\\_{a\\_{l k}} \\log \\frac{\\exp \\left(\\phi\\left(f\\left(x\\_{i}^{a_{l}}\\right), h\\left(x\\_{i}^{a\\_{k}}\\right)\\right) / \\tau\\right)}{\\sum\\_{m=1}^{M} \\exp \\left(\\phi\\left(f\\left(x\\_{i}^{a\\_{l}}\\right), h\\left(x\\_{m}^{a\\_{k}}\\right)\\right) / \\tau\\right)}+\\alpha \\sum\\_{a\\_{l k}, a\\_{q t}} K L\\left(p^{d o\\left(a\\_{l k}\\right)}, p^{d o\\left(a\\_{q t}\\right)}\\right)\r
$$\r
\r
with $M$ the number of points we use to construct the contrast set and $\\alpha$ the weighting of the invariance penalty. The shorthand $p^{d o(a)}$ is used for $p^{d o(a)}\\left(Y^{R}=j \\mid f\\left(x\\_{i}\\right)\\right)$. The Figure shows a schematic of the RELIC objective.""" ;
    skos:prefLabel "ReLIC" .

:ReLU a skos:Concept ;
    rdfs:seeAlso <https://github.com/DimTrigkakis/Python-Net/blob/efb81b2f828da5a81b77a141245efdb0d5bcfbf8/incredibleMathFunctions.py#L12-L13> ;
    skos:altLabel "Rectified Linear Units" ;
    skos:definition """**Rectified Linear Units**, or **ReLUs**, are a type of activation function that are linear in the positive dimension, but zero in the negative dimension. The kink in the function is the source of the non-linearity. Linearity in the positive dimension has the attractive property that it prevents non-saturation of gradients (contrast with [sigmoid activations](https://paperswithcode.com/method/sigmoid-activation)), although for half of the real line its gradient is zero.\r
\r
$$ f\\left(x\\right) = \\max\\left(0, x\\right) $$""" ;
    skos:prefLabel "ReLU" .

:ReLU6 a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1704.04861v1> ;
    rdfs:seeAlso <https://github.com/pytorch/pytorch/blob/96aaa311c0251d24decb9dc5da4957b7c590af6f/torch/nn/modules/activation.py#L246> ;
    skos:definition """**ReLU6** is a modification of the [rectified linear unit](https://paperswithcode.com/method/relu) where we limit the activation to a maximum size of $6$. This is due to increased robustness when used with low-precision computation.\r
\r
Image Credit: [PyTorch](https://pytorch.org/docs/master/generated/torch.nn.ReLU6.html)""" ;
    skos:prefLabel "ReLU6" .

:ReLUN a skos:Concept ;
    dcterms:source <https://doi.org/10.20944/preprints202301.0463.v1> ;
    skos:altLabel "Rectified Linear Unit N" ;
    skos:definition """The **Rectified Linear Unit N**, or **ReLUN**, is a modification of **[ReLU6](https://paperswithcode.com/method/relu6)** activation function that has trainable parameter **n**.\r
\r
$$ReLUN(x) = min(max(0, x), n)$$""" ;
    skos:prefLabel "ReLUN" .

:ReZero a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2003.04887v2> ;
    skos:definition """**ReZero** is a [normalization](https://paperswithcode.com/methods/category/normalization) approach that dynamically facilitates well-behaved gradients and arbitrarily deep signal propagation. The idea is simple: ReZero initializes each layer to perform the identity operation. For each layer,  a [residual connection](https://paperswithcode.com/method/residual-connectio) is introduced for the input signal $x$ and one trainable parameter $\\alpha$ that modulates the non-trivial transformation of a layer $F(\\mathbf{x})$:\r
\r
$$\r
\\mathbf{x}\\_{i+1}=\\mathbf{x}\\_{i}+\\alpha_{i} F\\left(\\mathbf{x}\\_{i}\\right)\r
$$\r
\r
where $\\alpha=0$ at the beginning of training. Initially the gradients for all parameters defining $F$ vanish, but dynamically evolve to suitable values during initial stages of training. The architecture is illustrated in the Figure.""" ;
    skos:prefLabel "ReZero" .

:RealFormer a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2012.11747v3> ;
    skos:definition "**RealFormer** is a type of [Transformer](https://paperswithcode.com/methods/category/transformers) based on the idea of [residual](https://paperswithcode.com/method/residual-connection) attention. It adds skip edges to the backbone [Transformer](https://paperswithcode.com/method/transformer) to create multiple direct paths, one for each type of attention module. It adds no parameters or hyper-parameters. Specifically, RealFormer uses a Post-[LN](https://paperswithcode.com/method/layer-normalization) style Transformer as backbone and adds skip edges to connect [Multi-Head Attention](https://paperswithcode.com/method/multi-head-attention) modules in adjacent layers." ;
    skos:prefLabel "RealFormer" .

:RealNVP a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1605.08803v3> ;
    rdfs:seeAlso <https://github.com/chrischute/real-nvp/blob/df51ad570baf681e77df4d2265c0f1eb1b5b646c/models/real_nvp/real_nvp.py#L9> ;
    skos:definition "**RealNVP** is a generative model that utilises real-valued non-volume preserving (real NVP) transformations for density estimation. The model can perform efficient and exact inference, sampling and log-density estimation of data points." ;
    skos:prefLabel "RealNVP" .

:ReasonBERT a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2109.04912v1> ;
    skos:definition """**ReasonBERT** is a pre-training method that augments language models with the ability to reason over long-range relations and multiple, possibly hybrid, contexts. It utilizes distant supervision to automatically connect multiple pieces of text and tables to create pre-training examples that require long-range reasoning. Different types of reasoning are simulated, including intersecting multiple pieces of evidence, bridging from one piece of evidence to another, and detecting unanswerable cases. \r
\r
Specifically, given a query sentence containing an entity pair, if we mask one of the entities, another sentence or table that contains the same pair of entities can likely be used as evidence to recover the masked entity. Moreover, to encourage deeper reasoning, multiple pieces of evidence are collected that are jointly used to recover the masked entities in the query sentence, allowing for the scattering of the masked entities among different pieces of evidence to mimic different types of reasoning. \r
\r
The Figure illustrates several examples using such distant supervision. In Ex. 1, a model needs to check multiple constraints (i.e., intersection reasoning type) and find “the beach soccer competition that is established in 1998.” In Ex. 2, a model needs to find “the type of the band that released Awaken the Guardian,” by first inferring the name of the band “Fates Warning” (i.e., bridging reasoning type). \r
\r
The masked entities in a query sentence are replaced with the [QUESTION] tokens. The new pre-training objective, span reasoning, then extracts the masked entities from the provided evidence. Existing LMs like [BERT](https://paperswithcode.com/method/bert) and [RoBERTa](https://paperswithcode.com/method/roberta) are augmented by continuing to train them with the new objective, which leads to ReasonBERT. Then query sentence and textual evidence are encoded via the LM. When tabular evidence is present, the structure-aware [transformer](https://paperswithcode.com/method/transformer) [TAPAS](https://paperswithcode.com/method/tapas) is used as the encoder to capture the table structure.""" ;
    skos:prefLabel "ReasonBERT" .

:RecurrentDropout a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1603.05118v2> ;
    rdfs:seeAlso <https://github.com/ssemeniuta/drop-rnn/blob/fec39a29c3c315e8f6d4529c799620ac0ffe7176/layers.py#L143> ;
    skos:definition "**Recurrent Dropout** is a regularization method for [recurrent neural networks](https://paperswithcode.com/methods/category/recurrent-neural-networks). [Dropout](https://paperswithcode.com/method/dropout) is applied to the updates to [LSTM](https://paperswithcode.com/method/lstm) memory cells (or [GRU](https://paperswithcode.com/method/gru) states), i.e. it drops out the input/update gate in LSTM/GRU." ;
    skos:prefLabel "Recurrent Dropout" .

:RecurrentEntityNetwork a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1612.03969v3> ;
    rdfs:seeAlso <https://github.com/siddk/entity-network/blob/b18fbb7c749e98192a77132af9e537e399ac24c7/model/entity_network.py#L27> ;
    skos:definition """The **Recurrent Entity Network** is equipped with a dynamic long-term memory which allows it to maintain and update a representation of the state of the world as it receives new data. For language understanding tasks, it can reason on-the-fly as it reads text, not just when it is required to answer a question or respond as is the case for a [Memory Network](https://paperswithcode.com/method/memory-network). Like a [Neural Turing Machine](https://paperswithcode.com/method/neural-turing-machine) or Differentiable Neural Computer, it maintains a fixed size memory and can learn to perform location and content-based read and write operations.  However, unlike those models it has a simple parallel  architecture in which several memory locations can be updated simultaneously. \r
\r
The model consists of a fixed number of dynamic memory cells, each containing a vector key $w_j$ and a vector value (or content) $h_j$. Each cell is associated with its own processor, a simple gated recurrent network that may update the cell value given an input. If each cell learns to represent a concept or entity in the world, one can imagine a gating mechanism that, based on the key and content of the memory cells, will only modify the cells that concern the entities mentioned in the input. There is no direct interaction between the memory cells, hence the system can be seen as multiple identical processors functioning in parallel, with distributed local memory. \r
\r
The sharing of these parameters reflects an invariance of these laws across object instances, similarly to how the [weight tying](https://paperswithcode.com/method/weight-tying) scheme in a CNN reflects an invariance of image statistics across locations. Their hidden state is updated only when new information relevant to their concept is received, and remains otherwise unchanged. The keys used in the addressing/gating mechanism also correspond to concepts or entities, but are modified only during learning, not during inference.""" ;
    skos:prefLabel "Recurrent Entity Network" .

:Reduction-A a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1602.07261v2> ;
    rdfs:seeAlso <https://github.com/Cadene/pretrained-models.pytorch/blob/8aae3d8f1135b6b13fed79c1d431e3449fdbf6e0/pretrainedmodels/models/inceptionresnetv2.py#L120> ;
    skos:definition "**Reduction-A** is an image model block used in the [Inception-v4](https://paperswithcode.com/method/inception-v4) architecture." ;
    skos:prefLabel "Reduction-A" .

:Reduction-B a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1602.07261v2> ;
    rdfs:seeAlso <https://github.com/kentsommer/keras-inceptionV4/blob/ef1db6f09b6511779c05fab47d374741bc89b5ee/inception_v4.py#L136> ;
    skos:definition "**Reduction-B** is an image model block used in the [Inception-v4](https://paperswithcode.com/method/inception-v4) architecture." ;
    skos:prefLabel "Reduction-B" .

:Reformer a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2001.04451v2> ;
    skos:definition """**Reformer** is a [Transformer](https://paperswithcode.com/method/transformer) based architecture that seeks to make efficiency improvements. [Dot-product attention](https://paperswithcode.com/method/dot-product-attention) is replaced by one that uses locality-sensitive hashing, changing its complexity\r
from O($L^2$) to O($L\\log L$), where $L$ is the length of the sequence. Furthermore, Reformers use reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of $N$ times, where $N$ is the number of layers.""" ;
    skos:prefLabel "Reformer" .

:RegNetX a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2003.13678v1> ;
    rdfs:seeAlso <https://github.com/facebookresearch/pycls/blob/ecfb53186b426002020f1a580c3d7d7ad723e283/pycls/models/regnet.py#L50> ;
    skos:definition """**RegNetX** is a convolutional network design space with simple, regular models with parameters: depth $d$, initial width $w\\_{0} > 0$, and slope $w\\_{a} > 0$, and generates a different block width $u\\_{j}$ for each block $j < d$. The key restriction for the RegNet types of model is that there is a linear parameterisation of block widths (the design space only contains models with this linear structure):\r
\r
$$ u\\_{j} = w\\_{0} + w\\_{a}\\cdot{j} $$\r
\r
For **RegNetX** we have additional restrictions: we set $b = 1$ (the bottleneck ratio), $12 \\leq d \\leq 28$, and $w\\_{m} \\geq 2$ (the width multiplier).""" ;
    skos:prefLabel "RegNetX" .

:RegNetY a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2003.13678v1> ;
    rdfs:seeAlso <https://github.com/facebookresearch/pycls/blob/ecfb53186b426002020f1a580c3d7d7ad723e283/pycls/models/regnet.py#L50> ;
    skos:definition """**RegNetY** is a convolutional network design space with simple, regular models with parameters: depth $d$, initial width $w\\_{0} > 0$, and slope $w\\_{a} > 0$, and generates a different block width $u\\_{j}$ for each block $j < d$. The key restriction for the RegNet types of model is that there is a linear parameterisation of block widths (the design space only contains models with this linear structure):\r
\r
$$ u\\_{j} = w\\_{0} + w\\_{a}\\cdot{j} $$\r
\r
For **RegNetX** we have additional restrictions: we set $b = 1$ (the bottleneck ratio), $12 \\leq d \\leq 28$, and $w\\_{m} \\geq 2$ (the width multiplier).\r
\r
For **RegNetY** we make one change, which is to include Squeeze-and-Excitation blocks.""" ;
    skos:prefLabel "RegNetY" .

:RegionViT a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2106.02689v3> ;
    skos:definition "**RegionViT** consists of two tokenization processes that convert an image into regional (upper path) and local tokens (lower path). Each tokenization is a convolution with different patch sizes, the patch size of regional tokens is $28^2$ while $4^2$ is used for local tokens with dimensions projected to $C$, which means that one regional token covers $7^2$ local tokens based on the spatial locality, leading to the window size of a local region to $7^2$. At stage 1, two set of tokens are passed through the proposed regional-to-local transformer encoders. However, for the later stages, to balance the computational load and to have feature maps at different resolution, the approach uses a downsampling process to halve the spatial resolution while doubling the channel dimension like CNN on both regional and local tokens before going to the next stage. Finally, at the end of the network, it simply averages the remaining regional tokens as the final embedding for the classification while the detection uses all local tokens at each stage since it provides more fine-grained location information. By having the pyramid structure, the ViT can generate multi-scale features and hence it could be easily extended to more vision applications, e.g., object detection, rather than image classification only." ;
    skos:prefLabel "RegionViT" .

:RelDiff a skos:Concept ;
    dcterms:source <https://aclanthology.org/2021.findings-emnlp.311> ;
    skos:definition "RelDiff generates entity-relation-entity embeddings in a single embedding space. RelDiff adopts two fundamental vector algebraic operators to transform entity and relation embeddings from knowledge graphs into entity-relation-entity embeddings. In particular, RelDiff can encode finer-grained information about the relations than is captured when separate embeddings are learned for the entities and the relations." ;
    skos:prefLabel "RelDiff" .

:RelativePositionEncodings a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1803.02155v2> ;
    skos:definition """**Relative Position Encodings** are a type of position embeddings for [Transformer-based models](https://paperswithcode.com/methods/category/transformers) that attempts to exploit pairwise, relative positional information. Relative positional information is supplied to the model on two levels: values and keys. This becomes apparent in the two modified self-attention equations shown below. First, relative positional information is supplied to the model as an additional component to the keys\r
\r
$$ e\\_{ij} = \\frac{x\\_{i}W^{Q}\\left(x\\_{j}W^{K} + a^{K}\\_{ij}\\right)^{T}}{\\sqrt{d\\_{z}}} $$\r
\r
Here $a$ is an edge representation for the inputs $x\\_{i}$ and $x\\_{j}$. The [softmax](https://paperswithcode.com/method/softmax) operation remains unchanged from vanilla self-attention. Then relative positional information is supplied again as a sub-component of the values matrix:\r
\r
$$ z\\_{i} = \\sum^{n}\\_{j=1}\\alpha\\_{ij}\\left(x\\_{j}W^{V} + a\\_{ij}^{V}\\right)$$\r
\r
In other words, instead of simply combining semantic embeddings with absolute positional ones, relative positional information is added to keys and values on the fly during attention calculation.\r
\r
Source: [Jake Tae](https://jaketae.github.io/study/relative-positional-encoding/)\r
\r
Image Source: [Relative Positional Encoding for Transformers with Linear Complexity](https://www.youtube.com/watch?v=qajudaEHuq8""" ;
    skos:prefLabel "Relative Position Encodings" .

:RelativisticGAN a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1807.00734v3> ;
    rdfs:seeAlso <https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/relativistic_gan/relativistic_gan.py> ;
    skos:definition """A **Relativistic GAN** is a type of generative adversarial network. It has a relativistic discriminator which estimates the probability that the given real data is more realistic than a randomly sampled fake data. The idea is to endow GANs with the property that the probability of real data being real ($D\\left(x\\_{r}\\right)$) should decrease as the probability of fake data being real ($D\\left(x\\_{f}\\right)$) increases.\r
\r
With a standard [GAN](https://paperswithcode.com/method/gan), we can achieve this as follows. The standard GAN discriminator can be defined, in term of the non-transformed layer $C\\left(x\\right)$, as $D\\left(x\\right) = \\text{sigmoid}\\left(C\\left(x\\right)\\right)$. A simple way to make discriminator relativistic - having the output of $D$ depend on both real and fake data - is to sample from real/fake data pairs $\\tilde{x} = \\left(x\\_{r}, x\\_{f}\\right)$ and define it as $D\\left(\\tilde{x}\\right) = \\text{sigmoid}\\left(C\\left(x\\_{r}\\right) − C\\left(x\\_{f}\\right)\\right)$. The modification can be interpreted as: the discriminator estimates the probability\r
that the given real data is more realistic than a randomly sampled fake data.\r
\r
More generally a Relativistic GAN can be interpreted as having a discriminator of the form $a\\left(C\\left(x\\_{r}\\right)−C\\left(x\\_{f}\\right)\\right)$, where $a$ is the activation function, to be relativistic.""" ;
    skos:prefLabel "Relativistic GAN" .

:ReliabilityBalancing a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2305.01486v3> ;
    skos:definition "" ;
    skos:prefLabel "Reliability Balancing" .

:Rendezvous a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2109.03223v2> ;
    skos:altLabel "Multi-head of Mixed Attention" ;
    skos:definition "Multi-heads of both self and cross attentions" ;
    skos:prefLabel "Rendezvous" .

:RepPoints a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1904.11490v2> ;
    skos:definition "**RepPoints** is a representation for object detection that consists of a set of points which indicate the spatial extent of an object and semantically significant local areas. This representation is learned via weak localization supervision from rectangular ground-truth boxes and implicit recognition feedback. Based on the richer RepPoints representation, the authors develop an anchor-free object detector that yields improved performance compared to using bounding boxes." ;
    skos:prefLabel "RepPoints" .

:RepVGG a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2101.03697v3> ;
    skos:definition """**RepVGG** is a [VGG](https://paperswithcode.com/method/vgg)-style convolutional architecture. It has the following advantages:\r
\r
- The model has a VGG-like plain (a.k.a. feed-forward) topology 1 without any branches. I.e., every layer takes\r
the output of its only preceding layer as input and feeds the output into its only following layer.\r
- The model’s body uses only 3 × 3 conv and [ReLU](https://paperswithcode.com/method/relu).\r
- The concrete architecture (including the specific depth and layer widths) is instantiated with no automatic\r
search, manual refinement, compound scaling, nor other heavy designs.""" ;
    skos:prefLabel "RepVGG" .

:ReplacingEligibilityTrace a skos:Concept ;
    skos:definition """In a **Replacing Eligibility Trace**, each time the state is revisited, the trace is reset to $1$ regardless of the presence of a prior trace.. For the memory vector $\\textbf{e}\\_{t} \\in \\mathbb{R}^{b} \\geq \\textbf{0}$:\r
\r
$$\\mathbf{e\\_{0}} = \\textbf{0}$$\r
\r
$$\\textbf{e}\\_{t} = \\gamma\\lambda{e}\\_{t-1}\\left(s\\right) \\text{ if } s \\neq s\\_{t}$$\r
\r
$$\\textbf{e}\\_{t} = 1 \\text{ if } s = s\\_{t}$$\r
\r
They can be seen as crude approximation to dutch traces, which have largely superseded them as they perform better than replacing traces and have a clearer theoretical basis. Accumulating traces remain of interest for nonlinear function approximations where dutch traces are not available.\r
\r
Source: Sutton and Barto, Reinforcement Learning, 2nd Edition""" ;
    skos:prefLabel "Replacing Eligibility Trace" .

:Res2Net a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1904.01169v3> ;
    rdfs:seeAlso <https://github.com/Res2Net/Res2Net-PretrainedModels/blob/3b9b078ae4c261d227449fe18504315c0740795a/res2net.py#L98> ;
    skos:definition """**Res2Net** is an image model that employs a variation on bottleneck residual blocks. The motivation is to be able to represent features at multiple scales. This is achieved through a novel building block for CNNs that constructs hierarchical residual-like connections within one single [residual block](https://paperswithcode.com/method/residual-block).\r
This represents multi-scale features at a granular level and increases the range of receptive fields for each network layer.""" ;
    skos:prefLabel "Res2Net" .

:Res2NetBlock a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1904.01169v3> ;
    rdfs:seeAlso <https://github.com/Res2Net/Res2Net-PretrainedModels/blob/d56af39dfc977bcbf3216871d20e2a9e16801c8a/res2net.py#L20> ;
    skos:definition """A **Res2Net Block** is an image model block that constructs hierarchical residual-like connections\r
within one single [residual block](https://paperswithcode.com/method/residual-block). It was proposed as part of the [Res2Net](https://paperswithcode.com/method/res2net) CNN architecture.\r
\r
The block represents multi-scale features at a granular level and increases the range of receptive fields for each network layer. The $3 \\times 3$ filters of $n$ channels is replaced with a set of smaller filter groups, each with $w$ channels. These smaller filter groups are connected in a hierarchical residual-like style to increase the number of scales that the output features can represent. Specifically, we divide input feature maps into several groups. A group of filters first extracts features from a group of input feature maps. Output features of the previous group are then sent to the next group of filters along with another group of input feature maps. \r
\r
This process repeats several times until all input feature maps are processed. Finally, feature maps from all groups are concatenated and sent to another group of $1 \\times 1$ filters to fuse information altogether. Along with any possible path in which input features are transformed to output features, the equivalent receptive field increases whenever it passes a $3 \\times 3$ filter, resulting in many equivalent feature scales due to combination effects.\r
\r
One way of thinking of these blocks is that they expose a new dimension, **scale**,  alongside the existing dimensions of depth, width, and cardinality.""" ;
    skos:prefLabel "Res2Net Block" .

:ResMLP a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2105.03404v2> ;
    skos:altLabel "Residual Multi-Layer Perceptrons" ;
    skos:definition """**Residual Multi-Layer Perceptrons**, or **ResMLP**, is an architecture built entirely upon [multi-layer perceptrons](https://paperswithcode.com/methods/category/feedforward-networks) for image classification. It is a simple [residual network](https://paperswithcode.com/method/residual-connection) that alternates (i) a [linear layer](https://paperswithcode.com/method/linear-layer) in which image patches interact, independently and identically across channels, and (ii) a two-layer [feed-forward network](https://paperswithcode.com/method/feedforward-network) in which channels interact independently per patch. At the end of the network, the patch representations are average pooled, and fed to a linear classifier.\r
\r
[Layer normalization](https://paperswithcode.com/method/layer-normalization) is replaced with a simpler [affine transformation](https://paperswithcode.com/method/affine-operator), thanks to the absence of self-attention layers which makes training more stable. The affine operator is applied at the beginning ("pre-normalization") and end ("post-normalization") of each residual block. As a pre-normalization, Aff replaces LayerNorm without using channel-wise statistics. Initialization is achieved as $\\mathbf{\\alpha}=\\mathbf{1}$, and $\\mathbf{\\beta}=\\mathbf{0}$. As a post-normalization, Aff is similar to [LayerScale](https://paperswithcode.com/method/layerscale) and $\\mathbf{\\alpha}$ is initialized with the same small value.""" ;
    skos:prefLabel "ResMLP" .

:ResNeSt a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2004.08955v2> ;
    rdfs:seeAlso <https://github.com/zhanghang1989/ResNeSt/blob/5fe47e93bd7e098d15bc278d8ab4812b82b49414/resnest/torch/resnet.py#L129> ;
    skos:definition "A **ResNest** is a variant on a [ResNet](https://paperswithcode.com/method/resnet), which instead stacks Split-Attention blocks. The cardinal group representations are then concatenated along the channel dimension: $V = \\text{Concat}${$V^{1},V^{2},\\cdots{V}^{K}$}. As in standard residual blocks, the final output $Y$ of otheur Split-Attention block is produced using a shortcut connection: $Y=V+X$, if the input and output feature-map share the same shape.  For blocks with a stride, an appropriate transformation $\\mathcal{T}$ is applied to the shortcut connection to align the output shapes:  $Y=V+\\mathcal{T}(X)$. For example, $\\mathcal{T}$ can be strided [convolution](https://paperswithcode.com/method/convolution) or combined convolution-with-pooling." ;
    skos:prefLabel "ResNeSt" .

:ResNeXt a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1611.05431v2> ;
    rdfs:seeAlso <https://github.com/pytorch/vision/blob/6db1569c89094cf23f3bc41f79275c45e9fcb3f3/torchvision/models/resnet.py#L124> ;
    skos:definition """A **ResNeXt** repeats a building block that aggregates a set of transformations with the same topology. Compared to a [ResNet](https://paperswithcode.com/method/resnet), it exposes a new dimension,  *cardinality* (the size of the set of transformations) $C$, as an essential factor in addition to the dimensions of depth and width. \r
\r
Formally, a set of aggregated transformations can be represented as: $\\mathcal{F}(x)=\\sum_{i=1}^{C}\\mathcal{T}_i(x)$, where $\\mathcal{T}_i(x)$ can be an arbitrary function. Analogous to a simple neuron, $\\mathcal{T}_i$ should project $x$ into an (optionally low-dimensional) embedding and then transform it.""" ;
    skos:prefLabel "ResNeXt" .

:ResNeXt-Elastic a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1812.05262v2> ;
    rdfs:seeAlso <https://github.com/allenai/elastic/blob/57345c600c63fbde163c41929d6d6dd894d408ce/models/resnext.py#L173> ;
    skos:definition "**ResNeXt-Elastic** is a convolutional neural network that is a modification of a [ResNeXt](https://paperswithcode.com/method/resnext) with elastic blocks (extra upsampling and downsampling)." ;
    skos:prefLabel "ResNeXt-Elastic" .

:ResNeXtBlock a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1611.05431v2> ;
    rdfs:seeAlso <https://github.com/pytorch/vision/blob/1aef87d01eec2c0989458387fa04baebcc86ea7b/torchvision/models/resnet.py#L75> ;
    skos:definition """A **ResNeXt Block** is a type of [residual block](https://paperswithcode.com/method/residual-block) used as part of the [ResNeXt](https://paperswithcode.com/method/resnext) CNN architecture. It uses a "split-transform-merge" strategy (branched paths within a single module) similar to an [Inception module](https://paperswithcode.com/method/inception-module), i.e. it aggregates a set of transformations. Compared to a Residual Block, it exposes a new dimension,  *cardinality* (size of set of transformations) $C$, as an essential factor in addition to depth and width. \r
\r
Formally, a set of aggregated transformations can be represented as: $\\mathcal{F}(x)=\\sum_{i=1}^{C}\\mathcal{T}_i(x)$, where $\\mathcal{T}_i(x)$ can be an arbitrary function. Analogous to a simple neuron, $\\mathcal{T}_i$ should project $x$ into an (optionally low-dimensional) embedding and then transform it.""" ;
    skos:prefLabel "ResNeXt Block" .

:ResNet a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1512.03385v1> ;
    rdfs:seeAlso <https://github.com/pytorch/vision/blob/6db1569c89094cf23f3bc41f79275c45e9fcb3f3/torchvision/models/resnet.py#L124> ;
    skos:altLabel "Residual Network" ;
    skos:definition """**Residual Networks**, or **ResNets**, learn residual functions with reference to the layer inputs, instead of learning unreferenced functions. Instead of hoping each few stacked layers directly fit a desired underlying mapping, residual nets let these layers fit a residual mapping. They stack [residual blocks](https://paperswithcode.com/method/residual-block) ontop of each other to form network: e.g. a ResNet-50 has fifty layers using these blocks. \r
\r
Formally, denoting the desired underlying mapping as $\\mathcal{H}(x)$, we let the stacked nonlinear layers fit another mapping of $\\mathcal{F}(x):=\\mathcal{H}(x)-x$. The original mapping is recast into $\\mathcal{F}(x)+x$.\r
\r
There is empirical evidence that these types of network are easier to optimize, and can gain accuracy from considerably increased depth.""" ;
    skos:prefLabel "ResNet" .

:ResNet-D a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1812.01187v2> ;
    rdfs:seeAlso <https://github.com/Linchunhui/Tricks-and-new-ResNet/blob/0e523a582d185f4da6598a145e8bee1e14d6aeeb/ResNet-D.py#L157> ;
    skos:definition "**ResNet-D** is a modification on the [ResNet](https://paperswithcode.com/method/resnet) architecture that utilises an [average pooling](https://paperswithcode.com/method/average-pooling) tweak for downsampling. The motivation is that in the unmodified ResNet, the 1 × 1 [convolution](https://paperswithcode.com/method/convolution) for the downsampling block ignores 3/4 of input feature maps, so this is modified so no information will be ignored" ;
    skos:prefLabel "ResNet-D" .

:ResNet-RS a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2103.07579v1> ;
    skos:definition """**ResNet-RS** is a family of [ResNet](https://paperswithcode.com/method/resnet) architectures that are 1.7x faster than [EfficientNets](https://paperswithcode.com/method/efficientnet) on TPUs, while achieving similar accuracies on ImageNet. The authors propose two new scaling strategies: (1) scale model depth in regimes where overfitting can occur (width scaling is preferable otherwise); (2) increase image resolution more slowly than previously recommended.\r
\r
Additional improvements include the use of a [cosine learning rate schedule](https://paperswithcode.com/method/cosine-annealing), [label smoothing](https://paperswithcode.com/method/label-smoothing), [stochastic depth](https://paperswithcode.com/method/stochastic-depth), [RandAugment](https://paperswithcode.com/method/randaugment), decreased [weight decay](https://paperswithcode.com/method/weight-decay), [squeeze-and-excitation](https://paperswithcode.com/method/squeeze-and-excitation-block) and the use of the [ResNet-D](https://paperswithcode.com/method/resnet-d) architecture.""" ;
    skos:prefLabel "ResNet-RS" .

:ResidualBlock a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1512.03385v1> ;
    rdfs:seeAlso <https://github.com/pytorch/vision/blob/1aef87d01eec2c0989458387fa04baebcc86ea7b/torchvision/models/resnet.py#L35> ;
    skos:definition """**Residual Blocks** are skip-connection blocks that learn residual functions with reference to the layer inputs, instead of learning unreferenced functions. They were introduced as part of the [ResNet](https://paperswithcode.com/method/resnet) architecture.\r
 \r
Formally, denoting the desired underlying mapping as $\\mathcal{H}({x})$, we let the stacked nonlinear layers fit another mapping of $\\mathcal{F}({x}):=\\mathcal{H}({x})-{x}$. The original mapping is recast into $\\mathcal{F}({x})+{x}$. The $\\mathcal{F}({x})$ acts like a residual, hence the name 'residual block'.\r
\r
The intuition is that it is easier to optimize the residual mapping than to optimize the original, unreferenced mapping. To the extreme, if an identity mapping were optimal, it would be easier to push the residual to zero than to fit an identity mapping by a stack of nonlinear layers. Having skip connections allows the network to more easily learn identity-like mappings.\r
\r
Note that in practice, [Bottleneck Residual Blocks](https://paperswithcode.com/method/bottleneck-residual-block) are used for deeper ResNets, such as ResNet-50 and ResNet-101, as these bottleneck blocks are less computationally intensive.""" ;
    skos:prefLabel "Residual Block" .

:ResidualConnection a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1512.03385v1> ;
    rdfs:seeAlso <https://github.com/pytorch/vision/blob/7c077f6a986f05383bcb86b535aedb5a63dd5c4b/torchvision/models/resnet.py#L118> ;
    skos:definition """**Residual Connections** are a type of skip-connection that learn residual functions with reference to the layer inputs, instead of learning unreferenced functions. \r
\r
Formally, denoting the desired underlying mapping as $\\mathcal{H}({x})$, we let the stacked nonlinear layers fit another mapping of $\\mathcal{F}({x}):=\\mathcal{H}({x})-{x}$. The original mapping is recast into $\\mathcal{F}({x})+{x}$.\r
\r
The intuition is that it is easier to optimize the residual mapping than to optimize the original, unreferenced mapping. To the extreme, if an identity mapping were optimal, it would be easier to push the residual to zero than to fit an identity mapping by a stack of nonlinear layers.""" ;
    skos:prefLabel "Residual Connection" .

:ResidualGRU a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1608.05148v2> ;
    skos:definition "A **Residual GRU** is a [gated recurrent unit (GRU)](https://paperswithcode.com/method/gru) that incorporates the idea of residual connections from [ResNets](https://paperswithcode.com/method/resnet)." ;
    skos:prefLabel "Residual GRU" .

:ResidualNormalDistribution a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2007.03898v3> ;
    skos:definition "**Residual Normal Distributions** are used to help the optimization of VAEs, preventing optimization from entering an unstable region. This can happen due to sharp gradients caused in situations where the encoder and decoder produce distributions far away from each other. The residual distribution parameterizes $q\\left(\\mathbf{z}|\\mathbf{x}\\right)$ relative to $p\\left(\\mathbf{z}\\right)$. Let $p\\left(z^{i}\\_{l}|\\mathbf{z}\\_{<l}\\right) := N \\left(\\mu\\_{i}\\left(\\mathbf{z}\\_{<l}\\right), \\sigma\\_{i}\\left(\\mathbf{z}\\_{<l}\\right)\\right)$ be a Normal distribution for the $i$th variable in $\\mathbf{z}\\_{l}$ in prior. Define $q\\left(z^{i}\\_{l}|\\mathbf{z}\\_{<l}, x\\right) := N\\left(\\mu\\_{i}\\left(\\mathbf{z}\\_{<l}\\right) + \\Delta\\mu\\_{i}\\left(\\mathbf{z}\\_{<l}, x\\right), \\sigma\\_{i}\\left(\\mathbf{z}\\_{<l}\\right) \\cdot \\Delta\\sigma\\_{i}\\left(\\mathbf{z}\\_{<l}, x\\right) \\right)$, where $\\Delta\\mu\\_{i}\\left(\\mathbf{z}\\_{<l}, \\mathbf{x}\\right)$ and $\\Delta\\sigma\\_{i}\\left(\\mathbf{z}\\_{<l}, \\mathbf{x}\\right)$ are the relative location and scale of the approximate posterior with respect to the prior. With this parameterization, when the prior moves, the approximate posterior moves accordingly, if not changed." ;
    skos:prefLabel "Residual Normal Distribution" .

:ResidualSRM a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1903.10829v1> ;
    rdfs:seeAlso <https://github.com/EvgenyKashin/SRMnet/blob/a7ebdbe47a489c3e604ef44a84d361ef7b679e17/models/layer_blocks.py#L27> ;
    skos:definition "A **Residual SRM** is a module for convolutional neural networks that uses a [Style-based Recalibration Module](https://paperswithcode.com/method/style-based-recalibration-module) within a [residual block](https://paperswithcode.com/method/residual-block) like structure. The Style-based Recalibration Module (SRM) adaptively recalibrates intermediate feature maps by exploiting their styles." ;
    skos:prefLabel "Residual SRM" .

:RestrictedBoltzmannMachine a skos:Concept ;
    skos:definition """**Restricted Boltzmann Machines**, or **RBMs**, are two-layer generative neural networks that learn a probability distribution over the inputs. They are a special class of Boltzmann Machine in that they have a restricted number of connections between visible and hidden units. Every node in the visible layer is connected to every node in the hidden layer, but no nodes in the same group are connected. RBMs are usually trained using the contrastive divergence learning procedure.\r
\r
Image Source: [here](https://medium.com/datatype/restricted-boltzmann-machine-a-complete-analysis-part-1-introduction-model-formulation-1a4404873b3)""" ;
    skos:prefLabel "Restricted Boltzmann Machine" .

:RetinaMask a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1901.03353v1> ;
    rdfs:seeAlso <https://github.com/chengyangfu/retinamask> ;
    skos:definition "**RetinaMask** is a one-stage object detection method that improves upon [RetinaNet](https://paperswithcode.com/method/retinanet) by adding the task of instance mask prediction during training, as well as an [adaptive loss](https://paperswithcode.com/method/adaptive-loss) that improves robustness to parameter choice during training, and including more difficult examples in training." ;
    skos:prefLabel "RetinaMask" .

:RetinaNet a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1708.02002v2> ;
    rdfs:seeAlso <https://github.com/facebookresearch/Detectron/blob/8170b25b425967f8f1c7d715bea3c5b8d9536cd8/detectron/modeling/retinanet_heads.py> ;
    skos:definition """**RetinaNet** is a one-stage object detection model that utilizes a [focal loss](https://paperswithcode.com/method/focal-loss) function to address class imbalance during training. Focal loss applies a modulating term to the cross entropy loss in order to focus learning on hard negative examples. RetinaNet is a single, unified network composed of a *backbone* network and two task-specific *subnetworks*. The backbone is responsible for computing a convolutional feature map over an entire input image and is an off-the-self convolutional network. The first subnet performs convolutional object classification on the backbone's output; the second subnet performs convolutional bounding box regression. The two subnetworks feature a simple design that the authors propose specifically for one-stage, dense detection. \r
\r
We can see the motivation for focal loss by comparing with two-stage object detectors. Here class imbalance is addressed by a two-stage cascade and sampling heuristics. The proposal stage (e.g., [Selective Search](https://paperswithcode.com/method/selective-search), [EdgeBoxes](https://paperswithcode.com/method/edgeboxes), [DeepMask](https://paperswithcode.com/method/deepmask), [RPN](https://paperswithcode.com/method/rpn)) rapidly narrows down the number of candidate object locations to a small number (e.g., 1-2k), filtering out most background samples. In the second classification stage, sampling heuristics, such as a fixed foreground-to-background ratio, or online hard example mining ([OHEM](https://paperswithcode.com/method/ohem)), are performed to maintain a\r
manageable balance between foreground and background.\r
\r
In contrast, a one-stage detector must process a much larger set of candidate object locations regularly sampled across an image. To tackle this, RetinaNet uses a focal loss function, a dynamically scaled cross entropy loss, where the scaling factor decays to zero as confidence in the correct class increases. Intuitively, this scaling factor can automatically down-weight the contribution of easy examples during training and rapidly focus the model on hard examples. \r
\r
Formally, the Focal Loss adds a factor $(1 - p\\_{t})^\\gamma$ to the standard cross entropy criterion. Setting $\\gamma>0$ reduces the relative loss for well-classified examples ($p\\_{t}>.5$), putting more focus on hard, misclassified examples. Here there is tunable *focusing* parameter $\\gamma \\ge 0$. \r
\r
$$ {\\text{FL}(p\\_{t}) = - (1 - p\\_{t})^\\gamma \\log\\left(p\\_{t}\\right)} $$""" ;
    skos:prefLabel "RetinaNet" .

:RetinaNet-RS a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2107.00057v1> ;
    skos:definition "**RetinaNet-RS** is an object detection model produced through a model scaling method based on changing the the input resolution and [ResNet](https://paperswithcode.com/method/resnet) backbone depth. For [RetinaNet](https://paperswithcode.com/method/retinanet), we scale up input resolution from 512 to 768 and the ResNet backbone depth from 50 to 152. As RetinaNet performs dense one-stage object detection, the authors find scaling up input resolution leads to large resolution feature maps hence more anchors to process. This results in a higher capacity dense prediction heads and expensive NMS. Scaling stops at input resolution 768 × 768 for RetinaNet." ;
    skos:prefLabel "RetinaNet-RS" .

:Retrace a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1606.02647v2> ;
    skos:definition """**Retrace** is an off-policy Q-value estimation algorithm which has guaranteed convergence for a target and behaviour policy $\\left(\\pi, \\beta\\right)$. With off-policy rollout for TD learning, we must use importance sampling for the update:\r
\r
$$ \\Delta{Q}^{\\text{imp}}\\left(S\\_{t}, A\\_{t}\\right) = \\gamma^{t}\\prod\\_{1\\leq{\\tau}\\leq{t}}\\frac{\\pi\\left(A\\_{\\tau}\\mid{S\\_{\\tau}}\\right)}{\\beta\\left(A\\_{\\tau}\\mid{S\\_{\\tau}}\\right)}\\delta\\_{t} $$\r
\r
This product term can lead to high variance, so Retrace modifies $\\Delta{Q}$ to have importance weights truncated by no more than a constant $c$:\r
\r
$$ \\Delta{Q}^{\\text{imp}}\\left(S\\_{t}, A\\_{t}\\right) = \\gamma^{t}\\prod\\_{1\\leq{\\tau}\\leq{t}}\\min\\left(c, \\frac{\\pi\\left(A\\_{\\tau}\\mid{S\\_{\\tau}}\\right)}{\\beta\\left(A\\_{\\tau}\\mid{S\\_{\\tau}}\\right)}\\right)\\delta\\_{t} $$""" ;
    skos:prefLabel "Retrace" .

:RevNet a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1707.04585v1> ;
    rdfs:seeAlso <https://github.com/osmr/imgclsmob/blob/68335927ba27f2356093b985bada0bc3989836b1/pytorch/pytorchcv/models/revnet.py#L319> ;
    skos:definition """A **Reversible Residual Network**, or **RevNet**, is a variant of a [ResNet](https://paperswithcode.com/method/resnet) where each layer’s activations can be reconstructed exactly from the next layer’s. Therefore, the activations for most layers need not be stored in memory during backpropagation. The result is a network architecture whose activation storage requirements are independent of depth, and typically at least an order of magnitude smaller compared with equally sized ResNets.\r
\r
RevNets are composed of a series of reversible blocks. Units in each layer are partitioned into two groups, denoted $x\\_{1}$ and $x\\_{2}$; the authors find what works best is partitioning the channels. Each reversible block takes inputs $\\left(x\\_{1}, x\\_{2}\\right)$ and produces outputs $\\left(y\\_{1}, y\\_{2}\\right)$ according to the following additive coupling rules – inspired the transformation in [NICE](https://paperswithcode.com/method/nice) (nonlinear independent components estimation) – and residual functions $F$ and $G$ analogous to those in standard ResNets:\r
\r
$$y\\_{1} = x\\_{1} + F\\left(x\\_{2}\\right)$$\r
$$y\\_{2} = x\\_{2} + G\\left(y\\_{1}\\right)$$\r
\r
Each layer’s activations can be reconstructed from the next layer’s activations as follows:\r
\r
$$ x\\_{2} = y\\_{2} − G\\left(y\\_{1}\\right)$$\r
$$ x\\_{1} = y\\_{1} − F\\left(x\\_{2}\\right)$$\r
\r
Note that unlike residual blocks, reversible blocks must have a stride of 1 because otherwise the layer\r
discards information, and therefore cannot be reversible. Standard ResNet architectures typically\r
have a handful of layers with a larger stride. If we define a RevNet architecture analogously, the\r
activations must be stored explicitly for all non-reversible layers.""" ;
    skos:prefLabel "RevNet" .

:RevSilo a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2206.14098v2> ;
    skos:definition "Invertible multi-input multi-output coupling module. In RevBiFPN it is used as a bidirectional multi-scale feature pyramid fusion module that is invertible." ;
    skos:prefLabel "RevSilo" .

:ReversibleResidualBlock a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1707.04585v1> ;
    rdfs:seeAlso <https://github.com/osmr/imgclsmob/blob/68335927ba27f2356093b985bada0bc3989836b1/pytorch/pytorchcv/models/revnet.py#L99> ;
    skos:definition """**Reversible Residual Blocks** are skip-connection blocks that learn *reversible* residual functions with reference to the layer inputs. It is proposed as part of the [RevNet](https://paperswithcode.com/method/revnet) CNN architecture. Units in each layer are partitioned into two groups, denoted $x\\_{1}$ and $x\\_{2}$; the authors find what works best is partitioning the channels. Each reversible block takes inputs $\\left(x\\_{1}, x\\_{2}\\right)$ and produces outputs $\\left(y\\_{1}, y\\_{2}\\right)$ according to the following additive coupling rules – inspired by the transformation in [NICE](https://paperswithcode.com/method/nice) (nonlinear independent components estimation) – and residual functions $F$ and $G$ analogous to those in standard [ResNets](https://paperswithcode.com/method/resnet):\r
\r
$$y\\_{1} = x\\_{1} + F\\left(x\\_{2}\\right)$$\r
$$y\\_{2} = x\\_{2} + G\\left(y\\_{1}\\right)$$\r
\r
Each layer’s activations can be reconstructed from the next layer’s activations as follows:\r
\r
$$ x\\_{2} = y\\_{2} − G\\left(y\\_{1}\\right)$$\r
$$ x\\_{1} = y\\_{1} − F\\left(x\\_{2}\\right)$$""" ;
    skos:prefLabel "Reversible Residual Block" .

:RevisionNetwork a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2104.05376v2> ;
    skos:definition """**Revision Network** is a style transfer module that aims to revise the rough stylized image via generating residual details image $r_{c s}$, while the final stylized image is generated by combining $r\\_{c s}$ and rough stylized image $\\bar{x}\\_{c s}$. This procedure ensures that the distribution of global style pattern in $\\bar{x}\\_{c s}$ is properly kept. Meanwhile, learning to revise local style patterns with residual details image is easier for the Revision Network.\r
\r
As shown in the Figure, the Revision Network is designed as a simple yet effective encoder-decoder architecture, with only one down-sampling and one up-sampling layer. Further, a [patch discriminator](https://paperswithcode.com/method/patchgan) is used to help Revision Network to capture fine patch textures under adversarial learning setting. The patch discriminator $D$ is defined following SinGAN, where $D$ owns 5 convolution layers and 32 hidden channels. A relatively shallow $D$ is chosen to (1) avoid overfitting since we only have one style image and (2) control the receptive field to ensure D can only capture local patterns.""" ;
    skos:prefLabel "Revision Network" .

:RoBERTa a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1907.11692v1> ;
    skos:definition """**RoBERTa** is an extension of [BERT](https://paperswithcode.com/method/bert) with changes to the pretraining procedure. The modifications include: \r
\r
- training the model longer, with bigger batches, over more data\r
- removing the next sentence prediction objective\r
- training on longer sequences\r
- dynamically changing the masking pattern applied to the training data. The authors also collect a large new dataset ($\\text{CC-News}$) of comparable size to other privately used datasets, to better control for training set size effects""" ;
    skos:prefLabel "RoBERTa" .

:RoIAlign a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1703.06870v3> ;
    rdfs:seeAlso <https://github.com/facebookresearch/detectron2/blob/bb9f5d8e613358519c9865609ab3fe7b6571f2ba/detectron2/layers/roi_align.py#L51> ;
    skos:definition "**Region of Interest Align**, or **RoIAlign**, is an operation for extracting a small feature map from each RoI in detection and segmentation based tasks. It removes the harsh quantization of [RoI Pool](https://paperswithcode.com/method/roi-pooling), properly *aligning* the extracted features with the input. To avoid any quantization of the RoI boundaries or bins (using $x/16$ instead of $[x/16]$), RoIAlign uses bilinear interpolation to compute the exact values of the input features at four regularly sampled locations in each RoI bin, and the result is then aggregated (using max or average)." ;
    skos:prefLabel "RoIAlign" .

:RoIPool a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1311.2524v5> ;
    rdfs:seeAlso <https://github.com/pytorch/vision/blob/5e9ebe8dadc0ea2841a46cfcd82a93b4ce0d4519/torchvision/ops/roi_pool.py#L10> ;
    skos:definition """**Region of Interest Pooling**, or **RoIPool**, is an operation for extracting a small feature map (e.g., $7×7$) from each RoI in detection and segmentation based tasks. Features are extracted from each candidate box, and thereafter in models like [Fast R-CNN](https://paperswithcode.com/method/fast-r-cnn), are then classified and bounding box regression performed.\r
\r
The actual scaling to, e.g., $7×7$, occurs by dividing the region proposal into equally sized sections, finding the largest value in each section, and then copying these max values to the output buffer. In essence, **RoIPool** is [max pooling](https://paperswithcode.com/method/max-pooling) on a discrete grid based on a box.\r
\r
Image Source: [Joyce Xu](https://towardsdatascience.com/deep-learning-for-object-detection-a-comprehensive-review-73930816d8d9)""" ;
    skos:prefLabel "RoIPool" .

:RoITanh-polarTransform a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2102.02717v3> ;
    skos:definition "" ;
    skos:prefLabel "RoI Tanh-polar Transform" .

:RoIWarp a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1512.04412v1> ;
    skos:definition "**Region of Interest Warping**, or **RoIWarp**, is a form of [RoIPool](https://paperswithcode.com/method/roi-pooling) that is differentiable with respect to the box position. In practice, this takes the form of a RoIWarp layer followed by a standard [Max Pooling](https://paperswithcode.com/method/max-pooling) layer. The RoIWarp layer crops a feature map region and warps it into a target size by interpolation." ;
    skos:prefLabel "RoIWarp" .

:RobustPredictableControl a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2109.03214v1> ;
    skos:definition "**Robust Predictable Control**, or **RPC**, is an RL algorithm for learning policies that uses only a few bits of information. RPC brings together ideas from information bottlenecks, model-based RL, and bits-back coding. The main idea of RPC is that if the agent can accurately predict the future, then the agent will not need to observe as many bits from future observations. Precisely, the agent will learn a latent dynamics model that predicts the next representation using the current representation and action. In addition to predicting the future, the agent can also decrease the number of bits by changing its behavior. States where the dynamics are hard to predict will require more bits, so the agent will prefer visiting states where its learned model can accurately predict the next state." ;
    skos:prefLabel "Robust Predictable Control" .

:RotNet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2012.01985v2> ;
    skos:definition """**RotNet** is a self-supervision approach that relies on predicting image rotations as the pretext task\r
in order to learn image representations.""" ;
    skos:prefLabel "RotNet" .

:RotaryEmbeddings a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2104.09864v4> ;
    skos:altLabel "Rotary Position Embedding" ;
    skos:definition "**Rotary Position Embedding**, or **RoPE**, is a type of position embedding which encodes absolute positional information with rotation matrix and naturally incorporates explicit relative position dependency in self-attention formulation. Notably, RoPE comes with valuable properties such as flexibility of being expand to any sequence lengths, decaying inter-token dependency with increasing relative distances, and capability of equipping the linear self-attention with relative position encoding." ;
    skos:prefLabel "Rotary Embeddings" .

:RotatE a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1902.10197v1> ;
    skos:definition "**RotatE** is a method for generating graph embeddings which is able to model and infer various relation patterns including: symmetry/antisymmetry, inversion, and composition. Specifically, the RotatE model defines each relation as a rotation from the source entity to the target entity in the complex vector space. The RotatE model is trained using a [self-adversarial negative sampling](https://paperswithcode.com/method/self-adversarial-negative-sampling) technique." ;
    skos:prefLabel "RotatE" .

:RoutingAttention a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2003.05997v5> ;
    rdfs:seeAlso <https://github.com/lucidrains/routing-transformer/blob/bb572b6efe533ac30f30e495fc41e0454b21736e/routing_transformer/routing_transformer.py#L247> ;
    skos:definition """**Routed Attention** is an attention pattern proposed as part of the [Routing Transformer](https://paperswithcode.com/method/routing-transformer) architecture.  Each attention module\r
considers a clustering of the space: the current timestep only attends to context belonging to the same cluster. In other word, the current time-step query is routed to a limited number of context through its cluster assignment. This can be contrasted with [strided](https://paperswithcode.com/method/strided-attention) attention patterns and those proposed with the [Sparse Transformer](https://paperswithcode.com/method/sparse-transformer).\r
\r
In the image to the right, the rows represent the outputs while the columns represent the inputs. The different colors represent cluster memberships for the output token.""" ;
    skos:prefLabel "Routing Attention" .

:RoutingTransformer a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2003.05997v5> ;
    skos:definition "The **Routing Transformer** is a [Transformer](https://paperswithcode.com/method/transformer) that endows self-attention with a sparse routing module based on online k-means. Each attention module considers a clustering of the space: the current timestep only attends to context belonging to the same cluster. In other word, the current time-step query is routed to a limited number of context through its cluster assignment." ;
    skos:prefLabel "Routing Transformer" .

:S-GCN a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2011.07980v2> ;
    skos:altLabel "Spherical Graph Convolutional Network" ;
    skos:definition "" ;
    skos:prefLabel "S-GCN" .

:SABL a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1912.04260v2> ;
    skos:altLabel "Side-Aware Boundary Localization" ;
    skos:definition """**Side-Aware Boundary Localization (SABL)** is a methodology for precise localization in object detection where each side of the bounding box is respectively localized with a dedicated network branch. Empirically, the authors observe that when they manually annotate a bounding box for an object, it is often much easier to align each side of the box to the object boundary than to move the\r
box as a whole while tuning the size. Inspired by this observation, in SABL each side of the bounding box is respectively positioned based on its surrounding context. \r
\r
As shown in the Figure, the authors devise a bucketing scheme to improve the localization precision. For each side of a bounding box, this scheme divides the target space into multiple buckets, then determines the bounding box via two steps. Specifically, it first searches for the correct bucket, i.e., the one in which the boundary resides. Leveraging the centerline of the selected buckets as a\r
coarse estimate, fine regression is then performed by predicting the offsets. This scheme allows very precise localization even in the presence of displacements with large variance. Moreover, to preserve precisely localized bounding boxes in the non-maximal suppression procedure, the authors also propose to adjust the classification score based on the bucketing confidences, which leads to further performance gains.""" ;
    skos:prefLabel "SABL" .

:SAC a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2006.02334v2> ;
    skos:altLabel "Switchable Atrous Convolution" ;
    skos:definition "**Switchable Atrous Convolution (SAC)** softly switches the convolutional computation between different atrous rates and gathers the results using switch functions. The switch functions are spatially dependent, i.e., each location of the feature map might have different switches to control the outputs of SAC. To use SAC in a detector, we convert all the standard 3x3 convolutional layers in the bottom-up backbone to SAC." ;
    skos:prefLabel "SAC" .

:SAFRAN a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2012.05750v1> ;
    skos:altLabel "SAFRAN - Scalable and fast non-redundant rule application" ;
    skos:definition "SAFRAN is a rule application framework which aggregates rules through a scalable clustering algorithm." ;
    skos:prefLabel "SAFRAN" .

:SAG a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2210.00939v6> ;
    skos:altLabel "Self-Attention Guidance" ;
    skos:definition "" ;
    skos:prefLabel "SAG" .

:SAGA a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1407.0202v3> ;
    skos:definition "SAGA is a method in the spirit of SAG, SDCA, MISO and SVRG, a set of recently proposed incremental gradient algorithms with fast linear convergence rates. SAGA improves on the theory behind SAG and SVRG, with better theoretical convergence rates, and has support for composite objectives where a proximal operator is used on the regulariser. Unlike SDCA, SAGA supports non-strongly convex problems directly, and is adaptive to any inherent strong convexity of the problem." ;
    skos:prefLabel "SAGA" .

:SAGAN a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1805.08318v2> ;
    rdfs:seeAlso <https://github.com/heykeetae/Self-Attention-GAN/blob/master/sagan_models.py> ;
    skos:altLabel "Self-Attention GAN" ;
    skos:definition "The **Self-Attention Generative Adversarial Network**, or **SAGAN**, allows for attention-driven, long-range dependency modeling for image generation tasks. Traditional convolutional GANs generate high-resolution details as a function of only spatially local points in lower-resolution feature maps. In SAGAN, details can be generated using cues from all feature locations. Moreover, the discriminator can check that highly detailed features in distant portions of the image are consistent with each other." ;
    skos:prefLabel "SAGAN" .

:SAGANSelf-AttentionModule a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1805.08318v2> ;
    rdfs:seeAlso <https://github.com/heykeetae/Self-Attention-GAN/blob/8714a54ba5027d680190791ba3a6bb08f9c9a129/sagan_models.py#L8> ;
    skos:definition """The **SAGAN Self-Attention Module** is a self-attention module used in the [Self-Attention GAN](https://paperswithcode.com/method/sagan) architecture for image synthesis. In the module, image features from the previous hidden layer $\\textbf{x} \\in \\mathbb{R}^{C\\text{x}N}$ are first transformed into two feature spaces $\\textbf{f}$, $\\textbf{g}$ to calculate the attention, where $\\textbf{f(x) = W}\\_{\\textbf{f}}{\\textbf{x}}$, $\\textbf{g}(\\textbf{x})=\\textbf{W}\\_{\\textbf{g}}\\textbf{x}$. We then calculate:\r
\r
$$\\beta_{j, i} = \\frac{\\exp\\left(s_{ij}\\right)}{\\sum^{N}\\_{i=1}\\exp\\left(s_{ij}\\right)} $$\r
\r
$$ \\text{where } s_{ij} = \\textbf{f}(\\textbf{x}\\_{i})^{T}\\textbf{g}(\\textbf{x}\\_{i}) $$\r
\r
and $\\beta_{j, i}$ indicates the extent to which the model attends to the $i$th location when synthesizing the $j$th region. Here, $C$ is the number of channels and $N$ is the number of feature\r
locations of features from the previous hidden layer. The output of the attention layer is $\\textbf{o} = \\left(\\textbf{o}\\_{\\textbf{1}}, \\textbf{o}\\_{\\textbf{2}}, \\ldots, \\textbf{o}\\_{\\textbf{j}} , \\ldots, \\textbf{o}\\_{\\textbf{N}}\\right) \\in \\mathbb{R}^{C\\text{x}N}$ , where,\r
\r
$$ \\textbf{o}\\_{\\textbf{j}} = \\textbf{v}\\left(\\sum^{N}\\_{i=1}\\beta_{j, i}\\textbf{h}\\left(\\textbf{x}\\_{\\textbf{i}}\\right)\\right) $$\r
\r
$$ \\textbf{h}\\left(\\textbf{x}\\_{\\textbf{i}}\\right) = \\textbf{W}\\_{\\textbf{h}}\\textbf{x}\\_{\\textbf{i}} $$\r
\r
$$ \\textbf{v}\\left(\\textbf{x}\\_{\\textbf{i}}\\right) = \\textbf{W}\\_{\\textbf{v}}\\textbf{x}\\_{\\textbf{i}} $$\r
\r
In the above formulation, $\\textbf{W}\\_{\\textbf{g}} \\in \\mathbb{R}^{\\bar{C}\\text{x}C}$, $\\mathbf{W}\\_{f} \\in \\mathbb{R}^{\\bar{C}\\text{x}C}$, $\\textbf{W}\\_{\\textbf{h}} \\in \\mathbb{R}^{\\bar{C}\\text{x}C}$ and $\\textbf{W}\\_{\\textbf{v}} \\in \\mathbb{R}^{C\\text{x}\\bar{C}}$ are the learned weight matrices, which are implemented as $1$×$1$ convolutions. The authors choose  $\\bar{C} = C/8$.\r
\r
In addition, the module further multiplies the output of the attention layer by a scale parameter and adds back the input feature map. Therefore, the final output is given by,\r
\r
$$\\textbf{y}\\_{\\textbf{i}} = \\gamma\\textbf{o}\\_{\\textbf{i}} + \\textbf{x}\\_{\\textbf{i}}$$\r
\r
where $\\gamma$ is a learnable scalar and it is initialized as 0. Introducing $\\gamma$ allows the network to first rely on the cues in the local neighborhood – since this is easier – and then gradually learn to assign more weight to the non-local evidence.""" ;
    skos:prefLabel "SAGAN Self-Attention Module" .

:SAINT a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2106.01342v1> ;
    skos:definition """**SAINT** is a hybrid deep learning approach to solving tabular data problems. SAINT performs attention over both rows and columns, and it includes an enhanced embedding method. The architecture, pre-training and training pipeline are as follows: \r
\r
- $L$ layers with 2 attention blocks each, one self-attention block, and a novel intersample attention blocks that computes attention across samples are used.\r
- For pre-training, this involves minimizing the contrastive and denoising losses between a given data point and its views generated by [CutMix](https://paperswithcode.com/method/cutmix) and [mixup](https://paperswithcode.com/method/mixup). During finetuning/regular training, data passes through an embedding layer and then the SAINT model. Lastly, the contextual embeddings from SAINT are used to pass only the embedding corresponding to the CLS token through an [MLP](https://paperswithcode.com/method/feedforward-network) to obtain the final prediction.""" ;
    skos:prefLabel "SAINT" .

:SANet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2004.13621v1> ;
    skos:altLabel "Self-Attention Network" ;
    skos:definition "**Self-Attention Network** (**SANet**) proposes two variations of self-attention used for image recognition: 1) pairwise self-attention which generalizes standard [dot-product attention](https://paperswithcode.com/method/dot-product-attention) and is fundamentally a set operator, and 2) patchwise self-attention which is strictly more powerful than [convolution](https://paperswithcode.com/method/convolution)." ;
    skos:prefLabel "SANet" .

:SASA a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1906.05909v1> ;
    skos:altLabel "Stand-Alone Self Attention" ;
    skos:definition "**Stand-Alone Self Attention** (SASA) replaces all instances of spatial [convolution](https://paperswithcode.com/method/convolution) with a form of self-attention applied to [ResNet](https://paperswithcode.com/method/resnet) producing a fully, stand-alone self-attentional model." ;
    skos:prefLabel "SASA" .

:SBERT a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1908.10084v1> ;
    skos:altLabel "Sentence-BERT" ;
    skos:definition "" ;
    skos:prefLabel "SBERT" .

:SC-GPT a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2002.12328v1> ;
    skos:definition "**SC-GPT** is a multi-layer [Transformer](http://paperswithcode.com/method/transformer) neural language model, trained in three steps: (i) Pre-trained on plain text, similar to [GPT-2](http://paperswithcode.com/method/gpt-2); (ii) Continuously pretrained on large amounts of dialog-act labeled utterances corpora to acquire the ability of controllable generation; (iii) Fine-tuned for a target domain using very limited amounts of domain labels. Unlike [GPT-2](http://paperswithcode.com/method/gpt-2), SC-GPT generates semantically controlled responses that are conditioned on the given semantic form, similar to SC-[LSTM](https://paperswithcode.com/method/lstm) but requiring much less domain labels to generalize to new domains. It is pre-trained on a large set of annotated NLG corpus to acquire the controllable generation ability, and fine-tuned with only a few domain-specific labels to adapt to new domains." ;
    skos:prefLabel "SC-GPT" .

:SCA a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2210.04883v1> ;
    skos:altLabel "Semantic Cross Attention" ;
    skos:definition """Semantic Cross Attention (SCA) is based on cross attention, which we restrict with respect to a semantic mask.\r
\r
The goal of SCA is two-fold depending on what is the query and what is the key. Either it allows to give the feature map information from a semantically restricted set of latents or, respectively, it allows a set of latents to retrieve information in a semantically restricted region of the feature map. \r
\r
SCA is defined as:  \r
\r
\\begin{equation}\r
    \\text{SCA}(I_{1}, I_{2}, I_{3}) = \\sigma\\left(\\frac{QK^T\\odot I_{3} +\\tau \\left(1-I_{3}\\right)}{\\sqrt{d_{in}}}\\right)V \\quad ,\r
\\end{equation}\r
\r
where $I_{1},I_{2},I_{3}$ the inputs, with $I_{1}$ attending $I_{2}$, and $I_{3}$ the mask that forces tokens from $I_1$ to attend only specific tokens from $I_2$. The attention values requiring masking are filled with $-\\infty$ before the softmax. (In practice $\\tau{=}-10^9$),  $Q {=} W_QI_{1}$, $K {=} W_KI_{2}$ and $V {=} W_VI_{2}$ the queries, keys and values, and $d_{in}$ the internal attention dimension. $\\sigma(.)$ is the softmax operation.\r
\r
Let $X\\in\\mathbb{R}^{n\\times C}$ be the feature map with n the number of pixels, and C the number of channels. Let $Z\\in\\mathbb{R}^{m\\times d}$ be a set of $m$ latents of dimension $d$ and $s$ the number of semantic labels. Each semantic label is attributed $k$ latents, such that $m=k\\times s$. Each semantic label mask is assigned $k$ copies in $S{\\in}\\{0;1\\}^{n \\times m}$. \r
\r
We can differentiate 3 types of SCA:\r
\r
(a) SCA with pixels $X$ attending latents $Z$: $\\text{SCA}(X, Z, S)$, where $W_{Q} {\\in} \\mathbb{R}^{n\\times d_{in}}$ and $W_{K}, W_{V} {\\in} \\mathbb{R}^{m\\times d_{in}}$.\r
The idea is to force the pixels from a semantic region to attend latents that are associated with the same label. \r
\r
(b) SCA with latents $Z$ attending pixels $X$: $\\text{SCA}(Z, X, S)$, where $W_{Q}{\\in} \\mathbb{R}^{m\\times d_{in}}$, $W_{K}, W_{V} {\\in} \\mathbb{R}^{n\\times d_{in}}$. \r
The idea is to semantically mask attention values to enforce latents to attend semantically corresponding pixels.\r
\r
(c) SCA with latents $Z$ attending themselves: $\\text{SCA}(Z, Z, M)$, where $W_{Q}, W_{K}, W_{V} {\\in} \\mathbb{R}^{n\\times d_{in}}$. We denote $M\\in\\mathbb{N}^{m\\times m}$ this mask, with $M_{\\text{latents}}(i,j) {=} 1$ if the semantic label of latent $i$ is the same as the one of latent $j$; $0$ otherwise.\r
The idea is to let the latents only attend latents that share the same semantic label.""" ;
    skos:prefLabel "SCA" .

:SCA-CNN a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1611.05594v2> ;
    skos:altLabel "Spatial and Channel-wise Attention-based Convolutional Neural Network" ;
    skos:definition """As CNN features are naturally spatial, channel-wise and multi-layer, \r
Chen et al. proposed a novel spatial and channel-wise attention-based convolutional neural network (SCA-CNN). \r
It was designed for the task of image captioning, and uses an encoder-decoder framework where a CNN first encodes an input image into a vector and then an LSTM decodes the vector into a sequence of words. Given an input feature map $X$ and the previous time step LSTM hidden state $h_{t-1} \\in \\mathbb{R}^d$, a spatial attention mechanism pays more attention to the semantically useful regions, guided by LSTM hidden state $h_{t-1}$. The  spatial attention model is:\r
\r
\\begin{align}\r
a(h_{t-1}, X) &= \\tanh(Conv_1^{1 \\times 1}(X) \\oplus W_1 h_{t-1})\r
\\end{align}\r
\r
\\begin{align}\r
\\Phi_s(h_{t-1}, X) &= \\text{Softmax}(Conv_2^{1 \\times 1}(a(h_{t-1}, X)))    \r
\\end{align}\r
\r
where $\\oplus$ represents  addition of a matrix and a vector. Similarly, channel-wise attention aggregates global information first, and then computes a channel-wise attention weight vector with the hidden state $h_{t-1}$:\r
\\begin{align}\r
b(h_{t-1}, X) &= \\tanh((W_2\\text{GAP}(X)+b_2)\\oplus W_1h_{t-1})\r
\\end{align}\r
\\begin{align}\r
\\Phi_c(h_{t-1}, X) &= \\text{Softmax}(W_3(b(h_{t-1}, X))+b_3)    \r
\\end{align}\r
Overall, the  SCA mechanism can be written in one of two ways. If channel-wise attention is applied before spatial attention, we have\r
\\begin{align}\r
Y &= f(X,\\Phi_s(h_{t-1}, X \\Phi_c(h_{t-1}, X)), \\Phi_c(h_{t-1}, X)) \r
\\end{align}\r
and  if spatial attention comes first:\r
\\begin{align}\r
Y &= f(X,\\Phi_s(h_{t-1}, X), \\Phi_c(h_{t-1}, X \\Phi_s(h_{t-1}, X)))\r
\\end{align}\r
where $f(\\cdot)$ denotes the modulate function which takes the feature map $X$ and attention maps as input and then outputs the modulated feature map $Y$.\r
\r
Unlike previous attention mechanisms which consider each image region equally and use global spatial information to tell the network where to focus, SCA-Net leverages the semantic vector to produce the spatial attention map as well as the channel-wise attention weight vector. Being more than a powerful attention model, SCA-CNN also provides a better understanding of where and what the model should focus on during sentence generation.""" ;
    skos:prefLabel "SCA-CNN" .

:SCAN-clustering a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2005.12320v2> ;
    skos:altLabel "Semantic Clustering by Adopting Nearest Neighbours" ;
    skos:definition """SCAN automatically groups images into semantically meaningful clusters when ground-truth annotations are absent. SCAN is a two-step approach where feature learning and clustering are decoupled. First, a self-supervised task is employed to obtain semantically meaningful features. Second, the obtained features are used as a prior in a learnable clustering  approach.\r
\r
Image source: [Gansbeke et al.](https://arxiv.org/pdf/2005.12320v2.pdf)""" ;
    skos:prefLabel "SCAN-clustering" .

:SCARF a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2106.15147v2> ;
    skos:definition "SCARF is a simple, widely-applicable technique for contrastive learning, where views are formed by corrupting a random subset of features. When applied to pre-train deep neural networks on the 69 real-world, tabular classification datasets from the OpenML-CC18 benchmark, SCARF not only improves classification accuracy in the fully-supervised setting but does so also in the presence of label noise and in the semi-supervised setting where only a fraction of the available training data is labeled." ;
    skos:prefLabel "SCARF" .

:SCARLET a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1908.06022v6> ;
    rdfs:seeAlso <https://github.com/xiaomi-automl/SCARLET-NAS/blob/bfcbc1c7c82266c2350836ff3c577248be9b000b/models/Scarlet_A.py#L6> ;
    skos:definition "**SCARLET** is a type of convolutional neural architecture learnt by the [SCARLET-NAS](https://paperswithcode.com/method/scarlet-nas) [neural architecture search](https://paperswithcode.com/method/neural-architecture-search) method. The three variants are SCARLET-A, SCARLET-B and SCARLET-C. The basic building block is MBConvs from [MobileNetV2](https://paperswithcode.com/method/mobilenetv2). Squeeze-and-excitation layers are also experimented with." ;
    skos:prefLabel "SCARLET" .

:SCARLET-NAS a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1908.06022v6> ;
    rdfs:seeAlso <https://github.com/xiaomi-automl/SCARLET-NAS> ;
    skos:definition "**SCARLET-NAS** is a type of [neural architecture search](https://paperswithcode.com/method/neural-architecture-search) that utilises a learnable stabilizer to calibrate feature deviation, named the Equivariant Learnable Stabilizer (ELS). Previous one-shot approaches can be limited by fixed-depth search spaces. With SCARLET-NAS, we use the equivariant learnable stabilizer on each skip connection. This can lead to improved convergence, more reliable evaluation, and retained equivalence. The third benefit is deemed most important by the authors for scalability." ;
    skos:prefLabel "SCARLET-NAS" .

:SCCL a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2103.12953v2> ;
    skos:altLabel "Supporting Clustering with Contrastive Learning" ;
    skos:definition "**SCCL**, or **Supporting Clustering with Contrastive Learning**, is a framework to leverage contrastive learning to promote better separation in unsupervised clustering. It combines the top-down clustering with the bottom-up instance-wise contrastive learning to achieve better inter-cluster distance and intra-cluster distance. During training, we jointly optimize a clustering loss over the original data instances and an instance-wise contrastive loss over the associated augmented pairs." ;
    skos:prefLabel "SCCL" .

:SCN a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2002.10392v2> ;
    skos:altLabel "Self-Cure Network" ;
    skos:definition "**Self-Cure Network**, or **SCN**, is a method for suppressing uncertainties for large-scale facial expression recognition, prventing deep networks from overfitting uncertain facial images. Specifically, SCN suppresses the uncertainty from two different aspects: 1) a self-attention mechanism over mini-batch to weight each training sample with a ranking regularization, and 2) a careful relabeling mechanism to modify the labels of these samples in the lowest-ranked group." ;
    skos:prefLabel "SCN" .

:SCNN_UNet_ConvLSTM a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2110.04079v5> ;
    skos:altLabel "Spatial CNN with UNet based Encoder-decoder and ConvLSTM" ;
    skos:definition "Spatial CNN with UNet based Encoder-decoder and ConvLSTM" ;
    skos:prefLabel "SCNN_UNet_ConvLSTM" .

:SCNet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2012.10150v1> ;
    skos:definition "**Sample Consistency Network (SCNet)** is a method for instance segmentation which ensures the IoU distribution of the samples at training time are as close to that at inference time. To this end, only the outputs of the last box stage are used for mask predictions at both training and inference. The Figure shows the IoU distribution of the samples going to the mask branch at training time with/without sample consistency compared to that at inference time." ;
    skos:prefLabel "SCNet" .

:SCST a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1612.00563v2> ;
    skos:altLabel "Self-critical Sequence Training" ;
    skos:definition "" ;
    skos:prefLabel "SCST" .

:SDAE a skos:Concept ;
    rdfs:seeAlso <https://github.com/lisa-lab/DeepLearningTutorials/blob/master/code/SdA.py> ;
    skos:altLabel "Stacked Denoising Autoencoder" ;
    skos:definition """The Stacked Denoising Autoencoder (SdA) is an extension of the stacked autoencoder [Bengio07] and it was introduced in [Vincent08].\r
\r
Denoising autoencoders can be stacked to form a deep network by feeding the latent representation (output code) of the [denoising autoencoder](https://paperswithcode.com/method/denoising-autoencoder) found on the layer below as input to the current layer. The unsupervised pre-training of such an architecture is done one layer at a time. Each layer is trained as a denoising autoencoder by minimizing the error in reconstructing its input (which is the output code of the previous layer). Once the first k layers are trained, we can train the k+1-th layer because we can now compute the code or latent representation from the layer below.\r
\r
Once all layers are pre-trained, the network goes through a second stage of training called fine-tuning. Here we consider supervised fine-tuning where we want to minimize prediction error on a supervised task. For this, we first add a [logistic regression](https://paperswithcode.com/method/logistic-regression) layer on top of the network (more precisely on the output code of the output layer). We then train the entire network as we would train a multilayer perceptron. At this point, we only consider the encoding parts of each auto-encoder. This stage is supervised, since now we use the target class during training. (See the Multilayer Perceptron for details on the multilayer perceptron.)\r
\r
This can be easily implemented in Theano, using the class defined previously for a denoising autoencoder. We can see the stacked denoising autoencoder as having two facades: a list of autoencoders, and an MLP. During pre-training we use the first facade, i.e., we treat our model as a list of autoencoders, and train each autoencoder seperately. In the second stage of training, we use the second facade. These two facades are linked because:\r
* the autoencoders and the sigmoid layers of the MLP share parameters, and\r
* the latent representations computed by intermediate layers of the MLP are fed as input to the autoencoders.\r
\r
Extracted from [webpage](http://deeplearning.net/tutorial/SdA.html)\r
\r
Image: [Jigar Bandaria](https://miro.medium.com/max/701/1*wbaL5CvUkVkZxlSUsRS5IQ.png)\r
\r
**Source**:\r
\r
Image: [Jigar Bandaria](https://blog.insightdatascience.com/brain-mri-image-segmentation-using-stacked-denoising-autoencoders-4e91417688f6)\r
\r
Webpage: [deeplearning.net](http://deeplearning.net/tutorial/SdA.html)\r
\r
Webpage: [www.iro.umontreal.ca](http://www.iro.umontreal.ca/~pift6266/H10/notes/SdA.html)\r
\r
Paper:\r
\r
[Vincent, H. Larochelle Y. Bengio and P.A. Manzagol, Extracting and Composing Robust Features with Denoising Autoencoders](https://doi.org/10.1145/1390156.1390294)\r
\r
[Vincent, H. Larochelle Y. Bengio and P.A. Manzagol, Extracting and Composing Robust Features with Denoising Autoencoders](http://www.iro.umontreal.ca/~lisa/publications2/index.php/publications/show/217)""" ;
    skos:prefLabel "SDAE" .

:SDNE a skos:Concept ;
    dcterms:source <https://www.kdd.org/kdd2016/papers/files/rfp0191-wangAemb.pdf> ;
    skos:altLabel "Structural Deep Network Embedding" ;
    skos:definition "" ;
    skos:prefLabel "SDNE" .

:SEAM a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2004.04581v1> ;
    skos:altLabel "Self-supervised Equivariant Attention Mechanism" ;
    skos:definition "**Self-supervised Equivariant Attention Mechanism**, or **SEAM**, is an attention mechanism for weakly supervised semantic segmentation. The SEAM applies consistency regularization on CAMs from various transformed images to provide self-supervision for network learning. To further improve the network prediction consistency, SEAM introduces the pixel correlation module (PCM), which captures context appearance information for each pixel and revises original CAMs by learned affinity attention maps. The SEAM is implemented by a [siamese network](https://paperswithcode.com/method/siamese-network) with equivariant cross regularization (ECR) loss, which regularizes the original CAMs and the revised CAMs on different branches." ;
    skos:prefLabel "SEAM" .

:SEEDRL a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1910.06591v2> ;
    skos:definition "**SEED** (Scalable, Efficient, Deep-RL) is a scalable reinforcement learning agent. It utilizes an architecture that features centralized inference and an optimized communication layer. SEED adopts two state of the art distributed algorithms, [IMPALA](https://paperswithcode.com/method/impala)/[V-trace](https://paperswithcode.com/method/v-trace) (policy gradients) and R2D2 ([Q-learning](https://paperswithcode.com/method/q-learning))." ;
    skos:prefLabel "SEED RL" .

:SEER a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2103.01988v2> ;
    skos:definition "**SEER** is a self-supervised learning approach for training large models on random, uncurated images with no supervision. It trains [RegNet-Y](https://paperswithcode.com/method/regnet-y) architectures with the [SwAV](https://paperswithcode.com/method/swav). Several adjustments are made to self-supervised training to make it work at a larger scale, including using a [cosine learning schedule](https://paperswithcode.com/method/cosine-annealing)" ;
    skos:prefLabel "SEER" .

:SELU a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1706.02515v5> ;
    rdfs:seeAlso <https://github.com/pytorch/pytorch/blob/96aaa311c0251d24decb9dc5da4957b7c590af6f/torch/nn/modules/activation.py#L507> ;
    skos:altLabel "Scaled Exponential Linear Unit" ;
    skos:definition """**Scaled Exponential Linear Units**, or **SELUs**, are activation functions that induce self-normalizing properties.\r
\r
The SELU activation function is given by \r
\r
$$f\\left(x\\right) = \\lambda{x} \\text{ if } x \\geq{0}$$\r
$$f\\left(x\\right) = \\lambda{\\alpha\\left(\\exp\\left(x\\right) -1 \\right)} \\text{ if } x < 0 $$\r
\r
with $\\alpha \\approx 1.6733$ and $\\lambda \\approx 1.0507$.""" ;
    skos:prefLabel "SELU" .

:SENet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1709.01507v4> ;
    rdfs:seeAlso <https://github.com/osmr/imgclsmob/blob/68335927ba27f2356093b985bada0bc3989836b1/pytorch/pytorchcv/models/senet.py#L167> ;
    skos:definition "A **SENet** is a convolutional neural network architecture that employs squeeze-and-excitation blocks to enable the network to perform dynamic channel-wise feature recalibration." ;
    skos:prefLabel "SENet" .

:SERLU a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1807.10117v2> ;
    rdfs:seeAlso <https://github.com/mattresnick/Custom-PyTorch-Resources/blob/2e3b05d7e0f75cea22902e9d62ed9869ce4c1c70/ActivationFunctions.py#L10> ;
    skos:definition """**SERLU**, or **Scaled Exponentially-Regularized Linear Unit**, is a type of activation function. The new function introduces a bump-shaped function in the region of negative input. The bump-shaped function has approximately zero response to large negative input while being able to push the output of SERLU towards zero mean statistically.\r
\r
$$ \\text{SERLU}\\left(x\\right)) = \\lambda\\_{serlu}x \\text{ if } x \\geq 0 $$\r
$$ \\text{SERLU}\\left(x\\right)) = \\lambda\\_{serlu}\\alpha\\_{serlu}xe^{x} \\text{ if } x < 0 $$\r
\r
where the two parameters $\\lambda\\_{serlu} > 0$ and $\\alpha\\_{serlu} > 0$ remain to be specified.""" ;
    skos:prefLabel "SERLU" .

:SESAMEDiscriminator a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2004.04977v2> ;
    skos:definition "Extends [PatchGAN](https://paperswithcode.com/method/patchgan) discriminator for the task of layout2image generation. The discriminator is comprised of two processing streams: one for the RGB image and one for its semantics, which are fused together at the later stages of the discriminator." ;
    skos:prefLabel "SESAME Discriminator" .

:SETR a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2012.15840v3> ;
    skos:altLabel "Segmentation Transformer" ;
    skos:definition """**Segmentation Transformer**, or **SETR**, is a [Transformer](https://paperswithcode.com/methods/category/transformers)-based segmentation model. The transformer-alone encoder treats an input image as a sequence of image patches represented by learned patch embedding, and transforms the sequence with global self-attention modeling for discriminative feature representation learning. Concretely, we first decompose an image into a grid of fixed-sized patches, forming a sequence of patches. With a linear embedding layer applied to the flattened pixel vectors of every patch, we then obtain a sequence of feature embedding vectors as the input to a transformer. Given the learned features from the encoder\r
transformer, a decoder is then used to recover the original image resolution. Crucially there is no downsampling in spatial resolution but global context modeling at every layer of the encoder transformer.""" ;
    skos:prefLabel "SETR" .

:SETSe a skos:Concept ;
    skos:altLabel "Strain Elevation Tension Spring embedding" ;
    skos:definition """SETSe is a deterministic physics based graph embedding algorithm. It embeds weighted feature rich networks. It treats each edge as a spring and each node as a bead whose movement is constrained by the graph adjacency matrix so that the nodes move in parallel planes enforcing a minimum distance between neighboring nodes. The node features act as forces moving the nodes up and down. The network converges to the embedded state when the force produced by each node is equal and opposite to the sum of the forces exerted by its edges, creating a net force of 0.\r
\r
SETSe has no conventional loss function and does not attempt to place similar nodes close to each other.""" ;
    skos:prefLabel "SETSe" .

:SFAM a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1811.04533v3> ;
    rdfs:seeAlso <https://github.com/qijiezhao/M2Det/blob/ade4f3d12979800c367bf1e46d2e316e73a87514/layers/nn_utils.py#L133> ;
    skos:altLabel "Scale-wise Feature Aggregation Module" ;
    skos:definition """**SFAM**, or **Scale-wise Feature Aggregation Module**, is a feature extraction block from the [M2Det](https://paperswithcode.com/method/m2det) architecture. It aims to aggregate the multi-level multi-scale features generated by [Thinned U-Shaped Modules](https://paperswithcode.com/method/tum) into a multi-level feature pyramid. \r
\r
The first stage of SFAM is to concatenate features of the equivalent scale together along the channel dimension. The aggregated feature pyramid can be presented as $\\mathbf{X} =[\\mathbf{X}\\_1,\\mathbf{X}\\_2,\\dots,\\mathbf{X}\\_i]$, where $\\mathbf{X}\\_i = \\text{Concat}(\\mathbf{x}\\_i^1,\\mathbf{x}\\_i^2,\\dots,\\mathbf{x}\\_i^L) \\in \\mathbb{R}^{W\\_{i}\\times H\\_{i}\\times C}$ refers to the features of the $i$-th largest scale. Here, each scale in the aggregated pyramid contains features from multi-level depths. \r
\r
However, simple concatenation operations are not adaptive enough. In the second stage, we introduce a channel-wise attention module to encourage features to focus on channels that they benefit most. Following Squeeze-and-Excitation, we use [global average pooling](https://paperswithcode.com/method/global-average-pooling) to generate channel-wise statistics $\\mathbf{z} \\in \\mathbb{R}^C$ at the squeeze step. And to fully capture channel-wise dependencies, the following excitation step learns the attention mechanism via two fully connected layers:\r
\r
$$\r
\\mathbf{s} = \\mathbf{F}\\_{ex}(\\mathbf{z},\\mathbf{W}) = \\sigma(\\mathbf{W}\\_{2} \\delta(\\mathbf{W}\\_{1}\\mathbf{z})),\r
$$\r
\r
where $\\sigma$ refers to the [ReLU](https://paperswithcode.com/method/relu) function, $\\delta$ refers to the sigmoid function, $\\mathbf{W}\\_{1} \\in \\mathbb{R}^{\\frac{C}{r}\\times C}$ , $\\mathbf{W}\\_{2} \\in \\mathbb{R}^{C\\times \\frac{C}{r}}$, r is the reduction ratio ($r=16$ in our experiments). The final output is obtained by reweighting the input $\\mathbf{X}$ with activation $\\mathbf{s}$:\r
\r
$$\r
\\tilde{\\mathbf{X}}_i^c = \\mathbf{F}\\_{scale}(\\mathbf{X}\\_i^c,s_c) = s_c \\cdot \\mathbf{X}_i^c,\r
$$\r
\r
where $\\tilde{\\mathbf{X}\\_i} = [\\tilde{\\mathbf{X}}\\_i^1,\\tilde{\\mathbf{X}}\\_i^2,...,\\tilde{\\mathbf{X}}\\_i^C]$, each of the features is enhanced or weakened by the rescaling operation.""" ;
    skos:prefLabel "SFAM" .

:SFT a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2010.13002v2> ;
    skos:altLabel "Shrink and Fine-Tune" ;
    skos:definition "**Shrink and Fine-Tune**, or **SFT**, is a type of distillation that avoids explicit distillation by copying parameters to a student student model and then fine-tuning. Specifically it extracts a student model from the maximally spaced layers of a fine-tuned teacher. Each layer $l \\in L'$ is copied fully from $L$. For example, when creating a [BART](https://paperswithcode.com/method/bart) student with 3 decoder layers from the 12 encoder layer 12 decoder layer teacher, we copy the teacher’s full $Enc^{L}$ and decoder layers 0, 6, and 11 to the student. When deciding which layers to copy, we break ties arbitrarily; copying layers 0, 5, and 11 might work just as well. When copy only 1 decoder layer, we copy layer 0. This was found this to work better than copying layer 11. The impact of initialization on performance is measured experimentally in Section 6.1. After initialization, the student model continues to fine-tune on the summarization dataset, with the objective of minimizing $\\mathcal{L}\\_{Data}$." ;
    skos:prefLabel "SFT" .

:SGD a skos:Concept ;
    rdfs:seeAlso <https://github.com/pytorch/pytorch/blob/4e0ac120e9a8b096069c2f892488d630a5c8f358/torch/optim/sgd.py#L97-L112> ;
    skos:altLabel "Stochastic Gradient Descent" ;
    skos:definition """**Stochastic Gradient Descent** is an iterative optimization technique that uses minibatches of data to form an expectation of the gradient, rather than the full gradient using all available data. That is for weights $w$ and a loss function $L$ we have:\r
\r
$$ w\\_{t+1} = w\\_{t} - \\eta\\hat{\\nabla}\\_{w}{L(w\\_{t})} $$\r
\r
Where $\\eta$ is a learning rate. SGD reduces redundancy compared to batch gradient descent - which recomputes gradients for similar examples before each parameter update - so it is usually much faster.\r
\r
(Image Source: [here](http://rasbt.github.io/mlxtend/user_guide/general_concepts/gradient-optimization/))""" ;
    skos:prefLabel "SGD" .

:SGDW a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1711.05101v3> ;
    rdfs:seeAlso <https://github.com/jettify/pytorch-optimizer/blob/155246597d66dd774156599be0f07a8c6f7758aa/torch_optimizer/sgdw.py#L10> ;
    skos:definition """**SGDW** is a stochastic optimization technique that decouples [weight decay](https://paperswithcode.com/method/weight-decay) from the gradient update:\r
\r
$$ g\\_{t} =  \\nabla{f\\_{t}}\\left(\\theta\\_{t-1}\\right) + \\lambda\\theta\\_{t-1}$$\r
\r
$$ m\\_{t} =  \\beta\\_{1}m\\_{t-1} + \\eta\\_{t}\\alpha{g}\\_{t}$$\r
\r
$$ \\theta\\_{t} = \\theta\\_{t-1} - m\\_{t} - \\eta\\_{t}\\lambda\\theta\\_{t-1}$$""" ;
    skos:prefLabel "SGDW" .

:SGDwithMomentum a skos:Concept ;
    skos:definition """### Why SGD with  Momentum?\r
In deep learning, we have used stochastic gradient descent as one of the optimizers because at the end we will find the minimum weight and bias at which the model loss is lowest. In the SGD we have some issues in which the SGD does not work perfectly because in deep learning we got a non-convex cost function graph and if use the simple SGD then it leads to low performance. There are 3 main reasons why it does not work:\r
\r
<img src="https://www.cs.umd.edu/~tomg/img/landscapes/shortHighRes.png" alt="Non-convex graph" style="width:400px;height :300px;" />\r
\r
1) We end up in local minima and not able to reach global minima\r
At the start, we randomly start at some point and we are going to end up at the local minimum and not able to reach the global minimum.\r
\r
2) Saddle Point will be the stop for reaching global minima\r
A saddle point is a point where in one direction the surface goes in the upward direction and in another direction it goes downwards. So that the slope is changing very gradually so the speed of changing is going to slow and as result, the training also going to slow.\r
\r
3) High curvature can be a reason\r
The larger radius leads to low curvature and vice-versa. It will be difficult to traverse in the large curvature which was generally high in non-convex optimization.\r
By using the SGD with Momentum optimizer we can overcome the problems like high curvature, consistent gradient, and noisy gradient.\r
\r
### What is SGD with Momentum?\r
SGD with  Momentum is one of the optimizers which is used to improve the performance of the neural network.\r
\r
Let's take an example and understand the intuition behind the optimizer suppose we have a ball which is sliding from the start of the slope as it goes the speed of the bowl is increased over time. If we have one point A and we want to reach point B and we don't know in which direction to move but we ask for the 4 points which have already reached point B. If all 4 points are pointing you in the same direction then the confidence of the A is more and it goes in the direction pointed very fast. This is the main concept behind the SGD with Momentum.\r
\r
<img src="https://cdn-images-1.medium.com/max/1000/1*zNbZqU_uDIV13c9ZCJOEXA.jpeg" alt="Non-convex graph" style="width:400px;height :250px;" />\r
### How does SGD with Momentum work?\r
So first to understand the concept of exponentially weighted moving average (EWMA). It was a technique through which try to find the trend in time series data. The formula of the EWMA is :\r
\r
<img src="https://cdn-images-1.medium.com/max/1000/1*O9Wcq-mbRgNOdRNTivSefw.png" alt="Non-convex graph" style="width:400px;height :100px;" />\r
\r
 In the formula, β represents the weightage that is going to assign to the past values of the gradient. The values of β is from 0 < β < 1. If the value of the beta is 0.5 then it means that the 1/1–0.5 = 2 so it represents that the calculated average was from the previous 2 readings. \r
\r
The value of Vt depends on β. The higher the value of β the more we try to get an average of more past data and vice-versa. For example, let's take the value of β 0.98 and 0.5 for two different scenarios so if we do 1/1-β then we get 50 and 10 respectively so it was clear that to calculate the average we take past 50 and 10 outcomes respectively for both cases.\r
Now in SGD with Momentum, we use the same concept of EWMA. Here we introduce the term velocity v which is used to denote the change in the gradient to get to the global minima. The change in the weights is denoted by the formula:\r
\r
<img src="https://cdn-images-1.medium.com/max/1000/0*i_r3u7LACa6dQyXd" alt="Non-convex graph" style="width:400px;height :100px;" />\r
\r
the β part of the V formula denotes and is useful to compute the confidence or we can say the past velocity for calculating Vt we have to calculate Vt-1 and for calculating Vt-1 we have to calculate Vt-2 and likewise. So we are using the history of velocity to calculate the momentum and this is the part that provides acceleration to the formula.\r
\r
<img src="https://cdn-images-1.medium.com/max/1000/1*L5lNKxAHLPYNc6-Zs4Vscw.png" alt="Non-convex graph" style="width:300px;height :100px;" />\r
\r
Here we have to consider two cases:\r
1. β=0 then, as per the formula weight updating is going to just work as a Stochastic gradient descent. Here we called β a decaying factor because it is defining the speed of past velocity.\r
\r
2. β=1 then, there will be no decay. It involves the dynamic equilibrium which is not desired so we generally use the value of β like 0.9,0.99or 0.5 only.\r
\r
### Advantages of SGD with Momentum :\r
1. Momentum is faster than stochastic gradient descent the training will be faster than SGD.\r
2. Local minima can be an escape and reach global minima due to the momentum involved.\r
\r
<img src="https://cdn-images-1.medium.com/max/1000/1*Nb39bHHUWGXqgisr2WcLGQ.gif" alt="Non-convex graph" style="width:400px;height :300px;" />\r
\r
Here in the video, we can see that purple is SGD with Momentum and light blue is for SGD the SGD with Momentum can reach global minima whereas SGD is stuck in local minima.\r
But there is a catch, the momentum itself can be a problem sometimes because of the high momentum after reaching global minima it is still fluctuating and take some time to get stable at global minima. And that kind of behavior leads to time consumption which makes SGD with Momentum slower than other optimization out there but still faster than SGD.""" ;
    skos:prefLabel "SGD with Momentum" .

:SGPCS a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2206.08083v4> ;
    skos:altLabel "Self-training Guided Prototypical Cross-domain Self-supervised learning" ;
    skos:definition """Model to adapt:\r
We use Ultra Fast Structure-aware Deep Lane Detection (UFLD) as baseline and strictly adopt its training scheme and hyperparameters. UFLD treats lane detection as a row-based classification problem and utilizes the row anchors defined by TuSimple.\r
\r
Unsupervised Domain Adaptation with SGPCS:\r
SGPCS builds upon PCS  and performs in-domain contrastive learning and crossdomain self-supervised learning via cluster prototypes. \r
\r
We reformulate the pseudo label selection mechanism from SGADA. For each lane, we select the highest confidence value from the griding cells of each row anchor. Based on their griding cell position, the confidence values are divided into two cases: absent lane points and present lane points. Thereby, the last griding cell represents absent lane points as in. For each case, we calculate the mean confidence over the corresponding lanes. We then use the thresholds defined by SGADA to decide whether the prediction is treated as a pseudo label.\r
\r
Our overall objective function comprises the in-domain and cross-domain loss from PCS, the losses defined by UFLD, and our adopted pseudo loss from SGADA. We adjust the momentum for memory bank feature updates to 0.5 and use spherical K-means with K = 2,500 to cluster them into prototypes.""" ;
    skos:prefLabel "SGPCS" .

:SHA-RNN a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1911.11423v2> ;
    skos:altLabel "Single Headed Attention RNN" ;
    skos:definition "**SHA-RNN**, or **Single Headed Attention RNN**, is a recurrent neural network, and language model when combined with an embedding input and [softmax](https://paperswithcode.com/method/softmax) classifier, based on a core [LSTM](https://paperswithcode.com/method/lstm) component and a [single-headed attention](https://paperswithcode.com/method/single-headed-attention) module. Other design choices include a Boom feedforward layer and the use of [layer normalization](https://paperswithcode.com/method/layer-normalization). The guiding principles of the author were to ensure simplicity in the architecture and to keep computational costs bounded (the model was originally trained with a single GPU)." ;
    skos:prefLabel "SHA-RNN" .

:SHAP a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1705.07874v2> ;
    rdfs:seeAlso <https://github.com/slundberg/shap> ;
    skos:altLabel "Shapley Additive Explanations" ;
    skos:definition "**SHAP**, or **SHapley Additive exPlanations**, is a game theoretic approach to explain the output of any machine learning model. It connects optimal credit allocation with local explanations using the classic Shapley values from game theory and their related extensions. Shapley values are approximating using Kernel SHAP, which uses a weighting kernel for the approximation, and DeepSHAP, which uses DeepLift to approximate them." ;
    skos:prefLabel "SHAP" .

:SIFA a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2002.02255v1> ;
    skos:altLabel "Synergistic Image and Feature Alignment" ;
    skos:definition "**Synergistic Image and Feature Alignment** is an unsupervised domain adaptation framework that conducts synergistic alignment of domains from both image and feature perspectives. In SIFA, we simultaneously transform the appearance of images across domains and enhance domain-invariance of the extracted features by leveraging adversarial learning in multiple aspects and with a deeply supervised mechanism. The feature encoder is shared between both adaptive perspectives to leverage their mutual benefits via end-to-end learning." ;
    skos:prefLabel "SIFA" .

:SIG a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2007.00674v3> ;
    rdfs:seeAlso <https://github.com/biweidai/SIG> ;
    skos:altLabel "Sliced Iterative Generator" ;
    skos:definition """The **Sliced Iterative Generator (SIG)** is an iterative generative model that is a Normalizing Flow (NF), but shares the advantages of Generative Adversarial Networks (GANs). The model is based on iterative Optimal Transport of a series of 1D slices through the data space, matching on each slice the probability distribution function (PDF) of the samples to the data. To improve the efficiency, the directions of the orthogonal slices are chosen to maximize the PDF difference between the generated samples and the data using Wasserstein distance at each iteration. A patch based approach is adopted to model the images in a hierarchical way, enabling the model to scale well to high dimensions. \r
\r
Unlike GANs, SIG has a NF structure and allows efficient likelihood evaluations that can be used in downstream tasks. While SIG has a deep neural network architecture, the approach deviates significantly from the current deep learning paradigm, as it does not use concepts such as mini-batching, stochastic gradient descent, gradient back-propagation through deep layers, or non-convex loss function optimization. SIG is very insensitive to hyper-parameter tuning, making it a useful generator tool for ML experts and non-experts alike.""" ;
    skos:prefLabel "SIG" .

:SIRM a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2001.00572v2> ;
    skos:altLabel "Skim and Intensive Reading Model" ;
    skos:definition "**Skim and Intensive Reading Model**, or **SIRM**, is a deep neural network for figuring out implied textual meaning. It consists of two main components, namely the skim reading component and intensive reading component. N-gram features are quickly extracted from the skim reading component, which is a combination of several convolutional neural networks, as skim (entire) information. An intensive reading component enables a hierarchical investigation for both local (sentence) and global (paragraph) representation, which encapsulates the current embedding and the contextual information with a dense connection." ;
    skos:prefLabel "SIRM" .

:SKEP a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2005.05635v2> ;
    skos:definition """**SKEP** is a self-supervised pre-training method for sentiment analysis. With the help of automatically-mined knowledge, SKEP conducts sentiment masking and constructs three sentiment knowledge prediction objectives, so as to embed sentiment information at the word, polarity and aspect level into pre-trained sentiment representation. In particular, the prediction of aspect-sentiment pairs is converted into multi-label classification, aiming to capture the dependency between words in a pair.\r
\r
SKEP contains two parts: (1) Sentiment masking recognizes the sentiment information of an input sequence based on automatically-mined sentiment knowledge, and produces a corrupted version by removing these informations. (2) Sentiment pre-training objectives require the transformer to recover the removed information from the corrupted version. The three prediction objectives on top are jointly optimized: Sentiment Word (SW) prediction (on $\\left.\\mathrm{x}\\_{9}\\right)$, Word Polarity (SP) prediction (on $\\mathrm{x}\\_{6}$ and $\\mathbf{x}\\_{9}$ ), Aspect-Sentiment pairs (AP) prediction (on $\\mathbf{x}\\_{1}$ ). Here, the smiley denotes positive polarity. Notably, on $\\mathrm{x}\\_{6}$, only SP is calculated without SW, as its original word has been predicted in the pair prediction on $\\mathbf{x}\\_{1}$.""" ;
    skos:prefLabel "SKEP" .

:SKNet a skos:Concept ;
    rdfs:seeAlso <https://github.com/osmr/imgclsmob/blob/68335927ba27f2356093b985bada0bc3989836b1/pytorch/pytorchcv/models/sknet.py#L172> ;
    skos:definition "**SKNet** is a type of convolutional neural network that employs [selective kernel](https://paperswithcode.com/method/selective-kernel) units, with selective kernel convolutions, in its architecture. This allows for a type of attention where the network can learn to attend to different receptive fields." ;
    skos:prefLabel "SKNet" .

:SLAMB a skos:Concept ;
    dcterms:source <https://proceedings.mlr.press/v202/xu23v.html> ;
    skos:altLabel "Sparse Layer-wise Adaptive Moments optimizer for large Batch training" ;
    skos:definition "Please enter a description about the method here" ;
    skos:prefLabel "SLAMB" .

:SLR a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2012.10079v2> ;
    skos:altLabel "Surrogate Lagrangian Relaxation" ;
    skos:definition "Please enter a description about the method here" ;
    skos:prefLabel "SLR" .

:SM3 a skos:Concept ;
    dcterms:source <http://papers.nips.cc/paper/9168-memory-efficient-adaptive-optimization> ;
    skos:definition """# Memory-Efficient Adaptive Optimization\r
\r
Source: https://arxiv.org/abs/1901.11150\r
\r
Adaptive gradient-based optimizers such as [AdaGrad](https://paperswithcode.com/method/adagrad) and [Adam](https://paperswithcode.com/method/adam) are among the\r
defacto methods of choice in modern machine learning.These methods tune the learning rate for each parameter during the optimization process using cumulative second-order statistics. These methods provide superior convergence properties and are very attractive in large scale applications due to their moderate time and space requirements which are linear in the number of parameters.\r
\r
\r
However, the recent advances in natural language processing such as [BERT](https://paperswithcode.com/method/bert) and GPT2 show that models with 10<sup>8</sup> to 10<sup>10</sup> parameters, trained with adaptive optimization methods, achieve state-of-the-art results. In such cases, the memory overhead of the optimizer can restrict the size of the model that can be used as well as the batch size, both of which can have a dramatic effect on the quality of the final model.\r
\r
\r
Here we construct a new adaptive optimization method that retains most of the benefits of standard per-parameter adaptivity while significantly reducing memory overhead.\r
\r
\r
We observe that in standard neural networks that certain entries of the stochastic gradients have (on average) similar values, and exhibit what we refer to as an activation pattern. For example, in gradients of embedding layers of deep networks, an entire row (or column) is either zero or non-zero. Similarly, in intermediate layers we often observe that gradients associated with the same unit are of similar order of magnitude. In these cases, a similar phenomenon is observed in the second-order statistics maintained by adaptive methods. With this key observation, to reduce the memory overhead of the optimizer our method takes in a cover set of the parameters. Cover sets are typically selected in practice such that parameters in each of the sets have second order statistics of similar magnitude. Our method is general enough that it can easily be extended to arbitrary cover sets. For parameters of deep networks that are organized as a collection of tensors, we form a cover consisting of slices of codimension one for each tensor. Thus, for an m x n parameter matrix, the cover consists of rows and columns of the matrix. The memory requirements therefore drop from mxn to merely m+n. For a parameter tensor of rank p, with dimensions n<sub>1</sub>  ...   n<sub>p</sub>, the reduction in memory consumption is even more pronounced, dropping from product of all the dimensions to the sum of all dimensions. This virtually eliminates the memory overhead associated with maintaining the adaptive learning rates!\r
\r
Another practical aspect worthy of note is that our method does not require an external hand engineered learning rate decay schedule but instead relies on the per parameter adaptivity that is natural to its update rule which makes it easier to tune. We provide details in the supplementary section of the paper.\r
\r
## Advice on using SM3 on your model\r
\r
### Learning rate warm-up:\r
\r
```python\r
learning_rate = lr_constant * tf.minimum(1.0, (warm_up_step / global_step) ** p)\r
```\r
\r
* p = 1, linear ramp up of learning rate.\r
* p = 2, quadratic ramp up of learning rate [preferred].\r
\r
We typically set `warm_up_step` as 5% of overall steps. Initially, the norm of the preconditioned gradient is much larger than norm of the weights. Learning rate warmup allows us to heuristically fix this scale mismatch.\r
\r
### Learning rate decay:\r
\r
We make use accumulated gradient squares for the decay. This means that each coordinate gets its own natural decay based on the scales of the gradients over time. Hence, users need not put in an external learning rate decay schedule. Moreover, we found in our experiments with translation and language models that this approach is superior to a hand-tuned learning rate decay schedules which is typically combined with exponential moving averages of the gradient squares.\r
\r
Having said that if users want to add exponential moving averages instead of the standard accumulated gradient squares - It's easy to modify the optimizer implementation to switch to exponential moving averages.\r
\r
For rank > 1:\r
\r
|            from                     |                  to                 |\r
|-------------------------------------|-------------------------------------|\r
|  current_accumulator += grad * grad |  current_accumulator = beta * current_accumulator + (1-beta) * grad * grad |\r
\r
\r
For rank <= 1:\r
\r
\r
|            from                     |                  to                 |\r
|-------------------------------------|-------------------------------------|\r
|  current_accumulator = tf.assign_add(accumulator, grad * grad) |   current_accumulator = tf.assign(accumulator, beta * accumulator + (1-beta) * (grad * grad)) |\r
\r
\r
### [Polyak averaging](https://paperswithcode.com/method/polyak-averaging) of parameters: \r
It's useful to run [polyak averaging](https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage) of the parameters. These parameters are then used in inference / serving. Using the averaged parameters instead of the last iterate typically improves the overall performance of the model.\r
\r
An **alternative** to polyak averaging which does not make use of extra memory is to decay the learning rate from the constant to zero for the last 10% of the steps of your entire training run, we term the phase a **cool-down** phase of the model. As training makes smaller and smaller steps the final iterate can be thought of as an average iterate.""" ;
    skos:prefLabel "SM3" .

:SMA a skos:Concept ;
    rdfs:seeAlso <https://aliasgharheidari.com/SMA.html> ;
    skos:altLabel "Slime Mould Algorithm" ;
    skos:definition """**Slime Mould Algorithm** (**SMA**) is a new stochastic optimizer proposed based on the oscillation mode of slime mould in nature. SMA has several new features with a unique mathematical model that uses adaptive weights to simulate the process of producing positive and negative feedback of the propagation wave of slime mould based on bio-oscillator to form the optimal path for connecting food with excellent exploratory ability and exploitation propensity.\r
\r
🔗 The source codes of SMA are publicly available at [https://aliasgharheidari.com/SMA.html](https://aliasgharheidari.com/SMA.html)""" ;
    skos:prefLabel "SMA" .

:SMITH a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2004.12297v2> ;
    skos:altLabel "Siamese Multi-depth Transformer-based Hierarchical Encoder" ;
    skos:definition "**SMITH**, or **Siamese Multi-depth Transformer-based Hierarchical Encoder**, is a [Transformer](https://paperswithcode.com/methods/category/transformers)-based model for document representation learning and matching. It contains several design choices to adapt [self-attention models](https://paperswithcode.com/methods/category/attention-modules) for long text inputs. For the model pre-training, a masked sentence block language modeling task is used in addition to the original masked word language model task used in [BERT](https://paperswithcode.com/method/bert), to capture sentence block relations within a document. Given a sequence of sentence block representation, the document level Transformers learn the contextual representation for each sentence block and the final document representation." ;
    skos:prefLabel "SMITH" .

:SMOT a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2010.16031v1> ;
    skos:altLabel "Single-Shot Multi-Object Tracker" ;
    skos:definition """**Single-Shot Multi-Object Tracker** or **SMOT**, is a tracking framework that converts any single-shot detector (SSD) model into an online multiple object tracker, which emphasizes simultaneously detecting and tracking of the object paths. Contrary to the existing tracking by detection approaches which suffer from errors made by the object detectors, SMOT adopts the recently proposed scheme of tracking by re-detection.\r
\r
The proposed SMOT consists of two stages. The first stage generates temporally consecutive tracklets by exploring the temporal and spatial correlations from previous frame. The second stage performs online linking of the tracklets to generate a face track for each person (better view in color).""" ;
    skos:prefLabel "SMOT" .

:SMOTE a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1106.1813v1> ;
    skos:altLabel "Synthetic Minority Over-sampling Technique." ;
    skos:definition """Perhaps the most widely used approach to synthesizing new examples is called the Synthetic Minority Oversampling Technique, or SMOTE for short. This technique was described by Nitesh Chawla, et al. in their 2002 paper named for the technique titled “SMOTE: Synthetic Minority Over-sampling Technique.”\r
\r
SMOTE works by selecting examples that are close in the feature space, drawing a line between the examples in the feature space and drawing a new sample at a point along that line.""" ;
    skos:prefLabel "SMOTE" .

:SNAIL a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1707.03141v3> ;
    skos:altLabel "Simple Neural Attention Meta-Learner" ;
    skos:definition "The **Simple Neural Attention Meta-Learner**, or **SNAIL**, combines the benefits of temporal convolutions and attention to solve meta-learning tasks. They introduce positional dependence through temporal convolutions to make the model applicable to reinforcement tasks - where the observations, actions, and rewards are intrinsically sequential. They also introduce attention in order to provide pinpoint access over an infinitely large context. SNAIL is constructing by combining the two: we use temporal convolutions to produce the context over which we use a causal attention operation." ;
    skos:prefLabel "SNAIL" .

:SNGAN a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1802.05957v1> ;
    rdfs:seeAlso <https://github.com/christiancosgrove/pytorch-spectral-normalization-gan/blob/master/model_resnet.py> ;
    skos:altLabel "Spectrally Normalised GAN" ;
    skos:definition "**SNGAN**, or **Spectrally Normalised GAN**, is a type of generative adversarial network that uses [spectral normalization](https://paperswithcode.com/method/spectral-normalization), a type of [weight normalization](https://paperswithcode.com/method/weight-normalization), to stabilise the training of the discriminator." ;
    skos:prefLabel "SNGAN" .

:SNIP a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1711.08189v2> ;
    skos:definition """**SNIP**, or **Scale Normalization for Image Pyramids**, is a multi-scale training scheme that selectively back-propagates the gradients of object instances of different sizes as a function of the image scale. SNIP is a modified version of MST where only the object instances that have a resolution close to the pre-training dataset, which is typically 224x224, are used for training the detector. In multi-scale training (MST), each image is observed at different resolutions therefore, at a high resolution (like 1400x2000) large objects are hard to classify and at a low resolution (like 480x800) small objects are hard to classify. Fortunately, each object instance appears at several different scales and some of those appearances fall in the desired scale range. In order to eliminate extreme scale objects, either too large or too small, training is only performed on objects that fall in the desired scale range and the remainder are simply ignored during back-propagation. Effectively, SNIP uses all the object instances during training, which helps capture all the variations in appearance and\r
pose, while reducing the domain-shift in the scale-space for the pre-trained network.""" ;
    skos:prefLabel "SNIP" .

:SNIPER a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1805.09300v3> ;
    skos:definition "**SNIPER** is a multi-scale training approach for instance-level recognition tasks like object detection and instance-level segmentation. Instead of processing all pixels in an image pyramid, SNIPER selectively processes context regions around the ground-truth objects (a.k.a chips). This can help to speed up multi-scale training as it operates on low-resolution chips. Due to its memory-efficient design, SNIPER can benefit from [Batch Normalization](https://paperswithcode.com/method/batch-normalization) during training and it makes larger batch-sizes possible for instance-level recognition tasks on a single GPU." ;
    skos:prefLabel "SNIPER" .

:SNN a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1706.02515v5> ;
    skos:altLabel "Spiking Neural Networks" ;
    skos:definition "**Spiking Neural Networks** (**SNNs**)  are a class of artificial neural networks inspired by the structure and functioning of the brain's neural networks. Unlike traditional artificial neural networks that operate based on continuous firing rates, SNNs simulate the behavior of individual neurons through discrete spikes or action potentials. These spikes are triggered when the neuron's membrane potential reaches a certain threshold, and they propagate through the network, communicating information and triggering subsequent neuron activations. This spike-based communication allows SNNs to capture the temporal dynamics of information processing and exhibit asynchronous, event-driven behavior, making them well-suited for tasks such as temporal pattern recognition, event detection, and real-time processing. SNNs have gained attention due to their potential in efficiently processing and encoding information, offering advantages in energy efficiency, robustness, and compatibility with neuromorphic hardware architectures." ;
    skos:prefLabel "SNN" .

:SNet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1903.11752v3> ;
    rdfs:seeAlso <https://github.com/ouyanghuiyu/Thundernet_Pytorch/blob/ab66b733a39c9d1c60b5373f84f861d9627d8c20/lib/model/faster_rcnn/Snet.py#L6> ;
    skos:definition "**SNet** is a convolutional neural network architecture and object detection backbone used for the [ThunderNet](https://paperswithcode.com/method/thundernet) two-stage object detector. SNet uses ShuffleNetV2 basic blocks but replaces all 3×3 depthwise convolutions with 5×5 depthwise convolutions." ;
    skos:prefLabel "SNet" .

:SOHO a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2104.03135v2> ;
    skos:definition "SOHO (“See Out of tHe bOx”) that takes a whole image as input, and learns vision-language representation in an end-to-end manner. SOHO does not require bounding box annotations which enables inference 10 times faster than region-based approaches. Text embeddings are used to extract textual embedding features. A trainable CNN is used to extract visual representations. SOHO learns to extract comprehensive yet compact image features through a visual dictionary (VD) that facilitates cross-modal understanding. VD is designed to represent consistent visual abstractions of similar semantics. It is updated on-the-fly and utilized in the proposed pre-training task Masked Visual Modeling (MVM)." ;
    skos:prefLabel "SOHO" .

:SOM a skos:Concept ;
    skos:altLabel "Self-Organizing Map" ;
    skos:definition """The **Self-Organizing Map (SOM)**, commonly also known as Kohonen network (Kohonen 1982, Kohonen 2001) is a computational method for the visualization and analysis of high-dimensional data, especially experimentally acquired information.\r
\r
Extracted from [scholarpedia](http://www.scholarpedia.org/article/Self-organizing_map)\r
\r
**Sources**:\r
\r
Image: [scholarpedia](http://www.scholarpedia.org/article/File:Somnbc.png)\r
\r
Paper: [Kohonen, T. Self-organized formation of topologically correct feature maps. Biol. Cybern. 43, 59–69 (1982)](https://doi.org/10.1007/BF00337288)\r
\r
Book: [Self-Organizing Maps](https://doi.org/10.1007/978-3-642-56927-2)""" ;
    skos:prefLabel "SOM" .

:SPADE a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1903.07291v2> ;
    skos:altLabel "Spatially-Adaptive Normalization" ;
    skos:definition "**SPADE**, or **Spatially-Adaptive Normalization** is a conditional normalization method for semantic image synthesis. Similar to [Batch Normalization](https://www.paperswithcode.com/method/batch-normalization), the activation is normalized in the channel-wise manner and then modulated with learned scale and bias. In the SPADE, the mask is first projected onto an embedding space and then convolved to produce the modulation parameters $\\gamma$ and $\\beta .$ Unlike prior conditional normalization methods, $\\gamma$ and $\\mathbf{\\beta}$ are not vectors, but tensors with spatial dimensions. The produced $\\gamma$ and $\\mathbf{\\beta}$ are multiplied and added to the normalized activation element-wise." ;
    skos:prefLabel "SPADE" .

:SPEED a skos:Concept ;
    rdfs:seeAlso <https://github.com/lorenzopapa5/SPEED> ;
    skos:altLabel "SPEED: Separable Pyramidal Pooling EncodEr-Decoder for Real-Time Monocular Depth Estimation on Low-Resource Settings" ;
    skos:definition """The monocular depth estimation (MDE) is the task of estimating depth from a single frame. This information is an essential knowledge in many computer vision tasks such as scene understanding and visual odometry, which are key components in autonomous and robotic systems. \r
Approaches based on the state of the art vision transformer architectures are extremely deep and complex not suitable for real-time inference operations on edge and autonomous systems equipped with low resources (i.e. robot indoor navigation and surveillance). This paper presents SPEED, a Separable Pyramidal pooling EncodEr-Decoder architecture designed to achieve real-time frequency performances on multiple hardware platforms. The proposed model is a fast-throughput deep architecture for MDE able to obtain depth estimations with high accuracy from low resolution images using minimum hardware resources (i.e. edge devices). Our encoder-decoder model exploits two depthwise separable pyramidal pooling layers, which allow to increase the inference frequency while reducing the overall computational complexity. The proposed method performs better than other fast-throughput architectures in terms of both accuracy and frame rates, achieving real-time performances over cloud CPU, TPU and the NVIDIA Jetson TX1 on two indoor benchmarks: the NYU Depth v2 and the DIML Kinect v2 datasets.""" ;
    skos:prefLabel "SPEED" .

:SPIN a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2205.13479v2> ;
    skos:altLabel "Spatiotemporal Point Inference Network" ;
    skos:definition "" ;
    skos:prefLabel "SPIN" .

:SPL a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2207.09869v1> ;
    skos:altLabel "Semi-Pseudo-Label" ;
    skos:definition "" ;
    skos:prefLabel "SPL" .

:SPNet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2003.13328v1> ;
    skos:altLabel "Strip Pooling Network" ;
    skos:definition """Spatial pooling usually operates on a small region which limits its capability to capture long-range dependencies and focus on distant regions. To overcome this, Hou et al. proposed  strip pooling, a novel pooling method capable of encoding long-range context in either horizontal or vertical spatial domains.  \r
\r
Strip pooling has two branches for horizontal  and vertical strip pooling. The horizontal strip pooling part first pools the input feature $F \\in \\mathcal{R}^{C \\times H \\times W}$ in the horizontal direction:\r
\\begin{align}\r
y^1 = \\text{GAP}^w (X) \r
\\end{align}\r
Then a 1D convolution with kernel size 3 is applied in $y$ to capture the relationship between different rows and channels. This is repeated $W$ times to make  the output $y_v$  consistent with the input shape:\r
\\begin{align}\r
    y_h = \\text{Expand}(\\text{Conv1D}(y^1))\r
\\end{align}\r
Vertical strip pooling is performed in a similar way. Finally, the outputs of the two branches are fused using element-wise summation to produce the attention map:\r
\\begin{align}\r
s &= \\sigma(Conv^{1\\times 1}(y_{v} + y_{h}))\r
\\end{align}\r
\\begin{align}\r
Y &= s  X\r
\\end{align}\r
\r
The strip pooling module (SPM) is further developed in the mixed pooling module (MPM). Both consider  spatial  and channel relationships to overcome the locality of convolutional neural networks.  SPNet achieves  state-of-the-art results for several complex semantic segmentation benchmarks.""" ;
    skos:prefLabel "SPNet" .

:SPP-Net a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1406.4729v4> ;
    rdfs:seeAlso <https://github.com/yueruchen/sppnet-pytorch/blob/2aaad15fd4828e78ce0dfa4c9daaba9d27cae9b8/cnn_with_spp.py#L9> ;
    skos:definition "**SPP-Net** is a convolutional neural architecture that employs [spatial pyramid pooling](https://paperswithcode.com/method/spatial-pyramid-pooling) to remove the fixed-size constraint of the network. Specifically, we add an SPP layer on top of the last convolutional layer. The SPP layer pools the features and generates fixed-length outputs, which are then fed into the fully-connected layers (or other classifiers). In other words, we perform some information aggregation at a deeper stage of the network hierarchy (between convolutional layers and fully-connected layers) to avoid the need for cropping or warping at the beginning." ;
    skos:prefLabel "SPP-Net" .

:SPS a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2207.09869v1> ;
    skos:altLabel "Semi-Pseudo-Label" ;
    skos:definition "" ;
    skos:prefLabel "SPS" .

:SRDC a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2003.08607v1> ;
    skos:altLabel "Structurally Regularized Deep Clustering" ;
    skos:definition "**Structurally Regularized Deep Clustering**, or **SRDC**, is a deep network based discriminative clustering method for domain adaptation that minimizes the KL divergence between predictive label distribution of the network and an introduced auxiliary one. Replacing the auxiliary distribution with that formed by ground-truth labels of source data implements the structural source regularization via a simple strategy of joint network training." ;
    skos:prefLabel "SRDC" .

:SRGAN a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1609.04802v5> ;
    rdfs:seeAlso <https://github.com/lizhuoq/SRGAN.git> ;
    skos:definition """**SRGAN** is a generative adversarial network for single image super-resolution. It uses a perceptual loss function which consists of an adversarial loss and a content loss. The adversarial loss pushes the solution to the natural image manifold using a discriminator network that is trained to differentiate between the super-resolved images and original photo-realistic images. In addition, the authors use a content loss motivated by perceptual similarity instead of similarity in pixel space. The actual networks - depicted in the Figure to the right - consist mainly of residual blocks for feature extraction.\r
\r
Formally we write the perceptual loss function as a weighted sum of a ([VGG](https://paperswithcode.com/method/vgg)) content loss $l^{SR}\\_{X}$ and an adversarial loss component $l^{SR}\\_{Gen}$:\r
\r
$$ l^{SR} = l^{SR}\\_{X} + 10^{-3}l^{SR}\\_{Gen} $$""" ;
    skos:prefLabel "SRGAN" .

:SRGANResidualBlock a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1609.04802v5> ;
    rdfs:seeAlso <https://github.com/Lornatang/SRGAN-PyTorch> ;
    skos:definition "**SRGAN Residual Block** is a residual block used in the [SRGAN](https://paperswithcode.com/method/srgan) generator for image super-resolution. It is similar to standard [residual blocks](https://paperswithcode.com/method/residual-block), although it uses a [PReLU](https://paperswithcode.com/method/prelu) activation function to help training (preventing sparse gradients during [GAN](https://paperswithcode.com/method/gan) training)." ;
    skos:prefLabel "SRGAN Residual Block" .

:SRM a skos:Concept ;
    dcterms:source <http://openaccess.thecvf.com/content_ICCV_2019/html/Lee_SRM_A_Style-Based_Recalibration_Module_for_Convolutional_Neural_Networks_ICCV_2019_paper.html> ;
    skos:altLabel "style-based recalibration module" ;
    skos:definition """SRM combines style transfer with an attention mechanism. Its main contribution is style pooling which utilizes both mean and standard deviation of the input features to improve its capability to capture global information. It also adopts a lightweight channel-wise fully-connected (CFC) layer, in place of the original fully-connected layer, to reduce the computational requirements.\r
Given an input feature map $X \\in \\mathbb{R}^{C \\times H \\times W}$, SRM first collects global information by using style pooling ($\\text{SP}(\\cdot)$) which combines global average pooling and global standard deviation pooling. \r
Then a channel-wise fully connected ($\\text{CFC}(\\cdot)$) layer (i.e. fully connected per channel), batch normalization $\\text{BN}$ and sigmoid function $\\sigma$ are used  to provide the attention vector. Finally,   as in an SE block, the input features are multiplied by the attention vector. Overall, an SRM can be written as:\r
\\begin{align}\r
    s = F_\\text{srm}(X, \\theta) & = \\sigma (\\text{BN}(\\text{CFC}(\\text{SP}(X))))\r
\\end{align}\r
\\begin{align}\r
    Y & = s  X\r
\\end{align}\r
The SRM block improves both squeeze and excitation modules, yet can be added after each residual unit like an SE block.""" ;
    skos:prefLabel "SRM" .

:SRMM a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2201.01652v3> ;
    skos:altLabel "Stochastic Regularized Majorization-Minimization" ;
    skos:definition "" ;
    skos:prefLabel "SRMM" .

:SRN a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1906.04659v3> ;
    skos:altLabel "Stable Rank Normalization" ;
    skos:definition """**Stable Rank Normalization (SRN)** is a weight-normalization scheme which minimizes the\r
stable rank of a linear operator. It simultaneously controls the Lipschitz constant and the stable rank of a linear operator. Stable rank is a softer version of the rank operator and is defined as the squared ratio of the Frobenius norm to the spectral norm.""" ;
    skos:prefLabel "SRN" .

:SRS a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2003.04679v1> ;
    skos:altLabel "Sticker Response Selector" ;
    skos:definition "**Sticker Response Selector**, or **SRS**, is a model for multi-turn dialog that automatically selects a sticker response. SRS first employs a convolutional based sticker image encoder and a self-attention based multi-turn dialog encoder to obtain the representation of stickers and utterances. Next, deep interaction network is proposed to conduct deep matching between the sticker with each utterance in the dialog history. SRS then learns the short-term and long-term dependency between all interaction results by a fusion network to output the the final matching score." ;
    skos:prefLabel "SRS" .

:SRU a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1709.02755v5> ;
    skos:definition """**SRU**, or **Simple Recurrent Unit**, is a recurrent neural unit with a light form of recurrence. SRU exhibits the same level of parallelism as [convolution](https://paperswithcode.com/method/convolution) and [feed-forward nets](https://paperswithcode.com/methods/category/feedforward-networks). This is achieved by balancing sequential dependence and independence: while the state computation of SRU is time-dependent, each state dimension is independent. This simplification enables CUDA-level optimizations that parallelize the computation across hidden dimensions and time steps, effectively using the full capacity of modern GPUs. \r
\r
SRU also replaces the use of convolutions (i.e., ngram filters), as in [QRNN](https://paperswithcode.com/method/qrnn) and KNN, with more recurrent connections. This retains modeling capacity, while using less computation (and hyper-parameters). Additionally, SRU improves the training of deep recurrent models by employing [highway connections](https://paperswithcode.com/method/highway-layer) and a parameter initialization scheme tailored for gradient propagation in deep architectures.\r
\r
A single layer of SRU involves the following computation:\r
\r
$$\r
\\mathbf{f}\\_{t} =\\sigma\\left(\\mathbf{W}\\_{f} \\mathbf{x}\\_{t}+\\mathbf{v}\\_{f} \\odot \\mathbf{c}\\_{t-1}+\\mathbf{b}\\_{f}\\right) \r
$$\r
\r
$$\r
\\mathbf{c}\\_{t} =\\mathbf{f}\\_{t} \\odot \\mathbf{c}\\_{t-1}+\\left(1-\\mathbf{f}\\_{t}\\right) \\odot\\left(\\mathbf{W} \\mathbf{x}\\_{t}\\right) \\\\\r
$$\r
\r
$$\r
\\mathbf{r}\\_{t} =\\sigma\\left(\\mathbf{W}\\_{r} \\mathbf{x}\\_{t}+\\mathbf{v}\\_{r} \\odot \\mathbf{c}\\_{t-1}+\\mathbf{b}\\_{r}\\right) \\\\\r
$$\r
\r
$$\r
\\mathbf{h}\\_{t} =\\mathbf{r}\\_{t} \\odot \\mathbf{c}\\_{t}+\\left(1-\\mathbf{r}\\_{t}\\right) \\odot \\mathbf{x}\\_{t}\r
$$\r
\r
where $\\mathbf{W}, \\mathbf{W}\\_{f}$ and $\\mathbf{W}\\_{r}$ are parameter matrices and $\\mathbf{v}\\_{f}, \\mathbf{v}\\_{r}, \\mathbf{b}\\_{f}$ and $\\mathbf{b}_{v}$ are parameter vectors to be learnt during training. The complete architecture decomposes to two sub-components: a light recurrence and a highway network,\r
\r
The light recurrence component successively reads the input vectors $\\mathbf{x}\\_{t}$ and computes the sequence of states $\\mathbf{c}\\_{t}$ capturing sequential information. The computation resembles other recurrent networks such as [LSTM](https://paperswithcode.com/method/lstm), [GRU](https://paperswithcode.com/method/gru) and RAN. Specifically, a forget gate $\\mathbf{f}\\_{t}$ controls the information flow and the state vector $\\mathbf{c}\\_{t}$ is determined by adaptively averaging the previous state $\\mathbf{c}\\_{t-1}$ and the current observation $\\mathbf{W} \\mathbf{x}_{+}$according to $\\mathbf{f}\\_{t}$.""" ;
    skos:prefLabel "SRU" .

<http://w3id.org/mlso/vocab/ml_algorithm/SRU++> a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2102.12459v3> ;
    skos:definition """**SRU++** is a self-attentive recurrent unit that combines fast recurrence and attention for sequence modeling, extending the [SRU](https://www.paperswithcode.com/method/sru) unit. The key modification of SRU++ is to incorporate more expressive non-linear operations into the recurrent network. Specifically, given the input sequence represented as a matrix $\\mathbf{X} \\in \\mathbb{R}^{L \\times d}$, the attention component computes the query, key and value representations using the following multiplications,\r
\r
$$\r
\\mathbf{Q} =\\mathbf{W}^{q} \\mathbf{X}^{\\top} \r
$$\r
\r
$$\r
\\mathbf{K} =\\mathbf{W}^{k} \\mathbf{Q} \\\\\r
$$\r
\r
$$\r
\\mathbf{V} =\\mathbf{W}^{v} \\mathbf{Q}\r
$$\r
\r
where $\\mathbf{W}^{q} \\in \\mathbb{R}^{d^{\\prime} \\times d}, \\mathbf{W}^{k}, \\mathbf{W}^{v} \\in \\mathbb{R}^{d^{\\prime} \\times d^{\\prime}}$ are model parameters. $d^{\\prime}$ is the attention dimension that is typically much smaller than $d$. Note that the keys $\\mathbf{K}$ and values $\\mathbf{V}$ are computed using $\\mathbf{Q}$ instead of $\\mathbf{X}$ such that the weight matrices $\\mathbf{W}^{k}$ and $\\mathbf{W}^{v}$ are significantly smaller. \r
\r
Next, we compute a weighted average output $\\mathbf{A} \\in \\mathbb{R}^{d^{\\prime} \\times L}$ using [scaled dot-product attention](https://paperswithcode.com/method/scaled):\r
\r
$$\r
\\mathbf{A}^{\\top}=\\operatorname{softmax}\\left(\\frac{\\mathbf{Q}^{\\top} \\mathbf{K}}{\\sqrt{d^{\\prime}}}\\right) \\mathbf{V}^{\\top}\r
$$\r
\r
The final output $U$ required by the elementwise recurrence is obtained by another linear projection,\r
\r
$$\r
\\mathbf{U}^{\\top}=\\mathbf{W}^{o}(\\mathbf{Q}+\\alpha \\cdot \\mathbf{A})\r
$$\r
\r
where $\\alpha \\in \\mathbb{R}$ is a learned scalar and $\\mathbf{W}\\_{o} \\in \\mathbb{R}^{3 d \\times d^{\\prime}}$ is a parameter matrix. $\\mathbf{Q}+\\alpha \\cdot \\mathbf{A}$ is a [residual connection](https://paperswithcode.com/method/residual-connection) which improves gradient propagation and stabilizes training. We initialize $\\alpha$ to zero and as a result,\r
\r
$$\r
\\mathbf{U}^{\\top}=\\mathbf{W}^{o} \\mathbf{Q}=\\left(\\mathbf{W}^{o} \\mathbf{W}^{q}\\right) \\mathbf{X}^{\\top}\r
$$\r
\r
initially falls back to a linear transformation of the input $X$ skipping the attention transformation. Intuitively, skipping attention encourages leveraging recurrence to capture sequential patterns during early stage of training. As $|\\alpha|$ grows, the attention mechanism can learn long-range dependencies for the model. In addition, $\\mathbf{W}^{o} \\mathbf{W}^{q}$ can be interpreted as applying a matrix factorization trick with a small inner dimension $d^{\\prime}<d$, reducing the total number of parameters. The Figure compares the differences of SRU, SRU with this factorization trick (but without attention), and SRU++.\r
\r
The last modification is adding [layer normalization](https://paperswithcode.com/method/layer-normalization) to each SRU++ layer. We apply normalization after the attention operation and before the matrix multiplication with $\\mathbf{W}^{o}$\r
\r
$$\r
\\mathbf{U}^{\\top}=\\mathbf{W}^{o} \\operatorname{layernorm}(\\mathbf{Q}+\\alpha \\cdot \\mathbf{A})\r
$$\r
\r
This implementation is post-layer normalization in which the normalization is added after the residual connection.""" ;
    skos:prefLabel "SRU++" .

:SReLU a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1512.07030v1> ;
    rdfs:seeAlso <https://github.com/ThomasHagebols/sReLU/blob/2180d2334dd2a39847e6bafe1033f80a655c5e07/srelu.py#L9> ;
    skos:altLabel "S-shaped ReLU" ;
    skos:definition """The **S-shaped Rectified Linear Unit**, or **SReLU**, is an activation function for neural networks. It learns both convex and non-convex functions, imitating the multiple function forms given by the two fundamental laws, namely  the Webner-Fechner law and the Stevens law, in psychophysics and neural sciences. Specifically, SReLU consists of three piecewise linear functions, which are formulated by four learnable parameters. \r
\r
The SReLU is defined as a mapping:\r
\r
$$ f\\left(x\\right) = t\\_{i}^{r}  + a^{r}\\_{i}\\left(x\\_{i}-t^{r}\\_{i}\\right) \\text{ if } x\\_{i} \\geq t^{r}\\_{i} $$\r
$$ f\\left(x\\right) = x\\_{i} \\text{ if } t^{r}\\_{i} > x > t\\_{i}^{l}$$\r
$$ f\\left(x\\right) = t\\_{i}^{l}  + a^{l}\\_{i}\\left(x\\_{i}-t^{l}\\_{i}\\right) \\text{ if } x\\_{i} \\leq t^{l}\\_{i} $$\r
\r
where $t^{l}\\_{i}$, $t^{r}\\_{i}$ and $a^{l}\\_{i}$ are learnable parameters of the network $i$ and indicates that the SReLU can differ in different channels. The parameter $a^{r}\\_{i}$ represents the slope of the right line with input above a set threshold. $t^{r}\\_{i}$ and $t^{l}\\_{i}$ are thresholds in positive and negative directions respectively.\r
\r
Source: [Activation Functions](https://arxiv.org/pdf/1811.03378.pdf)""" ;
    skos:prefLabel "SReLU" .

:SSD a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1512.02325v5> ;
    rdfs:seeAlso <https://github.com/amdegroot/ssd.pytorch/blob/5b0b77faa955c1917b0c710d770739ba8fbff9b7/ssd.py#L10> ;
    skos:definition """**SSD** is a single-stage object detection method that discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. \r
\r
The fundamental improvement in speed comes from eliminating bounding box proposals and the subsequent pixel or feature resampling stage. Improvements over competing single-stage methods include using a small convolutional filter to predict object categories and offsets in bounding box locations, using separate predictors (filters) for different aspect ratio detections, and applying these filters to multiple feature maps from the later stages of a network in order to perform detection at multiple scales.""" ;
    skos:prefLabel "SSD" .

:SSDS a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2207.12238v1> ;
    skos:altLabel "Self-Supervised Deep Supervision" ;
    skos:definition """The method exploits the finding that high correlation of segmentation performance among each U-Net's decoder layer -- with discriminative layer attached -- tends to have higher segmentation performance in the final segmentation map. By introducing an "Inter-layer Divergence Loss", based on Kulback-Liebler Divergence, to promotes the consistency between each discriminative output from decoder layers by minimizing the divergence.\r
\r
If we assume that each decoder layer is equivalent to PDE function parameterized by weight parameter $\\theta$:\r
\r
$Decoder_i(x;\\theta_i) \\equiv PDE(x;\\theta_i)$\r
\r
Then our objective is trying to make each discriminative output similar to each other:\r
\r
$PDE(x; \\theta_d) \\sim PDE(x; \\theta_i);\\text{ } 0 \\leq i < d$\r
\r
Hence the objective is to $\\text{minimize} \\sum_{i=0}^{d} D_{KL}(\\hat{y} || Decoder_i)$.""" ;
    skos:prefLabel "SSDS" .

:SSE a skos:Concept ;
    dcterms:source <https://icml.cc/Conferences/2018/Schedule?showEvent=2424> ;
    skos:altLabel "Stochastic Steady-state Embedding" ;
    skos:definition """Stochastic Steady-state Embedding (SSE) is an algorithm that can learn many steady-state algorithms over graphs. Different from graph neural network family models, SSE is trained stochastically which only requires 1-hop information, but can capture fixed point relationships efficiently and effectively.\r
\r
Description and Image from: [Learning Steady-States of Iterative Algorithms over Graphs](https://proceedings.mlr.press/v80/dai18a.html)""" ;
    skos:prefLabel "SSE" .

:SSFGregularization a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2102.10338v2> ;
    skos:altLabel "Stochastically Scaling Features and Gradients Regularization" ;
    skos:definition "Please enter a description about the method here" ;
    skos:prefLabel "SSFG regularization" .

:SSKD a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2108.05045v2> ;
    skos:altLabel "Semi-Supervised Knowledge Distillation" ;
    skos:definition "**Semi-Supervised Knowledge Distillation** is a type of knowledge distillation for person re-identification that exploits weakly annotated data by assigning soft pseudo labels to YouTube-Human to improve models' generalization ability. SSKD first trains a student model (e.g. [ResNet](https://paperswithcode.com/method/resnet)-50) and a teacher model (e.g. ResNet-101) using labeled data from multi-source domain datasets. Then, SSKD develops an [auxiliary classifier](https://paperswithcode.com/method/auxiliary-classifier) to imitate the soft predictions of unlabeled data generated by the teacher model. Meanwhile, the student model is also supervised by hard labels and predicted soft labels by the teacher model for labeled data." ;
    skos:prefLabel "SSKD" .

:SSTDA a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2003.02824v3> ;
    skos:altLabel "Self-Supervised Temporal Domain Adaptation" ;
    skos:definition "**Self-Supervised Temporal Domain Adaptation (SSTDA)** is a method for action segmentation with self-supervised temporal domain adaptation. It contains two self-supervised auxiliary tasks (binary and sequential domain prediction) to jointly align cross-domain feature spaces embedded with local and global temporal dynamics." ;
    skos:prefLabel "SSTDA" .

:STA-LSTM a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1611.06067v1> ;
    skos:altLabel "Spatio-Temporal Attention LSTM" ;
    skos:definition """In human action recognition, \r
each type of action  generally only depends \r
on a few specific kinematic joints. Furthermore, over time, multiple actions may be performed.\r
Motivated by these observations, Song et al. proposed \r
a joint spatial and temporal attention network based on LSTM, to adaptively find discriminative features and keyframes. \r
Its main attention-related components are a spatial attention sub-network, to select important regions, and a temporal attention sub-network, to select key frames. The spatial attention sub-network can be written as:\r
\\begin{align}\r
    s_{t} &= U_{s}\\tanh(W_{xs}X_{t} + W_{hs}h_{t-1}^{s} + b_{si}) + b_{so}\r
\\end{align}\r
\\begin{align}\r
    \\alpha_{t} &= \\text{Softmax}(s_{t})\r
\\end{align}\r
\\begin{align}\r
    Y_{t} &= \\alpha_{t}  X_{t} \r
\\end{align}\r
where $X_{t}$ is the input feature at time $t$, $U_{s}$, $W_{hs}$, $b_{si}$, and $b_{so}$ are learnable parameters, and $h_{t-1}^{s}$ is the hidden state at step $t-1$. Note that use of the hidden state $h$ means  the attention process takes  temporal relationships into consideration.\r
\r
The temporal attention sub-network is similar to the spatial branch and produces its attention map using:\r
\\begin{align}\r
    \\beta_{t} = \\delta(W_{xp}X_{t} + W_{hp}h_{t-1}^{p} + b_{p}). \r
\\end{align}\r
It adopts a ReLU function instead of a normalization function for ease of optimization. It also uses a regularized objective function to improve  convergence.\r
\r
Overall, this paper presents a joint spatiotemporal attention method\r
to focus on important joints and keyframes, \r
with excellent results on the action recognition task.""" ;
    skos:prefLabel "STA-LSTM" .

:STAC a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2005.04757v2> ;
    skos:definition "**STAC** is a semi-supervised framework for visual object detection along with a data augmentation strategy. STAC deploys highly confident pseudo labels of localized objects from an unlabeled image and updates the model by enforcing consistency via strong augmentations. We generate pseudo labels (i.e., bounding boxes and their class labels) for unlabeled data using test-time inference, including NMS , of the teacher model trained with labeled data. We then compute unsupervised loss with respect to pseudo labels whose confidence scores are above a threshold $\\tau$ . The strong augmentations are applied for augmentation consistency during the model training. Target boxes are augmented when global geometric transformations are used." ;
    skos:prefLabel "STAC" .

:STATEGAMEMAINTAINPICTUREBALANCEDPLAYSTABLE a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1801.09110v1> ;
    skos:altLabel "ATTEMPT THIS FATHINETUTE TO REPOPULATE ALREADY POPULATED  SYSTEM" ;
    skos:definition "" ;
    skos:prefLabel "STATEGAME MAINTAIN PICTURE BALANCED PLAY STABLE" .

:STD a skos:Concept ;
    dcterms:source <https://proceedings.mlr.press/v162/li22c.html> ;
    skos:altLabel "Spatial-Channel Token Distillation" ;
    skos:definition "The **Spatial-Channel Token Distillation** method is proposed to improve the spatial and channel mixing from a novel knowledge distillation (KD) perspective. To be specific, we design a special KD mechanism for MLP-like Vision Models called Spatial-channel Token Distillation (STD), which improves the information mixing in both the spatial and channel dimensions of MLP blocks. Instead of modifying the mixing operations themselves, STD adds spatial and channel tokens to image patches. After forward propagation, the tokens are concatenated for distillation with the teachers’ responses as targets. Each token works as an aggregator of its dimension. The objective of them is to encourage each mixing operation to extract maximal task-related information from their specific dimension." ;
    skos:prefLabel "STD" .

:STDC a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2104.13188v1> ;
    skos:altLabel "Short-Term Dense Concatenate" ;
    skos:definition """**STDC**, or **Short-Term Dense Concatenate**, is a module for semantic segmentation to extract deep features with scalable\r
receptive field and multi-scale information. It aims to remove structure redundancy in the BiSeNet architecture, specifically BiSeNet adds an extra path to encode spatial information which can be time-consuming,. Instead, STDC gradually reduces the dimension of feature maps and use the aggregation of them for image representation.\r
\r
We concatenate response maps from multiple continuous layers, each of which encodes input image/feature in different scales and respective fields, leading to multi-scale feature representation. To speed up, the filter size of layers is gradually reduced with negligible loss in segmentation performance.""" ;
    skos:prefLabel "STDC" .

:STMDA-RetinaNet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2210.00808v1> ;
    skos:altLabel "Self training multi target domain adaptive RetinaNet" ;
    skos:definition "" ;
    skos:prefLabel "STMDA-RetinaNet" .

:STN a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1506.02025v3> ;
    skos:altLabel "spatial transformer networks" ;
    skos:definition """spatial transformer networks uses an explicit procedure to learn invariance to translation, scaling, rotation and other more general warps, making the network pay attention to the most relevant regions. STN was the first attention mechanism to explicitly predict important regions and provide a deep neural network with transformation invariance.\r
\r
Taking a 2D image as an example, a 2D affine transformation can be formulated as followed, where A denotes a $ 2 \\times 3 $ learneable affine matrix:\r
\r
\\begin{align}\r
A = f_\\text{loc}(U) \r
\\end{align}\r
\\begin{align}\r
x_i^s = A x_i^t\r
\\end{align}\r
\r
Here, $U$ is the input feature map, and $f_\\text{loc}$ can be any differentiable function, such as a lightweight fully-connected network or convolutional neural network. $x_{i}^{s}$  is coordinates in the output feature map, while $x_{i}^{t}$ is corresponding coordinates in the input feature map and the $ A $ matrix is the learnable affine matrix. After obtaining the correspondence, the network can sample relevant input regions using the correspondence. \r
To ensure that the whole process is differentiable and can be updated in an end-to-end manner,  bilinear sampling is used to sample the input features.\r
\r
STNs focus on discriminative regions automatically\r
and  learn invariance to some geometric transformations.""" ;
    skos:prefLabel "STN" .

:STTP a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2103.04217v2> ;
    skos:altLabel "Spectral Tensor Train Parameterization" ;
    skos:definition "" ;
    skos:prefLabel "STTP" .

:STraTA a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2109.06270v2> ;
    skos:altLabel "Self-Training with Task Augmentation" ;
    skos:definition """**STraTA**, or **Self-Training with Task Augmentation**, is a self-training approach that builds on two key ideas for effective leverage of unlabeled data. First, STraTA uses task augmentation, a technique that synthesizes a large amount of data for auxiliary-task fine-tuning from target-task unlabeling texts. Second, STRATA performs self-training by further fine-tuning the strong base model created by task augmentation on a broad distribution of pseudo-labeled data.\r
\r
In task augmentation, we train an NLI data generation model and use it to synthesize a large amount of in-domain NLI training data for each given target task, which is then used for auxiliary (intermediate) fine-tuning. The self-training algorithm iteratively learns a better model using a concatenation of labeled and pseudo-labeled examples. At each iteration, we always start with the auxiliary-task model produced by task augmentation and train on a broad distribution of pseudo-labeled data.""" ;
    skos:prefLabel "STraTA" .

:SVDParameterization a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1803.09327v1> ;
    skos:altLabel "Singular Value Decomposition Parameterization" ;
    skos:definition "" ;
    skos:prefLabel "SVD Parameterization" .

:SVM a skos:Concept ;
    skos:altLabel "Support Vector Machine" ;
    skos:definition """A **Support Vector Machine**, or **SVM**, is a non-parametric supervised learning model. For non-linear classification and regression, they utilise the kernel trick to map inputs to high-dimensional feature spaces. SVMs construct a hyper-plane or set of hyper-planes in a high or infinite dimensional space, which can be used for classification, regression or other tasks. Intuitively, a good separation is achieved by the hyper-plane that has the largest distance to the nearest training data points of any class (so-called functional margin), since in general the larger the margin the lower the generalization error of the classifier. The figure to the right shows the decision function for a linearly separable problem, with three samples on the margin boundaries, called “support vectors”. \r
\r
Source: [scikit-learn](https://scikit-learn.org/stable/modules/svm.html)""" ;
    skos:prefLabel "SVM" .

:SVPG a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1704.02399v1> ;
    skos:altLabel "Stein Variational Policy Gradient" ;
    skos:definition """**Stein Variational Policy Gradient**, or **SVPG**, is a policy gradient based method in reinforcement learning that uses Stein Variational Gradient Descent to allow simultaneous exploitation and exploration of multiple policies. Unlike traditional policy optimization which attempts to learn a single policy, SVPG models a distribution of policy parameters, where samples from this distribution will represent strong policies.  SVPG optimizes this distribution of policy parameters with (relative) [entropy regularization](https://paperswithcode.com/method/entropy-regularization). The (relative) entropy term explicitly encourages exploration in the parameter space while also optimizing the expected utility of polices drawn from this distribution. Stein variational gradient descent (SVGD) is then used to optimize this distribution. SVGD leverages efficient deterministic dynamics to transport a set of particles to approximate given target posterior distributions. \r
\r
The update takes the form:\r
\r
$$ $$\r
\r
$$ \\nabla\\theta\\_i = \\frac{1} {n}\\sum\\_{j=1}^n \\nabla\\_{\\theta\\_{j}} \\left(\\frac{1}{\\alpha} J(\\theta\\_{j}) + \\log q\\_0(\\theta\\_j)\\right)k(\\theta\\_j, \\theta\\_i) + \\nabla\\_{\\theta\\_j} k(\\theta\\_j, \\theta\\_i)$$\r
\r
Note that here the magnitude of $\\alpha$ adjusts the relative importance between the policy gradient and the prior term $\\nabla_{\\theta_j} \\left(\\frac{1}{\\alpha} J(\\theta_j) + \\log q_0(\\theta_j)\\right)k(\\theta_j, \\theta_i)$ and the repulsive term $\\nabla_{\\theta_j} k(\\theta_j, \\theta_i)$. The repulsive functional is used to diversify particles to enable parameter exploration. A suitable $\\alpha$ provides a good trade-off between exploitation and exploration. If $\\alpha$ is too large, the Stein gradient would only drive the particles to be consistent with the prior $q_0$. As $\\alpha \\to 0$, this algorithm is reduced to running $n$ copies of independent policy gradient algorithms, if $\\{\\theta_i\\}$ are initialized very differently. A careful annealing scheme of $\\alpha$ allows efficient exploration in the beginning of training and later focuses on exploitation towards the end of training.""" ;
    skos:prefLabel "SVPG" .

:SaBN a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2102.11382v2> ;
    skos:altLabel "Sandwich Batch Normalization" ;
    skos:definition "Sandwich Batch Normalization (**SaBN**) is a frustratingly easy improvement of [Batch Normalization](https://paperswithcode.com/method/batch-normalization) (BN) with only a few lines of code changes. SaBN is motivated by addressing the inherent *feature distribution heterogeneity* that one can be identified in many tasks, which can arise from data heterogeneity (multiple input domains) or model heterogeneity (dynamic architectures, model conditioning, etc.). Our SaBN factorizes the BN affine layer into one shared *sandwich affine* layer, cascaded by several parallel *independent affine* layers. We demonstrate the prevailing effectiveness of SaBN as a **drop-in replacement in four tasks**: *conditional image generation*, *[neural architecture search](https://paperswithcode.com/method/neural-architecture-search)* (NAS), *adversarial training*, and *arbitrary style transfer*. Leveraging SaBN immediately achieves better Inception Score and FID on CIFAR-10 and ImageNet conditional image generation with three state-of-the-art GANs; boosts the performance of a state-of-the-art weight-sharing NAS algorithm significantly on NAS-Bench-201; substantially improves the robust and standard accuracies for adversarial defense; and produces superior arbitrary stylized results." ;
    skos:prefLabel "SaBN" .

:SampleRedistribution a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2105.04714v1> ;
    skos:definition """**Sample Redistribution** is a [data augmentation](https://paperswithcode.com/methods/category/image-data-augmentation) technique for face detection which augments training samples based on the statistics of benchmark datasets via large-scale cropping. During training data augmentation, square patches are cropped from the original images with a random size from the set $[0.3,1.0]$ of the short edge of the original images. To generate more positive samples for stride 8, the random size range is enlarged from $[0.3,1.0]$ to $[0.3,2.0]$. When the crop box is beyond the original image, average RGB values fill the missing pixels.\r
\r
The motivation is that for efficient [face detection](https://paperswithcode.com/task/face-detection) under a fixed VGA resolution (i.e. 640×480), most of the faces (78.93%) in [WIDER FACE](https://paperswithcode.com/dataset/wider-face-1) are smaller than 32×32 pixels, and thus they are predicted by shallow stages. To obtain more training samples for these shallow stages, Sample Redistribution (SR) is used.""" ;
    skos:prefLabel "Sample Redistribution" .

:SandwichTransformer a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1911.03864v2> ;
    rdfs:seeAlso <https://github.com/ofirpress/sandwich_transformer/blob/9d8e48fd1f9c10586a8a988e07bae4f8a327706f/models.py#L179> ;
    skos:definition """A **Sandwich Transformer** is a variant of a [Transformer](https://paperswithcode.com/method/transformer) that reorders sublayers in the architecture to achieve better performance. The reordering is based on the authors' analysis that models with more self-attention toward the bottom and more\r
feedforward sublayers toward the top tend to perform better in general.""" ;
    skos:prefLabel "Sandwich Transformer" .

:Sarsa a skos:Concept ;
    skos:definition """**Sarsa** is an on-policy TD control algorithm:\r
\r
$$Q\\left(S\\_{t}, A\\_{t}\\right) \\leftarrow Q\\left(S\\_{t}, A\\_{t}\\right) + \\alpha\\left[R_{t+1} + \\gamma{Q}\\left(S\\_{t+1}, A\\_{t+1}\\right) - Q\\left(S\\_{t}, A\\_{t}\\right)\\right] $$\r
\r
This update is done after every transition from a nonterminal state $S\\_{t}$. if $S\\_{t+1}$ is terminal, then $Q\\left(S\\_{t+1}, A\\_{t+1}\\right)$ is defined as zero.\r
\r
To design an on-policy control algorithm using Sarsa, we estimate $q\\_{\\pi}$ for a behaviour policy $\\pi$ and then change $\\pi$ towards greediness with respect to $q\\_{\\pi}$.\r
\r
Source: Sutton and Barto, Reinforcement Learning, 2nd Edition""" ;
    skos:prefLabel "Sarsa" .

:SarsaLambda a skos:Concept ;
    skos:definition """**Sarsa_INLINE_MATH_1** extends eligibility-traces to action-value methods. It has the same update rule as for **TD_INLINE_MATH_1** but we use the action-value form of the TD erorr:\r
\r
$$ \\delta\\_{t} = R\\_{t+1} + \\gamma\\hat{q}\\left(S\\_{t+1}, A\\_{t+1}, \\mathbb{w}\\_{t}\\right) - \\hat{q}\\left(S\\_{t}, A\\_{t}, \\mathbb{w}\\_{t}\\right) $$\r
\r
and the action-value form of the [eligibility trace](https://paperswithcode.com/method/eligibility-trace):\r
\r
$$ \\mathbb{z}\\_{-1} = \\mathbb{0} $$\r
\r
$$ \\mathbb{z}\\_{t} = \\gamma\\lambda\\mathbb{z}\\_{t-1} + \\nabla\\hat{q}\\left(S\\_{t}, A\\_{t}, \\mathbb{w}\\_{t} \\right), 0 \\leq t \\leq T$$\r
\r
Source: Sutton and Barto, Reinforcement Learning, 2nd Edition""" ;
    skos:prefLabel "Sarsa Lambda" .

:ScaleAggregationBlock a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1904.09460v1> ;
    rdfs:seeAlso <https://github.com/Eli-YiLi/ScaleNet/blob/2c27b4207691dbe72f7e19fd88bfccc5ce5b3080/pytorch/scalenet.py#L8> ;
    skos:definition """A **Scale Aggregation Block** concatenates feature maps at a wide range of scales. Feature maps for each scale are generated by a stack of downsampling, [convolution](https://paperswithcode.com/method/convolution) and upsampling operations. The proposed scale aggregation block is a standard computational module which readily replaces any given transformation $\\mathbf{Y}=\\mathbf{T}(\\mathbf{X})$, where $\\mathbf{X}\\in \\mathbb{R}^{H\\times W\\times C}$, $\\mathbf{Y}\\in \\mathbb{R}^{H\\times W\\times C_o}$ with $C$ and $C_o$ being the input and output channel number respectively. $\\mathbf{T}$ is any operator such as a convolution layer or a series of convolution layers. Assume we have $L$ scales. Each scale $l$ is generated by sequentially conducting a downsampling $\\mathbf{D}_l$, a transformation $\\mathbf{T}_l$ and an unsampling operator $\\mathbf{U}_l$:\r
\r
$$\r
\\mathbf{X}^{'}_l=\\mathbf{D}_l(\\mathbf{X}),\r
\\label{eq:eq_d}\r
$$\r
\r
$$\r
\\mathbf{Y}^{'}_l=\\mathbf{T}_l(\\mathbf{X}^{'}_l),\r
\\label{eq:eq_tl}\r
$$\r
\r
$$\r
\\mathbf{Y}_l=\\mathbf{U}_l(\\mathbf{Y}^{'}_l),\r
\\label{eq:eq_u}\r
$$\r
\r
where $\\mathbf{X}^{'}_l\\in \\mathbb{R}^{H_l\\times W_l\\times C}$,\r
$\\mathbf{Y}^{'}_l\\in \\mathbb{R}^{H_l\\times W_l\\times C_l}$, and\r
$\\mathbf{Y}_l\\in \\mathbb{R}^{H\\times W\\times C_l}$.\r
Notably, $\\mathbf{T}_l$ has the similar structure as $\\mathbf{T}$.\r
We can concatenate all $L$ scales together, getting\r
\r
$$\r
\\mathbf{Y}^{'}=\\Vert^L_1\\mathbf{U}_l(\\mathbf{T}_l(\\mathbf{D}_l(\\mathbf{X}))),\r
\\label{eq:eq_all}\r
$$\r
\r
where $\\Vert$ indicates concatenating feature maps along the channel dimension, and $\\mathbf{Y}^{'} \\in \\mathbb{R}^{H\\times W\\times \\sum^L_1 C_l}$ is the final output feature maps of the scale aggregation block.\r
\r
In the reference implementation, the downsampling $\\mathbf{D}_l$ with factor $s$ is implemented by a max pool layer with $s\\times s$ kernel size and  $s$ stride. The upsampling $\\mathbf{U}_l$ is implemented by resizing with the nearest neighbor  interpolation.""" ;
    skos:prefLabel "Scale Aggregation Block" .

:ScaleNet a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1904.09460v1> ;
    rdfs:seeAlso <https://github.com/Eli-YiLi/ScaleNet/blob/2c27b4207691dbe72f7e19fd88bfccc5ce5b3080/pytorch/scalenet.py#L72> ;
    skos:definition "**ScaleNet**, or a **Scale Aggregation Network**, is a type of convolutional neural network which learns a neuron allocation for aggregating multi-scale information in different building blocks of a deep network. The most informative output neurons in each block are preserved while others are discarded, and thus neurons for multiple scales are competitively and adaptively allocated. The scale aggregation (SA) block concatenates feature maps at a wide range of scales. Feature maps for each scale are generated by a stack of downsampling, [convolution](https://paperswithcode.com/method/convolution) and upsampling operations." ;
    skos:prefLabel "ScaleNet" .

:ScaledDot-ProductAttention a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1706.03762v7> ;
    rdfs:seeAlso <https://github.com/jadore801120/attention-is-all-you-need-pytorch/blob/5c0264915ab43485adc576f88971fc3d42b10445/transformer/Modules.py#L7> ;
    skos:definition """**Scaled dot-product attention** is an attention mechanism where the dot products are scaled down by $\\sqrt{d_k}$. Formally we have a query $Q$, a key $K$ and a value $V$ and calculate the attention as:\r
\r
$$ {\\text{Attention}}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^{T}}{\\sqrt{d_k}}\\right)V $$\r
\r
If we assume that $q$ and $k$ are $d_k$-dimensional vectors whose components are independent random variables with mean $0$ and variance $1$, then their dot product, $q \\cdot k = \\sum_{i=1}^{d_k} u_iv_i$, has mean $0$ and variance $d_k$.  Since we would prefer these values to have variance $1$, we divide by $\\sqrt{d_k}$.""" ;
    skos:prefLabel "Scaled Dot-Product Attention" .

:ScaledSoftSign a skos:Concept ;
    dcterms:source <https://doi.org/10.20944/preprints202301.0463.v1> ;
    skos:definition """The **ScaledSoftSign** is a modification of **[SoftSign](https://paperswithcode.com/method/softsign-activation)** activation function that has trainable parameters.\r
\r
$$ScaledSoftSign(x) = \\frac{\\alpha x}{\\beta + |x|}$$""" ;
    skos:prefLabel "ScaledSoftSign" .

:ScanSSD a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2003.08005v1> ;
    skos:definition "**ScanSSD** is a single-shot Detector ([SSD](https://paperswithcode.com/method/ssd)) for locating math formulas offset from text and embedded in textlines. It uses only visual features for detection: no formatting or typesetting information such as layout, font, or character labels are employed. Given a 600 dpi document page image, a Single Shot Detector (SSD) locates formulas at multiple scales using sliding windows, after which candidate detections are pooled to obtain page-level results." ;
    skos:prefLabel "ScanSSD" .

:ScatNet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1203.1513v2> ;
    skos:altLabel "Scattering Transform" ;
    skos:definition """A wavelet **scattering transform** computes a translation invariant representation, which is stable to deformation, using a deep [convolution](https://paperswithcode.com/method/convolution) network architecture. It computes non-linear invariants with modulus and averaging pooling functions. It helps to eliminate the image variability due to translation and is stable to deformations. \r
\r
Image source: [Bruna and Mallat](https://arxiv.org/pdf/1203.1513v2.pdf)""" ;
    skos:prefLabel "ScatNet" .

:ScatterConnection a skos:Concept ;
    skos:definition "A **Scatter Connection** is a type of connection that allows a vector to be \"scattered\" onto a layer representing a map, so that a vector at a specific location corresponds to objects of interest at that location (e.g. units in Starcraft II). This allows for the integration of spatial and non-spatial features." ;
    skos:prefLabel "Scatter Connection" .

:SchNet a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1706.08566v5> ;
    skos:altLabel "Schrödinger Network" ;
    skos:definition "**SchNet** is an end-to-end deep neural network architecture based on continuous-filter convolutions. It follows the deep tensor neural network framework, i.e. atom-wise representations are constructed by starting from embedding vectors that characterize the atom type before introducing the configuration of the system by a series of interaction blocks." ;
    skos:prefLabel "SchNet" .

:ScheduledDropPath a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1707.07012v4> ;
    rdfs:seeAlso <https://github.com/tensorflow/models/blob/e54fcee236a1258302342bd703ee27cbba0c12e3/research/slim/nets/nasnet/nasnet.py#L310> ;
    skos:definition "**ScheduledDropPath** is a modified version of [DropPath](https://paperswithcode.com/method/droppath). In DropPath, each path in the cell is stochastically dropped with some fixed probability during training. In ScheduledDropPath, each path in the cell is dropped out with a probability that is linearly increased over the course of training." ;
    skos:prefLabel "ScheduledDropPath" .

:SeesawLoss a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2008.10032v4> ;
    skos:definition """**Seesaw Loss** is a loss function for long-tailed instance segmentation. It dynamically re-balances the gradients of positive and negative samples on a tail class with two complementary factors: mitigation factor and compensation factor. The mitigation factor reduces punishments to tail categories w.r.t the ratio of cumulative training instances between different categories. Meanwhile, the compensation factor increases the penalty of misclassified instances to avoid false positives of tail categories. The synergy of the two factors enables Seesaw Loss to mitigate the overwhelming punishments on tail classes as well as compensate for the risk of misclassification caused by diminished penalties.\r
\r
$$ L\\_{seesaw}\\left(\\mathbf{x}\\right) = - \\sum^{C}\\_{i=1}y\\_{i}\\log\\left(\\hat{\\sigma}\\_{i}\\right) $$\r
\r
$$ \\text{with } \\hat{\\sigma\\_{i}} = \\frac{e^{z\\_{i}}}{- \\sum^{C}\\_{j\\neq{1}}\\mathcal{S}\\_{ij}e^{z\\_{j}}+e^{z\\_{i}} } $$\r
\r
Here $\\mathcal{S}\\_{ij}$ works as a tunable balancing factor between different classes. By a careful design of $\\mathcal{S}\\_{ij}$, Seesaw loss adjusts the punishments on class j from positive samples of class $i$. Seesaw loss determines $\\mathcal{S}\\_{ij}$ by a mitigation factor and a compensation factor, as:\r
\r
$$ \\mathcal{S}\\_{ij} =\\mathcal{M}\\_{ij} · \\mathcal{C}\\_{ij}  $$\r
\r
The mitigation factor $\\mathcal{M}\\_{ij}$ decreases the penalty on tail class $j$ according to a ratio of instance numbers between tail class $j$ and head class $i$. The compensation factor $\\mathcal{C}\\_{ij}$ increases the penalty on class $j$ whenever an instance of class $i$ is misclassified to class $j$.""" ;
    skos:prefLabel "Seesaw Loss" .

:SegFormer a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2105.15203v3> ;
    skos:definition "**SegFormer** is a [Transformer](https://paperswithcode.com/methods/category/transformers)-based framework for semantic segmentation that unifies Transformers with lightweight [multilayer perceptron](https://paperswithcode.com/method/feedforward-network) (MLP) decoders. SegFormer has two appealing features: 1) SegFormer comprises a novel hierarchically structured Transformer encoder which outputs multiscale features. It does not need positional encoding, thereby avoiding the interpolation of positional codes which leads to decreased performance when the testing resolution differs from training. 2) SegFormer avoids complex decoders. The proposed MLP decoder aggregates information from different layers, and thus combining both local attention and global attention to render powerful representations." ;
    skos:prefLabel "SegFormer" .

:SegNet a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1511.00561v3> ;
    rdfs:seeAlso <https://github.com/yassouali/pytorch_segmentation/blob/8b8e3ee20a3aa733cb19fc158ad5d7773ed6da7f/models/segnet.py#L9> ;
    skos:definition """**SegNet** is a semantic segmentation model. This core trainable segmentation architecture consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the\r
VGG16 network. The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature maps. Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to\r
perform non-linear upsampling.""" ;
    skos:prefLabel "SegNet" .

:SegSort a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1910.06962v2> ;
    skos:altLabel "Segment Sorting" ;
    skos:definition "" ;
    skos:prefLabel "SegSort" .

:SegregatedAttentionNetwork a skos:Concept ;
    dcterms:source <https://journals.flvc.org/FLAIRS/article/view/128538> ;
    skos:definition "" ;
    skos:prefLabel "Segregated Attention Network" .

:SelectiveKernel a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1903.06586v2> ;
    rdfs:seeAlso <https://github.com/osmr/imgclsmob/blob/68335927ba27f2356093b985bada0bc3989836b1/pytorch/pytorchcv/models/sknet.py#L15> ;
    skos:definition """A **Selective Kernel** unit is a bottleneck block consisting of a sequence of 1×1 [convolution](https://paperswithcode.com/method/convolution), SK convolution and 1×1 convolution. It was proposed as part of the [SKNet](https://paperswithcode.com/method/sknet) CNN architecture. In general, all the large kernel convolutions in the original bottleneck blocks in [ResNeXt](https://paperswithcode.com/method/resnext) are replaced by the proposed SK convolutions, enabling the network to choose appropriate receptive field sizes in an adaptive manner. \r
\r
In SK units, there are three important hyper-parameters which determine the final settings of SK convolutions: the number of paths $M$ that determines the number of choices of different kernels to be aggregated, the group number $G$ that controls the cardinality of each path, and the reduction ratio $r$ that controls the number of parameters in the fuse operator. One typical setting of SK convolutions is $\\text{SK}\\left[M, G, r\\right]$ to be $\\text{SK}\\left[2, 32, 16\\right]$.""" ;
    skos:prefLabel "Selective Kernel" .

:SelectiveKernelConvolution a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1903.06586v2> ;
    rdfs:seeAlso <https://github.com/osmr/imgclsmob/blob/68335927ba27f2356093b985bada0bc3989836b1/pytorch/pytorchcv/models/sknet.py#L15> ;
    skos:definition """A **Selective Kernel Convolution** is a [convolution](https://paperswithcode.com/method/convolution) that enables neurons to adaptively adjust their RF sizes among multiple kernels with different kernel sizes. Specifically, the SK convolution has three operators – Split, Fuse and Select. Multiple branches with different kernel sizes are fused using\r
[softmax](https://paperswithcode.com/method/softmax) attention that is guided by the information in these branches. Different attentions on these branches yield different sizes of the effective receptive fields of neurons in the fusion layer""" ;
    skos:prefLabel "Selective Kernel Convolution" .

:SelectiveSearch a skos:Concept ;
    skos:definition """**Selective Search** is a region proposal algorithm for object detection tasks. It starts by over-segmenting the image based on intensity of the pixels using a graph-based segmentation method by Felzenszwalb and Huttenlocher. Selective Search then takes these oversegments as initial input and performs the following steps\r
\r
1. Add all bounding boxes corresponding to segmented parts to the list of regional proposals\r
2. Group adjacent segments based on similarity\r
3. Go to step 1\r
\r
At each iteration, larger segments are formed and added to the list of region proposals. Hence we create region proposals from smaller segments to larger segments in a bottom-up approach. This is what we mean by computing “hierarchical” segmentations using Felzenszwalb and Huttenlocher’s oversegments.""" ;
    skos:prefLabel "Selective Search" .

:Self-AdjustingSmoothL1Loss a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1901.03353v1> ;
    rdfs:seeAlso <https://github.com/chengyangfu/retinamask/blob/ce1eac2bb9882797dcba2a9bc72f994bec04fbdf/maskrcnn_benchmark/layers/adjust_smooth_l1_loss.py#L7> ;
    skos:definition """**Self-Adjusting Smooth L1 Loss** is a loss function used in object detection that was introduced with [RetinaMask](https://paperswithcode.com/method/retinamask). This is an improved version of Smooth L1.  For Smooth L1 loss we have:\r
\r
$$ f(x) = 0.5  \\frac{x^{2}}{\\beta} \\text{ if } |x| < \\beta $$\r
$$ f(x) = |x| -0.5\\beta \\text{ otherwise } $$\r
\r
Here a point $\\beta$ splits the positive axis range into two parts: $L2$ loss is used for targets in range $[0, \\beta]$, and $L1$ loss is used beyond $\\beta$ to avoid over-penalizing  utliers. The overall function is smooth (continuous, together with its derivative). However, the choice of control point ($\\beta$) is heuristic and is usually done by hyper parameter search.\r
\r
Instead, with self-adjusting smooth L1 loss, inside the loss function the running mean and variance of the absolute loss are recorded. We use the running minibatch mean and variance with a momentum of $0.9$ to update these two parameters.""" ;
    skos:prefLabel "Self-Adjusting Smooth L1 Loss" .

:Self-AdversarialNegativeSampling a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1902.10197v1> ;
    skos:definition """**Self-Adversarial Negative Sampling** is a negative sampling technique used for methods like [word embeddings](https://paperswithcode.com/methods/category/word-embeddings) and [knowledge graph embeddings](https://paperswithcode.com/methods/category/graph-embeddings). The traditional negative sampling loss from word2vec for optimizing distance-based models be written as:\r
\r
$$ L = −\\log\\sigma\\left(\\gamma − d\\_{r}\\left(\\mathbf{h}, \\mathbf{t}\\right)\\right) − \\sum^{n}\\_{i=1}\\frac{1}{k}\\log\\sigma\\left(d\\_{r}\\left(\\mathbf{h}^{'}\\_{i}, \\mathbf{t}^{'}\\_{i}\\right) - \\gamma\\right) $$\r
\r
where $\\gamma$ is a fixed margin, $\\sigma$ is the sigmoid function, and $\\left(\\mathbf{h}^{'}\\_{i}, r, \\mathbf{t}^{'}\\_{i}\\right)$ is the $i$-th negative triplet. \r
\r
The negative sampling loss samples the negative triplets in a uniform way. Such a uniform negative sampling suffers the problem of inefficiency since many samples are obviously false as training goes on, which does not provide any meaningful information. Therefore, the authors propose an approach called self-adversarial negative sampling, which samples negative triples according to the current embedding model. Specifically, we sample negative triples from the following distribution:\r
\r
$$ p\\left(h^{'}\\_{j}, r, t^{'}\\_{j} | \\text{set}\\left(h\\_{i}, r\\_{i}, t\\_{i} \\right) \\right) = \\frac{\\exp\\alpha{f}\\_{r}\\left(\\mathbf{h}^{'}\\_{j}, \\mathbf{t}^{'}\\_{j}\\right)}{\\sum\\_{i=1}\\exp\\alpha{f}\\_{r}\\left(\\mathbf{h}^{'}\\_{i}, \\mathbf{t}^{'}\\_{i}\\right)} $$\r
\r
where $\\alpha$ is the temperature of sampling. Moreover, since the sampling procedure may be costly, the authors treat the above probability as the weight of the negative sample. Therefore, the final negative sampling loss with self-adversarial training takes the following form:\r
\r
$$ L = −\\log\\sigma\\left(\\gamma − d\\_{r}\\left(\\mathbf{h}, \\mathbf{t}\\right)\\right) − \\sum^{n}\\_{i=1}p\\left(h^{'}\\_{i}, r, t^{'}\\_{i}\\right)\\log\\sigma\\left(d\\_{r}\\left(\\mathbf{h}^{'}\\_{i}, \\mathbf{t}^{'}\\_{i}\\right) - \\gamma\\right) $$""" ;
    skos:prefLabel "Self-Adversarial Negative Sampling" .

:Self-AttentionGuidance a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2210.00939v6> ;
    skos:definition "" ;
    skos:prefLabel "Self-Attention Guidance" .

:Self-CalibratedConvolutions a skos:Concept ;
    dcterms:source <http://openaccess.thecvf.com/content_CVPR_2020/html/Liu_Improving_Convolutional_Networks_With_Self-Calibrated_Convolutions_CVPR_2020_paper.html> ;
    skos:definition """Liu et al. presented self-calibrated convolution as a means to enlarge the receptive field at each spatial location. \r
\r
Self-calibrated convolution is used together with a standard convolution. It first divides the input feature $X$ into $X_{1}$ and $X_{2}$ in the channel domain. The self-calibrated convolution first uses average pooling to reduce the input size and enlarge the receptive field:\r
\\begin{align}\r
T_{1} = AvgPool_{r}(X_{1}) \r
\\end{align}\r
where $r$ is the filter size and stride. Then a convolution is used to model the channel relationship and a bilinear interpolation operator $Up$ is used to upsample the feature map: \r
\r
\\begin{align}\r
X'_{1} = \\text{Up}(Conv_2(T_1))\r
\\end{align}\r
\r
Next, element-wise multiplication finishes the self-calibrated process:\r
\r
\\begin{align}\r
Y'_{1} = Conv_3(X_1) \\sigma(X_1 + X'_1)\r
\\end{align}\r
\r
Finally, the output feature map of is formed:\r
\\begin{align}\r
Y_{1} &= Conv_4(Y'_{1})\r
\\end{align}\r
\\begin{align}\r
Y_2 &= Conv_1(X_2)\r
\\end{align}\r
\\begin{align}\r
Y &= [Y_1; Y_2]\r
\\end{align}\r
Such self-calibrated convolution can enlarge the receptive field of a network and improve its adaptability. It achieves excellent results in image classification and certain downstream tasks such as instance segmentation, object detection and keypoint detection.""" ;
    skos:prefLabel "Self-Calibrated Convolutions" .

:Self-Learning a skos:Concept ;
    skos:altLabel "None" ;
    skos:definition "" ;
    skos:prefLabel "Self-Learning" .

:Self-adaptiveTraining a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2002.10319v2> ;
    skos:definition "**Self-adaptive Training** is a training algorithm that dynamically corrects problematic training labels by model predictions to improve generalization of deep learning for potentially corrupted training data. Accumulated predictions are used to augment the training dynamics. The use of an exponential-moving-average scheme alleviates the instability issue of model predictions, smooths out the training target during the training process and enables the algorithm to completely change the training labels if necessary." ;
    skos:prefLabel "Self-adaptive Training" .

:SemanticReasoningNetwork a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2003.12294v1> ;
    skos:definition "**Semantic reasoning network**, or **SRN**, is an end-to-end trainable framework for scene text recognition that consists of four parts: backbone network, parallel [visual attention](https://paperswithcode.com/method/visual-attention) module (PVAM), global semantic reasoning module (GSRM), and visual-semantic fusion decoder (VSFD). Given an input image, the backbone network is first used to extract 2D features $V$. Then, the PVAM is used to generate $N$ aligned 1-D features $G$, where each feature corresponds to a character in the text and captures the aligned visual information. These $N$ 1-D features $G$ are then fed into a GSRM to capture the semantic information $S$. Finally, the aligned visual features $G$ and the semantic information $S$ are fused by the VSFD to predict $N$ characters. For text string shorter than $N$, ’EOS’ are padded." ;
    skos:prefLabel "Semantic Reasoning Network" .

:SentencePiece a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1808.06226v1> ;
    skos:definition "**SentencePiece** is a subword tokenizer and detokenizer for natural language processing. It performs subword segmentation, supporting the byte-pair-encoding ([BPE](https://paperswithcode.com/method/bpe)) algorithm and unigram language model, and then converts this text into an id sequence guarantee perfect reproducibility of the normalization and subword segmentation." ;
    skos:prefLabel "SentencePiece" .

:SepFormer a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2010.13154v2> ;
    skos:definition """**SepFormer** is [Transformer](https://paperswithcode.com/methods/category/transformers)-based neural network for speech separation. The SepFormer learns short and long-term dependencies with a multi-scale approach that employs transformers. It is mainly composed of multi-head attention and feed-forward layers. A dual-path framework (introduced by DPRNN) is adopted and [RNNs](https://paperswithcode.com/methods/category/recurrent-neural-networks) are replaced with a multiscale pipeline composed of transformers that learn both short and long-term dependencies. The dual-path framework enables the mitigation of the quadratic complexity of transformers, as transformers in the dual-path framework process smaller chunks.\r
\r
The model is based on the learned-domain masking approach and employs an encoder, a decoder, and a masking network, as shown in the figure. The encoder is fully convolutional, while the decoder employs two Transformers embedded inside the dual-path processing block. The decoder finally reconstructs the separated signals in the time domain by using the masks predicted by the masking network.""" ;
    skos:prefLabel "SepFormer" .

:SeparateAndDiffuse a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2301.10752v2> ;
    skos:definition "" ;
    skos:prefLabel "Separate And Diffuse" .

:Seq2Edits a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2009.11136v1> ;
    skos:definition """**Seq2Edits** is an open-vocabulary approach to sequence editing for natural language processing (NLP) tasks with a high degree of overlap between input and output texts. In this approach, each sequence-to-sequence transduction is represented as a sequence of edit operations, where each operation either replaces an entire source span with target tokens or keeps it unchanged. For text normalization, sentence fusion, sentence splitting & rephrasing, text simplification, and grammatical error correction, the approach improves explainability by associating each edit operation with a human-readable tag.\r
\r
Rather than generating the target sentence as a series of tokens, the model predicts a sequence of edit operations that, when applied to the source sentence, yields the target sentence. Each edit operates on a span in the source sentence and either copies, deletes, or replaces it with one or more target tokens. Edits are generated auto-regressively from left to right using a modified [Transformer](https://paperswithcode.com/method/transformer) architecture to facilitate learning of long-range dependencies.""" ;
    skos:prefLabel "Seq2Edits" .

:Seq2Seq a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1409.3215v3> ;
    skos:altLabel "Sequence to Sequence" ;
    skos:definition """**Seq2Seq**, or **Sequence To Sequence**, is a model used in sequence prediction tasks, such as language modelling and machine translation. The idea is to use one [LSTM](https://paperswithcode.com/method/lstm), the *encoder*, to read the input sequence one timestep at a time, to obtain a large fixed dimensional vector representation (a context vector), and then to use another LSTM, the *decoder*, to extract the output sequence\r
from that vector. The second LSTM is essentially a recurrent neural network language model except that it is conditioned on the input sequence.\r
\r
(Note that this page refers to the original seq2seq not general sequence-to-sequence models)""" ;
    skos:prefLabel "Seq2Seq" .

:SeqINT a skos:Concept ;
    dcterms:source <https://www.sciencedirect.com/science/article/pii/S0306457323000110> ;
    skos:altLabel "Sequential Information Threading" ;
    skos:definition "Unsupervised machine learning approach for identifying information threads by leveraging answers to 5W1H questions from documents, the temporal relationships between documents and hierarchical agglomerative clustering (HAC)." ;
    skos:prefLabel "SeqINT" .

:Serf a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2108.09598v3> ;
    skos:definition """**Serf**, or **Log-Softplus ERror activation Function**, is a type of activation function which is self-regularized and nonmonotonic in nature. It belongs to the [Swish](https://paperswithcode.com/method/swish) family of functions. Serf is defined as:\r
\r
$$f\\left(x\\right) = x\\text{erf}\\left(\\ln\\left(1 + e^{x}\\right)\\right)$$""" ;
    skos:prefLabel "Serf" .

:SetTransformer a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1810.00825v3> ;
    skos:definition "Many machine learning tasks such as multiple instance learning, 3D shape recognition, and few-shot image classification are defined on sets of instances. Since solutions to such problems do not depend on the order of elements of the set, models used to address them should be permutation invariant. We present an attention-based neural network module, the Set Transformer, specifically designed to model interactions among elements in the input set. The model consists of an encoder and a decoder, both of which rely on attention mechanisms. In an effort to reduce computational complexity, we introduce an attention scheme inspired by inducing point methods from sparse Gaussian process literature. It reduces the computation time of self-attention from quadratic to linear in the number of elements in the set. We show that our model is theoretically attractive and we evaluate it on a range of tasks, demonstrating the state-of-the-art performance compared to recent methods for set-structured data." ;
    skos:prefLabel "Set Transformer" .

:Shake-ShakeRegularization a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1705.07485v2> ;
    rdfs:seeAlso <https://github.com/osmr/imgclsmob/blob/68335927ba27f2356093b985bada0bc3989836b1/pytorch/pytorchcv/models/shakeshakeresnet_cifar.py#L18> ;
    skos:definition """**Shake-Shake Regularization**  aims to improve the generalization ability of multi-branch networks by replacing the standard summation of parallel branches with a stochastic affine combination. A typical pre-activation [ResNet](https://paperswithcode.com/method/resnet) with 2 residual branches would follow this equation:\r
\r
$$x\\_{i+1} = x\\_{i} + \\mathcal{F}\\left(x\\_{i}, \\mathcal{W}\\_{i}^{\\left(1\\right)}\\right) + \\mathcal{F}\\left(x\\_{i}, \\mathcal{W}\\_{i}^{\\left(2\\right)}\\right) $$\r
\r
Shake-shake regularization introduces a random variable $\\alpha\\_{i}$  following a uniform distribution between 0 and 1 during training:\r
\r
$$x\\_{i+1} = x\\_{i} + \\alpha\\mathcal{F}\\left(x\\_{i}, \\mathcal{W}\\_{i}^{\\left(1\\right)}\\right) + \\left(1-\\alpha\\right)\\mathcal{F}\\left(x\\_{i}, \\mathcal{W}\\_{i}^{\\left(2\\right)}\\right) $$\r
\r
Following the same logic as for [dropout](https://paperswithcode.com/method/dropout), all $\\alpha\\_{i}$ are set to the expected value of $0.5$ at test time.""" ;
    skos:prefLabel "Shake-Shake Regularization" .

:ShakeDrop a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1802.02375v3> ;
    rdfs:seeAlso <https://github.com/owruby/shake-drop_pytorch/blob/4ef188475101dfc6f73191388e320969823f9e7d/models/shakedrop.py#L9> ;
    skos:definition """**ShakeDrop regularization** extends [Shake-Shake regularization](https://paperswithcode.com/method/shake-shake-regularization) and can be applied not only to [ResNeXt](https://paperswithcode.com/method/resnext) but also [ResNet](https://paperswithcode.com/method/resnet), [WideResNet](https://paperswithcode.com/method/wideresnet), and [PyramidNet](https://paperswithcode.com/method/pyramidnet). The proposed ShakeDrop is given as\r
\r
$$G\\left(x\\right) = x + \\left(b\\_{l} + \\alpha − b\\_{l}\\alpha\\right)F\\left(x\\right), \\text{ in train-fwd} $$\r
$$G\\left(x\\right) = x + \\left(b\\_{l} + \\beta − b\\_{l}\\beta\\right)F\\left(x\\right), \\text{ in train-bwd} $$\r
$$G\\left(x\\right) = x + E\\left[b\\_{l} + \\alpha − b\\_{l}\\alpha\\right]F\\left(x\\right), \\text{ in test} $$\r
\r
where $b\\_{l}$ is a Bernoulli random variable with probability $P\\left(b\\_{l} = 1\\right) = E\\left[b\\_{l}\r
\\right] = p\\_{l}$ given by the linear decay rule in each layer, and $\\alpha$ and $\\beta$ are independent uniform random variables in each element. \r
\r
The most effective ranges of $\\alpha$ and $\\beta$ were experimentally found to be different from those of Shake-Shake, and are $\\alpha$ = 0, $\\beta \\in \\left[0, 1\\right]$ and $\\alpha \\in \\left[−1, 1\\right]$, $\\beta \\in \\left[0, 1\\right]$.""" ;
    skos:prefLabel "ShakeDrop" .

:ShapeAdaptor a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2008.00892v2> ;
    rdfs:seeAlso <https://github.com/lorenmt/shape-adaptor/blob/7dc323d26b8c6fda8d0087023a0a60df0b8d5e91/model_list.py#L9> ;
    skos:definition """**Shape Adaptor** is a novel resizing module for neural networks. It is a drop-in enhancement built on top of traditional resizing layers, such as pooling, bilinear sampling, and strided [convolution](https://paperswithcode.com/method/convolution). This module allows for a learnable shaping factor which differs from the traditional resizing layers that are fixed and deterministic.\r
\r
Image Source: [Liu et al.](https://arxiv.org/pdf/2008.00892v2.pdf)""" ;
    skos:prefLabel "Shape Adaptor" .

:ShapeConv a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2108.10528v1> ;
    skos:definition "**ShapeConv**, or **Shape-aware Convolutional layer**, is a convolutional layer for processing the depth feature in indoor RGB-D semantic segmentation. The depth feature is firstly decomposed into a shape-component and a base-component, next two learnable weights are introduced to cooperate with them independently, and finally a [convolution](https://paperswithcode.com/method/convolution) is applied on the re-weighted combination of these two components." ;
    skos:prefLabel "ShapeConv" .

:Sharpness-AwareMinimization a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2010.01412v3> ;
    skos:definition "**Sharpness-Aware Minimization**, or **SAM**, is a procedure that improves model generalization by simultaneously minimizing loss value and loss sharpness. SAM functions by seeking parameters that lie in neighborhoods having uniformly low loss value (rather than parameters that only themselves have low loss value)." ;
    skos:prefLabel "Sharpness-Aware Minimization" .

:ShiLU a skos:Concept ;
    dcterms:source <https://doi.org/10.20944/preprints202301.0463.v1> ;
    skos:altLabel "Shifted Rectified Linear Unit" ;
    skos:definition """The **Shifted Rectified Linear Unit**, or **ShiLU**, is a modification of **[ReLU](https://paperswithcode.com/method/relu)** activation function that has trainable parameters.\r
\r
$$ShiLU(x) = \\alpha ReLU(x) + \\beta$$""" ;
    skos:prefLabel "ShiLU" .

:ShiftedSoftplus a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1706.08566v5> ;
    skos:definition "**Shifted Softplus** is an activation function ${\\rm ssp}(x) = \\ln( 0.5 e^{x} + 0.5 )$, which [SchNet](https://paperswithcode.com/method/schnet) employs as non-linearity throughout the network in order to obtain a smooth potential energy surface. The shifting ensures that ${\\rm ssp}(0) = 0$ and improves the convergence of the network. This activation function shows similarity to ELUs, while having infinite order of continuity." ;
    skos:prefLabel "Shifted Softplus" .

:Shuffle-T a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2106.03650v1> ;
    skos:altLabel "Shuffle Transformer" ;
    skos:definition """The **Shuffle Transformer Block** consists of the Shuffle Multi-Head Self-Attention module (ShuffleMHSA), the Neighbor-Window Connection module (NWC), and the MLP module. To introduce cross-window connections while maintaining the efficient computation of non-overlapping windows, a strategy which alternates between WMSA and Shuffle-WMSA in consecutive Shuffle Transformer blocks is proposed. The first window-based transformer block uses regular window partition strategy and the second window-based transformer block uses window-based selfattention with spatial shuffle. Besides, the Neighbor-Window Connection moduel (NWC) is added into each block for enhancing connections among neighborhood windows. Thus the proposed shuffle transformer block could build rich cross-window connections and augments representation. Finally, the consecutive Shuffle Transformer blocks are computed as:\r
\r
$$ x^{l}=\\mathbf{W M S A}\\left(\\mathbf{B N}\\left(z^{l-1}\\right)\\right)+z^{l-1} $$\r
\r
$$ y^{l}=\\mathbf{N W C}\\left(x^{l}\\right)+x^{l} $$\r
\r
$$ z^{l}=\\mathbf{M L P}\\left(\\mathbf{B N}\\left(y^{l}\\right)\\right)+y^{l} $$\r
\r
$$ x^{l+1}=\\mathbf{S h u f f l e - W M S A}\\left(\\mathbf{B N}\\left(z^{l}\\right)\\right)+z^{l} $$\r
\r
$$ y^{l+1}=\\mathbf{N W C}\\left(x^{l+1}\\right)+x^{l+1} $$\r
\r
$$ z^{l+1}=\\mathbf{M L P}\\left(\\mathbf{B N}\\left(y^{l+1}\\right)\\right)+y^{l+1} $$\r
\r
where $x^l$, $y^l$ and $z^l$ denote the output features of the (Shuffle-)WMSA module, the Neighbor-Window Connection module and the MLP module for block $l$, respectively; WMSA and Shuffle-WMSA denote\r
window-based multi-head self-attention without/with spatial shuffle, respectively.""" ;
    skos:prefLabel "Shuffle-T" .

:ShuffleNet a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1707.01083v2> ;
    rdfs:seeAlso <https://github.com/mindspore-ecosystem/mindcv/blob/main/mindcv/models/shufflenetv1.py> ;
    skos:definition "**ShuffleNet** is a convolutional neural network designed specially for mobile devices with very limited computing power. The architecture utilizes two new operations, pointwise group [convolution](https://paperswithcode.com/method/convolution) and [channel shuffle](https://paperswithcode.com/method/channel-shuffle), to reduce computation cost while maintaining accuracy." ;
    skos:prefLabel "ShuffleNet" .

:ShuffleNetBlock a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1707.01083v2> ;
    rdfs:seeAlso <https://github.com/osmr/imgclsmob/blob/c03fa67de3c9e454e9b6d35fe9cbb6b15c28fda7/pytorch/pytorchcv/models/shufflenet.py#L18> ;
    skos:definition "A **ShuffleNet Block** is an image model block that utilises a [channel shuffle](https://paperswithcode.com/method/channel-shuffle) operation, along with depthwise convolutions, for an efficient architectural design. It was proposed as part of the [ShuffleNet](https://paperswithcode.com/method/shufflenet) architecture. The starting point is the [Residual Block](https://paperswithcode.com/method/residual-block) unit from [ResNets](https://paperswithcode.com/method/resnet), which is then modified with a pointwise group [convolution](https://paperswithcode.com/method/convolution) and a channel shuffle operation." ;
    skos:prefLabel "ShuffleNet Block" .

:ShuffleNetV2Block a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1807.11164v1> ;
    rdfs:seeAlso <https://github.com/pytorch/vision/blob/227027d5abc8eacb110c93b5b5c2f4ea5dd401d6/torchvision/models/shufflenetv2.py#L36> ;
    skos:definition """**ShuffleNet V2 Block** is an image model block used in the [ShuffleNet V2](https://paperswithcode.com/method/shufflenet-v2) architecture, where speed is the metric optimized for (instead of indirect ones like FLOPs). It utilizes a simple operator called channel split. At the beginning of each unit, the input of $c$ feature channels are split into two branches with $c - c'$ and $c'$ channels, respectively. Following **G3**, one branch remains as identity. The other branch consists of three convolutions with the same input and output channels to satisfy **G1**. The two $1\\times1$ convolutions are no longer group-wise, unlike the original [ShuffleNet](https://paperswithcode.com/method/shufflenet). This is partially to follow **G2**, and partially because the split operation already produces two groups. After [convolution](https://paperswithcode.com/method/convolution), the two branches are concatenated. So, the number of channels keeps the same (G1). The same “[channel shuffle](https://paperswithcode.com/method/channel-shuffle)” operation as in ShuffleNet is then used to enable information communication between the two branches.\r
\r
The motivation behind channel split is that alternative architectures, where pointwise group convolutions and bottleneck structures are used, lead to increased memory access cost. Additionally more network fragmentation with group convolutions reduces parallelism (less friendly for GPU), and the element-wise addition operation, while they have low FLOPs, have high memory access cost. Channel split is an alternative where we can maintain a large number of equally wide channels (equally wide minimizes memory access cost) without having dense convolutions or too many groups.""" ;
    skos:prefLabel "ShuffleNet V2 Block" .

:ShuffleNetV2DownsamplingBlock a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1807.11164v1> ;
    rdfs:seeAlso <https://github.com/osmr/imgclsmob/blob/c03fa67de3c9e454e9b6d35fe9cbb6b15c28fda7/pytorch/pytorchcv/models/shufflenetv2.py#L16> ;
    skos:definition "**ShuffleNet V2 Downsampling Block** is a block for spatial downsampling used in the [ShuffleNet V2](https://paperswithcode.com/method/shufflenet-v2) architecture. Unlike the regular [ShuffleNet](https://paperswithcode.com/method/shufflenet) V2 block, the channel split operator is removed so the number of output channels is doubled." ;
    skos:prefLabel "ShuffleNet V2 Downsampling Block" .

:ShuffleNetv2 a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1807.11164v1> ;
    rdfs:seeAlso <https://github.com/pytorch/vision/blob/6db1569c89094cf23f3bc41f79275c45e9fcb3f3/torchvision/models/shufflenetv2.py#L86> ;
    skos:definition "**ShuffleNet v2** is a convolutional neural network optimized for a direct metric (speed) rather than indirect metrics like FLOPs. It builds upon [ShuffleNet v1](https://paperswithcode.com/method/shufflenet), which utilised pointwise group convolutions, bottleneck-like structures, and a [channel shuffle](https://paperswithcode.com/method/channel-shuffle) operation. Differences are shown in the Figure to the right, including a new channel split operation and moving the channel shuffle operation further down the block." ;
    skos:prefLabel "ShuffleNet v2" .

:SiLU a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1702.03118v3> ;
    rdfs:seeAlso <https://github.com/hendrycks/GELUs/blob/master/twitter_pos.py#L178> ;
    skos:altLabel "Sigmoid Linear Unit" ;
    skos:definition """** Sigmoid Linear Units**, or **SiLUs**, are activation functions for\r
neural networks. The activation of the SiLU is computed by the sigmoid function multiplied by its input, or $$ x\\sigma(x).$$\r
\r
See [Gaussian Error Linear Units](https://arxiv.org/abs/1606.08415) ([GELUs](https://paperswithcode.com/method/gelu)) where the SiLU was originally coined, and see [Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning](https://arxiv.org/abs/1702.03118) and [Swish: a Self-Gated Activation Function](https://arxiv.org/abs/1710.05941v1) where the SiLU was experimented with later.""" ;
    skos:prefLabel "SiLU" .

:SiameseNetwork a skos:Concept ;
    skos:definition """A **Siamese Network** consists of twin networks which accept distinct inputs but are joined by an energy function at the top. This function computes a metric between the highest level feature representation on each side. The parameters between the twin networks are tied. [Weight tying](https://paperswithcode.com/method/weight-tying) guarantees that two extremely similar images are not mapped by each network to very different locations in feature space because each network computes the same function. The network is symmetric, so that whenever we present two distinct images to the twin networks, the top conjoining layer will compute the same metric as if we were to we present the same two images but to the opposite twins.\r
\r
Intuitively instead of trying to classify inputs, a siamese network learns to differentiate between inputs, learning their similarity. The loss function used is usually a form of contrastive loss.\r
\r
Source: [Koch et al](https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf)""" ;
    skos:prefLabel "Siamese Network" .

:SiameseU-Net a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2008.11201v1> ;
    rdfs:seeAlso <https://github.com/previtus/ChangeDetectionProject/blob/master/demo/_ChangeDetection_prediction_example.ipynb> ;
    skos:definition "Siamese U-Net model with a pre-trained ResNet34 architecture as an encoder for data efficient Change Detection" ;
    skos:prefLabel "Siamese U-Net" .

:SigmoidActivation a skos:Concept ;
    rdfs:seeAlso <https://github.com/pytorch/pytorch/blob/96aaa311c0251d24decb9dc5da4957b7c590af6f/torch/nn/modules/activation.py#L277> ;
    skos:definition """**Sigmoid Activations** are a type of activation function for neural networks:\r
\r
$$f\\left(x\\right) = \\frac{1}{\\left(1+\\exp\\left(-x\\right)\\right)}$$\r
\r
Some drawbacks of this activation that have been noted in the literature are: sharp damp gradients during backpropagation from deeper hidden layers to inputs, gradient saturation, and slow convergence.""" ;
    skos:prefLabel "Sigmoid Activation" .

:SimAdapter a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2105.11905v2> ;
    skos:definition """**SimAdapter** is a module for explicitly learning knowledge from adapters. SimAdapter aims to learn the similarities between the source and target languages during fine-tuning using the adapters, and the similarity is based on an [attention mechanism](https://paperswithcode.com/methods/category/attention-mechanisms-1). \r
\r
The detailed composition of the SimAdapter is shown in the Figure. By taking the language-agnostic representations from the backbone model as the query, and the language-specific outputs from multiple adapter as the keys and values, the final output for SimAdapter over attention are computed as (For notation simplicity, we omit the layer index $l$ below):\r
\r
$$\r
\\operatorname{SimAdapter}\\left(\\mathbf{z}, \\mathbf{a}\\_{\\left\\(S\\_{1}, S\\_{2}, \\ldots, S\\_{N}\\right\\)}\\right)=\\sum_{i=1}^{N} \\operatorname{Attn}\\left(\\mathbf{z}, \\mathbf{a}\\_{S\\_{i}}\\right) \\cdot\\left(\\mathbf{a}\\_{S\\_{i}} \\mathbf{W}\\_{V}\\right)\r
$$\r
\r
where SimAdapter $(\\cdot)$ and $\\operatorname{Attn}(\\cdot)$ denotes the SimAdapter and attention operations, respectively. Specifically, the attention operation is computed as:\r
\r
$$\r
\\operatorname{Attn}(\\mathbf{z}, \\mathbf{a})=\\operatorname{Softmax}\\left(\\frac{\\left(\\mathbf{z} \\mathbf{W}\\_{Q}\\right)\\left(\\mathbf{a} \\mathbf{W}\\_{K}\\right)^{\\top}}{\\tau}\\right)\r
$$\r
\r
where $\\tau$ is the temperature coefficient, $\\mathbf{W}\\_{Q}, \\mathbf{W}\\_{K}, \\mathbf{W}\\_{V}$ are attention matrices. Note that while $\\mathbf{W}\\_{Q}, \\mathbf{W}\\_{K}$ are initialized randomly, $\\mathbf{W}\\_{V}$ is initialized with a diagonal of ones and the rest of the matrix with small weights $(1 e-6)$ to retain the adapter representations. Furthermore, a regularization term is introduced to avoid drastic feature changes:\r
\r
$$\r
\\mathcal{L}\\_{\\mathrm{reg}}=\\sum\\_{i, j}\\left(\\left(\\mathbf{I}\\_{V}\\right)\\_{i, j}-\\left(\\mathbf{W}\\_{V}\\right)_{i, j}\\right)^{2}\r
$$\r
\r
where $\\mathbf{I}\\_{V}$ is the identity matrix with the same size as $\\mathbf{W}\\_{V}$""" ;
    skos:prefLabel "SimAdapter" .

:SimAug a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2004.02022v2> ;
    skos:altLabel "Simulation as Augmentation" ;
    skos:definition "**SimAug**, or **Simulation as Augmentation**, is a data augmentation method for trajectory prediction. It augments the representation such that it is robust to the variances in semantic scenes and camera views.  First, to deal with the gap between real and synthetic semantic scene, it represents each training trajectory by high-level scene semantic segmentation features, and defends the model from adversarial examples generated by whitebox attack methods. Second, to overcome the changes in camera views, it generates multiple views for the same trajectory, and encourages the model to focus on the “hardest” view to which the model has learned. The classification loss is adopted and the view with the highest loss is favored during training. Finally, the augmented trajectory is computed as a convex combination of the trajectories generated in previous steps. The trajectory prediction model is built on a multi-scale representation and the final model is trained to minimize the empirical vicinal risk over the distribution of augmented trajectories." ;
    skos:prefLabel "SimAug" .

:SimCLR a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2002.05709v3> ;
    rdfs:seeAlso <https://github.com/google-research/simclr> ;
    skos:definition """**SimCLR** is a framework for contrastive learning of visual representations. It learns representations by maximizing agreement between differently augmented views of the same data example via a contrastive loss in the latent space. It consists of:\r
\r
- A stochastic data augmentation module that transforms any given data example randomly resulting in two correlated views of the same example, denoted $\\mathbf{\\tilde{x}\\_{i}}$ and $\\mathbf{\\tilde{x}\\_{j}}$, which is considered a positive pair. SimCLR sequentially applies three simple augmentations: random cropping followed by resize back to the original size, random color distortions, and [random Gaussian blur](https://paperswithcode.com/method/random-gaussian-blur). The authors find random crop and color distortion is crucial to achieve good performance.\r
\r
- A neural network base encoder $f\\left(·\\right)$ that extracts representation vectors from augmented data examples. The framework allows various choices of the network architecture without any constraints. The authors opt for simplicity and adopt [ResNet](https://paperswithcode.com/method/resnet) to obtain $h\\_{i} = f\\left(\\mathbf{\\tilde{x}}\\_{i}\\right) = \\text{ResNet}\\left(\\mathbf{\\tilde{x}}\\_{i}\\right)$ where $h\\_{i} \\in \\mathbb{R}^{d}$ is the output after the [average pooling](https://paperswithcode.com/method/average-pooling) layer.\r
\r
- A small neural network projection head $g\\left(·\\right)$ that maps representations to the space where contrastive loss is applied. Authors use a MLP with one hidden layer to obtain $z\\_{i} = g\\left(h\\_{i}\\right) = W^{(2)}\\sigma\\left(W^{(1)}h\\_{i}\\right)$ where $\\sigma$ is a [ReLU](https://paperswithcode.com/method/relu) nonlinearity. The authors find it beneficial to define the contrastive loss on $z\\_{i}$’s rather than $h\\_{i}$’s.\r
\r
- A contrastive loss function defined for a contrastive prediction task. Given a set {$\\mathbf{\\tilde{x}}\\_{k}$} including a positive pair of examples $\\mathbf{\\tilde{x}}\\_{i}$ and $\\mathbf{\\tilde{x}\\_{j}}$ , the contrastive prediction task aims to identify $\\mathbf{\\tilde{x}}\\_{j}$ in {$\\mathbf{\\tilde{x}}\\_{k}$}$\\_{k\\neq{i}}$ for a given $\\mathbf{\\tilde{x}}\\_{i}$.\r
\r
A minibatch of $N$ examples is randomly sampled and the contrastive prediction task is defined on pairs of augmented examples derived from the minibatch, resulting in $2N$ data points. Negative examples are not sampled explicitly. Instead, given a positive pair, the other $2(N − 1)$ augmented examples within a minibatch are treated as negative examples. A [NT-Xent](https://paperswithcode.com/method/nt-xent) (the normalized\r
temperature-scaled cross entropy loss) loss function is used (see components).""" ;
    skos:prefLabel "SimCLR" .

:SimCLRv2 a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2006.10029v2> ;
    skos:definition """**SimCLRv2** is a semi-supervised learning method for learning from few labeled examples while making best use of a large amount of unlabeled data. It is a modification of a recently proposed contrastive learning framework, [SimCLR](https://www.paperswithcode.com/method/simclr). It improves upon it in three major ways:\r
\r
1. To fully leverage the power of general pre-training, larger [ResNet](https://paperswithcode.com/method/resnet) models are explored. Unlike SimCLR and other previous work, whose largest model is ResNet-50 (4×), SimCLRv2 trains models that are deeper but less wide. The largest model trained is a 152 layer ResNet with 3× wider channels and [selective kernels](https://paperswithcode.com/method/selective-kernel-convolution) (SK), a channel-wise attention mechanism that improves the parameter efficiency of the network. By scaling up the model from ResNet-50 to ResNet-152 (3×+SK), a 29% relative improvement is obtained in top-1 accuracy when fine-tuned on 1% of labeled examples.\r
\r
2. The capacity of the non-linear network $g(·)$ (a.k.a. projection head) is increased, by making it deeper. Furthermore, instead of throwing away $g(·)$ entirely after pre-training as in SimCLR, fine-tuning occurs from a middle layer. This small change yields a significant improvement for both linear evaluation and fine-tuning with only a few labeled examples. Compared to SimCLR with 2-layer projection head, by using a 3-layer projection head and fine-tuning from the 1st layer of projection head, it results in as much as 14% relative improvement in top-1 accuracy when fine-tuned on 1% of labeled examples.\r
\r
3. The memory mechanism of [MoCo v2](https://paperswithcode.com/method/moco-v2) is incorporated, which designates a memory network (with a moving average of weights for stabilization) whose output will be buffered as negative examples. Since training is based on large mini-batch which already supplies many contrasting negative examples, this change yields an improvement of ∼1% for linear evaluation as well as when fine-tuning on 1% of labeled examples.""" ;
    skos:prefLabel "SimCLRv2" .

:SimCSE a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2104.08821v4> ;
    skos:definition "**SimCSE** is a contrastive learning framework for generating sentence embeddings. It utilizes an unsupervised approach, which takes an input sentence and predicts itself in contrastive objective, with only standard [dropout](https://paperswithcode.com/method/dropout) used as noise. The authors find that dropout acts as minimal “data augmentation” of hidden representations, while removing it leads to a representation collapse. Afterwards a supervised approach is used, which incorporates annotated pairs from natural language inference datasets into the contrastive framework, by using “entailment” pairs as positives and “contradiction" ;
    skos:prefLabel "SimCSE" .

:SimVLM a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2108.10904v3> ;
    skos:altLabel "Simple Visual Language Model" ;
    skos:definition """SimVLM is a minimalist pretraining framework to reduce training complexity by exploiting large-scale weak supervision. It is trained end-to-end with a single prefix language modeling (PrefixLM) objective. PrefixLM enables bidirectional attention within the prefix sequence, and thus it is applicable for both decoder-only\r
and encoder-decoder sequence-to-sequence language models.""" ;
    skos:prefLabel "SimVLM" .

:SimpleNet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1608.06037v8> ;
    rdfs:seeAlso <https://github.com/Coderx7/SimpleNet_Pytorch/blob/5d13ddbba6ae531ced26469c6b0f0ec18665d5ec/models/simplenet.py#L12> ;
    skos:definition "**SimpleNet** is a convolutional neural network with 13 layers. The network employs a homogeneous design utilizing 3 × 3 kernels for convolutional layer and 2 × 2 kernels for pooling operations. The only layers which do not use 3 × 3 kernels are 11th and 12th layers, these layers, utilize 1 × 1 convolutional kernels. Feature-map down-sampling is carried out using nonoverlaping 2 × 2 max-pooling. In order to cope with the problem of vanishing gradient and also over-fitting, SimpleNet also uses batch-normalization with moving average fraction of 0.95 before any [ReLU](https://paperswithcode.com/method/relu) non-linearity." ;
    skos:prefLabel "SimpleNet" .

:Single-HeadedAttention a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1911.11423v2> ;
    rdfs:seeAlso <https://github.com/Smerity/sha-rnn/blob/218d748022dbcf32d50bbbb4d151a9b6de3f8bba/model.py#L53> ;
    skos:definition "**Single-Headed Attention** is a single-headed attention module used in the [SHA-RNN](https://paperswithcode.com/method/sha-rnn) language model. The principle design reasons for single-headedness were simplicity (avoiding running out of memory) and scepticism about the benefits of using multiple heads." ;
    skos:prefLabel "Single-Headed Attention" .

:Single-pathNAS a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1904.02877v1> ;
    rdfs:seeAlso <https://github.com/skmhrk1209/Single-Path-NAS-PyTorch/blob/514abca4c0ce350cc920b9b9277790598774829e/models.py#L60> ;
    skos:definition """**Single-Path NAS** is a convolutional neural network architecture discovered through the Single-Path [neural architecture search](https://paperswithcode.com/method/neural-architecture-search) approach. The NAS utilises a single-path search space. Specifically, compared to previous differentiable  NAS methods, Single-Path NAS uses one single-path over-parameterized  ConvNet to encode all architectural decisions with shared convolutional kernel parameters. The approach is built upon the  observation that different candidate convolutional operations in NAS  can be viewed as subsets of a single superkernel. Without having to  choose among different paths/operations as in multi-path methods, we instead  solve the NAS problem as finding which subset of kernel weights to use in each ConvNet layer. By sharing the convolutional kernel weights,  we encode all candidate NAS operations into a single superkernel.\r
\r
The architecture itself uses the [inverted residual block](https://paperswithcode.com/method/inverted-residual-block) from [MobileNetV2](https://paperswithcode.com/method/mobilenetv2) as its basic building block.""" ;
    skos:prefLabel "Single-path NAS" .

:SingularValueClipping a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1611.06624v3> ;
    rdfs:seeAlso <https://github.com/pfnet-research/tgan/blob/53fde1bcb59e2a57f76b9b26097226ec0423811f/updaters/tgan_updater_wgan.py#L10> ;
    skos:definition """**Singular Value Clipping (SVC)** is an adversarial training technique used by [TGAN](https://paperswithcode.com/method/tgan) to enforce the 1-Lipschitz constraint of the [WGAN](https://paperswithcode.com/method/wgan) objective. It is a constraint to all linear layers in the discriminator that satisfies the spectral norm of weight parameter $W$ is equal or less than one. This\r
means that the singular values of weight matrix are all one or less. Therefore singular value decomposition (SVD) is performed after a parameter update, replacing all the singular values larger than one with one, and the parameters are reconstructed with them. The same operation is applied to convolutional layers by interpreting a higher order tensor in weight parameter as a matrix $\\hat{W}$.""" ;
    skos:prefLabel "Singular Value Clipping" .

:SinkhornTransformer a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2002.11296v1> ;
    skos:definition "The **Sinkhorn Transformer** is a type of [transformer](https://paperswithcode.com/method/transformer) that uses [Sparse Sinkhorn Attention](https://paperswithcode.com/method/sparse-sinkhorn-attention) as a building block. This component is a plug-in replacement for dense fully-connected attention (as well as local attention, and sparse attention alternatives), and allows for reduced memory complexity as well as sparse attention." ;
    skos:prefLabel "Sinkhorn Transformer" .

:Siren a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2006.09661v1> ;
    skos:altLabel "Sinusoidal Representation Network" ;
    skos:definition """**Siren**, or **Sinusoidal Representation Network**, is a periodic activation function for implicit neural representations. Specifically it uses the sine as a periodic activation function:\r
\r
$$ \\Phi\\left(x\\right) = \\textbf{W}\\_{n}\\left(\\phi\\_{n-1} \\circ \\phi\\_{n-2} \\circ \\dots \\circ \\phi\\_{0} \\right) $$""" ;
    skos:prefLabel "Siren" .

:Skip-gramWord2Vec a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1301.3781v3> ;
    skos:definition """**Skip-gram Word2Vec** is an architecture for computing word embeddings. Instead of using surrounding words to predict the center word, as with CBow Word2Vec, Skip-gram Word2Vec uses the central word to predict the surrounding words.\r
\r
The skip-gram objective function sums the log probabilities of the surrounding $n$ words to the left and right of the target word $w\\_{t}$ to produce the following objective:\r
\r
$$J\\_\\theta = \\frac{1}{T}\\sum^{T}\\_{t=1}\\sum\\_{-n\\leq{j}\\leq{n}, \\neq{0}}\\log{p}\\left(w\\_{j+1}\\mid{w\\_{t}}\\right)$$""" ;
    skos:prefLabel "Skip-gram Word2Vec" .

:SkipInit a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2002.10444v3> ;
    skos:definition """**SkipInit** is a method that aims to allow [normalization](https://paperswithcode.com/methods/category/normalization)-free training of neural networks by downscaling [residual branches](https://paperswithcode.com/method/residual-block) at initialization.  This is achieved by including a learnable scalar multiplier at the end of each residual branch, initialized to $\\alpha$.\r
\r
The method is motivated by theoretical findings that [batch normalization](https://paperswithcode.com/method/batch-normalization) downscales the hidden activations on the residual branch by a factor on the order of the square root of the network depth (at initialization). Therefore, as the depth of a residual network is increased, the residual blocks are increasingly dominated by the [skip connection](https://paperswithcode.com/method/residual-connection), which drives the functions computed by residual blocks closer to the identity, preserving signal propagation and ensuring well-behaved gradients. This leads to the proposed method which can achieve this property through an [initialization](https://paperswithcode.com/methods/category/initialization) strategy rather than a [normalization](https://paperswithcode.com/methods/category/normalization) strategy.""" ;
    skos:prefLabel "SkipInit" .

:SlantedTriangularLearningRates a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1801.06146v5> ;
    skos:definition "**Slanted Triangular Learning Rates (STLR)** is a learning rate schedule which first linearly increases the learning rate and then linearly decays it, which can be seen in Figure to the right. It is a modification of Triangular Learning Rates, with a short increase and a long decay period." ;
    skos:prefLabel "Slanted Triangular Learning Rates" .

:SlidingWindowAttention a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2004.05150v2> ;
    rdfs:seeAlso <https://github.com/allenai/longformer/blob/74523f7f2897b3a5ffcdb537f67a03f83aa1affb/longformer/sliding_chunks.py#L40> ;
    skos:definition """**Sliding Window Attention** is an attention pattern for attention-based models. It was proposed as part of the [Longformer](https://paperswithcode.com/method/longformer) architecture. It is motivated by the fact that non-sparse attention in the original [Transformer](https://paperswithcode.com/method/transformer) formulation has a [self-attention component](https://paperswithcode.com/method/scaled) with $O\\left(n^{2}\\right)$ time and memory complexity where $n$ is the input sequence length and thus, is not efficient to scale to long inputs. Given the importance of local context, the sliding window attention pattern employs a fixed-size window attention surrounding each token. Using multiple stacked layers of such windowed attention results in a large receptive field, where top layers have access to all input locations and have the capacity to build representations that incorporate information across the entire input. \r
\r
More formally, in this attention pattern, given a fixed window size $w$, each token attends to $\\frac{1}{2}w$ tokens on each side. The computation complexity of this pattern is $O\\left(n×w\\right)$,\r
which scales linearly with input sequence length $n$. To make this attention pattern efficient, $w$ should be small compared with $n$. But a model with typical multiple stacked transformers will have a large receptive field. This is analogous to CNNs where stacking layers of small kernels leads to high level features that are built from a large portion of the input (receptive field)\r
\r
In this case, with a transformer of $l$ layers, the receptive field size is $l × w$ (assuming\r
$w$ is fixed for all layers). Depending on the application, it might be helpful to use different values of $w$ for each layer to balance between efficiency and model representation capacity.""" ;
    skos:prefLabel "Sliding Window Attention" .

:SlotAttention a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2006.15055v2> ;
    skos:definition "**Slot Attention** is an architectural component that interfaces with perceptual representations such as the output of a convolutional neural network and produces a set of task-dependent abstract representations which we call slots. These slots are exchangeable and can bind to any object in the input by specializing through a competitive procedure over multiple rounds of attention. Using an iterative attention mechanism, slots produces a set of output vectors with permutation symmetry. Unlike capsules used in Capsule Networks, slots produced by Slot Attention do not specialize to one particular type or class of object, which could harm generalization. Instead, they act akin to object files, i.e., slots use a common representational format: each slot can store (and bind to) any object in the input. This allows Slot Attention to generalize in a systematic way to unseen compositions, more objects, and more slots." ;
    skos:prefLabel "Slot Attention" .

:SlowMo a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1910.00643v2> ;
    skos:definition "**Slow Momentum** (SlowMo) is a distributed optimization method where workers periodically synchronize and perform a momentum update, after multiple iterations of a base optimization algorithm.  Periodically, after taking some number $\\tau$ of base algorithm steps, workers average their parameters using ALLREDUCE and perform a momentum update." ;
    skos:prefLabel "SlowMo" .

:SmeLU a skos:Concept ;
    skos:altLabel "Smooth ReLU" ;
    skos:definition "Please enter a description about the method here" ;
    skos:prefLabel "SmeLU" .

:Smish a skos:Concept ;
    skos:definition """Smish is an activation function defined as $f(x)=x\\cdot \\text{tanh}(\\ln(1+\\sigma(x)))$ where $\\sigma(x)$ denotes the sigmoid function. A parameterized version was also described in the form $f(x)=\\alpha x\\cdot \\text{tanh}(\\ln(1+\\sigma(\\beta x)))$.\r
\r
Paper: Smish: A Novel Activation Function for Deep Learning Methods\r
\r
Source: https://www.mdpi.com/2079-9292/11/4/540""" ;
    skos:prefLabel "Smish" .

:SmoothStep a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2002.07772v2> ;
    skos:definition "Please enter a description about the method here" ;
    skos:prefLabel "Smooth Step" .

:SnapshotEnsembles a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1704.00109v1> ;
    skos:altLabel "Snapshot Ensembles: Train 1, get M for free" ;
    skos:definition "The  overhead  cost  of  training  multiple  deep  neural networks  could  be  very  high  in  terms  of  the  training  time, hardware, and computational resource requirement and often acts  as  obstacle  for  creating  deep  ensembles.  To  overcome these barriers Huang et al. proposed a unique method to create  ensemble  which  at  the  cost  of  training  one  model, yields  multiple  constituent  model  snapshots  that  can  be ensembled together to create a strong learner. The core idea behind the concept is to make the model converge to several local minima along the optimization path and save the model parameters at these local minima points. During the training phase, a neural network would traverse through many such points. The lowest of all such local minima is known as the Global Minima. The larger the model, more are the number of parameters and larger the number of local minima points. This implies, there are discrete sets of weights and biases, at which  the  model  is  making  fewer  errors.  So,  every  such minimum  can  be  considered a  weak  but  a  potential learner model for the problem being solved. Multiple such snapshot of  weights  and  biases  are  recorded  which  can  later  be ensembled to get a better generalized model which makes the least amount of mistakes." ;
    skos:prefLabel "Snapshot Ensembles" .

:Social-STGCNN a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2002.11927v3> ;
    skos:definition "**Social-STGCNN** is a method for human trajectory prediction. Pedestrian trajectories are not only influenced by the pedestrian itself but also by interaction with surrounding objects." ;
    skos:prefLabel "Social-STGCNN" .

:Soft-NMS a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1704.04503v2> ;
    skos:definition """Non-maximum suppression is an integral part of the object detection pipeline. First, it sorts all detection boxes on the basis of their scores. The detection box $M$ with the maximum score is selected and all other detection boxes with a significant overlap (using a pre-defined threshold)\r
with $M$ are suppressed. This process is recursively applied on the remaining boxes. As per the design of the algorithm, if an object lies within the predefined overlap threshold, it leads to a miss. \r
\r
**Soft-NMS** solves this problem by decaying the detection scores of all other objects as a continuous function of their overlap with M. Hence, no object is eliminated in this process.""" ;
    skos:prefLabel "Soft-NMS" .

:SoftActor-Critic\(AutotunedTemperature\) a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1812.05905v2> ;
    skos:definition "**Soft Actor Critic (Autotuned Temperature** is a modification of the [SAC](https://paperswithcode.com/method/soft-actor-critic) reinforcement learning algorithm. [SAC](https://paperswithcode.com/method/sac) can suffer from brittleness to the temperature hyperparameter. Unlike in conventional reinforcement learning, where the optimal policy is independent of scaling of the reward function, in maximum entropy reinforcement learning the scaling factor has to be compensated by the choice a of suitable temperature, and a sub-optimal temperature can drastically degrade performance. To resolve this issue, SAC with Autotuned Temperature has an automatic gradient-based temperature tuning method that adjusts the expected entropy over the visited states to match a target value." ;
    skos:prefLabel "Soft Actor-Critic (Autotuned Temperature)" .

:SoftActorCritic a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1801.01290v2> ;
    skos:definition """**Soft Actor Critic**, or **SAC**, is an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as [Q-learning methods](https://paperswithcode.com/method/q-learning). [SAC](https://paperswithcode.com/method/sac) combines off-policy updates with a stable stochastic actor-critic formulation.\r
\r
The SAC objective has a number of advantages. First, the policy is incentivized to explore more widely, while giving up on clearly unpromising avenues. Second, the policy can capture multiple modes of near-optimal behavior. In problem settings where multiple actions seem equally attractive, the policy will commit equal probability mass to those actions. Lastly, the authors present evidence that it improves learning speed over state-of-art methods that optimize the conventional RL objective function.""" ;
    skos:prefLabel "Soft Actor Critic" .

:SoftPool a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2101.00440v3> ;
    rdfs:seeAlso <https://github.com/alexandrosstergiou/SoftPool> ;
    skos:altLabel "Soft Pooling" ;
    skos:definition "SoftPool: a fast and efficient method that sums exponentially weighted activations. Compared to a range of other pooling methods, SoftPool retains more information in the downsampled activation maps. More refined downsampling leads to better classification accuracy." ;
    skos:prefLabel "SoftPool" .

:SoftSplitandSoftComposition a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2109.02974v1> ;
    skos:definition "**Soft Split and Soft Composition** are video frame based operations used in the [FuseFormer](https://paperswithcode.com/method/fuseformer) architecture, specifically the [FuseFormer blocks](https://paperswithcode.com/method/fuseformer-block). We softly split each frame into overlapped patches and then softly composite them back, by using an unfold and fold operator with patch size $k$ being greater than patch stride $s$. When compositing patches back to its original spatial shape, we add up feature values at each overlapping spatial location of neighboring patches." ;
    skos:prefLabel "Soft Split and Soft Composition" .

:Softmax a skos:Concept ;
    skos:definition """The **Softmax** output function transforms a previous layer's output into a vector of probabilities. It is commonly used for multiclass classification.  Given an input vector $x$ and a weighting vector $w$ we have:\r
\r
$$ P(y=j \\mid{x}) = \\frac{e^{x^{T}w_{j}}}{\\sum^{K}_{k=1}e^{x^{T}wk}} $$""" ;
    skos:prefLabel "Softmax" .

:Softplus a skos:Concept ;
    rdfs:seeAlso <https://github.com/pytorch/pytorch/blob/96aaa311c0251d24decb9dc5da4957b7c590af6f/torch/nn/modules/activation.py#L723> ;
    skos:definition "**Softplus** is an activation function $f\\left(x\\right) = \\log\\left(1+\\exp\\left(x\\right)\\right)$. It can be viewed as a smooth version of [ReLU](https://paperswithcode.com/method/relu)." ;
    skos:prefLabel "Softplus" .

:SoftsignActivation a skos:Concept ;
    rdfs:seeAlso <https://github.com/pytorch/pytorch/blob/96aaa311c0251d24decb9dc5da4957b7c590af6f/torch/nn/modules/activation.py#L1032> ;
    skos:definition """**Softsign** is an activation function for neural networks:\r
\r
$$ f\\left(x\\right) = \\left(\\frac{x}{|x|+1}\\right)$$\r
\r
Image Source: [Sefik Ilkin Serengil](https://sefiks.com/2017/11/10/softsign-as-a-neural-networks-activation-function/)""" ;
    skos:prefLabel "Softsign Activation" .

:SongNet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2004.08022v2> ;
    skos:definition "**SongNet** is an auto-regressive [Transformer](https://paperswithcode.com/method/transformer)-based language model for rigid format text detection. Sets of symbols are tailor-designed to improve the modeling performance especially on format, rhyme, and sentence integrity. The attention mechanism is improved to impel the model to capture some future information on the format. A pre-training and fine-tuning framework is designed to further improve the generation quality." ;
    skos:prefLabel "SongNet" .

:SortCutSinkhornAttention a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2002.11296v1> ;
    rdfs:seeAlso <https://github.com/lucidrains/sinkhorn-transformer/blob/3eaa76e99efeee75cf8298defaaef51621c55ff4/sinkhorn_transformer/sinkhorn_transformer.py#L286> ;
    skos:definition """**SortCut Sinkhorn Attention** is a variant of [Sparse Sinkhorn Attention](https://paperswithcode.com/method/sparse-sinkhorn-attention) where a post-sorting truncation of the input sequence is performed, essentially performing a hard top-k operation on the input sequence blocks within the computational graph. While most attention models mainly re-weight or assign near-zero weights during training, this allows for explicitly and dynamically truncate the input sequence. Specifically:\r
\r
$$ Y = \\text{Softmax}\\left(Q{\\psi\\_{S}}\\left(K\\right)^{T}\\_{\\left[:n\\right]}\\right)\\psi\\_{S}\\left(V\\right)\\_{\\left[:n\\right]} $$\r
\r
where $n$ is the Sortfut budget hyperparameter.""" ;
    skos:prefLabel "SortCut Sinkhorn Attention" .

:SourceHypothesisTransfer a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2002.08546v6> ;
    skos:definition "**Source Hypothesis Transfer**, or **SHOT**, is a representation learning framework for unsupervised domain adaptation. SHOT freezes the classifier module (hypothesis) of the source model and learns the target-specific feature extraction module by exploiting both information maximization and self-supervised pseudo-labeling to implicitly align representations from the target domains to the source hypothesis." ;
    skos:prefLabel "Source Hypothesis Transfer" .

:Span-BasedDynamicConvolution a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2008.02496v3> ;
    skos:definition """**Span-Based Dynamic Convolution** is a type of convolution used in the [ConvBERT](https://paperswithcode.com/method/convbert) architecture to capture local dependencies between tokens.  Kernels are generated by taking in a local span of current token, which better utilizes local dependency and discriminates different meanings of the same token (e.g., if “a” is in front of “can” in the input sentence, “can” is apparently a noun not a verb).\r
\r
Specifically, with [classic convolution](https://paperswithcode.com/method/convolution), we would have fixed parameters shared for all input tokens. [Dynamic convolution](https://paperswithcode.com/method/dynamicconv) is therefore preferable because it has  higher flexibility in capturing local dependencies of different tokens. Dynamic convolution uses a kernel generator to produce different kernels for different input tokens. However, such dynamic convolution cannot differentiate the same tokens within different context and\r
generate the same kernels (e.g., the three “can” in Figure (b)).\r
\r
Therefore the span-based dynamic convolution is developed to produce more adaptive convolution kernels by receiving an input span instead of only a single token, which enables discrimination of generated kernels for the same tokens within different context. For example, as shown in Figure (c), span-based dynamic convolution produces different kernels for different “can” tokens.""" ;
    skos:prefLabel "Span-Based Dynamic Convolution" .

:SparseAutoencoder a skos:Concept ;
    skos:definition """A **Sparse Autoencoder** is a type of autoencoder that employs sparsity to achieve an information bottleneck. Specifically the loss function is constructed so that activations are penalized within a layer. The sparsity constraint can be imposed with [L1 regularization](https://paperswithcode.com/method/l1-regularization) or a KL divergence between expected average neuron activation to an ideal distribution $p$.\r
\r
Image: [Jeff Jordan](https://www.jeremyjordan.me/autoencoders/). Read his blog post (click) for a detailed summary of autoencoders.""" ;
    skos:prefLabel "Sparse Autoencoder" .

:SparseConvolutions a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1409.6070v1> ;
    skos:definition "" ;
    skos:prefLabel "Sparse Convolutions" .

:SparseR-CNN a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2011.12450v2> ;
    skos:definition """**Sparse R-CNN** is a purely sparse method for object detection in images, without object positional candidates enumerating\r
on all(dense) image grids nor object queries interacting with global(dense) image feature.\r
\r
As shown in the Figure, object candidates are given with a fixed small set of learnable bounding boxes represented by 4-d coordinate. For the example of the COCO dataset, 100 boxes and 400 parameters are needed in total, rather than the predicted ones from hundreds of thousands of candidates in a Region Proposal Network ([RPN](https://paperswithcode.com/method/rpn)). These sparse candidates are used as proposal boxes to extract the feature of Region of Interest (RoI) by [RoIPool](https://paperswithcode.com/method/roi-pooling) or [RoIAlign](https://paperswithcode.com/method/roi-align).""" ;
    skos:prefLabel "Sparse R-CNN" .

:SparseSinkhornAttention a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2002.11296v1> ;
    rdfs:seeAlso <https://github.com/lucidrains/sinkhorn-transformer/blob/3eaa76e99efeee75cf8298defaaef51621c55ff4/sinkhorn_transformer/sinkhorn_transformer.py#L286> ;
    skos:definition "**Sparse Sinkhorn Attention** is an attention mechanism that reduces the memory complexity of the [dot-product attention mechanism](https://paperswithcode.com/method/scaled) and is capable of learning sparse attention outputs. It is based on the idea of differentiable sorting of internal representations within the self-attention module. SSA incorporates a meta sorting network that learns to rearrange and sort input sequences. Sinkhorn normalization is used to normalize the rows and columns of the sorting matrix. The actual SSA attention mechanism then acts on the block sorted sequences." ;
    skos:prefLabel "Sparse Sinkhorn Attention" .

:SparseSwitchableNormalization a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1903.03793v1> ;
    rdfs:seeAlso <https://github.com/switchablenorms/Sparse_SwitchNorm/blob/875db480b5ce755cd569c2ce54655b637891ce58/utils/sparse_switchable_norm.py#L6> ;
    skos:definition "**Sparse Switchable Normalization (SSN)** is a variant on [Switchable Normalization](https://paperswithcode.com/method/switchable-normalization) where the importance ratios are constrained to be sparse. Unlike $\\ell_1$ and $\\ell_0$ constraints that impose difficulties in optimization, the constrained optimization problem is turned into feed-forward computation through [SparseMax](https://paperswithcode.com/method/sparsemax), which is a sparse version of [softmax](https://paperswithcode.com/method/softmax)." ;
    skos:prefLabel "Sparse Switchable Normalization" .

:SparseTransformer a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1904.10509v1> ;
    skos:definition "A **Sparse Transformer** is a [Transformer](https://paperswithcode.com/method/transformer) based architecture which utilises sparse factorizations of the attention matrix to reduce time/memory to $O(n \\sqrt{n})$. Other changes to the Transformer architecture include: (a) a restructured [residual block](https://paperswithcode.com/method/residual-block) and weight initialization, (b) A set of sparse attention kernels which efficiently compute subsets of the attention matrix, (c) recomputation of attention weights during the backwards pass to reduce memory usage" ;
    skos:prefLabel "Sparse Transformer" .

:Sparsemax a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1602.02068v2> ;
    rdfs:seeAlso <https://github.com/vene/sparse-structured-attention/blob/e89a2162bdde3a86b7dfdba22e292ea3bd3880d3/pytorch/torchsparseattn/sparsemax.py#L47> ;
    skos:definition """**Sparsemax** is a type of activation/output function similar to the traditional [softmax](https://paperswithcode.com/method/softmax), but able to output sparse probabilities. \r
\r
$$ \\text{sparsemax}\\left(z\\right) = \\arg\\_{p∈\\Delta^{K−1}}\\min||\\mathbf{p} - \\mathbf{z}||^{2} $$""" ;
    skos:prefLabel "Sparsemax" .

<http://w3id.org/mlso/vocab/ml_algorithm/Spatial&TemporalAttention> a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1611.06067v1> ;
    skos:definition "Spatial & temporal attention combines the advantages of spatial attention and temporal attention as it adaptively selects both important regions and key frames. Some works compute temporal attention and spatial attention separately, while others produce joint spatio & temporal attention maps. Further works focusing on capturing pairwise relations." ;
    skos:prefLabel "Spatial & Temporal Attention" .

:Spatial-ReductionAttention a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2102.12122v2> ;
    skos:definition """**Spatial-Reduction Attention**, or **SRA**, is a [multi-head attention](https://paperswithcode.com/method/multi-head-attention) module used in the [Pyramid Vision Transformer](https://paperswithcode.com/method/pvt) architecture which reduces the spatial scale of the key $K$ and value $V$ before the attention operation. This reduces the computational/memory overhead. Details of the SRA in the stage $i$ can be formulated as follows:\r
\r
$$\r
\\text{SRA}(Q, K, V)=\\text { Concat }\\left(\\operatorname{head}\\_{0}, \\ldots \\text { head }\\_{N\\_{i}}\\right) W^{O} $$\r
\r
$$\\text{ head}\\_{j}=\\text { Attention }\\left(Q W\\_{j}^{Q}, \\operatorname{SR}(K) W\\_{j}^{K}, \\operatorname{SR}(V) W\\_{j}^{V}\\right)\r
$$\r
\r
where Concat $(\\cdot)$ is the concatenation operation. $W\\_{j}^{Q} \\in \\mathbb{R}^{C\\_{i} \\times d\\_{\\text {head }}}$, $W\\_{j}^{K} \\in \\mathbb{R}^{C\\_{i} \\times d\\_{\\text {head }}}$, $W\\_{j}^{V} \\in \\mathbb{R}^{C\\_{i} \\times d\\_{\\text {head }}}$, and $W^{O} \\in \\mathbb{R}^{C\\_{i} \\times C\\_{i}}$ are linear projection parameters. $N\\_{i}$ is the head number of the attention layer in Stage $i$. Therefore, the dimension of each head (i.e. $\\left.d\\_{\\text {head }}\\right)$ is equal to $\\frac{C\\_{i}}{N\\_{i}} . \\text{SR}(\\cdot)$ is the operation for reducing the spatial dimension of the input sequence ($K$ or $V$ ), which is written as:\r
\r
$$\r
\\text{SR}(\\mathbf{x})=\\text{Norm}\\left(\\operatorname{Reshape}\\left(\\mathbf{x}, R\\_{i}\\right) W^{S}\\right)\r
$$\r
\r
Here, $\\mathbf{x} \\in \\mathbb{R}^{\\left(H\\_{i} W\\_{i}\\right) \\times C\\_{i}}$ represents a input sequence, and $R\\_{i}$ denotes the reduction ratio of the attention layers in Stage $i .$ Reshape $\\left(\\mathbf{x}, R\\_{i}\\right)$ is an operation of reshaping the input sequence $\\mathbf{x}$ to a sequence of size $\\frac{H\\_{i} W\\_{i}}{R\\_{i}^{2}} \\times\\left(R\\_{i}^{2} C\\_{i}\\right)$. $W\\_{S} \\in \\mathbb{R}^{\\left(R\\_{i}^{2} C\\_{i}\\right) \\times C\\_{i}}$ is a linear projection that reduces the dimension of the input sequence to $C\\_{i}$. $\\text{Norm}(\\cdot)$ refers to layer normalization.""" ;
    skos:prefLabel "Spatial-Reduction Attention" .

:SpatialAttention-GuidedMask a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1911.06667v6> ;
    rdfs:seeAlso <https://github.com/youngwanLEE/CenterMask/blob/4a7a421913ecd57ddba23b27410d0a2f14565f33/maskrcnn_benchmark/modeling/make_layers.py#L199> ;
    skos:definition """**A Spatial Attention-Guided Mask** is a module for [instance segmentation](https://paperswithcode.com/task/instance-segmentation) that predicts a segmentation mask on each detected box with a spatial attention map that helps to focus on informative pixels and suppress noise. The goal is to guide the mask head for spotlighting meaningful pixels and repressing uninformative ones. \r
\r
Once features inside the predicted RoIs are extracted by [RoIAlign](https://paperswithcode.com/method/roi-align) with 14×14 resolution, those features are fed into four conv layers and the [spatial attention module](https://paperswithcode.com/method/spatial-attention-module) (SAM) sequentially. To exploit the spatial attention map $A\\_{sag}\\left(X\\_{i}\\right) \\in \\mathcal{R}^{1\\times{W}\\times{H}}$ as a feature descriptor given input feature map $X\\_{i} \\in \\mathcal{R}^{C×W×H}$, the SAM first generates pooled features $P\\_{avg}, P\\_{max} \\in \\mathcal{R}^{1\\times{W}\\times{H}}$ by both average and [max pooling](https://paperswithcode.com/method/max-pooling) operations respectively along the channel axis and aggregates them via concatenation. Then it is followed by a 3 × 3 conv layer and normalized by the sigmoid function. The computation process\r
is summarized as follow:\r
\r
$$\r
A\\_{sag}\\left(X\\_{i}\\right) = \\sigma\\left(F\\_{3\\times{3}}(P\\_{max} \\cdot P\\_{avg})\\right)\r
$$\r
\r
where $\\sigma$ denotes the sigmoid function, $F\\_{3\\times{3}}$ is 3 × 3 conv layer and $\\cdot$ represents the concatenate operation. Finally, the attention guided feature map $X\\_{sag} ∈ \\mathcal{R}^{C\\times{W}\\times{H}}$ is computed as:\r
\r
$$\r
X\\_{sag} = A\\_{sag}\\left(X\\_{i}\\right) \\otimes X\\_{i}\r
$$\r
\r
where ⊗ denotes element-wise multiplication. After then, a 2 × 2 deconv upsamples the spatially attended feature map to 28 × 28 resolution. Lastly, a 1 × 1 conv is applied for predicting class-specific masks.""" ;
    skos:prefLabel "Spatial Attention-Guided Mask" .

:SpatialAttentionModule a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1807.06521v2> ;
    rdfs:seeAlso <https://github.com/Jongchan/attention-module/blob/5d3a54af0f6688bedca3f179593dff8da63e8274/MODELS/cbam.py#L72> ;
    skos:definition """A **Spatial Attention Module** is a module for spatial attention in convolutional neural networks. It generates a spatial attention map by utilizing the inter-spatial relationship of features. Different from the [channel attention](https://paperswithcode.com/method/channel-attention-module), the spatial attention focuses on where is an informative part, which is complementary to the channel attention. To compute the spatial attention, we first apply average-pooling and max-pooling operations along the channel axis and concatenate them to generate an efficient feature descriptor. On the concatenated feature descriptor, we apply a [convolution](https://paperswithcode.com/method/convolution) layer to generate a spatial attention map $\\textbf{M}\\_{s}\\left(F\\right) \\in \\mathcal{R}^{H×W}$ which encodes where to emphasize or suppress. \r
\r
We aggregate channel information of a feature map by using two pooling operations, generating two 2D maps: $\\mathbf{F}^{s}\\_{avg} \\in \\mathbb{R}^{1\\times{H}\\times{W}}$ and $\\mathbf{F}^{s}\\_{max} \\in \\mathbb{R}^{1\\times{H}\\times{W}}$. Each denotes average-pooled features and max-pooled features across the channel. Those are then concatenated and convolved by a standard convolution layer, producing the 2D spatial attention map. In short, the spatial attention is computed as:\r
\r
$$ \\textbf{M}\\_{s}\\left(F\\right) = \\sigma\\left(f^{7x7}\\left(\\left[\\text{AvgPool}\\left(F\\right);\\text{MaxPool}\\left(F\\right)\\right]\\right)\\right) $$\r
\r
$$ \\textbf{M}\\_{s}\\left(F\\right) = \\sigma\\left(f^{7x7}\\left(\\left[\\mathbf{F}^{s}\\_{avg};\\mathbf{F}^{s}\\_{max} \\right]\\right)\\right) $$\r
\r
where $\\sigma$ denotes the sigmoid function and $f^{7×7}$ represents a convolution operation with the filter size of 7 × 7.""" ;
    skos:prefLabel "Spatial Attention Module" .

:SpatialAttentionModule\(ThunderNet\) a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1903.11752v3> ;
    rdfs:seeAlso <https://github.com/ouyanghuiyu/Thundernet_Pytorch/blob/ab66b733a39c9d1c60b5373f84f861d9627d8c20/lib/model/faster_rcnn/modules.py#L209> ;
    skos:definition """**Spatial Attention Module (SAM)** is a feature extraction module for object detection used in [ThunderNet](https://paperswithcode.com/method/thundernet).\r
\r
The ThunderNet SAM explicitly re-weights the feature map before RoI warping over the spatial dimensions. The key idea of SAM is to use the knowledge from [RPN](https://paperswithcode.com/method/rpn) to refine the feature distribution of the feature map. RPN is trained to recognize foreground regions under the supervision of ground truths. Therefore, the intermediate features in RPN can be used to distinguish foreground features from background features. SAM accepts two inputs: the intermediate feature map from RPN $\\mathcal{F}^{RPN}$ and the thin feature map from the [Context Enhancement Module](https://paperswithcode.com/method/context-enhancement-module) $\\mathcal{F}^{CEM}$. The output of SAM $\\mathcal{F}^{SAM}$ is defined as:\r
\r
$$ \\mathcal{F}^{SAM} = \\mathcal{F}^{CEM} * \\text{sigmoid}\\left(\\theta\\left(\\mathcal{F}^{RPN}\\right)\\right) $$\r
\r
Here $\\theta\\left(·\\right)$ is a dimension transformation to match the number of channels in both feature maps. The sigmoid function is used to constrain the values within $\\left[0, 1\\right]$. At last, $\\mathcal{F}^{CEM}$ is re-weighted by the generated feature map for better feature distribution. For computational efficiency, we simply apply a 1×1 [convolution](https://paperswithcode.com/method/convolution) as $\\theta\\left(·\\right)$, so the computational cost of CEM is negligible. The Figure to the right shows the structure of SAM. \r
\r
SAM has two functions. The first one is to refine the feature distribution by strengthening foreground features and suppressing background features. The second one is to stabilize the training of RPN as SAM enables extra gradient flow from [R-CNN](https://paperswithcode.com/method/r-cnn) subnet to RPN. As a result, RPN receives additional supervision from RCNN subnet, which helps the training of RPN.""" ;
    skos:prefLabel "Spatial Attention Module (ThunderNet)" .

:SpatialBroadcastDecoder a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1901.07017v2> ;
    skos:definition """Spatial Broadcast Decoder is an architecture that aims to improve disentangling, reconstruction accuracy, and generalization to held-out regions in data space. It provides a particularly dramatic\r
benefit when applied to datasets with small objects.\r
\r
Source: [Watters et al.](https://arxiv.org/pdf/1901.07017v2.pdf)\r
\r
Image source: [Watters et al.](https://arxiv.org/pdf/1901.07017v2.pdf)""" ;
    skos:prefLabel "Spatial Broadcast Decoder" .

:SpatialDropout a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1411.4280v3> ;
    rdfs:seeAlso <https://github.com/kklosowski/EterChess/blob/0374cfc78c20877e13bcd035e5fe3185f8769371/PythonAPI/app.py#L27> ;
    skos:definition """**SpatialDropout** is a type of [dropout](https://paperswithcode.com/method/dropout) for convolutional networks. For a given [convolution](https://paperswithcode.com/method/convolution) feature tensor of size $n\\_{\\text{feats}}$×height×width, we perform only $n\\_{\\text{feats}}$ dropout\r
trials and extend the dropout value across the entire feature map. Therefore, adjacent pixels in the dropped-out feature\r
map are either all 0 (dropped-out) or all active as illustrated in the figure to the right.""" ;
    skos:prefLabel "SpatialDropout" .

:SpatialFeatureTransform a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1804.02815v1> ;
    skos:definition """**Spatial Feature Transform**, or **SFT**, is a layer that generates affine transformation parameters for spatial-wise feature modulation, and was originally proposed within the context of image super-resolution. A Spatial Feature Transform (SFT) layer learns a mapping function $\\mathcal{M}$ that outputs a modulation parameter pair $(\\mathbf{\\gamma}, \\mathbf{\\beta})$ based on some prior condition $\\Psi$. The learned parameter pair adaptively influences the outputs by applying an affine transformation spatially to each intermediate feature maps in an SR network. During testing, only a single forward pass is needed to generate the HR image given the LR input and segmentation probability maps.\r
\r
More precisely, the prior $\\Psi$ is modeled by a pair of affine transformation parameters $(\\mathbf{\\gamma}, \\mathbf{\\beta})$ through a mapping function $\\mathcal{M}: \\Psi \\mapsto(\\mathbf{\\gamma}, \\mathbf{\\beta})$. Consequently,\r
\r
$$\r
\\hat{\\mathbf{y}}=G_{\\mathbf{\\theta}}(\\mathbf{x} \\mid \\mathbf{\\gamma}, \\mathbf{\\beta}), \\quad(\\mathbf{\\gamma}, \\mathbf{\\beta})=\\mathcal{M}(\\Psi)\r
$$\r
\r
After obtaining $(\\mathbf{\\gamma}, \\mathbf{\\beta})$ from conditions, the transformation is carried out by scaling and shifting feature maps of a specific layer:\r
\r
$$\r
\\operatorname{SFT}(\\mathbf{F} \\mid \\mathbf{\\gamma}, \\mathbf{\\beta})=\\mathbf{\\gamma} \\odot \\mathbf{F}+\\mathbf{\\beta}\r
$$\r
\r
where $\\mathbf{F}$ denotes the feature maps, whose dimension is the same as $\\gamma$ and $\\mathbf{\\beta}$, and $\\odot$ is referred to element-wise multiplication, i.e., Hadamard product. Since the spatial dimensions are preserved, the SFT layer not only performs feature-wise manipulation but also spatial-wise transformation.""" ;
    skos:prefLabel "Spatial Feature Transform" .

:SpatialGatingUnit a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2105.08050v2> ;
    skos:definition """**Spatial Gating Unit**, or **SGU**, is a gating unit used in the [gMLP](https://paperswithcode.com/method/gmlp) architecture to captures spatial interactions. To enable cross-token interactions, it is necessary for the layer $s(\\cdot)$ to contain a contraction operation over the spatial dimension. The layer $s(\\cdot)$ is formulated as the output of linear gating:\r
\r
$$\r
s(Z)=Z \\odot f\\_{W, b}(Z)\r
$$\r
\r
where $\\odot$ denotes element-wise multiplication. For training stability, the authors find it critical to initialize $W$ as near-zero values and $b$ as ones, meaning that $f\\_{W, b}(Z) \\approx 1$ and therefore $s(Z) \\approx Z$ at the beginning of training. This initialization ensures each [gMLP](https://paperswithcode.com/method/gmlp) block behaves like a regular [FFN](https://paperswithcode.com/method/gmlp) at the early stage of training, where each token is processed independently, and only gradually injects spatial information across tokens during the course of learning.\r
\r
The authors find it further effective to split $Z$ into two independent parts $\\left(Z\\_{1}, Z\\_{2}\\right)$ along the channel dimension for the gating function and for the multiplicative bypass:\r
\r
$$\r
s(Z)=Z\\_{1} \\odot f\\_{W, b}\\left(Z\\_{2}\\right)\r
$$\r
\r
They also normalize the input to $f\\_{W, b}$ which empirically improved the stability of large NLP models.""" ;
    skos:prefLabel "Spatial Gating Unit" .

:SpatialGroup-wiseEnhance a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1905.09646v2> ;
    rdfs:seeAlso <https://github.com/implus/PytorchInsight/blob/67cf5a9f31d40fc80a6dd12234a50bd4d335ace7/classification/models/imagenet/resnet_sge.py#L6> ;
    skos:definition """**Spatial Group-wise Enhance** is a module for convolutional neural networks that can adjust the\r
importance of each sub-feature by generating an attention factor for each spatial location in each semantic group, so that every individual group can autonomously enhance its learnt expression and suppress possible noise\r
\r
Inside each feature group, we model a spatial enhance mechanism inside each feature group, by scaling the feature vectors over all the locations with an attention mask. This attention mask is designed to suppress the possible noise and highlight the correct semantic feature regions. Different from other popular attention methods, it utilises the similarity between the global statistical feature and the local ones of each location as the source of generation for the attention masks.""" ;
    skos:prefLabel "Spatial Group-wise Enhance" .

:SpatialPropagation a skos:Concept ;
    dcterms:source <https://dl.acm.org/doi/abs/10.1145/3503161.3548312> ;
    skos:altLabel "Surface Nomral-based Spatial Propagation" ;
    skos:definition "Inspired by the spatial propagation mechanism utilized in the depth completion task \\cite{NLSPN}, we introduce a normal incorporated non-local disparity propagation module in which we hub NDP to generate non-local affinities and offsets for spatial propagation at the disparity level. The motivation lies that the sampled pixels for edges and occluded regions are supposed to be selected. The propagation process aggregates disparities via plane affinity relations, which alleviates the phenomenon of disparity blurring at object edges due to frontal parallel windows. And the disparities in occluded areas are also optimized at the same time by being propagated from non-occluded areas where the predicted disparities are with high confidence." ;
    skos:prefLabel "Spatial Propagation" .

:SpatialPyramidPooling a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1406.4729v4> ;
    rdfs:seeAlso <https://github.com/yueruchen/sppnet-pytorch/blob/270529337baa5211538bf553bda222b9140838b3/spp_layer.py#L2> ;
    skos:definition "** Spatial Pyramid Pooling (SPP)** is a pooling layer that removes the fixed-size constraint of the network, i.e. a CNN does not require a fixed-size input image. Specifically, we add an SPP layer on top of the last convolutional layer. The SPP layer pools the features and generates fixed-length outputs, which are then fed into the fully-connected layers (or other classifiers). In other words, we perform some information aggregation at a deeper stage of the network hierarchy (between convolutional layers and fully-connected layers) to avoid the need for cropping or warping at the beginning." ;
    skos:prefLabel "Spatial Pyramid Pooling" .

:SpatialTransformer a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1506.02025v3> ;
    rdfs:seeAlso <https://github.com/kevinzakka/spatial-transformer-network/blob/375f99046383316b18edfb5c575dc390c4ee3193/stn/transformer.py#L4> ;
    skos:definition """A **Spatial Transformer** is an image model block that explicitly allows the spatial manipulation of data within a [convolutional neural network](https://paperswithcode.com/methods/category/convolutional-neural-networks). It gives CNNs the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. Unlike pooling layers, where the receptive fields are fixed and local, the spatial transformer module is a dynamic mechanism that can actively spatially transform an image (or a feature map) by producing an appropriate transformation for each input sample. The transformation is then performed on the entire feature map (non-locally) and can include scaling, cropping, rotations, as well as non-rigid deformations.\r
\r
The architecture is shown in the Figure to the right. The input feature map $U$ is passed to a localisation network which regresses the transformation parameters $\\theta$. The regular spatial grid $G$ over $V$ is transformed to the sampling grid $T\\_{\\theta}\\left(G\\right)$, which is applied to $U$, producing the warped output feature map $V$. The combination of the localisation network and sampling mechanism defines a spatial transformer.""" ;
    skos:prefLabel "Spatial Transformer" .

:SpatiallySeparableConvolution a skos:Concept ;
    skos:definition """A **Spatially Separable Convolution** decomposes a [convolution](https://paperswithcode.com/method/convolution) into two separate operations. In regular convolution, if we have a 3 x 3 kernel then we directly convolve this with the image. We can divide a 3 x 3 kernel into a 3 x 1 kernel and a 1 x 3 kernel. Then, in spatially separable convolution, we first convolve the 3 x 1 kernel then the 1 x 3 kernel. This requires 6 instead of 9 parameters compared to regular convolution, and so it is more parameter efficient (additionally less matrix multiplications are required).\r
\r
Image Source: [Kunlun Bai](https://towardsdatascience.com/a-comprehensive-introduction-to-different-types-of-convolutions-in-deep-learning-669281e58215)""" ;
    skos:prefLabel "Spatially Separable Convolution" .

:SpatiallySeparableSelf-Attention a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2104.13840v4> ;
    skos:definition """**Spatially Separable Self-Attention**, or **SSSA**, is an [attention module](https://paperswithcode.com/methods/category/attention-modules) used in the [Twins-SVT](https://paperswithcode.com/method/twins-svt) architecture that aims to reduce the computational complexity of [vision transformers](https://paperswithcode.com/methods/category/vision-transformer) for dense prediction tasks (given high-resolution inputs). SSSA is composed of [locally-grouped self-attention](https://paperswithcode.com/method/locally-grouped-self-attention) (LSA) and [global sub-sampled attention](https://paperswithcode.com/method/global-sub-sampled-attention) (GSA).\r
\r
Formally, spatially separable self-attention (SSSA) can be written as:\r
\r
$$\r
\\hat{\\mathbf{z}}\\_{i j}^{l}=\\text { LSA }\\left(\\text { LayerNorm }\\left(\\mathbf{z}\\_{i j}^{l-1}\\right)\\right)+\\mathbf{z}\\_{i j}^{l-1} $$\r
\r
$$\\mathbf{z}\\_{i j}^{l}=\\mathrm{FFN}\\left(\\operatorname{LayerNorm}\\left(\\hat{\\mathbf{z}}\\_{i j}^{l}\\right)\\right)+\\hat{\\mathbf{z}}\\_{i j}^{l} $$\r
\r
$$ \\hat{\\mathbf{z}}^{l+1}=\\text { GSA }\\left(\\text { LayerNorm }\\left(\\mathbf{z}^{l}\\right)\\right)+\\mathbf{z}^{l} $$\r
\r
$$ \\mathbf{z}^{l+1}=\\text { FFN }\\left(\\text { LayerNorm }\\left(\\hat{\\mathbf{z}}^{l+1}\\right)\\right)+\\hat{\\mathbf{z}}^{l+1}$$\r
\r
$$i \\in\\{1,2, \\ldots ., m\\}, j \\in\\{1,2, \\ldots ., n\\}\r
$$\r
\r
where LSA means locally-grouped self-attention within a sub-window; GSA is the global sub-sampled attention by interacting with the representative keys (generated by the sub-sampling functions) from each sub-window $\\hat{\\mathbf{z}}\\_{i j} \\in \\mathcal{R}^{k\\_{1} \\times k\\_{2} \\times C} .$ Both LSA and GSA have multiple heads as in the standard self-attention.""" ;
    skos:prefLabel "Spatially Separable Self-Attention" .

:SpecGAN a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1802.04208v3> ;
    skos:definition """**SpecGAN** is a generative adversarial network method for spectrogram-based, frequency-domain audio generation. The problem is suited for GANs designed for image generation. The model can be approximately inverted. \r
\r
To process audio into suitable spectrograms, the authors perform the short-time Fourier transform with 16 ms windows and 8ms stride, resulting in 128 frequency bins, linearly spaced from 0 to 8 kHz. They take the magnitude of the resultant spectra and scale amplitude values logarithmically to better-align with human perception. They then normalize each frequency bin to have zero mean and unit variance. They clip the spectra to $3$ standard deviations and rescale to $\\left[−1, 1\\right]$.\r
\r
They then use the [DCGAN](https://paperswithcode.com/method/dcgan) approach on the result spectra.""" ;
    skos:prefLabel "SpecGAN" .

:Spectral-NormalizedIdentityPriors a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2010.01791v1> ;
    skos:definition "**Spectral-Normalized Identity Priors**, or **SNIP**, is a structured pruning approach that penalizes an entire [residual module](https://paperswithcode.com/method/residual-connection) in a [Transformer model](https://paperswithcode.com/method/residual-connection) toward an identity mapping. It is applicable to any structured module, including a single [attention head](https://paperswithcode.com/method/scaled), an [entire attention block](https://paperswithcode.com/method/multi-head-attention), or a [feed-forward subnetwork](https://paperswithcode.com/method/position-wise-feed-forward-layer). The method identifies and discards unimportant non-linear mappings in the [residual connections](https://paperswithcode.com/method/residual-connection) by applying a thresholding operator on the function norm. Furthermore, [spectral normalization](https://paperswithcode.com/method/spectral-normalization) to stabilize the distribution of the post-activation values of the [Transformer](https://paperswithcode.com/method/transformer) layers, further improving the pruning effectiveness of the proposed methodology." ;
    skos:prefLabel "Spectral-Normalized Identity Priors" .

:SpectralDropout a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1711.08591v1> ;
    skos:definition "Please enter a description about the method here" ;
    skos:prefLabel "Spectral Dropout" .

:SpectralNormalization a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1802.05957v1> ;
    rdfs:seeAlso <https://github.com/christiancosgrove/pytorch-spectral-normalization-gan/blob/12dcf945a6359301d63d1e0da3708cd0f0590b19/spectral_normalization.py#L14> ;
    skos:definition """**Spectral Normalization** is a normalization technique used for generative adversarial networks, used to stabilize training of the discriminator. Spectral normalization has the convenient property that the Lipschitz constant is the only hyper-parameter to be tuned.\r
\r
It controls the Lipschitz constant of the discriminator $f$ by constraining the spectral norm of each layer $g : \\textbf{h}\\_{in} \\rightarrow \\textbf{h}_{out}$. The Lipschitz norm $\\Vert{g}\\Vert\\_{\\text{Lip}}$ is equal to $\\sup\\_{\\textbf{h}}\\sigma\\left(\\nabla{g}\\left(\\textbf{h}\\right)\\right)$, where $\\sigma\\left(a\\right)$ is the spectral norm of the matrix $A$ ($L\\_{2}$ matrix norm of $A$):\r
\r
$$ \\sigma\\left(a\\right) = \\max\\_{\\textbf{h}:\\textbf{h}\\neq{0}}\\frac{\\Vert{A\\textbf{h}}\\Vert\\_{2}}{\\Vert\\textbf{h}\\Vert\\_{2}} = \\max\\_{\\Vert\\textbf{h}\\Vert\\_{2}\\leq{1}}{\\Vert{A\\textbf{h}}\\Vert\\_{2}} $$\r
\r
which is equivalent to the largest singular value of $A$. Therefore for a [linear layer](https://paperswithcode.com/method/linear-layer) $g\\left(\\textbf{h}\\right) = W\\textbf{h}$ the norm is given by $\\Vert{g}\\Vert\\_{\\text{Lip}} = \\sup\\_{\\textbf{h}}\\sigma\\left(\\nabla{g}\\left(\\textbf{h}\\right)\\right) = \\sup\\_{\\textbf{h}}\\sigma\\left(W\\right) = \\sigma\\left(W\\right) $. Spectral normalization normalizes the spectral norm of the weight matrix $W$ so it satisfies the Lipschitz constraint $\\sigma\\left(W\\right) = 1$:\r
\r
$$ \\bar{W}\\_{\\text{SN}}\\left(W\\right) = W / \\sigma\\left(W\\right) $$""" ;
    skos:prefLabel "Spectral Normalization" .

:SpineNet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1912.05027v3> ;
    rdfs:seeAlso <https://github.com/lucifer443/SpineNet-Pytorch/blob/a7059eff295dcee16d719b381f80af8eb3fe42f6/mmdet/models/backbones/spinenet.py#L151> ;
    skos:definition "**SpineNet** is a convolutional neural network backbone with scale-permuted intermediate features and cross-scale connections that is learned on an object detection task by [Neural Architecture Search](https://paperswithcode.com/method/neural-architecture-search)." ;
    skos:prefLabel "SpineNet" .

:SplitAttention a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2004.08955v2> ;
    rdfs:seeAlso <https://github.com/zhanghang1989/ResNeSt/blob/5fe47e93bd7e098d15bc278d8ab4812b82b49414/resnest/torch/splat.py#L11> ;
    skos:definition """A **Split Attention** block enables attention across feature-map groups. As in [ResNeXt blocks](https://paperswithcode.com/method/resnext-block), the feature can be divided into several groups, and the number of feature-map groups is given by a cardinality hyperparameter $K$. The resulting feature-map groups are called cardinal groups. Split Attention blocks introduce a new radix hyperparameter $R$ that indicates the number of splits within a cardinal group, so the total number of feature groups is $G = KR$. We may apply a series of transformations {$\\mathcal{F}\\_1, \\mathcal{F}\\_2, \\cdots\\mathcal{F}\\_G$} to each individual group, then the intermediate representation of each group is $U\\_i = \\mathcal{F}\\_i\\left(X\\right)$, for $i \\in$ {$1, 2, \\cdots{G}$}.\r
\r
A combined representation for each cardinal group can be obtained by fusing via an element-wise summation across multiple splits. The representation for $k$-th cardinal group is \r
$\\hat{U}^k = \\sum_{j=R(k-1)+1}^{R k} U_j $, where $\\hat{U}^k \\in \\mathbb{R}^{H\\times W\\times C/K}$ for $k\\in{1,2,...K}$, and $H$, $W$ and $C$ are the block output feature-map sizes. \r
Global contextual information with embedded channel-wise statistics can be gathered with [global average pooling](https://paperswithcode.com/method/global-average-pooling) across spatial dimensions  $s^k\\in\\mathbb{R}^{C/K}$. Here the $c$-th component is calculated as:\r
\r
$$\r
    s^k\\_c = \\frac{1}{H\\times W} \\sum\\_{i=1}^H\\sum\\_{j=1}^W \\hat{U}^k\\_c(i, j).\r
$$\r
\r
A weighted fusion of the cardinal group representation $V^k\\in\\mathbb{R}^{H\\times W\\times C/K}$ is aggregated using [channel-wise soft attention](https://paperswithcode.com/method/channel-wise-soft-attention), where each feature-map channel is produced using a weighted combination over splits. The $c$-th channel is calculated as:\r
\r
$$\r
    V^k_c=\\sum_{i=1}^R a^k_i(c) U_{R(k-1)+i} ,\r
$$\r
\r
where $a_i^k(c)$ denotes a (soft) assignment weight given by:\r
\r
$$\r
a_i^k(c) =\r
\\begin{cases}\r
  \\frac{exp(\\mathcal{G}^c_i(s^k))}{\\sum_{j=0}^R exp(\\mathcal{G}^c_j(s^k))} & \\quad\\textrm{if } R>1, \\\\\r
   \\frac{1}{1+exp(-\\mathcal{G}^c_i(s^k))} & \\quad\\textrm{if } R=1,\\\\\r
\\end{cases}\r
$$\r
\r
and mapping $\\mathcal{G}_i^c$ determines the weight of each split for the $c$-th channel based on the global context representation $s^k$.""" ;
    skos:prefLabel "Split Attention" .

:SpreadsheetCoder a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2106.15339v1> ;
    skos:definition "**SpreadsheetCoder** is a neural network architecture for spreadsheet formula prediction. It is a [BERT](https://paperswithcode.com/method/bert)-based model architecture to represent the tabular context in both row-based and column-based formats. A [BERT](https://paperswithcode.com/method/bert) encoder computes an embedding vector for each input token, incorporating the contextual information from nearby rows and columns. The BERT encoder is initialized from the weights pre-trained on English text corpora, which is beneficial for encoding table headers. To handle cell references, a two-stage decoding process is used inspired by sketch learning for program synthesis. The decoder first generates a formula sketch, which does not include concrete cell references, and then predicts the corresponding cell ranges to generate the complete formula" ;
    skos:prefLabel "SpreadsheetCoder" .

:SquaredReLU a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2109.08668v2> ;
    skos:definition """**Squared ReLU** is an activation function used in the [Primer](https://paperswithcode.com/method/primer) architecture in the feedforward block of the [Transformer](https://paperswithcode.com/methods/category/transformers) layer. It is simply squared [ReLU](https://paperswithcode.com/method/relu) activations.\r
\r
The effectiveness of higher order polynomials can also be observed in other effective [Transformer](https://paperswithcode.com/method/transformer) nonlinearities, such as [GLU](https://paperswithcode.com/method/glu) variants like [ReGLU](https://paperswithcode.com/method/reglu) and point-wise activations like [approximate GELU](https://paperswithcode.com/method/gelu). However, squared ReLU has drastically different asymptotics as $x \\rightarrow \\inf$ compared to the most commonly used activation functions: [ReLU](https://paperswithcode.com/method/relu), [GELU](https://paperswithcode.com/method/gelu) and [Swish](https://paperswithcode.com/method/swish). Squared ReLU does have significant overlap with ReGLU and in fact is equivalent when ReGLU’s $U$ and $V$ weight matrices are the same and squared ReLU is immediately preceded by a linear transformation with weight matrix $U$. This leads the authors to believe that squared ReLUs capture the benefits of these GLU variants, while being simpler, without additional parameters, and delivering better quality.""" ;
    skos:prefLabel "Squared ReLU" .

:Squeeze-and-ExcitationBlock a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1709.01507v4> ;
    rdfs:seeAlso <https://github.com/osmr/imgclsmob/blob/68335927ba27f2356093b985bada0bc3989836b1/pytorch/pytorchcv/models/common.py#L731> ;
    skos:definition """The **Squeeze-and-Excitation Block** is an architectural unit designed to improve the representational power of a network by enabling it to perform dynamic channel-wise feature recalibration. The process is:\r
\r
- The block has a convolutional block as an input.\r
- Each channel is "squeezed" into a single numeric value using [average pooling](https://paperswithcode.com/method/average-pooling).\r
- A dense layer followed by a [ReLU](https://paperswithcode.com/method/relu) adds non-linearity and output channel complexity is reduced by a ratio.\r
- Another dense layer followed by a sigmoid gives each channel a smooth gating function.\r
- Finally, we weight each feature map of the convolutional block based on the side network; the "excitation".""" ;
    skos:prefLabel "Squeeze-and-Excitation Block" .

:SqueezeBERT a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2006.11316v1> ;
    skos:definition "**SqueezeBERT** is an efficient architectural variant of [BERT](https://paperswithcode.com/method/bert) for natural language processing that uses [grouped convolutions](https://paperswithcode.com/method/grouped-convolution). It is much like BERT-base, but with positional feedforward connection layers implemented as convolutions, and grouped [convolution](https://paperswithcode.com/method/convolution) for many of the layers." ;
    skos:prefLabel "SqueezeBERT" .

:SqueezeNeXt a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1803.10615v2> ;
    rdfs:seeAlso <https://github.com/osmr/imgclsmob/blob/c03fa67de3c9e454e9b6d35fe9cbb6b15c28fda7/pytorch/pytorchcv/models/squeezenext.py#L125> ;
    skos:definition "**SqueezeNeXt** is a type of convolutional neural network that uses the [SqueezeNet](https://paperswithcode.com/method/squeezenet) architecture as a baseline, but makes a number of changes. First, a more aggressive channel reduction is used by incorporating a two-stage squeeze module. This significantly reduces the total number of parameters used with the 3×3 convolutions. Secondly, it uses separable 3 × 3 convolutions to further reduce the model size, and removes the additional 1×1 branch after the squeeze module. Thirdly, the network use an element-wise addition skip connection similar to that of [ResNet](https://paperswithcode.com/method/resnet) architecture." ;
    skos:prefLabel "SqueezeNeXt" .

:SqueezeNeXtBlock a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1803.10615v2> ;
    rdfs:seeAlso <https://github.com/osmr/imgclsmob/blob/c03fa67de3c9e454e9b6d35fe9cbb6b15c28fda7/pytorch/pytorchcv/models/squeezenext.py#L14> ;
    skos:definition "A **SqueezeNeXt Block** is a two-stage bottleneck module used in the [SqueezeNeXt](https://paperswithcode.com/method/squeezenext) architecture to reduce the number of input channels to the 3 × 3 [convolution](https://paperswithcode.com/method/convolution). We decompose with separable convolutions to further reduce the number of parameters (orange parts), followed by a 1 × 1 expansion module." ;
    skos:prefLabel "SqueezeNeXt Block" .

:SqueezeNet a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1602.07360v4> ;
    rdfs:seeAlso <https://github.com/pytorch/vision/blob/6db1569c89094cf23f3bc41f79275c45e9fcb3f3/torchvision/models/squeezenet.py#L37> ;
    skos:definition "**SqueezeNet** is a convolutional neural network that employs design strategies to reduce the number of parameters, notably with the use of fire modules that \"squeeze\" parameters using 1x1 convolutions." ;
    skos:prefLabel "SqueezeNet" .

:Sscs a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2108.10576v1> ;
    skos:altLabel "Support-set Based Cross-Supervision" ;
    skos:definition """**Sscs**, or **Support-set Based Cross-Supervision**, is a module for video grounding which consists of two main components: a discriminative contrastive objective and a generative caption objective. The contrastive objective aims to learn effective representations by contrastive learning, while the caption objective can train a powerful video encoder supervised by texts. Due to the co-existence of some visual entities in both ground-truth and background intervals, i.e., mutual exclusion, naively contrastive learning is unsuitable to video grounding. This problem is addressed by boosting the cross-supervision with the support-set concept, which collects visual information from the whole video and eliminates the mutual exclusion of entities.\r
\r
Specifically, in the Figure to the right, two video-text pairs { $V\\_{i}, L\\_{i}$}, {$V\\_{j} , L\\_{j}$ } in the batch are presented for clarity. After feeding them into a video and text encoder, the clip-level and sentence-level embedding ( {$X\\_{i}, Y\\_{i}$} and {$X\\_{j} , Y\\_{j}$} ) in a shared space are acquired. Base on the support-set module, the weighted average of $X\\_{i}$ and $X\\_{j}$ is computed to obtain $\\bar{X}\\_{i}$, $\\bar{X}\\_{j}$ respectively. Finally, the contrastive and caption objectives are combined to pull close the representations of the clips and text from the same samples and push away those from other pairs""" ;
    skos:prefLabel "Sscs" .

:StackedHourglassNetwork a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1603.06937v2> ;
    skos:definition "**Stacked Hourglass Networks** are a type of convolutional neural network for pose estimation. They are based on the successive steps of pooling and upsampling that are done to produce a final set of predictions." ;
    skos:prefLabel "Stacked Hourglass Network" .

:StarReLU a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2210.13452v2> ;
    rdfs:seeAlso <https://github.com/sail-sg/metaformer> ;
    skos:definition """$s \\cdot (\\mathrm{ReLU}(x))^2 + b$\r
\r
where $s \\in \\mathbb{R}$ and $b \\in \\mathbb{R}$ are shared for all channels and can be set as constants (s=0.8944, b=-0.4472) or learnable parameters.""" ;
    skos:prefLabel "StarReLU" .

:State-AwareTracker a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2003.00482v1> ;
    skos:definition "**State-Aware Tracker** is a pipeline for semi-supervised video object segmentation. It takes each target object as a tracklet, which not only makes the pipeline more efficient but also filters distractors to facilitate target modeling. For more stable and robust performance over video sequences, SAT gets awareness for each state and makes self-adaptation via two feedback loops. One loop assists SAT in generating more stable tracklets. The other loop helps to construct a more robust and holistic target representation." ;
    skos:prefLabel "State-Aware Tracker" .

:StepDecay a skos:Concept ;
    skos:definition """**Step Decay** is a learning rate schedule that drops the learning rate by a factor every few epochs, where the number of epochs is a hyperparameter.\r
\r
Image Credit: [Suki Lau](https://towardsdatascience.com/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1)""" ;
    skos:prefLabel "Step Decay" .

:StereoLayers a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2201.05023v2> ;
    skos:definition "" ;
    skos:prefLabel "StereoLayers" .

:StoGCN a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1710.10568v3> ;
    skos:definition "StoGCN is a control variate based algorithm which allow sampling an arbitrarily small neighbor size. Presents new theoretical guarantee for the algorithms to converge to a local optimum of GCN." ;
    skos:prefLabel "StoGCN" .

:StochasticDepth a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1603.09382v3> ;
    rdfs:seeAlso <https://github.com/osmr/imgclsmob/blob/69d63f99a4929d61a98e0975a4ab45b554835b9e/gluon/gluoncv2/models/resdropresnet_cifar.py#L17> ;
    skos:definition """**Stochastic Depth** aims to shrink the depth of a network during training, while\r
keeping it unchanged during testing. This is achieved by randomly dropping entire [ResBlocks](https://paperswithcode.com/method/residual-block) during training and bypassing their transformations through skip connections. \r
\r
Let $b\\_{l} \\in$ {$0, 1$} denote a Bernoulli random variable, which indicates whether the $l$th ResBlock is active ($b\\_{l} = 1$) or inactive ($b\\_{l} = 0$). Further, let us denote the “survival” probability of ResBlock $l$ as $p\\_{l} = \\text{Pr}\\left(b\\_{l} = 1\\right)$. With this definition we can bypass the $l$th ResBlock by multiplying its function $f\\_{l}$ with $b\\_{l}$ and we extend the update rule to:\r
\r
$$ H\\_{l} = \\text{ReLU}\\left(b\\_{l}f\\_{l}\\left(H\\_{l-1}\\right) + \\text{id}\\left(H\\_{l-1}\\right)\\right) $$\r
\r
If $b\\_{l} = 1$, this reduces to the original [ResNet](https://paperswithcode.com/method/resnet) update and this ResBlock remains unchanged. If $b\\_{l} = 0$, the ResBlock reduces to the identity function, $H\\_{l} = \\text{id}\\left((H\\_{l}−1\\right)$.""" ;
    skos:prefLabel "Stochastic Depth" .

:StochasticDuelingNetwork a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1611.01224v2> ;
    skos:definition "A **Stochastic Dueling Network**, or **SDN**, is an architecture for learning a value function $V$. The SDN learns both $V$ and $Q$ off-policy while maintaining consistency between the two estimates. At each time step it outputs a stochastic estimate of $Q$ and a deterministic estimate of $V$." ;
    skos:prefLabel "Stochastic Dueling Network" .

:StochasticGradientVariationalBayes a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1312.6114v10> ;
    skos:definition "" ;
    skos:prefLabel "Stochastic Gradient Variational Bayes" .

:StochasticWeightAveraging a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1803.05407v3> ;
    rdfs:seeAlso <https://github.com/timgaripov/swa/blob/f54af44f7af3cadd628412bbc34383e94e56ae53/train.py#L154> ;
    skos:definition "**Stochastic Weight Averaging** is an optimization procedure that averages multiple points along the trajectory of [SGD](https://paperswithcode.com/method/sgd), with a cyclical or constant learning rate. On the one hand it averages weights, but it also has the property that, with a cyclical or constant learning rate, SGD proposals are approximately sampling from the loss surface of the network, leading to stochastic weights and helping to discover broader optima." ;
    skos:prefLabel "Stochastic Weight Averaging" .

:StreaMRAK a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2108.10411v2> ;
    skos:definition "**StreaMRAK** is a streaming version of kernel ridge regression. It divdes the problem into several levels of resolution, which allows continual refinement to the predictions." ;
    skos:prefLabel "StreaMRAK" .

:StreamingModule a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1904.09290v1> ;
    skos:definition "" ;
    skos:prefLabel "Streaming Module" .

:StridedAttention a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1904.10509v1> ;
    rdfs:seeAlso <https://github.com/openai/sparse_attention/blob/b68b367288ce5cabb3acec21afea2db54feedb9a/attention.py#L231> ;
    skos:definition """**Strided Attention** is a factorized attention pattern that has one head attend to the previous\r
$l$ locations, and the other head attend to every $l$th location, where $l$ is the stride and chosen to be close to $\\sqrt{n}$. It was proposed as part of the [Sparse Transformer](https://paperswithcode.com/method/sparse-transformer) architecture.\r
\r
A self-attention layer maps a matrix of input embeddings $X$ to an output matrix and is parameterized by a connectivity pattern $S = \\text{set}\\left(S\\_{1}, \\dots, S\\_{n}\\right)$, where $S\\_{i}$ denotes the set of indices of the input vectors to which the $i$th output vector attends. The output vector is a weighted sum of transformations of the input vectors:\r
\r
$$ \\text{Attend}\\left(X, S\\right) = \\left(a\\left(\\mathbf{x}\\_{i}, S\\_{i}\\right)\\right)\\_{i\\in\\text{set}\\left(1,\\dots,n\\right)}$$\r
\r
$$ a\\left(\\mathbf{x}\\_{i}, S\\_{i}\\right) = \\text{softmax}\\left(\\frac{\\left(W\\_{q}\\mathbf{x}\\_{i}\\right)K^{T}\\_{S\\_{i}}}{\\sqrt{d}}\\right)V\\_{S\\_{i}} $$\r
\r
$$ K\\_{Si} = \\left(W\\_{k}\\mathbf{x}\\_{j}\\right)\\_{j\\in{S\\_{i}}} $$\r
\r
$$ V\\_{Si} = \\left(W\\_{v}\\mathbf{x}\\_{j}\\right)\\_{j\\in{S\\_{i}}} $$\r
\r
Here $W\\_{q}$, $W\\_{k}$, and $W\\_{v}$ represent the weight matrices which transform a given $x\\_{i}$ into a query, key, or value, and $d$ is the inner dimension of the queries and keys. The output at each position is a sum of the values weighted by the scaled dot-product similarity of the keys and queries.\r
\r
Full self-attention for autoregressive models defines $S\\_{i} = \\text{set}\\left(j : j \\leq i\\right)$, allowing every element to attend to all previous positions and its own position.\r
\r
Factorized self-attention instead has $p$ separate attention heads, where the $m$th head defines a subset of the indices $A\\_{i}^{(m)} ⊂ \\text{set}\\left(j : j \\leq i\\right)$ and lets $S\\_{i} = A\\_{i}^{(m)}$. The goal with the Sparse [Transformer](https://paperswithcode.com/method/transformer) was to find efficient choices for the subset $A$.\r
\r
Formally for Strided Attention, $A^{(1)}\\_{i} = ${$t, t + 1, ..., i$} for $t = \\max\\left(0, i − l\\right)$, and $A^{(2)}\\_{i} = ${$j : (i − j) \\mod l = 0$}. The $i$-th output vector of the attention head attends to all input vectors either from $A^{(1)}\\_{i}$ or $A^{(2)}\\_{i}$. This pattern can be visualized in the figure to the right.\r
\r
This formulation is convenient if the data naturally has a structure that aligns with the stride, like images or some types of music. For data without a periodic structure, like text, however, the authors find that the network can fail to properly route information with the strided pattern, as spatial coordinates for an element do not necessarily correlate with the positions where the element may be most relevant in the future.""" ;
    skos:prefLabel "Strided Attention" .

:StridedEESP a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1811.11431v3> ;
    rdfs:seeAlso <https://github.com/osmr/imgclsmob/blob/e8d8e31ad8250142170cbc10d5e7c6a583bd9585/pytorch/pytorchcv/models/espnetv2.py#L94> ;
    skos:definition "A **Strided EESP** unit is based on the [EESP Unit](https://paperswithcode.com/method/eesp) but is modified to learn representations more efficiently at multiple scales. Depth-wise dilated convolutions are given strides, an [average pooling](https://paperswithcode.com/method/average-pooling) operation is added instead of an identity connection, and the element-wise addition operation is replaced with a concatenation operation, which helps in expanding the dimensions of feature maps efficiently." ;
    skos:prefLabel "Strided EESP" .

:StripPooling a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2003.13328v1> ;
    skos:definition "**Strip Pooling** is a pooling strategy for scene parsing which considers a long but narrow kernel, i.e., $1\\times{N}$ or $N\\times{1}$. As an alternative to global pooling, strip pooling offers two advantages. First, it deploys a long kernel shape along one spatial dimension and hence enables capturing long-range relations of isolated regions. Second, it keeps a narrow kernel shape along the other spatial dimension, which facilitates capturing local context and prevents irrelevant regions from interfering the label prediction. Integrating such long but narrow pooling kernels enables the scene parsing networks to simultaneously aggregate both global and local context. This is essentially different from the traditional spatial pooling which collects context from a fixed square region." ;
    skos:prefLabel "Strip Pooling" .

:StruBERT a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2203.14278v1> ;
    skos:altLabel "StruBERT: Structure-aware BERT for Table Search and Matching" ;
    skos:definition "A large amount of information is stored in data tables. Users can search for data tables using a keyword-based query. A table is composed primarily of data values that are organized in rows and columns providing implicit structural information. A table is usually accompanied by secondary information such as the caption, page title, etc., that form the textual information. Understanding the connection between the textual and structural information is an important yet neglected aspect in table retrieval as previous methods treat each source of information independently. In addition, users can search for data tables that are similar to an existing table, and this setting can be seen as a content-based table retrieval. In this paper, we propose StruBERT, a structure-aware BERT model that fuses the textual and structural information of a data table to produce context-aware representations for both textual and tabular content of a data table. StruBERT features are integrated in a new end-to-end neural ranking model to solve three table-related downstream tasks: keyword- and content-based table retrieval, and table similarity. We evaluate our approach using three datasets, and we demonstrate substantial improvements in terms of retrieval and classification metrics over state-of-the-art methods." ;
    skos:prefLabel "StruBERT" .

:Style-basedRecalibrationModule a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1903.10829v1> ;
    rdfs:seeAlso <https://github.com/EvgenyKashin/SRMnet/blob/a7ebdbe47a489c3e604ef44a84d361ef7b679e17/models/layer_blocks.py#L27> ;
    skos:definition """A **Style-based Recalibration Module (SRM)** is a module for convolutional neural networks that adaptively recalibrates intermediate feature maps by exploiting their styles. SRM first extracts the style information from each channel of the feature maps by style pooling, then estimates per-channel recalibration weight via channel-independent style integration. By incorporating the relative importance of individual styles into feature maps, SRM is aimed at enhancing the representational ability of a CNN.\r
\r
The overall structure of SRM is illustrated in the Figure to the right. It consists of two main components: style pooling and style integration. The style pooling operator extracts style features\r
from each channel by summarizing feature responses across spatial dimensions. It is followed by the style integration operator, which produces example-specific style weights by utilizing the style features via channel-wise operation. The style weights finally recalibrate the feature maps to either\r
emphasize or suppress their information.""" ;
    skos:prefLabel "Style-based Recalibration Module" .

:StyleALAE a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2004.04467v1> ;
    skos:definition """**StyleALAE** is a type of [adversarial latent autoencoder](https://paperswithcode.com/method/alae) that uses a [StyleGAN](https://paperswithcode.com/method/stylegan) based generator. For this the latent space $\\mathcal{W}$ plays the same role as the intermediate latent space in [StyleGAN](https://paperswithcode.com/method/stylegan). Therefore, the $G$ network becomes the part of StyleGAN depicted on the right side of the Figure. The left side is a\r
novel architecture that we designed to be the encoder $E$. The StyleALAE encoder has [Instance Normalization](https://paperswithcode.com/method/instance-normalization) (IN) layers to extract multiscale style information that is combined into a latent code $w$ via a learnable multilinear map.""" ;
    skos:prefLabel "StyleALAE" .

:StyleGAN a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1812.04948v3> ;
    skos:definition "**StyleGAN** is a type of generative adversarial network. It uses an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature; in particular, the use of [adaptive instance normalization](https://paperswithcode.com/method/adaptive-instance-normalization). Otherwise it follows Progressive [GAN](https://paperswithcode.com/method/gan) in using a progressively growing training regime. Other quirks include the fact it generates from a fixed value tensor not stochastically generated latent variables as in regular GANs. The stochastically generated latent variables are used as style vectors in the adaptive [instance normalization](https://paperswithcode.com/method/instance-normalization) at each resolution after being transformed by an 8-layer [feedforward network](https://paperswithcode.com/method/feedforward-network). Lastly, it employs a form of regularization called mixing regularization, which mixes two style latent variables during training." ;
    skos:prefLabel "StyleGAN" .

:StyleGAN2 a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1912.04958v2> ;
    rdfs:seeAlso <https://github.com/NVlabs/stylegan2> ;
    skos:definition "**StyleGAN2** is a generative adversarial network that builds on [StyleGAN](https://paperswithcode.com/method/stylegan) with several improvements. First, [adaptive instance normalization](https://paperswithcode.com/method/adaptive-instance-normalization) is redesigned and replaced with a normalization technique called [weight demodulation](https://paperswithcode.com/method/weight-demodulation). Secondly, an improved training scheme upon progressively growing is introduced, which achieves the same goal - training starts by focusing on low-resolution images and then progressively shifts focus to higher and higher resolutions - without changing the network topology during training. Additionally, new types of regularization like lazy regularization and [path length regularization](https://paperswithcode.com/method/path-length-regularization) are proposed." ;
    skos:prefLabel "StyleGAN2" .

:StyleMapGAN a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2104.14754v2> ;
    skos:definition "**StyleMapGAN** is a generative adversarial network for real-time image editing. The intermediate latent space has spatial dimensions, and a spatially variant modulation replaces [AdaIN](https://paperswithcode.com/method/adaptive-instance-normalization). It aims to make the embedding through an encoder more accurate than existing optimization-based methods while maintaining the properties of GANs." ;
    skos:prefLabel "StyleMapGAN" .

:StyleSwin a skos:Concept ;
    rdfs:seeAlso <https://github.com/microsoft/StyleSwin> ;
    skos:altLabel "StyleSwin: Transformer-based GAN for High-resolution Image Generation" ;
    skos:definition "Despite the tantalizing success in a broad of vision tasks, transformers have not yet demonstrated on-par ability as ConvNets in high-resolution image generative modeling. In this paper, we seek to explore using pure transformers to build a generative adversarial network for high-resolution image synthesis. To this end, we believe that local attention is crucial to strike the balance between computational efficiency and modeling capacity. Hence, the proposed generator adopts Swin transformer in a style-based architecture. To achieve a larger receptive field, we propose double attention which simultaneously leverages the context of the local and the shifted windows, leading to improved generation quality. Moreover, we show that offering the knowledge of the absolute position that has been lost in window-based transformers greatly benefits the generation quality. The proposed StyleSwin is scalable to high resolutions, with both the coarse geometry and fine structures benefit from the strong expressivity of transformers. However, blocking artifacts occur during high-resolution synthesis because performing the local attention in a block-wise manner may break the spatial coherency. To solve this, we empirically investigate various solutions, among which we find that employing a wavelet discriminator to examine the spectral discrepancy effectively suppresses the artifacts. Extensive experiments show the superiority over prior transformer-based GANs, especially on high resolutions, e.g., 1024x1024. The StyleSwin, without complex training strategies, excels over StyleGAN on CelebA-HQ 1024x1024, and achieves on-par performance on FFHQ 1024x1024, proving the promise of using transformers for high-resolution image generation." ;
    skos:prefLabel "StyleSwin" .

:StyleTransferModule a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1703.06868v2> ;
    skos:definition "Modules used in [GAN](https://paperswithcode.com/method/gan)'s style transfer." ;
    skos:prefLabel "Style Transfer Module" .

:Subformer a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2101.00234v3> ;
    skos:definition "**Subformer** is a [Transformer](https://paperswithcode.com/method/transformer) that combines sandwich-style parameter sharing, which overcomes naive cross-layer parameter sharing in generative models, and self-attentive embedding factorization (SAFE). In SAFE, a small self-attention layer is used to reduce embedding parameter count." ;
    skos:prefLabel "Subformer" .

:SubmanifoldConvolution a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1711.10275v1> ;
    rdfs:seeAlso <https://github.com/facebookresearch/SparseConvNet/blob/159e5f9a2349c776c422dce9f5b4493519303dc2/sparseconvnet/submanifoldConvolution.py#L14> ;
    skos:definition "**Submanifold Convolution (SC)** is a spatially sparse [convolution](https://paperswithcode.com/method/convolution) operation used for tasks with sparse data like semantic segmentation of 3D point clouds. An SC convolution computes the set of active sites in the same way as a regular convolution: it looks for the presence of any active sites in its receptive field of size $f^{d}$. If the input has size $l$ then the output will have size $\\left(l − f + s\\right)/s$. Unlike a regular convolution, an SC convolution discards the ground state for non-active sites by assuming that the input from those sites is zero. For more details see the [paper](https://paperswithcode.com/paper/3d-semantic-segmentation-with-submanifold), or the official code [here](https://github.com/facebookresearch/SparseConvNet)." ;
    skos:prefLabel "Submanifold Convolution" .

:SuperpixelGridMasks a skos:Concept ;
    skos:altLabel "SuperpixelGridCut, SuperpixelGridMean, SuperpixelGridMix" ;
    skos:definition "Karim Hammoudi, Adnane Cabani, Bouthaina Slika, Halim Benhabiles, Fadi Dornaika and Mahmoud Melkemi. SuperpixelGridCut, SuperpixelGridMean and SuperpixelGridMix Data Augmentation, arXiv:2204.08458, 2022. https://doi.org/10.48550/arxiv.2204.08458" ;
    skos:prefLabel "SuperpixelGridMasks" .

:SupervisedContrastiveLoss a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2004.11362v5> ;
    rdfs:seeAlso <https://github.com/HobbitLong/SupContrast/blob/8117ece43eea5a38fb154f587601ceeadbe416b3/losses.py#L11> ;
    skos:definition """**Supervised Contrastive Loss** is an alternative loss function to cross entropy that the authors argue can leverage label information more effectively. Clusters of points belonging to the same class are pulled together in embedding space, while simultaneously pushing apart clusters of samples from different classes.\r
\r
$$\r
  \\mathcal{L}^{sup}=\\sum_{i=1}^{2N}\\mathcal{L}_i^{sup}\r
  \\label{eqn:total_supervised_loss}\r
$$\r
\r
$$\r
  \\mathcal{L}\\_i^{sup}=\\frac{-1}{2N\\_{\\boldsymbol{\\tilde{y}}\\_i}-1}\\sum\\_{j=1}^{2N}\\mathbf{1}\\_{i\\neq j}\\cdot\\mathbf{1}\\_{\\boldsymbol{\\tilde{y}}\\_i=\\boldsymbol{\\tilde{y}}_j}\\cdot\\log{\\frac{\\exp{\\left(\\boldsymbol{z}\\_i\\cdot\\boldsymbol{z}\\_j/\\tau\\right)}}{\\sum\\_{k=1}^{2N}\\mathbf{1}\\_{i\\neq k}\\cdot\\exp{\\left(\\boldsymbol{z}\\_i\\cdot\\boldsymbol{z}\\_k/\\tau\\right)}}}\r
$$\r
\r
where $N_{\\boldsymbol{\\tilde{y}}_i}$ is the total number of images in the minibatch that have the same label, $\\boldsymbol{\\tilde{y}}_i$, as the anchor, $i$. This loss has important properties well suited for supervised learning: (a) generalization to an arbitrary number of positives, (b) contrastive power increases with more negatives.""" ;
    skos:prefLabel "Supervised Contrastive Loss" .

:SwAV a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2006.09882v5> ;
    rdfs:seeAlso <https://github.com/facebookresearch/swav> ;
    skos:altLabel "Swapping Assignments between Views" ;
    skos:definition "**SwaV**, or **Swapping Assignments Between Views**, is a self-supervised learning approach that takes advantage of contrastive methods without requiring to compute pairwise comparisons. Specifically, it simultaneously clusters the data while enforcing consistency between cluster assignments produced for different augmentations (or views) of the same image, instead of comparing features directly as in contrastive learning. Simply put, SwaV uses a swapped prediction mechanism where we predict the cluster assignment of a view from the representation of another view." ;
    skos:prefLabel "SwAV" .

:SwiGLU a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2002.05202v1> ;
    skos:definition """**SwiGLU** is an activation function which is a variant of [GLU](https://paperswithcode.com/method/glu). The definition is as follows:\r
\r
$$ \\text{SwiGLU}\\left(x, W, V, b, c, \\beta\\right) = \\text{Swish}\\_{\\beta}\\left(xW + b\\right) \\otimes \\left(xV + c\\right) $$""" ;
    skos:prefLabel "SwiGLU" .

:SwinTransformer a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2103.14030v2> ;
    rdfs:seeAlso <https://github.com/microsoft/Swin-Transformer/blob/2622619f70760b60a42b996f5fcbe7c9d2e7ca57/models/swin_transformer.py#L458> ;
    skos:definition "The **Swin Transformer** is a type of [Vision Transformer](https://paperswithcode.com/method/vision-transformer). It builds hierarchical feature maps by merging image patches (shown in gray) in deeper layers and has linear computation complexity to input image size due to computation of self-attention only within each local window (shown in red). It can thus serve as a general-purpose backbone for both image classification and dense recognition tasks. In contrast, previous vision Transformers produce feature maps of a single low resolution and have quadratic computation complexity to input image size due to computation of self-attention globally." ;
    skos:prefLabel "Swin Transformer" .

:Swish a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1710.05941v2> ;
    rdfs:seeAlso <https://github.com/pytorch/pytorch/blob/96aaa311c0251d24decb9dc5da4957b7c590af6f/torch/nn/modules/activation.py#L352> ;
    skos:definition """**Swish** is an activation function, $f(x) = x \\cdot \\text{sigmoid}(\\beta x)$, where $\\beta$ a learnable parameter. Nearly all implementations do not use the learnable parameter $\\beta$, in which case the activation function is $x\\sigma(x)$ ("Swish-1").\r
\r
The function $x\\sigma(x)$ is exactly the [SiLU](https://paperswithcode.com/method/silu), which was introduced by other authors before the swish.\r
See [Gaussian Error Linear Units](https://arxiv.org/abs/1606.08415) ([GELUs](https://paperswithcode.com/method/gelu)) where the SiLU (Sigmoid Linear Unit) was originally coined, and see [Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning](https://arxiv.org/abs/1702.03118) and [Swish: a Self-Gated Activation Function](https://arxiv.org/abs/1710.05941v1) where the same activation function was experimented with later.""" ;
    skos:prefLabel "Swish" .

:SwitchFFN a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2101.03961v3> ;
    skos:definition "A **Switch FFN** is a sparse layer that operates independently on tokens within an input sequence. It is shown in the blue block in the figure. We diagram two tokens ($x\\_{1}$ = “More” and $x\\_{2}$ = “Parameters” below) being routed (solid lines) across four FFN experts, where the router independently routes each token. The switch FFN layer returns the output of the selected FFN multiplied by the router gate value (dotted-line)." ;
    skos:prefLabel "Switch FFN" .

:SwitchTransformer a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2101.03961v3> ;
    skos:definition "**Switch Transformer** is a sparsely-activated expert [Transformer](https://paperswithcode.com/methods/category/transformers) model that aims to simplify and improve over Mixture of Experts. Through distillation of sparse pre-trained and specialized fine-tuned models into small dense models, it reduces the model size by up to 99% while preserving 30% of the quality gains of the large sparse teacher. It also uses selective precision training that enables training with lower bfloat16 precision, as well as an initialization scheme that allows for scaling to a larger number of experts, and also increased regularization that improves sparse model fine-tuning and multi-task training." ;
    skos:prefLabel "Switch Transformer" .

:SwitchableNormalization a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1806.10779v5> ;
    rdfs:seeAlso <https://github.com/switchablenorms/Switchable-Normalization/blob/5472286952ba5519a7d3229d9ad899a5c1b2e5b1/devkit/ops/switchable_norm.py#L60> ;
    skos:definition "**Switchable Normalization** combines three types of statistics estimated channel-wise, layer-wise, and minibatch-wise by using [instance normalization](https://paperswithcode.com/method/instance-normalization), [layer normalization](https://paperswithcode.com/method/layer-normalization), and [batch normalization](https://paperswithcode.com/method/batch-normalization) respectively. [Switchable Normalization](https://paperswithcode.com/method/switchable-normalization) switches among them by learning their importance weights." ;
    skos:prefLabel "Switchable Normalization" .

:Sym-NCO a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2205.13209v2> ;
    skos:definition "" ;
    skos:prefLabel "Sym-NCO" .

:SymbolicDeepLearning a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2006.11287v2> ;
    rdfs:seeAlso <https://github.com/MilesCranmer/symbolic_deep_learning> ;
    skos:definition """This is a general approach to convert a neural network into an analytic equation. The technique works as follows:\r
\r
1. Encourage sparse latent representations\r
2. Apply symbolic regression to approximate the transformations between in/latent/out layers\r
3. Compose the symbolic expressions.\r
\r
In the [paper](https://arxiv.org/abs/2006.11287), we show that we find the correct known equations, including force laws and Hamiltonians, can be extracted from the neural network. We then apply our method to a non-trivial cosmology example-a detailed dark matter simulation-and discover a new analytic formula which can predict the concentration of dark matter from the mass distribution of nearby cosmic structures. The symbolic expressions extracted from the GNN using our technique also generalized to out-of-distribution data better than the GNN itself. Our approach offers alternative directions for interpreting neural networks and discovering novel physical principles from the representations they learn.""" ;
    skos:prefLabel "Symbolic Deep Learning" .

:Symbolicrulelearning a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2012.05750v1> ;
    skos:definition "Symbolic rule learning methods find regularities in data that can be expressed in the form of 'if-then' rules based on symbolic representations of the data." ;
    skos:prefLabel "Symbolic rule learning" .

:SymmNet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2002.08681v2> ;
    skos:altLabel "Domain-Symmetric Network" ;
    skos:definition "**Domain-Symmetric Network**, or **SymmNet**, is an algorithm for unsupervised multi-class domain adaptation. It features an adversarial strategy of domain confusion and discrimination." ;
    skos:prefLabel "SymmNet" .

:SynaNN a skos:Concept ;
    dcterms:source <https://openreview.net/forum?id=ryGpEiAcFQ> ;
    skos:altLabel "Synaptic Neural Network" ;
    skos:definition "A Synaptic Neural Network (SynaNN) consists of synapses and neurons. Inspired by the synapse research of neuroscience, we built a synapse model with a nonlinear and log-concave synapse function of excitatory and inhibitory probabilities of channels." ;
    skos:prefLabel "SynaNN" .

:SyncBN a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1803.08904v1> ;
    rdfs:seeAlso <https://github.com/pytorch/pytorch/blob/881c1adfcd916b6cd5de91bc343eb86aff88cc80/torch/nn/modules/batchnorm.py#L353> ;
    skos:altLabel "Synchronized Batch Normalization" ;
    skos:definition "**Synchronized Batch Normalization (SyncBN)** is a type of [batch normalization](https://paperswithcode.com/method/batch-normalization) used for multi-GPU training. Standard batch normalization only normalizes the data within each device (GPU). SyncBN normalizes the input within the whole mini-batch." ;
    skos:prefLabel "SyncBN" .

:SyntaxHeatParseTree a skos:Concept ;
    dcterms:source <https://aclanthology.org/2020.emnlp-main.18> ;
    rdfs:seeAlso <https://github.com/ART-Group-it/KERMIT> ;
    skos:definition "Syntax Heat Parse Tree are heatmaps over parse trees, similar to [\"heat trees\"](https://doi.org/10.1371/journal.pcbi.1005404) in biology." ;
    skos:prefLabel "Syntax Heat Parse Tree" .

:Synthesizer a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2005.00743v3> ;
    skos:definition """The  **Synthesizer** is a model that learns synthetic attention weights without token-token interactions. Unlike [Transformers](https://paperswithcode.com/method/transformer), the model eschews dot product self-attention but also content-based self-attention altogether. Synthesizer learns to synthesize the self-alignment matrix instead of manually computing pairwise dot products. It is transformation-based, only relies on simple feed-forward layers, and completely dispenses with dot products and explicit token-token interactions. \r
\r
This new module employed by the Synthesizer is called "Synthetic Attention": a new way of learning to attend without explicitly attending (i.e., without dot product attention or [content-based attention](https://paperswithcode.com/method/content-based-attention)). Instead, Synthesizer generate the alignment matrix independent of token-token dependencies.""" ;
    skos:prefLabel "Synthesizer" .

:T-D a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1801.10198v1> ;
    skos:altLabel "Transformer Decoder" ;
    skos:definition """[Transformer](https://paperswithcode.com/method/transformer)-Decoder is a modification to Transformer-Encoder-Decoder for long sequences that drops the encoder\r
module, combines the input and output sequences into a single ”sentence” and is trained as a standard language model. It is used in [GPT](https://paperswithcode.com/method/gpt) and later revisions.""" ;
    skos:prefLabel "T-D" .

:T-Fixup a skos:Concept ;
    dcterms:source <https://proceedings.icml.cc/static/paper_files/icml/2020/5691-Paper.pdf> ;
    skos:definition """**T-Fixup** is an [initialization](https://paperswithcode.com/methods/category/initialization) method for [Transformers](https://paperswithcode.com/methods/category/transformers) that aims to remove the need for [layer normalization](https://paperswithcode.com/method/layer-normalization) and [warmup](https://paperswithcode.com/method/linear-warmup). The initialization procedure is as follows:\r
\r
- Apply [Xavier initialization](https://paperswithcode.com/method/xavier-initialization) for all parameters excluding input embeddings. Use Gaussian initialization $\\mathcal{N}\\left(0, d^{-\\frac{1}{2}}\\right)$ for input embeddings where $d$ is the embedding dimension.\r
- Scale $\\mathbf{v}\\_{d}$ and $\\mathbf{w}\\_{d}$ matrices in each decoder [attention block](https://paperswithcode.com/method/multi-head-attention), weight matrices in each decoder [MLP block](https://paperswithcode.com/method/position-wise-feed-forward-layer) and input embeddings $\\mathbf{x}$ and $\\mathbf{y}$ in encoder and decoder by $(9 N)^{-\\frac{1}{4}}$\r
- Scale $\\mathbf{v}\\_{e}$ and $\\mathbf{w}\\_{e}$ matrices in each encoder [attention block](https://paperswithcode.com/method/multi-head-attention) and weight matrices in each encoder [MLP block](https://paperswithcode.com/method/position-wise-feed-forward-layer) by $0.67 N^{-\\frac{1}{4}}$""" ;
    skos:prefLabel "T-Fixup" .

:T2T-ViT a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2101.11986v3> ;
    rdfs:seeAlso <https://github.com/yitu-opensource/T2T-ViT/blob/main/models/t2t_vit.py> ;
    skos:altLabel "Tokens-To-Token Vision Transformer" ;
    skos:definition "**T2T-ViT** (Tokens-To-Token Vision Transformer) is a type of [Vision Transformer](https://paperswithcode.com/method/vision-transformer) which incorporates 1) a layerwise Tokens-to-Token (T2T) transformation to progressively structurize the image to tokens by recursively aggregating neighboring Tokens into one Token (Tokens-to-Token), such that local structure represented by surrounding tokens can be modeled and tokens length can be reduced; 2) an efficient backbone with a deep-narrow structure for vision [transformer](https://paperswithcode.com/method/transformer) motivated by CNN architecture design after empirical study." ;
    skos:prefLabel "T2T-ViT" .

:T5 a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1910.10683v3> ;
    skos:definition """**T5**, or **Text-to-Text Transfer Transformer**, is a [Transformer](https://paperswithcode.com/method/transformer) based architecture that uses a text-to-text approach. Every task – including translation, question answering, and classification – is cast as feeding the model text as input and training it to generate some target text. This allows for the use of the same model, loss function, hyperparameters, etc. across our diverse set of tasks. The changes compared to [BERT](https://paperswithcode.com/method/bert) include:\r
\r
- adding a *causal* decoder to the bidirectional architecture.\r
- replacing the fill-in-the-blank cloze task with a mix of alternative pre-training tasks.""" ;
    skos:prefLabel "T5" .

:TABBIE a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2105.02584v1> ;
    skos:definition "**TABBIE** is a pretraining objective (*corrupt cell detection*) that learns exclusively from tabular data. Unlike other approaches, TABBIE provides embeddings of all table substructures (cells, rows, and columns). TABBIE can be seen as a table embedding model trained to detect corrupted cells, inspired by the [ELECTRA](https://www.paperswithcode.com/method/electra) objective function." ;
    skos:prefLabel "TABBIE" .

:TAM a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2005.06803v3> ;
    skos:altLabel "Temporal Adaptive Module" ;
    skos:definition """TAM is designed to capture complex temporal relationships both  efficiently and  flexibly,\r
It adopts an adaptive kernel instead of self-attention to capture  global contextual information, with lower time complexity \r
than GLTR.\r
\r
TAM has two branches, a local branch and a global branch. Given the input feature map $X\\in \\mathbb{R}^{C\\times T\\times H\\times W}$,  global spatial average pooling $\\text{GAP}$ is first applied to the feature map to ensure TAM has a low computational cost. Then the local branch in TAM employs several 1D convolutions with ReLU nonlinearity across the temporal domain to produce location-sensitive importance maps for enhancing frame-wise features.\r
The local branch can be written as\r
\\begin{align}\r
    s &= \\sigma(\\text{Conv1D}(\\delta(\\text{Conv1D}(\\text{GAP}(X)))))\r
\\end{align}\r
\\begin{align}\r
    X^1 &= s X\r
\\end{align}\r
Unlike the local branch, the global branch is location invariant and focuses on generating a channel-wise adaptive kernel based on global temporal information in each channel. For the $c$-th channel, the  kernel can be written as\r
\r
\\begin{align}\r
    \\Theta_c = \\text{Softmax}(\\text{FC}_2(\\delta(\\text{FC}_1(\\text{GAP}(X)_c)))) \r
\\end{align}\r
\r
where $\\Theta_c \\in \\mathbb{R}^{K}$ and $K$ is the adaptive kernel size. Finally, TAM  convolves the adaptive kernel $\\Theta$ with $ X_\\text{out}^1$:\r
\\begin{align}\r
    Y = \\Theta \\otimes  X^1\r
\\end{align}\r
\r
With the help of the local branch and global branch,\r
TAM can capture the complex temporal structures in video and \r
enhance per-frame features at low computational cost.\r
Due to its flexibility and lightweight design,\r
TAM can be added to any existing 2D CNNs.""" ;
    skos:prefLabel "TAM" .

:TAPAS a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2004.02349v2> ;
    skos:definition "**TAPAS** is a weakly supervised question answering model that reasons over tables without generating logical forms. TAPAS predicts a minimal program by selecting a subset of the table cells and a possible aggregation operation to be executed on top of them. Consequently, TAPAS can learn operations from natural language, without the need to specify them in some formalism. This is implemented by extending [BERT](https://paperswithcode.com/method/bert)’s architecture with additional embeddings that capture tabular structure, and with two classification layers for selecting cells and predicting a corresponding aggregation operator." ;
    skos:prefLabel "TAPAS" .

:TAPEX a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2107.07653v3> ;
    rdfs:seeAlso <https://github.com/microsoft/Table-Pretraining> ;
    skos:altLabel "Table Pre-training via Execution" ;
    skos:definition "TAPEX is a conceptually simple and empirically powerful pre-training approach to empower existing models with table reasoning skills. TAPEX realizes table pre-training by learning a neural SQL executor over a synthetic corpus, which is obtained by automatically synthesising executable SQL queries." ;
    skos:prefLabel "TAPEX" .

:TD-Gammon a skos:Concept ;
    skos:definition """**TD-Gammon** is a game-learning architecture for playing backgammon. It involves the use of a $TD\\left(\\lambda\\right)$ learning algorithm and a feedforward neural network.\r
\r
Credit: [Temporal Difference Learning and\r
TD-Gammon](https://cling.csd.uwo.ca/cs346a/extra/tdgammon.pdf)""" ;
    skos:prefLabel "TD-Gammon" .

:TD-VAE a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1806.03107v3> ;
    skos:definition "**TD-VAE**, or **Temporal Difference VAE**, is a generative sequence model that learns representations containing explicit beliefs about states several steps into the future, and that can be rolled out directly without single-step transitions. TD-VAE is trained on pairs of temporally separated time points, using an analogue of [temporal difference learning](https://paperswithcode.com/method/td-lambda) used in reinforcement learning." ;
    skos:prefLabel "TD-VAE" .

:TD3 a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1802.09477v3> ;
    skos:altLabel "Twin Delayed Deep Deterministic" ;
    skos:definition "**TD3** builds on the [DDPG](https://paperswithcode.com/method/ddpg) algorithm for reinforcement learning, with a couple of modifications aimed at tackling overestimation bias with the value function. In particular, it utilises [clipped double Q-learning](https://paperswithcode.com/method/clipped-double-q-learning), delayed update of target and policy networks, and [target policy smoothing](https://paperswithcode.com/method/target-policy-smoothing) (which is similar to a [SARSA](https://paperswithcode.com/method/sarsa) based update; a safer update, as they provide higher value to actions resistant to perturbations)." ;
    skos:prefLabel "TD3" .

:TDLambda a skos:Concept ;
    skos:definition """**TD_INLINE_MATH_1** is a generalisation of **TD_INLINE_MATH_2** reinforcement learning algorithms, but it employs an [eligibility trace](https://paperswithcode.com/method/eligibility-trace) $\\lambda$ and $\\lambda$-weighted returns. The eligibility trace vector is initialized to zero at the beginning of the episode, and it is incremented on each time step by the value gradient, and then fades away by $\\gamma\\lambda$:\r
\r
$$ \\textbf{z}\\_{-1} = \\mathbf{0} $$\r
$$ \\textbf{z}\\_{t} = \\gamma\\lambda\\textbf{z}\\_{t-1} + \\nabla\\hat{v}\\left(S\\_{t}, \\mathbf{w}\\_{t}\\right), 0 \\leq t \\leq T$$\r
\r
The eligibility trace keeps track of which components of the weight vector contribute to recent state valuations. Here $\\nabla\\hat{v}\\left(S\\_{t}, \\mathbf{w}\\_{t}\\right)$ is the feature vector.\r
\r
The TD error for state-value prediction is:\r
\r
$$ \\delta\\_{t} = R\\_{t+1} + \\gamma\\hat{v}\\left\\(S\\_{t+1}, \\mathbf{w}\\_{t}\\right) - \\hat{v}\\left(S\\_{t}, \\mathbf{w}\\_{t}\\right) $$\r
\r
In **TD_INLINE_MATH_1**, the weight vector is updated on each step proportional to the scalar TD error and the vector eligibility trace:\r
\r
$$ \\mathbf{w}\\_{t+1} = \\mathbf{w}\\_{t} + \\alpha\\delta\\mathbf{z}\\_{t}  $$\r
\r
Source: Sutton and Barto, Reinforcement Learning, 2nd Edition""" ;
    skos:prefLabel "TD Lambda" .

:TDN a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2012.10071v2> ;
    skos:altLabel "Temporaral Difference Network" ;
    skos:definition "**TDN**, or **Temporaral Difference Network**, is an action recognition model that aims to capture multi-scale temporal information. To fully capture temporal information over the entire video, the TDN is established with a two-level difference modeling paradigm. Specifically, for local motion modeling, temporal difference over consecutive frames is used to supply 2D CNNs with finer motion pattern, while for global motion modeling, temporal difference across segments is incorporated to capture long-range structure for motion feature excitation." ;
    skos:prefLabel "TDN" .

:TE2Rules a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2206.14359v4> ;
    skos:altLabel "Tree Ensemble to Rules" ;
    skos:definition "A method to convert a Tree Ensemble model into a Rule list. This makes the AI model more transparent." ;
    skos:prefLabel "TE2Rules" .

:TFGW a skos:Concept ;
    skos:altLabel "Template based Graph Neural Network with Optimal Transport Distances" ;
    skos:definition "" ;
    skos:prefLabel "TFGW" .

:TGAN a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1611.06624v3> ;
    rdfs:seeAlso <https://github.com/pfnet-research/tgan> ;
    skos:definition """**TGAN** is a type of generative adversarial network that is capable of learning representation from an unlabeled video dataset and producing a new video. The generator consists of two sub networks\r
called a temporal generator and an image generator. Specifically, the temporal generator first yields a set of latent variables, each of which corresponds to a latent variable for the image generator. Then, the image generator transforms these latent variables into a video which has the same number of frames as the variables. The model comprised of the temporal and image generators can not only enable to efficiently capture the time series, but also be easily extended to frame interpolation. The authors opt for a [WGAN](https://paperswithcode.com/method/wgan) as the basic [GAN](https://paperswithcode.com/method/gan) structure and objective, but use [singular value clipping](https://paperswithcode.com/method/singular-value-clipping) to enforce the Lipschitz constraint.""" ;
    skos:prefLabel "TGAN" .

:TGN a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2006.10637v3> ;
    skos:altLabel "Temporal Graph Network" ;
    skos:definition "**Temporal Graph Network**, or **TGN**, is a framework for deep learning on dynamic graphs represented as sequences of timed events. The memory (state) of the model at time $t$ consists of a vector $\\mathbf{s}_i(t)$ for each node $i$ the model has seen so far. The memory of a node is updated after an event (e.g. interaction with another node or node-wise change), and its purpose is to represent the node's history in a compressed format. Thanks to this specific module, TGNs have the capability to memorize long term dependencies for each node in the graph. When a new node is encountered, its memory is initialized as the zero vector, and it is then updated for each event involving the node, even after the model has finished training." ;
    skos:prefLabel "TGN" .

:TILDEv2 a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2108.08513v2> ;
    skos:definition """**TILDEv2** is a [BERT](https://paperswithcode.com/method/bert)-based re-ranking method that stems from [TILDE](https://dl.acm.org/doi/abs/10.1145/3404835.3462922) but that addresses its limitations. It relies on contextualized exact term matching with expanded passages. This requires to only store in the index the score of tokens that appear in the expanded passages (rather than all the vocabulary), thus producing indexes that are 99% smaller than those of the original.\r
\r
Specifically, TILDE is modified in the following aspects:\r
\r
- **Exact Term Matching**. The query likelihood matching originally employed in TILDE, expands passages into the BERT vocabulary size, resulting in large indexes. To overcome this issue, estimating relevance scores is achieved with contextualized exact term matching. This allows the model to index tokens only present in the passage, thus reducing the index size. In addition to this, we replace the query likelihood loss function, with the Noise contrastive estimation (NCE) loss that allows to better leverage negative training samples. \r
 \r
- **Passage Expansion**. To overcome the vocabulary mismatch problem that affects exact term matching methods, passage expansion is used to expand the original passage collection. Passages in the collection are expanded using deep LMs with a limited number of tokens. This requires TILDEv2 to only index a few extra tokens in addition to those in the original passages.""" ;
    skos:prefLabel "TILDEv2" .

:TLC a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2112.04491v4> ;
    skos:altLabel "Test-time Local Converter" ;
    skos:definition "TLC convert the global operation to a local one so that it extract representations based on local spatial region of features as in training phase." ;
    skos:prefLabel "TLC" .

:TNT a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2103.00112v3> ;
    skos:altLabel "Transformer in Transformer" ;
    skos:definition """[Transformer](https://paperswithcode.com/method/transformer) is a type of self-attention-based neural networks originally applied for NLP tasks. Recently, pure transformer-based models are proposed to solve computer vision problems. These visual transformers usually view an image as a sequence of patches while they ignore the intrinsic structure information inside each patch. In this paper, we propose a novel Transformer-iN-Transformer (TNT) model for modeling both patch-level and pixel-level representation. In each TNT block, an outer transformer block is utilized to process patch embeddings, and an inner transformer block extracts local features from pixel embeddings. The pixel-level feature is projected to the space of patch embedding by a linear transformation layer and then added into the patch. By stacking the TNT blocks, we build the TNT model for image recognition.\r
\r
Image source: [Han et al.](https://arxiv.org/pdf/2103.00112v1.pdf)""" ;
    skos:prefLabel "TNT" .

:TPN a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2004.03548v2> ;
    skos:altLabel "Temporal Pyramid Network" ;
    skos:definition "**Temporal Pyramid Network**, or **TPN**, is a pyramid level module for action recognition at the feature-level, which can be flexibly integrated into 2D or 3D backbone networks in a plug-and-play manner. The source of features and the fusion of features form a feature hierarchy for the backbone so that it can capture action instances at various tempos. In the TPN, a Backbone Network is used to extract multiple level features, a Spatial Semantic Modulation spatially downsamples features to align semantics, a Temporal Rate Modulation temporally downsamples features to adjust relative tempo among levels, Information Flow aggregates features in various directions to enhance and enrich level-wise representations and Final Prediction rescales and concatenates all levels of pyramid along channel dimension." ;
    skos:prefLabel "TPN" .

:TRPO a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1502.05477v5> ;
    skos:altLabel "Trust Region Policy Optimization" ;
    skos:definition """**Trust Region Policy Optimization**, or **TRPO**, is a policy gradient method in reinforcement learning that avoids parameter updates that change the policy too much with a KL divergence constraint on the size of the policy update at each iteration.\r
\r
Take the case of off-policy reinforcement learning, where the policy $\\beta$ for collecting trajectories on rollout workers is different from the policy $\\pi$ to optimize for. The objective function in an off-policy model measures the total advantage over the state visitation distribution and actions, while the mismatch between the training data distribution and the true policy state distribution is compensated with an importance sampling estimator:\r
\r
$$ J\\left(\\theta\\right) = \\sum\\_{s\\in{S}}p^{\\pi\\_{\\theta\\_{old}}}\\sum\\_{a\\in\\mathcal{A}}\\left(\\pi\\_{\\theta}\\left(a\\mid{s}\\right)\\hat{A}\\_{\\theta\\_{old}}\\left(s, a\\right)\\right) $$\r
\r
$$ J\\left(\\theta\\right) = \\sum\\_{s\\in{S}}p^{\\pi\\_{\\theta\\_{old}}}\\sum\\_{a\\in\\mathcal{A}}\\left(\\beta\\left(a\\mid{s}\\right)\\frac{\\pi\\_{\\theta}\\left(a\\mid{s}\\right)}{\\beta\\left(a\\mid{s}\\right)}\\hat{A}\\_{\\theta\\_{old}}\\left(s, a\\right)\\right) $$\r
\r
$$ J\\left(\\theta\\right) = \\mathbb{E}\\_{s\\sim{p}^{\\pi\\_{\\theta\\_{old}}}, a\\sim{\\beta}} \\left(\\frac{\\pi\\_{\\theta}\\left(a\\mid{s}\\right)}{\\beta\\left(a\\mid{s}\\right)}\\hat{A}\\_{\\theta\\_{old}}\\left(s, a\\right)\\right)$$\r
\r
When training on policy, theoretically the policy for collecting data is same as the policy that we want to optimize. However, when rollout workers and optimizers are running in parallel asynchronously, the behavior policy can get stale. TRPO considers this subtle difference: It labels the behavior policy as $\\pi\\_{\\theta\\_{old}}\\left(a\\mid{s}\\right)$ and thus the objective function becomes:\r
\r
$$ J\\left(\\theta\\right) = \\mathbb{E}\\_{s\\sim{p}^{\\pi\\_{\\theta\\_{old}}}, a\\sim{\\pi\\_{\\theta\\_{old}}}} \\left(\\frac{\\pi\\_{\\theta}\\left(a\\mid{s}\\right)}{\\pi\\_{\\theta\\_{old}}\\left(a\\mid{s}\\right)}\\hat{A}\\_{\\theta\\_{old}}\\left(s, a\\right)\\right)$$\r
\r
TRPO aims to maximize the objective function $J\\left(\\theta\\right)$ subject to a trust region constraint which enforces the distance between old and new policies measured by KL-divergence to be small enough, within a parameter $\\delta$:\r
\r
$$ \\mathbb{E}\\_{s\\sim{p}^{\\pi\\_{\\theta\\_{old}}}} \\left[D\\_{KL}\\left(\\pi\\_{\\theta\\_{old}}\\left(.\\mid{s}\\right)\\mid\\mid\\pi\\_{\\theta}\\left(.\\mid{s}\\right)\\right)\\right] \\leq \\delta$$""" ;
    skos:prefLabel "TRPO" .

:TResNet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2003.13630v3> ;
    rdfs:seeAlso <https://github.com/mrT23/TResNet/blob/c57e3d47d2d2441aaaf5daf532a6331925ca07a2/src/models/tresnet/tresnet.py#L114> ;
    skos:definition "A **TResNet** is a variant on a [ResNet](https://paperswithcode.com/method/resnet) that aim to boost accuracy while maintaining GPU training and inference efficiency.  They contain several design tricks including a SpaceToDepth stem, [Anti-Alias downsampling](https://paperswithcode.com/method/anti-alias-downsampling), In-Place Activated BatchNorm, Blocks selection and squeeze-and-excitation layers." ;
    skos:prefLabel "TResNet" .

:TS a skos:Concept ;
    rdfs:seeAlso <https://github.com/mchelali/TemporalStability> ;
    skos:altLabel "Spatio-temporal stability analysis" ;
    skos:definition "Spatio-temporal features extraction that measure the stabilty. The proposed method is based on a compression algorithm named Run Length Encoding. The workflow of the method is presented bellow." ;
    skos:prefLabel "TS" .

:TSDAE a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2104.06979v3> ;
    skos:definition """**TSDAE** is an unsupervised sentence embedding method. During training, TSDAE encodes corrupted sentences into fixed-sized vectors and requires the decoder to reconstruct the original sentences from this sentence embedding. For good reconstruction quality, the semantics must be captured well in the sentence embedding from the encoder. Later, at inference, we only use the encoder for creating sentence embeddings.\r
\r
The model architecture of TSDAE is a modified [encoder-decoder Transformer](https://paperswithcode.com/methods/category/autoencoding-transformers) where the key and value of the cross-attention are both confined to the sentence embedding only. Formally, the formulation of the modified cross-attention is:\r
\r
$$\r
H^{(k)}=\\text { Attention }\\left(H^{(k-1)},\\left[s^{T}\\right],\\left[s^{T}\\right]\\right)\r
$$\r
\r
$$\r
\\operatorname{Attention}(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d}}\\right) V\r
$$\r
\r
where $H^{(k)} \\in \\mathbb{R}^{t \\times d}$ is the decoder hidden states within $t$ decoding steps at the $k$-th layer, $d$ is the size of the sentence embedding, $\\left[s^{T}\\right] \\in \\mathbb{R}^{1 \\times d}$ is a one-row matrix including the sentence embedding vector and $Q, K$ and $V$ are the query, key and value, respectively. By exploring different configurations on the STS benchmark dataset, the authors discover that the best combination is: (1) adopting deletion as the input noise and setting the deletion ratio to $0.6,(2)$ using the output of the [CLS] token as fixed-sized sentence representation (3) tying the encoder and decoder parameters during training.""" ;
    skos:prefLabel "TSDAE" .

:TSRUc a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2003.04035v3> ;
    skos:definition """**TSRUc**, or **Transformation-based Spatial Recurrent Unit c**, is a modification of a [ConvGRU](https://paperswithcode.com/method/cgru) used in the [TriVD-GAN](https://paperswithcode.com/method/trivd-gan) architecture for video generation.\r
\r
Instead of computing the reset gate $r$ and resetting $h\\_{t−1}$, the TSRUc computes the parameters of a transformation $\\theta$, which we use to warp $h\\_{t−1}$. The rest of our model is unchanged (with $\\hat{h}\\_{t-1}$ playing the role of $h'\\_{t}$ in $c$’s update equation from ConvGRU. The TSRUc module is described by the following equations:\r
\r
$$ \\theta\\_{h,x} = f\\left(h\\_{t−1}, x\\_{t}\\right) $$\r
\r
$$ \\hat{h}\\_{t-1} = w\\left(h\\_{t-1}; \\theta\\_{h, x}\\right) $$\r
\r
$$ c = \\rho\\left(W\\_{c} \\star\\_{n}\\left[\\hat{h}\\_{t-1};x\\_{t}\\right] + b\\_{c} \\right) $$\r
\r
$$ u = \\sigma\\left(W\\_{u} \\star\\_{n}\\left[h\\_{t-1};x\\_{t}\\right] + b\\_{u} \\right) $$\r
\r
$$ h\\_{t} = u \\odot h\\_{t-1} + \\left(1-u\\right) \\odot c $$\r
\r
In these equations $\\sigma$ and $\\rho$ are the elementwise sigmoid and [ReLU](https://paperswithcode.com/method/relu) functions respectively and the $\\star\\_{n}$ represents a [convolution](https://paperswithcode.com/method/convolution) with a kernel of size $n \\times n$. Brackets are used to represent a feature concatenation.""" ;
    skos:prefLabel "TSRUc" .

:TSRUp a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2003.04035v3> ;
    skos:definition """**TSRUp**, or **Transformation-based Spatial Recurrent Unit p**, is a modification of a [ConvGRU](https://paperswithcode.com/method/cgru) used in the [TriVD-GAN](https://paperswithcode.com/method/trivd-gan) architecture for video generation.\r
\r
It largely follows [TSRUc](https://paperswithcode.com/method/tsruc), but computes $\\theta$, $u$ and $c$ in parallel given $x\\_{t}$ and $h\\_{t−1}$, yielding the following replacement for the $c$ update equation:\r
\r
$$ c = \\rho\\left(W\\_{c} \\star\\_{n}\\left[h\\_{t-1}; x\\_{t}\\right] + b\\_{c} \\right) $$\r
\r
In these equations $\\sigma$ and $\\rho$ are the elementwise sigmoid and [ReLU](https://paperswithcode.com/method/relu) functions respectively and the $\\star\\_{n}$ represents a [convolution](https://paperswithcode.com/method/convolution) with a kernel of size $n \\times n$. Brackets are used to represent a feature concatenation.""" ;
    skos:prefLabel "TSRUp" .

:TSRUs a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2003.04035v3> ;
    skos:definition """**TSRUs**, or **Transformation-based Spatial Recurrent Unit p**, is a modification of a [ConvGRU](https://paperswithcode.com/method/cgru) used in the [TriVD-GAN](https://paperswithcode.com/method/trivd-gan) architecture for video generation.\r
\r
It largely follows [TSRUc](https://paperswithcode.com/method/tsruc), but computes each intermediate output in a fully sequential manner: like in TSRUc, $c$ is given access to $\\hat{h}\\_{t-1}$, but additionally, $u$ is given access to both outputs $\\hat{h}\\_{t-1}$ and $c$, so as to make an informed decision prior to mixing. This yields the following replacement for $u$:\r
\r
$$ u = \\sigma\\left(W\\_{u} \\star\\_{n}\\left[\\hat{h}\\_{t-1};c\\right] + b\\_{u} \\right) $$\r
\r
In these equations $\\sigma$ and $\\rho$ are the elementwise sigmoid and [ReLU](https://paperswithcode.com/method/relu) functions respectively and the $\\star\\_{n}$ represents a [convolution](https://paperswithcode.com/method/convolution) with a kernel of size $n \\times n$. Brackets are used to represent a feature concatenation.""" ;
    skos:prefLabel "TSRUs" .

:TTUR a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1706.08500v6> ;
    rdfs:seeAlso <https://github.com/yccyenchicheng/pytorch-WGAN-GP-TTUR-CelebA/blob/70707ecd5772249fbcdd3935dc1c937bbbc112e1/main.py#L42> ;
    skos:altLabel "Two Time-scale Update Rule" ;
    skos:definition "The **Two Time-scale Update Rule (TTUR)** is an update rule for generative adversarial networks trained with stochastic gradient descent. TTUR has an individual learning rate for both the discriminator and the generator. The main premise is that the discriminator converges to a local minimum when the generator is fixed. If the generator changes slowly enough, then the discriminator still converges, since the generator perturbations are small. Besides ensuring convergence, the performance may also improve since the discriminator must first learn new patterns before they are transferred to the generator. In contrast, a generator which is overly fast, drives the discriminator steadily into new regions without capturing its gathered information." ;
    skos:prefLabel "TTUR" .

:TUM a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1811.04533v3> ;
    rdfs:seeAlso <https://github.com/qijiezhao/M2Det/blob/ade4f3d12979800c367bf1e46d2e316e73a87514/layers/nn_utils.py#L26> ;
    skos:altLabel "Thinned U-shape Module" ;
    skos:definition """**Thinned U-shape Module**, or **TUM**, is a feature extraction block used for object detection models. It was introduced as part of the [M2Det](https://paperswithcode.com/method/m2det) architecture. Different from [FPN](https://paperswithcode.com/method/fpn) and [RetinaNet](https://paperswithcode.com/method/retinanet), TUM adopts a thinner U-shape structure as illustrated in the Figure to the right. The encoder is a series of 3x3 [convolution](https://paperswithcode.com/method/convolution) layers with stride 2. And the decoder takes the outputs of these layers as its reference set of feature maps, while the original FPN chooses the output of the last layer of each stage in [ResNet](https://paperswithcode.com/method/resnet) backbone. \r
\r
In addition, with TUM, we add [1x1 convolution](https://paperswithcode.com/method/1x1-convolution) layers after the upsample and element-wise sum operation at the decoder branch to enhance learning ability and keep smoothness for the features. In the context of M2Det, all of the outputs in the decoder of each TUM form the multi-scale features of the current level. As a whole, the outputs of stacked TUMs form the multi-level multi-scale features, while the front TUM mainly provides shallow-level features, the middle TUM provides medium-level features, and the back TUM provides deep-level features.""" ;
    skos:prefLabel "TUM" .

:TURL a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2006.14806v2> ;
    skos:altLabel "TURL: Table Understanding through Representation Learning" ;
    skos:definition """Relational tables on the Web store a vast amount of knowledge. Owing to the wealth of such tables, there has been tremendous progress on a variety of tasks in the area of table understanding. However, existing work generally relies on heavily-engineered task- specific features and model architectures. In this paper, we present TURL, a novel framework that introduces the pre-training/fine- tuning paradigm to relational Web tables. During pre-training, our framework learns deep contextualized representations on relational tables in an unsupervised manner. Its universal model design with pre-trained representations can be applied to a wide range of tasks with minimal task-specific fine-tuning.\r
Specifically, we propose a structure-aware Transformer encoder to model the row-column structure of relational tables, and present a new Masked Entity Recovery (MER) objective for pre-training to capture the semantics and knowledge in large-scale unlabeled data. We systematically evaluate TURL with a benchmark consisting of 6 different tasks for table understanding (e.g., relation extraction, cell filling). We show that TURL generalizes well to all tasks and substantially outperforms existing methods in almost all instances.""" ;
    skos:prefLabel "TURL" .

:TWEC a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1906.02376v1> ;
    skos:altLabel "Temporal Word Embeddings with a Compass" ;
    skos:definition "TWEC is a method to generate temporal word embeddings: this method is efficient and it is based on a simple heuristic: we train an atemporal word embedding, the compass and we use this embedding to freeze one of the layers of the CBOW architecture. The frozen architecture is then used to train time-specific slices that are all comparable after training." ;
    skos:prefLabel "TWEC" .

:TaBERT a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2005.08314v1> ;
    skos:definition """**TaBERT** is a pretrained language model (LM) that jointly learns representations for natural language sentences and (semi-)structured tables. TaBERT is trained on a large corpus of 26 million tables and their English contexts. \r
\r
In summary, TaBERT's process for learning representations for NL sentences is as follows: Given an utterance $u$ and a table $T$, TaBERT first creates a content snapshot of $T$. This snapshot consists of sampled rows that summarize the information in $T$ most relevant to the input utterance. The model then linearizes each row in the snapshot, concatenates each linearized row with the utterance, and uses the concatenated string as input to a Transformer model, which outputs row-wise encoding vectors of utterance tokens and cells. The encodings for all the rows in the snapshot are fed into a series of vertical self-attention layers, where a cell representation (or an utterance token representation) is computed by attending to vertically-aligned vectors of the same column (or the same NL token). Finally, representations for each utterance token and column are generated from a pooling layer.""" ;
    skos:prefLabel "TaBERT" .

:TaLKConvolution a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2002.03184v2> ;
    skos:altLabel "Time-aware Large Kernel Convolution" ;
    skos:definition "A **Time-aware Large Kernel (TaLK) convolution** is a type of temporal [convolution](https://paperswithcode.com/method/convolution) that learns the kernel size of a summation kernel for each time-step instead of learning the kernel weights as in a typical convolution operation. For each time-step, a function is responsible for predicting the appropriate size of neighbor representations to use in the form of left and right offsets relative to the time-step." ;
    skos:prefLabel "TaLK Convolution" .

:TabNN a skos:Concept ;
    dcterms:source <https://openreview.net/forum?id=r1eJssCqY7> ;
    skos:definition "TabNN is a universal neural network solution to derive effective NN architectures for tabular data in all kinds of tasks automatically. Specifically, the design of TabNN follows two principles: to explicitly leverage expressive feature combinations and to reduce model complexity. Since GBDT has empirically proven its strength in modeling tabular data, GBDT is used to power the implementation of TabNN." ;
    skos:prefLabel "TabNN" .

:TabNet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1908.07442v5> ;
    skos:definition """**TabNet** is a deep tabular data learning architecture that uses sequential attention to choose which features to reason from at each decision step.\r
\r
The TabNet encoder is composed of a feature transformer, an attentive transformer and feature masking. A split block\r
divides the processed representation to be used by the attentive transformer of the subsequent step as well as for the overall output. For each step, the feature selection mask provides interpretable information about the model’s functionality, and the masks can be aggregated to obtain global feature important attribution. The TabNet decoder is composed of a feature transformer block at each step. \r
\r
In the feature transformer block, a 4-layer network is used, where 2 are shared across all decision steps and 2 are decision step-dependent. Each layer is composed of a fully-connected (FC) layer, BN and GLU nonlinearity. An attentive transformer block example – a single layer mapping is modulated with a prior scale information which aggregates how much each feature has been used before the current decision step. sparsemax is used for normalization of the coefficients, resulting in sparse selection of the salient features.""" ;
    skos:prefLabel "TabNet" .

:TabTransformer a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2012.06678v1> ;
    skos:definition """**TabTransformer** is a deep tabular data modeling architecture for supervised and semi-supervised learning. The TabTransformer is built upon self-attention based Transformers. The Transformer layers transform the embeddings of categorical features into robust contextual embeddings to achieve higher prediction accuracy. \r
\r
As an overview, the architecture comprises a column embedding layer, a stack of $N$ [Transformer](/method/transformer) layers, and a multi-layer perceptron (MLP). The contextual embeddings (outputted by the Transformer layer) are concatenated along with continuous features which is inputted to an MLP. The loss function is then minimized  to learn all the parameters in an end-to-end learning.""" ;
    skos:prefLabel "TabTransformer" .

:Tacotron a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1703.10135v2> ;
    skos:definition """**Tacotron** is an end-to-end generative text-to-speech model that takes a character sequence as input and outputs the corresponding spectrogram. The backbone of Tacotron is a seq2seq model with attention. The Figure depicts the model, which includes an encoder, an attention-based decoder, and a post-processing net. At a high-level, the model takes characters as input and produces spectrogram\r
frames, which are then converted to waveforms.""" ;
    skos:prefLabel "Tacotron" .

:Tacotron2 a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1712.05884v2> ;
    rdfs:seeAlso <https://github.com/NVIDIA/tacotron2/blob/185cd24e046cc1304b4f8e564734d2498c6e2e6f/model.py#L457> ;
    skos:altLabel "Tacotron2" ;
    skos:definition """**Tacotron 2** is a neural network architecture for speech synthesis directly from text. It consists of two components:\r
\r
- a recurrent sequence-to-sequence feature prediction network with\r
attention which predicts a sequence of mel spectrogram frames from\r
an input character sequence\r
- a modified version of [WaveNet](https://paperswithcode.com/method/wavenet) which generates time-domain waveform samples conditioned on the\r
predicted mel spectrogram frames\r
\r
In contrast to the original [Tacotron](https://paperswithcode.com/method/tacotron), Tacotron 2 uses simpler building blocks, using vanilla [LSTM](https://paperswithcode.com/method/lstm) and convolutional layers in the encoder and decoder instead of [CBHG](https://paperswithcode.com/method/cbhg) stacks and [GRU](https://paperswithcode.com/method/gru) recurrent layers. Tacotron 2 does not use a “reduction factor”, i.e., each decoder step corresponds to a single spectrogram frame. Location-sensitive attention is used instead of [additive attention](https://paperswithcode.com/method/additive-attention).""" ;
    skos:prefLabel "Tacotron 2" .

:Talking-HeadsAttention a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2003.02436v1> ;
    skos:definition "**Talking-Heads Attention** is a variation on [multi-head attention](https://paperswithcode.com/method/multi-head-attention) which includes linear projections across the attention-heads dimension, immediately before and after the [softmax](https://paperswithcode.com/method/softmax) operation. In [multi-head attention](https://paperswithcode.com/method/multi-head-attention), the different attention heads perform separate computations, which are then summed at the end. Talking-Heads Attention breaks that separation. Two additional learned linear projections are inserted, $P\\_{l}$ and $P\\_{w}$, which transform the attention-logits and the attention weights respectively, moving information across attention heads. Instead of one \"heads\" dimension $h$ across the whole computation, we now have three separate heads dimensions: $h\\_{k}$, $h$, and $h\\_{v}$, which can optionally differ in size (number of \"heads\"). $h\\_{k}$ refers to the number of attention heads for the keys and the queries. $h$ refers to the number of attention heads for the logits and the weights, and $h\\_{v}$ refers to the number of attention heads for the values." ;
    skos:prefLabel "Talking-Heads Attention" .

:TanhActivation a skos:Concept ;
    rdfs:seeAlso <https://github.com/pytorch/pytorch/blob/96aaa311c0251d24decb9dc5da4957b7c590af6f/torch/nn/modules/activation.py#L329> ;
    skos:definition """**Tanh Activation** is an activation function used for neural networks:\r
\r
$$f\\left(x\\right) = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}$$\r
\r
Historically, the tanh function became preferred over the [sigmoid function](https://paperswithcode.com/method/sigmoid-activation) as it gave better performance for multi-layer neural networks. But it did not solve the vanishing gradient problem that sigmoids suffered, which was tackled more effectively with the introduction of [ReLU](https://paperswithcode.com/method/relu) activations.\r
\r
Image Source: [Junxi Feng](https://www.researchgate.net/profile/Junxi_Feng)""" ;
    skos:prefLabel "Tanh Activation" .

:TanhExp a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2003.09855v2> ;
    skos:altLabel "Tanh Exponential Activation Function" ;
    skos:definition """Lightweight or mobile neural networks used for real-time computer vision tasks contain fewer parameters than normal\r
networks, which lead to a constrained performance. In this work, we proposed a novel activation function named Tanh Exponential\r
Activation Function (TanhExp) which can improve the performance for these networks on image classification task significantly.\r
The definition of TanhExp is $f(x) = x tanh(e^x)$. We demonstrate the simplicity, efficiency, and robustness of TanhExp on various\r
datasets and network models and TanhExp outperforms its counterparts in both convergence speed and accuracy. Its behaviour\r
also remains stable even with noise added and dataset altered. We show that without increasing the size of the network, the\r
capacity of lightweight neural networks can be enhanced by TanhExp with only a few training epochs and no extra parameters\r
added.""" ;
    skos:prefLabel "TanhExp" .

:TargetPolicySmoothing a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1802.09477v3> ;
    rdfs:seeAlso <https://github.com/sfujim/TD3/blob/ade6260da88864d1ab0ed592588e090d3d97d679/main.py#L86> ;
    skos:definition """**Target Policy Smoothing** is a regularization strategy for the value function in reinforcement learning. Deterministic policies can overfit to narrow peaks in the value estimate, making them highly susceptible to functional approximation error, increasing the variance of the target. To reduce this variance, target policy smoothing adds a small amount of random noise to the target policy and averages over mini-batches - approximating a [SARSA](https://paperswithcode.com/method/sarsa)-like expectation/integral.\r
\r
The modified target update is:\r
\r
$$ y = r + \\gamma{Q}\\_{\\theta'}\\left(s', \\pi\\_{\\theta'}\\left(s'\\right) + \\epsilon \\right) $$\r
\r
$$ \\epsilon \\sim \\text{clip}\\left(\\mathcal{N}\\left(0, \\sigma\\right), -c, c \\right) $$\r
\r
where the added noise is clipped to keep the target close to the original action. The outcome is an algorithm reminiscent of [Expected SARSA](https://paperswithcode.com/method/expected-sarsa), where the value estimate is instead learned off-policy and the noise added to the target policy is chosen independently of the exploration policy. The value estimate learned is with respect to a noisy policy defined by the parameter $\\sigma$.""" ;
    skos:prefLabel "Target Policy Smoothing" .

:TargetedDropout a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1905.13678v5> ;
    skos:definition "Please enter a description about the method here" ;
    skos:prefLabel "Targeted Dropout" .

:TaxoExpan a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2001.09522v1> ;
    skos:definition "**TaxoExpan** is a self-supervised taxonomy expansion framework. It automatically generates a set of <query concept, anchor concept> pairs from the existing taxonomy as training data. Using such self-supervision data, TaxoExpan learns a model to predict whether a query concept is the direct hyponym of an anchor concept. TaxoExpan features: (1) a position-enhanced graph neural network that encodes the local structure of an anchor concept in the existing taxonomy, and (2) a noise-robust training objective that enables the learned model to be insensitive to the label noise in the self-supervision data." ;
    skos:prefLabel "TaxoExpan" .

:TayPO a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2003.06259v1> ;
    skos:altLabel "Taylor Expansion Policy Optimization" ;
    skos:definition """**TayPO**, or **Taylor Expansion Policy Optimization**, refers to a set of algorithms that apply the $k$-th order Taylor expansions for policy optimization. This generalizes prior work, including [TRPO](https://paperswithcode.com/method/trpo) as a special case. It can be thought of unifying ideas from trust-region policy optimization and off-policy corrections. Taylor expansions share high-level similarities with both trust region policy search and off-policy corrections. To get high-level intuitions of such similarities, consider a simple 1D example of Taylor expansions. Given a sufficiently smooth real-valued function on the real line $f : \\mathbb{R} \\rightarrow \\mathbb{R}$, the $k$-th order Taylor expansion of $f\\left(x\\right)$ at $x\\_{0}$ is \r
\r
$$f\\_{k}\\left(x\\right) = f\\left(x\\_{0}\\right)+\\sum^{k}\\_{i=1}\\left[f^{(i)}\\left(x\\_{0}\\right)/i!\\right]\\left(x−x\\_{0}\\right)^{i}$$\r
\r
where $f^{(i)}\\left(x\\_{0}\\right)$ are the $i$-th order derivatives at $x\\_{0}$. First, a common feature shared by Taylor expansions and trust-region policy search is the inherent notion of a trust region constraint. Indeed, in order for convergence to take place, a trust-region constraint is required $|x − x\\_{0}| < R\\left(f, x\\_{0}\\right)^{1}$. Second, when using the truncation as an approximation to the original function $f\\_{K}\\left(x\\right) \\approx f\\left(x\\right)$, Taylor expansions satisfy the requirement of off-policy evaluations: evaluate target policy with behavior data. Indeed, to evaluate the truncation $f\\_{K}\\left(x\\right)$ at any $x$ (target policy), we only require the behavior policy "data" at $x\\_{0}$ (i.e., derivatives $f^{(i)}\\left(x\\_{0}\\right)$).""" ;
    skos:prefLabel "TayPO" .

:Teacher-Tutor-StudentKnowledgeDistillation a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2103.04559v2> ;
    skos:definition "**Teacher-Tutor-Student Knowledge Distillation** is a method for image virtual try-on models. It treats fake images produced by the parser-based method as \"tutor knowledge\", where the artifacts can be corrected by real \"teacher knowledge\", which is extracted from the real person images in a self-supervised way. Other than using real images as supervisions, knowledge distillation is formulated in the try-on problem as distilling the appearance flows between the person image and the garment image, enabling the finding of dense correspondences between them to produce high-quality results." ;
    skos:prefLabel "Teacher-Tutor-Student Knowledge Distillation" .

:TemporalActivationRegularization a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1708.01009v1> ;
    rdfs:seeAlso <https://github.com/salesforce/awd-lstm-lm/blob/32fcb42562aeb5c7e6c9dec3f2a3baaaf68a5cb5/main.py#L202> ;
    skos:definition """**Temporal Activation Regularization (TAR)** is a type of slowness regularization for [RNNs](https://paperswithcode.com/methods/category/recurrent-neural-networks) that penalizes differences between states that have been explored in the past. Formally we minimize:\r
\r
$$\\beta{L\\_{2}}\\left(h\\_{t} - h\\_{t+1}\\right)$$\r
\r
where $L\\_{2}$ is the $L\\_{2}$ norm, $h_{t}$ is the output of the RNN at timestep $t$, and $\\beta$ is a scaling coefficient.""" ;
    skos:prefLabel "Temporal Activation Regularization" .

:TemporalDistributionCharacterization a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2108.04443v2> ;
    skos:definition """**Temporal Distribution Characterization**, or **TDC**, is a module used in the [AdaRNN](https://paperswithcode.com/method/adarnn) architecture to characterize the distributional information in a time series.\r
\r
Based on the principle of maximum entropy, maximizing the utilization of shared knowledge underlying a times series under temporal covariate shift can be done by finding periods which are most dissimilar to each other, which is also considered as the worst case of temporal covariate shift since the cross-period distributions are the most diverse. TDC achieves this goal for splitting the time-series by solving an optimization problem whose objective can be formulated as:\r
\r
$$\r
\\max \\_{0<K \\leq K\\_{0}} \\max \\_{n\\_{1}, \\cdots, n\\_{K}} \\frac{1}{K} \\sum_{1 \\leq i \\neq j \\leq K} d\\left(\\mathcal{D}\\_{i}, \\mathcal{D}\\_{j}\\right) \r
$$\r
\r
$$\r
\\text { s.t. } \\forall i, \\Delta_{1}<\\left|\\mathcal{D}\\_{i}\\right|<\\Delta_{2} ; \\sum_{i}\\left|\\mathcal{D}\\_{i}\\right|=n\r
$$\r
\r
where $d$ is a distance metric, $\\Delta\\_{1}$ and $\\Delta\\_{2}$ are predefined parameters to avoid trivial solutions (e.g., very small values or very large values may fail to capture the distribution information), and $K\\_{0}$ is the hyperparameter to avoid over-splitting. The metric $d(\\cdot, \\cdot)$ above can be any distance function, e.g., Euclidean or Editing distance, or some distribution-based distance / divergence, like MMD [14] and KL-divergence.\r
\r
The learning goal of the optimization problem (1) is to maximize the averaged period-wise distribution distances by searching $K$ and the corresponding periods so that the distributions of each period are as diverse as possible and the learned prediction model has better a more generalization ability.""" ;
    skos:prefLabel "Temporal Distribution Characterization" .

:TemporalDistributionMatching a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2108.04443v2> ;
    skos:definition """**Temporal Distribution Matching**, or **TDM**,  is a module used in the [AdaRNN](https://paperswithcode.com/method/adarnn) architecture to match the distributions of the discovered periods to build a time series prediction model $\\mathcal{M}$ Given the learned time periods, the TDM module is designed to learn the common knowledge shared by different periods via matching their distributions. Thus, the learned model $\\mathcal{M}$ is expected to generalize well on unseen test data compared with the methods which only rely on local or statistical information.\r
\r
Within the context of AdaRNN, Temporal Distribution Matching aims to adaptively match the distributions between the [RNN](https://paperswithcode.com/methods/category/recurrent-neural-networks) cells of two periods while capturing the temporal dependencies. TDM introduces the importance vector $\\mathbf{\\alpha} \\in \\mathbb{R}^{\\hat{V}}$ to learn the relative importance of $V$ hidden states inside the RNN, where all the hidden states are weighted with a normalized $\\alpha$. Note that for each pair of periods, there is an $\\mathbf{\\alpha}$, and we omit the subscript if there is no confusion. In this way, we can dynamically reduce the distribution divergence of cross-periods.\r
\r
Given a period-pair $\\left(\\mathcal{D}\\_{i}, \\mathcal{D}\\_{j}\\right)$, the loss of temporal distribution matching is formulated as:\r
\r
$$\r
\\mathcal{L}\\_{t d m}\\left(\\mathcal{D}\\_{i}, \\mathcal{D}\\_{j} ; \\theta\\right)=\\sum_{t=1}^{V} \\alpha\\_{i, j}^{t} d\\left(\\mathbf{h}\\_{i}^{t}, \\mathbf{h}\\_{j}^{t} ; \\theta\\right)\r
$$\r
\r
where $\\alpha\\_{i, j}^{t}$ denotes the distribution importance between the periods $\\mathcal{D}\\_{i}$ and $\\mathcal{D}\\_{j}$ at state $t$.\r
\r
All the hidden states of the RNN can be easily computed by following the standard RNN computation. Denote by $\\delta(\\cdot)$ the computation of a next hidden state based on a previous state. The state computation can be formulated as\r
\r
$$\r
\\mathbf{h}\\_{i}^{t}=\\delta\\left(\\mathbf{x}\\_{i}^{t}, \\mathbf{h}\\_{i}^{t-1}\\right)\r
$$\r
\r
The final objective of temporal distribution matching (one RNN layer) is:\r
\r
$$\r
\\mathcal{L}(\\theta, \\mathbf{\\alpha})=\\mathcal{L}\\_{\\text {pred }}(\\theta)+\\lambda \\frac{2}{K(K-1)} \\sum\\_{i, j}^{i \\neq j} \\mathcal{L}\\_{t d m}\\left(\\mathcal{D}\\_{i}, \\mathcal{D}\\_{j} ; \\theta, \\mathbf{\\alpha}\\right)\r
$$\r
\r
where $\\lambda$ is a trade-off hyper-parameter. Note that in the second term, we compute the average of the distribution distances of all pairwise periods. For computation, we take a mini-batch of $\\mathcal{D}_{i}$ and $\\mathcal{D}\\_{j}$ to perform forward operation in RNN layers and concatenate all hidden features. Then, we can perform TDM using the above equation.""" ;
    skos:prefLabel "Temporal Distribution Matching" .

:TemporalJittering a skos:Concept ;
    rdfs:seeAlso <https://github.com/pytorch/vision/blob/c808d163f6c65ca851db59e9966807a9220fc493/torchvision/datasets/samplers/clip_sampler.py#L150> ;
    skos:definition "**Temporal Jittering** is a method used in deep learning for video, where we sample multiple training clips from each video with random start times during at every epoch." ;
    skos:prefLabel "Temporal Jittering" .

:TemporalROIAlign a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2109.03495v2> ;
    skos:definition "**Temporal ROI Align** is an operator for extracting features from other frames' feature maps for current frame proposals by utilizing feature similarity. Considering the features of the same object instance are highly similar among frames in a video, the proposed operator implicitly extracts the most similar ROI features from support frames feature map for target frame proposals based on feature similarity." ;
    skos:prefLabel "Temporal ROIAlign" .

:Temporalattention a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1708.02286v2> ;
    skos:definition "Temporal attention can be seen as a dynamic time selection mechanism determining when to pay attention, and is thus usually used for video processing." ;
    skos:prefLabel "Temporal attention" .

:TemporallyConsistentSpatialAugmentation a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2008.03800v4> ;
    skos:definition "**Temporally Consistent Spatial Augmentation** is a video data augmentation technique used for contrastive learning in the [Contrastive Video Representation Learning](https://paperswithcode.com/method/cvrl) framework. It fixes the randomness of spatial augmentation across frames; this prevents spatial augmentation hurting learning if applied independently across frames, because in that case it breaks the natural motion. In contrast, having temporally consistent spatial augmentation does not break the natural motion in the frames." ;
    skos:prefLabel "Temporally Consistent Spatial Augmentation" .

:TernaryBERT a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2009.12812v3> ;
    skos:definition "**TernaryBERT** is a [Transformer](https://paperswithcode.com/methods/category/transformers)-based model which ternarizes the weights of a pretrained [BERT](https://paperswithcode.com/method/bert) model to $\\{-1,0,+1\\}$, with different granularities for word embedding and weights in the Transformer layer. Instead of directly using knowledge distillation to compress a model, it is used to improve the performance of ternarized student model with the same size as the teacher model. In this way, we transfer the knowledge from the highly-accurate teacher model to the ternarized student model with smaller capacity." ;
    skos:prefLabel "TernaryBERT" .

:TernaryWeightSplitting a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2012.15701v2> ;
    skos:definition """**Ternary Weight Splitting** is a ternarization approach used in [BinaryBERT](https://www.paperswithcode.com/method/binarybert) that exploits the flatness of ternary loss landscape as the optimization proxy of the binary model. We first train the half-sized ternary BERT to convergence, and then split both the latent full-precision weight $\\mathbf{w}^{t}$ and quantized $\\hat{\\mathbf{w}}^{t}$ to their binary counterparts $\\mathbf{w}\\_{1}^{b}, \\mathbf{w}\\_{2}^{b}$ and $\\hat{\\mathbf{w}}\\_{1}^{b}, \\hat{\\mathbf{w}}\\_{2}^{b}$ via the TWS operator. To inherit the performance of the ternary model after splitting, the TWS operator requires the splitting equivalency (i.e., the same output given the same input):\r
\r
$$\r
\\mathbf{w}^{t}=\\mathbf{w}\\_{1}^{b}+\\mathbf{w}\\_{2}^{b}, \\quad \\hat{\\mathbf{w}}^{t}=\\hat{\\mathbf{w}}\\_{1}^{b}+\\hat{\\mathbf{w}}\\_{2}^{b}\r
$$\r
\r
While solution to the above equation is not unique, we constrain the latent full-precision weights after splitting $\\mathbf{w}\\_{1}^{b}, \\mathbf{w}\\_{2}^{b}$ to satisfy $\\mathbf{w}^{t}=\\mathbf{w}\\_{1}^{b}+\\mathbf{w}\\_{2}^{b}$. See the paper for more details.""" ;
    skos:prefLabel "Ternary Weight Splitting" .

:ThunderNet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1903.11752v3> ;
    rdfs:seeAlso <https://github.com/ouyanghuiyu/Thundernet_Pytorch/tree/ab66b733a39c9d1c60b5373f84f861d9627d8c20> ;
    skos:definition "**ThunderNet** is a two-stage object detection model. The design of ThunderNet aims at the computationally expensive structures in state-of-the-art two-stage detectors. The backbone utilises a [ShuffleNetV2](https://paperswithcode.com/method/shufflenet-v2) inspired network called [SNet](https://paperswithcode.com/method/snet) designed for object detection. In the detection part, ThunderNet follows the detection head design in Light-Head [R-CNN](https://paperswithcode.com/method/r-cnn), and further compresses the [RPN](https://paperswithcode.com/method/rpn) and R-CNN subnet. To eliminate the performance degradation induced by small backbones and small feature maps, ThunderNet uses two new efficient architecture blocks, [Context Enhancement Module](https://paperswithcode.com/method/context-enhancement-module) (CEM) and [Spatial Attention Module](https://paperswithcode.com/method/spatial-attention-module) (SAM). CEM combines the feature maps from multiple scales to leverage local and global context information, while SAM uses the information learned in RPN to refine the feature distribution in RoI warping." ;
    skos:prefLabel "ThunderNet" .

:TimeSformer a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2102.05095v4> ;
    skos:definition "**TimeSformer** is a [convolution](https://paperswithcode.com/method/convolution)-free approach to video classification built exclusively on self-attention over space and time. It adapts the standard [Transformer](https://paperswithcode.com/method/transformer) architecture to video by enabling spatiotemporal feature learning directly from a sequence of frame-level patches. Specifically, the method adapts the image model [[Vision Transformer](https://paperswithcode.com/method/vision-transformer)](https//www.paperswithcode.com/method/vision-transformer) (ViT) to video by extending the self-attention mechanism from the image space to the space-time 3D volume. As in ViT, each patch is linearly mapped into an embedding and augmented with positional information. This makes it possible to interpret the resulting sequence of vector" ;
    skos:prefLabel "TimeSformer" .

:TinaFace a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2011.13183v3> ;
    skos:definition "**TinaFace** is a type of face detection method that is based on generic object detection. It consists of (a) Feature Extractor: [ResNet](https://paperswithcode.com/method/resnet)-50 and 6 level [Feature Pyramid Network](https://www.paperswithcode.com/method/fpn) to extract the multi-scale features of input image; (b) an Inception block to enhance receptive field; (c) Classification Head: 5 layers [FCN](https://paperswithcode.com/method/fcn) for classification of anchors; (d) Regression Head: 5 layers [FCN](https://paperswithcode.com/method/fcn) for regression of anchors to ground-truth objects boxes; (e) IoU Aware Head: a single convolutional layer for IoU prediction." ;
    skos:prefLabel "TinaFace" .

:TinyNet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2010.14819v2> ;
    skos:altLabel "Model Rubik's Cube: Twisting Resolution, Depth and Width for TinyNets" ;
    skos:definition "To obtain excellent deep neural architectures, a series of techniques are carefully designed in EfficientNets. The giant formula for simultaneously enlarging the resolution, depth and width provides us a Rubik's cube for neural networks. So that we can find networks with high efficiency and excellent performance by twisting the three dimensions. This paper aims to explore the twisting rules for obtaining deep neural networks with minimum model sizes and computational costs. Different from the network enlarging, we observe that resolution and depth are more important than width for tiny networks. Therefore, the original method, i.e., the compound scaling in [EfficientNet](https://paperswithcode.com/method/efficientnet) is no longer suitable. To this end, we summarize a tiny formula for downsizing neural architectures through a series of smaller models derived from the EfficientNet-B0 with the FLOPs constraint. Experimental results on the ImageNet benchmark illustrate that our TinyNet performs much better than the smaller version of EfficientNets using the inversed giant formula. For instance, our TinyNet-E achieves a 59.9% Top-1 accuracy with only 24M FLOPs, which is about 1.9% higher than that of the previous best [MobileNetV3](https://paperswithcode.com/method/mobilenetv3) with similar computational cost." ;
    skos:prefLabel "TinyNet" .

:Tofu a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1807.08887v2> ;
    skos:definition "**Tofu** is an intra-layer model parallel system that partitions very large DNN models across multiple GPU devices to reduce per-GPU memory footprint. Tofu is designed to partition a dataflow graph of fine-grained tensor operators used by platforms like MXNet and TensorFlow. To optimally partition different operators in a dataflow graph, Tofu uses a recursive search algorithm that minimizes the total communication cost." ;
    skos:prefLabel "Tofu" .

:TopKCopy a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2109.04901v1> ;
    skos:definition "**TopK Copy** is a cross-attention guided copy mechanism for entity extraction where only the Top-$k$ important attention heads are used for computing copy distributions. The motivation is that that attention heads may not equally important, and that some heads can be pruned out with a marginal decrease in overall performance. Attention probabilities produced by insignificant attention heads may be noisy. Thus, computing copy distributions without these heads could improve the model’s ability to infer the importance of each token in the input document." ;
    skos:prefLabel "TopK Copy" .

:TopographicVAE a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2109.01394v2> ;
    skos:definition "**Topographic VAE** is a method for efficiently training deep generative models with topographically organized latent variables. The model learns sets of approximately equivariant features (i.e. \"capsules\") directly from sequences and achieves higher likelihood on correspondingly transforming test sequences. The combined color/rotation transformation in input space $\\tau\\_{g}$ becomes encoded as a $\\mathrm{Roll}$ within the capsule dimension. The model is thus able decode unseen sequence elements by encoding a partial sequence and Rolling activations within the capsules. This resembles a commutative diagram." ;
    skos:prefLabel "Topographic VAE" .

:TorchBeast a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1910.03552v1> ;
    skos:definition "**TorchBeast** is a platform for reinforcement learning (RL) research in PyTorch. It implements a version of the popular [IMPALA](https://paperswithcode.com/method/impala) algorithm for fast, asynchronous, parallel training of RL agents." ;
    skos:prefLabel "TorchBeast" .

:TrIVD-GAN a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2003.04035v3> ;
    skos:definition """**TrIVD-GAN**, or **Transformation-based & TrIple Video Discriminator GAN**, is a type of generative adversarial network for video generation that builds upon [DVD-GAN](https://paperswithcode.com/method/dvd-gan). Improvements include a novel transformation-based recurrent unit (the TSRU) that makes the generator more expressive, and an improved discriminator architecture. \r
\r
In contrast with DVD-[GAN](https://paperswithcode.com/method/gan), TrIVD-GAN has an alternative split for the roles of the discriminators, with $\\mathcal{D}\\_{S}$ judging per-frame global structure, while $\\mathcal{D}\\_{T}$ critiques local spatiotemporal structure. This is achieved by downsampling the $k$ randomly sampled frames fed to $\\mathcal{D}\\_{S}$ by a factor $s$, and cropping $T \\times H/s \\times W/s$ clips inside the high resolution video fed to $\\mathcal{D}\\_{T}$, where $T, H, W, C$ correspond to time, height, width and channel dimension of the input. This further reduces the number of pixels to process per video,\r
from $k \\times H \\times W + T \\times H/s \\times W/s$ to $\\left(k + T\\right) \\times H/s \\times W/s$.""" ;
    skos:prefLabel "TrIVD-GAN" .

:TrOCR a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2109.10282v5> ;
    skos:definition "**TrOCR** is an end-to-end [Transformer](https://paperswithcode.com/methods/category/transformers)-based OCR model for text recognition with pre-trained CV and NLP models. It leverages the [Transformer](https://paperswithcode.com/method/transformer) architecture for both image understanding and wordpiece-level text generation. It first resizes the input text image into $384 × 384$ and then the image is split into a sequence of 16 patches which are used as the input to image Transformers.  Standard Transformer architecture with the [self-attention mechanism](https://paperswithcode.com/method/scaled) is leveraged on both encoder and decoder parts, where wordpiece units are generated as the recognized text from the input image." ;
    skos:prefLabel "TrOCR" .

:TraDeS a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2103.08808v1> ;
    skos:definition "**TradeS** is an online joint detection and tracking model, coined as TRACK to DEtect and Segment, exploiting tracking clues to assist detection end-to-end. TraDeS infers object tracking offset by a cost volume, which is used to propagate previous object features for improving current object detection and segmentation." ;
    skos:prefLabel "TraDeS" .

:Trans-Encoder a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2109.13059v4> ;
    skos:definition "Unsupervised knowledge distillation from a pretrained language model to *itself*, by alternating between its bi- and cross-encoder forms." ;
    skos:prefLabel "Trans-Encoder" .

:TransE a skos:Concept ;
    dcterms:source <http://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data> ;
    skos:definition "**TransE** is an energy-based model that produces knowledge base embeddings. It models relationships by interpreting them as translations operating on the low-dimensional embeddings of the entities. Relationships are represented as translations in the embedding space: if $\\left(h, \\mathcal{l}, t\\right)$ holds, the embedding of the tail entity $t$ should be close to the embedding of the head entity $h$ plus some vector that depends on the relationship $\\mathcal{l}$." ;
    skos:prefLabel "TransE" .

:TransductiveInference a skos:Concept ;
    dcterms:source <https://ieeexplore.ieee.org/abstract/document/6280886> ;
    skos:definition "" ;
    skos:prefLabel "Transductive Inference" .

:TransferQA a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2109.04655v1> ;
    skos:definition "**TransferQA** is a transferable generative QA model, built upon [T5](https://paperswithcode.com/method/t5) that combines extractive QA and multi-choice QA via a text-to-text [transformer](https://paperswithcode.com/method/transformer) framework, and tracks both categorical slots and non-categorical slots in DST. In addition, it introduces two effective ways to construct unanswerable questions, namely, negative question sampling and context truncation, which enable the model to handle “none” value slots in the zero-shot DST setting." ;
    skos:prefLabel "TransferQA" .

:Transformer a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1706.03762v7> ;
    rdfs:seeAlso <https://github.com/tunz/transformer-pytorch/blob/e7266679f0b32fd99135ea617213f986ceede056/model/transformer.py#L201> ;
    skos:definition "A **Transformer** is a model architecture that eschews recurrence and instead relies entirely on an [attention mechanism](https://paperswithcode.com/methods/category/attention-mechanisms-1) to draw global dependencies between input and output. Before Transformers, the dominant sequence transduction models were based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The Transformer also employs an encoder and decoder, but removing recurrence in favor of [attention mechanisms](https://paperswithcode.com/methods/category/attention-mechanisms-1) allows for significantly more parallelization than methods like [RNNs](https://paperswithcode.com/methods/category/recurrent-neural-networks) and [CNNs](https://paperswithcode.com/methods/category/convolutional-neural-networks)." ;
    skos:prefLabel "Transformer" .

:Transformer-XL a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1901.02860v3> ;
    skos:definition "**Transformer-XL** (meaning extra long) is a [Transformer](https://paperswithcode.com/method/transformer) architecture that introduces the notion of recurrence to the deep self-attention network. Instead of computing the hidden states from scratch for each new segment, Transformer-XL reuses the hidden states obtained in previous segments. The reused hidden states serve as memory for the current segment, which builds up a recurrent connection between the segments. As a result, modeling very long-term dependency becomes possible because information can be propagated through the recurrent connections. As an additional contribution, the Transformer-XL uses a new relative positional encoding formulation that generalizes to attention lengths longer than the one observed during training." ;
    skos:prefLabel "Transformer-XL" .

:Transposedconvolution a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1605.06211v1> ;
    skos:definition "" ;
    skos:prefLabel "Transposed convolution" .

:Tree-structuredParzenEstimatorApproach\(TPE\) a skos:Concept ;
    dcterms:source <https://www.researchgate.net/publication/285464863_Hyperopt_A_Python_library_for_optimizing_the_hyperparameters_of_machine_learning_algorithms> ;
    skos:definition "" ;
    skos:prefLabel "Tree-structured Parzen Estimator Approach (TPE)" .

:TridentNet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1901.01892v2> ;
    rdfs:seeAlso <https://github.com/facebookresearch/detectron2/tree/master/projects/TridentNet/> ;
    skos:definition """**TridentNet** is an object detection architecture that aims to generate scale-specific feature\r
maps with a uniform representational power.  A parallel multi-branch architecture is constructed in which each branch shares the same transformation parameters but with different receptive fields. A scale-aware training scheme is used to specialize each branch by sampling object instances of proper scales for training.""" ;
    skos:prefLabel "TridentNet" .

:TridentNetBlock a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1901.01892v2> ;
    rdfs:seeAlso <https://github.com/facebookresearch/detectron2/blob/d250fcc1b66d5a3686c15144480441b7abe31dec/projects/TridentNet/tridentnet/trident_backbone.py#L15> ;
    skos:definition """A **TridentNet Block** is a feature extractor used in object detection models. Instead of feeding in multi-scale inputs like the image pyramid, in a [TridentNet](https://paperswithcode.com/method/tridentnet) block we adapt the backbone network for different scales. These blocks create multiple scale-specific feature maps. With the help of dilated convolutions, different branches of trident blocks have the same network structure and share the\r
same parameters yet have different receptive fields. Furthermore, to avoid training objects with extreme scales, a scale-aware training scheme is employed to make each branch specific to a given scale range matching its receptive field. Weight sharing is used to prevent overfitting.""" ;
    skos:prefLabel "TridentNet Block" .

:TripletAttention a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2010.03045v2> ;
    rdfs:seeAlso <https://github.com/LandskapeAI/triplet-attention/blob/f07524e45db5eea1357c50316f30ab99a292d2f9/triplet_attention.py#L38> ;
    skos:definition "Triplet attention comprises of three branches each responsible for capturing crossdimension between the spatial dimensions and channel dimension of the input. Given an input tensor with shape (C × H × W), each branch is responsible for aggregating cross-dimensional interactive features between either the spatial dimension H or W and the channel dimension C." ;
    skos:prefLabel "Triplet Attention" .

:TripletEntropyLoss a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2012.03775v1> ;
    skos:definition "The Triplet Entropy Loss (TEL) training method aims to leverage both the strengths of Cross Entropy Loss (CEL) and [Triplet loss](https://paperswithcode.com/method/triplet-loss) during the training process, assuming that it would lead to better generalization. The TEL method though does not contain a pre-training step, but trains simultaneously with both CEL and Triplet losses." ;
    skos:prefLabel "Triplet Entropy Loss" .

:TripletLoss a skos:Concept ;
    dcterms:source <http://openaccess.thecvf.com/content_ECCV_2018/html/Xingping_Dong_Triplet_Loss_with_ECCV_2018_paper.html> ;
    skos:definition """The goal of **Triplet loss**, in the context of Siamese Networks, is to maximize the joint probability among all score-pairs i.e. the product of all probabilities. By using its negative logarithm, we can get the loss formulation as follows:\r
\r
$$\r
L\\_{t}\\left(\\mathcal{V}\\_{p}, \\mathcal{V}\\_{n}\\right)=-\\frac{1}{M N} \\sum\\_{i}^{M} \\sum\\_{j}^{N} \\log \\operatorname{prob}\\left(v p\\_{i}, v n\\_{j}\\right)\r
$$\r
\r
where the balance weight $1/MN$ is used to keep the loss with the same scale for different number of instance sets.""" ;
    skos:prefLabel "Triplet Loss" .

:TrueOnlineTDLambda a skos:Concept ;
    skos:definition """**True Online $TD\\left(\\lambda\\right)$** seeks to approximate the ideal online $\\lambda$-return algorithm. It seeks to invert this ideal forward-view algorithm to produce an efficient backward-view algorithm using eligibility traces. It uses dutch traces rather than accumulating traces.\r
\r
Source: [Sutton and Seijen](http://proceedings.mlr.press/v32/seijen14.pdf)""" ;
    skos:prefLabel "True Online TD Lambda" .

:TruncationTrick a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1706.00082v1> ;
    rdfs:seeAlso <https://github.com/ajbrock/BigGAN-PyTorch/blob/7b65e82d058bfe035fc4e299f322a1f83993e04c/TFHub/biggan_v1.py#L16> ;
    skos:definition """The **Truncation Trick** is a latent sampling procedure for generative adversarial networks, where we sample $z$ from a truncated normal (where values which fall outside a range are resampled to fall inside that range). \r
The original implementation was in [Megapixel Size Image Creation with GAN](https://paperswithcode.com/paper/megapixel-size-image-creation-using).\r
In [BigGAN](http://paperswithcode.com/method/biggan), the authors find this provides a boost to the Inception Score and FID.""" ;
    skos:prefLabel "Truncation Trick" .

:TuckER a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1901.09590v2> ;
    skos:definition "TuckER" ;
    skos:prefLabel "TuckER" .

:TuckER-RP a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2110.02834v1> ;
    skos:altLabel "TuckER with Relation Prediction" ;
    skos:definition "TuckER model trained with a relation prediction objective on top of the 1vsAll loss" ;
    skos:prefLabel "TuckER-RP" .

:TunableNetwork a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1811.10515v1> ;
    skos:definition "" ;
    skos:prefLabel "Tunable Network" .

:Twins-PCPVT a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2104.13840v4> ;
    skos:definition """**Twins-PCPVT** is a type of [vision transformer](https://paperswithcode.com/methods/category/vision-transformer) that combines global attention, specifically the global sub-sampled attention as proposed in [Pyramid Vision Transformer](https://paperswithcode.com/method/pvt), with [conditional position encodings](https://paperswithcode.com/method/conditional-positional-encoding) (CPE) to replace the [absolute position encodings](https://paperswithcode.com/method/absolute-position-encodings) used in PVT.\r
\r
The [position encoding generator](https://paperswithcode.com/method/positional-encoding-generator) (PEG), which generates the CPE, is placed after the first encoder block of each stage. The simplest form of PEG is used, i.e., a 2D [depth-wise convolution](https://paperswithcode.com/method/depthwise-convolution) without [batch normalization](https://paperswithcode.com/method/batch-normalization). For image-level classification, following [CPVT](https://paperswithcode.com/method/cpvt), the class token is removed and [global average pooling](https://paperswithcode.com/method/global-average-pooling) is used at the end of the stage. For other vision tasks, the design of PVT is followed.""" ;
    skos:prefLabel "Twins-PCPVT" .

:Twins-SVT a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2104.13840v4> ;
    skos:definition "**Twins-SVT** is a type of [vision transformer](https://paperswithcode.com/methods/category/vision-transformer) which utilizes a [spatially separable attention mechanism](https://paperswithcode.com/method/spatially-separable-self-attention) (SSAM) which is composed of two types of attention operations—(i) locally-grouped self-attention (LSA), and (ii) global sub-sampled attention (GSA), where LSA captures the fine-grained and short-distance information and GSA deals with the long-distance and global information. On top of this, it utilizes [conditional position encodings](https://paperswithcode.com/method/conditional-positional-encoding) as well as the architectural design of the [Pyramid Vision Transformer](https://paperswithcode.com/method/pvt)." ;
    skos:prefLabel "Twins-SVT" .

:Two-WayDenseLayer a skos:Concept ;
    dcterms:source <http://papers.nips.cc/paper/7466-pelee-a-real-time-object-detection-system-on-mobile-devices> ;
    rdfs:seeAlso <https://github.com/osmr/imgclsmob/blob/c03fa67de3c9e454e9b6d35fe9cbb6b15c28fda7/pytorch/pytorchcv/models/peleenet.py#L130> ;
    skos:definition "**Two-Way Dense Layer** is an image model block used in the [PeleeNet](https://paperswithcode.com/method/peleenet) architectures. Motivated by [GoogLeNet](https://paperswithcode.com/method/googlenet), the 2-way dense layer is used to get different scales of receptive fields. One way of the layer uses a 3x3 kernel size. The other way of the layer uses two stacked 3x3 [convolution](https://paperswithcode.com/method/convolution) to learn visual patterns for large objects." ;
    skos:prefLabel "Two-Way Dense Layer" .

:U-CAM a skos:Concept ;
    skos:altLabel "Uncertainty Class Activation Map (U-CAM) Using Gradient Certainty Method" ;
    skos:definition "Understanding and explaining deep learning models is an imperative task. Towards this, we propose a method that obtains gradient-based certainty estimates that also provide [visual attention](https://paperswithcode.com/method/visual-attention) maps. Particularly, we solve for visual question answering task. We incorporate modern probabilistic deep learning methods that we further improve by using the gradients for these estimates. These have two-fold benefits: a) improvement in obtaining the certainty estimates that correlate better with misclassified samples and b) improved attention maps that provide state-of-the-art results in terms of correlation with human attention regions. The improved attention maps result in consistent improvement for various methods for visual question answering. Therefore, the proposed technique can be thought of as a tool for obtaining improved certainty estimates and explanations for deep learning models." ;
    skos:prefLabel "U-CAM" .

:U-Net a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1505.04597v1> ;
    rdfs:seeAlso <https://github.com/milesial/Pytorch-UNet/blob/67bf11b4db4c5f2891bd7e8e7f58bcde8ee2d2db/unet/unet_model.py#L8> ;
    skos:definition """**U-Net** is an architecture for semantic segmentation. It consists of a contracting path and an expansive path. The contracting path follows the typical architecture of a convolutional network. It consists of the repeated application of two 3x3 convolutions (unpadded convolutions), each followed by a rectified linear unit ([ReLU](https://paperswithcode.com/method/relu)) and a 2x2 [max pooling](https://paperswithcode.com/method/max-pooling) operation with stride 2 for downsampling. At each downsampling step we double the number of feature channels. Every step in the expansive path consists of an upsampling of the feature map followed by a 2x2 [convolution](https://paperswithcode.com/method/convolution) (“up-convolution”) that halves the number of feature channels, a concatenation with the correspondingly cropped feature map from the contracting path, and two 3x3 convolutions, each followed by a ReLU. The cropping is necessary due to the loss of border pixels in every convolution. At the final layer a [1x1 convolution](https://paperswithcode.com/method/1x1-convolution) is used to map each 64-component feature vector to the desired number of classes. In total the network has 23 convolutional layers.\r
\r
[Original MATLAB Code](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-release-2015-10-02.tar.gz)""" ;
    skos:prefLabel "U-Net" .

:U-NetGAN a skos:Concept ;
    dcterms:source <http://openaccess.thecvf.com/content_CVPR_2020/html/Schonfeld_A_U-Net_Based_Discriminator_for_Generative_Adversarial_Networks_CVPR_2020_paper.html> ;
    skos:altLabel "U-Net Generative Adversarial Network" ;
    skos:definition "In contrast to typical GANs, a U-Net GAN uses a segmentation network as the discriminator. This segmentation network predicts two classes: real and fake. In doing so, the discriminator gives the generator region-specific feedback. This discriminator design also enables a  [CutMix](https://paperswithcode.com/method/cutmix)-based consistency regularization on the two-dimensional output of the U-Net GAN discriminator, which further improves image synthesis quality." ;
    skos:prefLabel "U-Net GAN" .

:U-RNNs a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2106.04419v2> ;
    rdfs:seeAlso <https://github.com/JosephGesnouin/Asymmetrical-Bi-RNNs-to-encode-pedestrian-trajectories> ;
    skos:altLabel "Asymmetrical Bi-RNN" ;
    skos:definition """An aspect of Bi-RNNs that could be undesirable is the architecture's symmetry in both time directions.\r
\r
 Bi-RNNs are often used in natural language processing, where the order of the words is almost exclusively determined by grammatical rules and not by temporal sequentiality.  However, in some cases, the data has a preferred direction in time: the forward direction. \r
\r
Another potential drawback of Bi-RNNs is that their output is simply the concatenation of two naive readings of the input in both directions. In consequence, Bi-RNNs never actually read an input by knowing what happens in the future. Conversely, the idea behind U-RNN, is to first do a backward pass, and then use during the forward pass information about the future.\r
\r
We accumulate information while knowing which part of the information will be useful in the future as it should be relevant to do so if the forward direction is the preferred direction of the data.\r
\r
The backward and forward hidden states $(h^b_t)$ and  $(h^f_t)$ are obtained according to these equations:\r
\r
\\begin{equation}\r
\\begin{aligned}\r
&h_{t-1}^{b}=R N N\\left(h_{t}^{b}, e_{t}, W_{b}\\right) \\\\\r
&h_{t+1}^{f}=R N N\\left(h_{t}^{f},\\left[e_{t}, h_{t}^{b}\\right], W_{f}\\right)\r
\\end{aligned}\r
\\end{equation}\r
\r
where $W_b$ and $W_f$ are learnable weights that are shared among pedestrians, and $[\\cdot, \\cdot]$ denotes concatenation. The last hidden state $h^f_{T_{obs}}$ is then used as the encoding of the sequence.""" ;
    skos:prefLabel "U-RNNs" .

:U2-Net a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2005.09007v3> ;
    skos:definition "**U2-Net** is a two-level nested U-structure architecture that is designed for salient object detection (SOD).  The architecture allows the network to go deeper, attain high resolution, without significantly increasing the memory and computation cost. This is achieved by a nested U-structure: on the bottom level, with a novel ReSidual U-block (RSU) module, which is able to extract intra-stage multi-scale features without degrading the feature map resolution; on the top level, there is a [U-Net](https://paperswithcode.com/method/u-net) like structure, in which each stage is filled by a RSU block." ;
    skos:prefLabel "U2-Net" .

:UCNet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2004.05763v1> ;
    skos:definition "**UCNet** is a probabilistic framework for RGB-D Saliency Detection that employs uncertainty by learning from the data labelling process. It utilizes conditional variational autoencoders to model human annotation uncertainty and generate multiple saliency maps for each input image by sampling in the latent space." ;
    skos:prefLabel "UCNet" .

:UCTransNet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2109.04335v3> ;
    skos:definition "**UCTransNet** is an end-to-end deep learning network for semantic segmentation that takes [U-Net](https://paperswithcode.com/method/u-net) as the main structure of the network. The original skip connections of U-Net are replaced by CTrans consisting of two components: [Channel-wise Cross fusion Transformer](https://paperswithcode.com/method/channel-wise-cross-fusion-transformer) ([CCT](https://paperswithcode.com/method/cct)) and [Channel-wise Cross Attention](https://paperswithcode.com/method/channel-wise-cross-attention) (CCA) to guide the fused multi-Scale channel-wise information to effectively connect to the decoder features for eliminating the ambiguity." ;
    skos:prefLabel "UCTransNet" .

:UFLoss a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2108.12460v1> ;
    skos:altLabel "Unsupervised Feature Loss" ;
    skos:definition "**UFLoss**, or **Unsupervised Feature Loss**, is a patch-based unsupervised learned feature loss for deep learning (DL) based reconstructions. The UFLoss provides instance-level discrimination by mapping similar instances to similar low-dimensional feature vectors using a pre-trained mapping network (UFLoss Network). The rationale of using features from large-patches (typically 40×40 pixels for a 300×300 pixels image) is that we want the UFLoss to capture mid-level structural and semantic features instead of using small patches (typically around 10×10 pixels), which only contain local edge information. On the other hand, the authors avoid using global features due to the fact that the training set (typically around 5000 slices) is usually not large enough to capture common and general features at a large-image scale." ;
    skos:prefLabel "UFLoss" .

:UL2 a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2205.05131v3> ;
    skos:definition "**UL2** is a unified framework for pretraining models that are universally effective across datasets and setups. UL2 uses Mixture-of-Denoisers (MoD), a pre-training objective that combines diverse pre-training paradigms together. UL2 introduces a notion of mode switching, wherein downstream fine-tuning is associated with specific pre-training schemes." ;
    skos:prefLabel "UL2" .

:ULMFiT a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1801.06146v5> ;
    skos:altLabel "Universal Language Model Fine-tuning" ;
    skos:definition """**Universal Language Model Fine-tuning**, or **ULMFiT**, is an architecture and transfer learning method that can be applied to NLP tasks. It involves a 3-layer [AWD-LSTM](https://paperswithcode.com/method/awd-lstm) architecture for its representations. The training consists of three steps: 1) general language model pre-training on a Wikipedia-based text, 2) fine-tuning the language model on a target task, and 3) fine-tuning the classifier on the target task.\r
\r
As different layers capture different types of information, they are fine-tuned to different extents using [discriminative fine-tuning](https://paperswithcode.com/method/discriminative-fine-tuning). Training is performed using [Slanted triangular learning rates](https://paperswithcode.com/method/slanted-triangular-learning-rates) (STLR), a learning rate scheduling strategy that first linearly increases the learning rate and then linearly decays it.\r
\r
Fine-tuning the target classifier is achieved in ULMFiT using gradual unfreezing. Rather than fine-tuning all layers at once, which risks catastrophic forgetting, ULMFiT gradually unfreezes the model starting from the last layer (i.e., closest to the output) as this contains the least general knowledge. First the last layer is unfrozen and all unfrozen layers are fine-tuned for one epoch. Then the next group of frozen layers is unfrozen and fine-tuned and repeat, until all layers are fine-tuned until convergence at the last iteration.""" ;
    skos:prefLabel "ULMFiT" .

:UNETR a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2103.10504v3> ;
    skos:altLabel "UNet Transformer" ;
    skos:definition "**UNETR**, or **UNet Transformer**, is a [Transformer](https://paperswithcode.com/methods/category/transformers)-based architecture for [medical image segmentation](https://paperswithcode.com/task/medical-image-segmentation) that utilizes a pure [transformer](https://paperswithcode.com/method/transformer) as the encoder to learn sequence representations of the input volume -- effectively capturing the global multi-scale information. The transformer encoder is directly connected to a decoder via [skip connections](https://paperswithcode.com/methods/category/skip-connections) at different resolutions like a [U-Net](https://paperswithcode.com/method/u-net) to compute the final semantic segmentation output." ;
    skos:prefLabel "UNETR" .

:UNIMO a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2012.15409v4> ;
    skos:definition """**UNIMO** is a multi-modal pre-training architecture that can effectively adapt to both single modal and multimodal understanding and generation tasks. UNIMO learns visual representations and textual representations simultaneously, and unifies them into the same semantic space via [cross-modal contrastive learning](https://paperswithcode.com/method/cmcl) (CMCL) based on a large-scale corpus of image collections, text corpus and image-text pairs. The CMCL aligns the visual representation and textual representation, and unifies them into the same semantic\r
space based on image-text pairs.""" ;
    skos:prefLabel "UNIMO" .

:UNITER a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1909.11740v3> ;
    skos:altLabel "UNiversal Image-TExt Representation Learning" ;
    skos:definition """UNITER or UNiversal Image-TExt Representation model is a large-scale pre-trained model for joint multimodal embedding. It is pre-trained using four image-text datasets COCO, Visual Genome, Conceptual Captions, and SBU Captions. It can power heterogeneous downstream V+L tasks with joint multimodal embeddings. \r
UNITER takes the visual regions of the image and textual tokens of the sentence as inputs. A faster R-CNN is used in Image Embedder to extract the visual features of each region and a Text Embedder is used to tokenize the input sentence into WordPieces.  \r
\r
It proposes WRA via the Optimal Transport to provide more fine-grained alignment between word tokens and image regions that is effective in calculating the minimum cost of transporting the contextualized image embeddings to word embeddings and vice versa. \r
\r
Four pretraining tasks were designed for this model. They are Masked Language Modeling (MLM), Masked Region Modeling (MRM, with three variants), Image-Text Matching (ITM), and Word-Region Alignment (WRA). This model is different from the previous models because it uses conditional masking on pre-training tasks.""" ;
    skos:prefLabel "UNITER" .

<http://w3id.org/mlso/vocab/ml_algorithm/UNet++> a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1807.10165v1> ;
    rdfs:seeAlso <https://github.com/MrGiovanni/UNetPlusPlus/blob/e145ba63862982bf1099cf2ec11d5466b434ae0b/pytorch/nnunet/network_architecture/generic_UNetPlusPlus.py#L167> ;
    skos:definition "UNet++ is an architecture for semantic segmentation based on the [U-Net](https://paperswithcode.com/method/u-net). Through the use of densely connected nested decoder sub-networks, it enhances extracted feature processing and was reported by its authors to outperform the U-Net in [Electron Microscopy (EM)](https://imagej.net/events/isbi-2012-segmentation-challenge), [Cell](https://acsjournals.onlinelibrary.wiley.com/doi/full/10.1002/cncy.21576), [Nuclei](https://www.kaggle.com/c/data-science-bowl-2018), [Brain Tumor](https://paperswithcode.com/dataset/brats-2013-1), [Liver](https://paperswithcode.com/dataset/lits17) and [Lung Nodule](https://paperswithcode.com/dataset/lidc-idri) medical image segmentation tasks." ;
    skos:prefLabel "UNet++" .

:UORO a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1702.05043v3> ;
    skos:altLabel "Unbiased Online Recurrent Optimization" ;
    skos:definition "" ;
    skos:prefLabel "UORO" .

:USE a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1907.04307v1> ;
    skos:altLabel "Multilingual Universal Sentence Encoder" ;
    skos:definition "" ;
    skos:prefLabel "USE" .

:UnifiedVLP a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1909.11059v3> ;
    skos:definition "Unified VLP is unified encoder-decoder model for general vision-language pre-training. The models uses a shared multi-layer transformers network for both encoding and decoding. The model is pre-trained on large amount of image-text pairs using the unsupervised learning objectives of two tasks: bidirectional and sequence-to-sequence (seq2seq) masked vision-language prediction. Model architecture for pre-training. For pre-training , the input comprises of image input, sentence input, and three special tokens ([CLS], [SEP], [STOP]). The image is processed as $N$ Region of Interests (RoIs) and region features are extracted. The sentence is tokenized and masked with [MASK] tokens for the later masked language modeling task. The model consists of 12 layers of Transformer blocks, each having a masked self-attention layer and feed-forward module, where the self-attention mask controls what input context the prediction conditions on. Two self-attention masks are implemented depending on whether the objective is bidirectional or seq2seq. The model is fine-tuned for image captioning and visual question answering." ;
    skos:prefLabel "Unified VLP" .

:UnigramSegmentation a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1804.10959v1> ;
    skos:definition """**Unigram Segmentation** is a subword segmentation algorithm based on a unigram language model. It provides multiple segmentations with probabilities. The language model allows for emulating the noise generated during the segmentation of actual data.\r
\r
The unigram language model makes an assumption that each subword occurs independently, and consequently, the probability of a subword sequence $\\mathbf{x} = (x_1,\\ldots,x_M)$ is\r
formulated as the product of the subword occurrence probabilities\r
$p(x_i)$:\r
\r
$$\r
  P(\\mathbf{x}) = \\prod_{i=1}^{M} p(x_i), \\\\\\\\\r
  \\forall i\\,\\, x_i \\in \\mathcal{V},\\,\\,\\,\r
  \\sum_{x \\in \\mathcal{V}} p(x) = 1, \\nonumber\r
$$\r
\r
where $\\mathcal{V}$ is a pre-determined vocabulary.  The most probable\r
segmentation $\\mathbf{x}^*$ for the input sentence $X$ is then given by:\r
\r
$$\r
  \\mathbf{x}^{*} = \\text{argmax}_{\\mathbf{x} \\in \\mathcal{S}(X)} P(\\mathbf{x}),\r
$$\r
\r
where $\\mathcal{S}(X)$ is a set of segmentation candidates built from\r
the input sentence $X$.  $\\mathbf{x}^*$ is obtained with the Viterbi\r
algorithm.""" ;
    skos:prefLabel "Unigram Segmentation" .

:UnitaryRNN a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1511.06464v4> ;
    skos:definition """A **Unitary RNN** is a recurrent neural network architecture that uses a unitary hidden to hidden matrix. Specifically they concern dynamics of the form:\r
\r
$$ h\\_{t} = f\\left(Wh\\_{t−1} + Vx\\_{t}\\right) $$\r
\r
where $W$ is a unitary matrix $\\left(W^{†}W = I\\right)$. The product of unitary matrices is a unitary matrix, so $W$ can be parameterised as a product of simpler unitary matrices:\r
\r
$$ h\\_{t} = f\\left(D\\_{3}R\\_{2}F^{−1}D\\_{2}PR\\_{1}FD\\_{1}h\\_{t−1} + Vxt\\right) $$\r
\r
where $D\\_{3}$, $D\\_{2}$, $D\\_{1}$ are learned diagonal complex matrices, and $R\\_{2}$, $R\\_{1}$ are learned reflection matrices. Matrices $F$ and $F^{−1}$ are the discrete Fourier transformation and its inverse. P is any constant random permutation. The activation function $f\\left(h\\right)$ applies a rectified linear unit with a learned bias to the modulus of each complex number. Only\r
the diagonal and reflection matrices, $D$ and $R$, are learned, so Unitary RNNs have fewer parameters than [LSTMs](https://paperswithcode.com/method/lstm) with comparable numbers of hidden units.\r
\r
Source: [Associative LSTMs](https://arxiv.org/pdf/1602.03032.pdf)""" ;
    skos:prefLabel "Unitary RNN" .

:UniversalProbing a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2210.13236v1> ;
    skos:altLabel "Massively multilingual probing based on Universal Dependencies" ;
    skos:definition "" ;
    skos:prefLabel "Universal Probing" .

:UniversalTransformer a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1807.03819v3> ;
    skos:definition "The **Universal Transformer** is a generalization of the [Transformer](https://paperswithcode.com/method/transformer) architecture. Universal Transformers combine the parallelizability and global receptive field of feed-forward sequence models like the Transformer with the recurrent inductive bias of [RNNs](https://paperswithcode.com/methods/category/recurrent-neural-networks). They also utilise a dynamic per-position halting mechanism." ;
    skos:prefLabel "Universal Transformer" .

:V-trace a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1802.01561v3> ;
    skos:definition """**V-trace** is an off-policy actor-critic reinforcement learning algorithm that helps tackle the lag between when actions are generated by the actors and when the learner estimates the gradient. Consider a trajectory $\\left(x\\_{t}, a\\_{t}, r\\_{t}\\right)^{t=s+n}\\_{t=s}$ generated by the actor following some policy $\\mu$. We can define the $n$-steps V-trace target for $V\\left(x\\_{s}\\right)$, our value approximation at state $x\\_{s}$ as:\r
\r
$$ v\\_{s} = V\\left(x\\_{s}\\right) + \\sum^{s+n-1}\\_{t=s}\\gamma^{t-s}\\left(\\prod^{t-1}\\_{i=s}c\\_{i}\\right)\\delta\\_{t}V $$\r
\r
Where $\\delta\\_{t}V = \\rho\\_{t}\\left(r\\_{t} + \\gamma{V}\\left(x\\_{t+1}\\right) - V\\left(x\\_{t}\\right)\\right)$ is a temporal difference algorithm for $V$, and $\\rho\\_{t} = \\text{min}\\left(\\bar{\\rho}, \\frac{\\pi\\left(a\\_{t}\\mid{x\\_{t}}\\right)}{\\mu\\left(a\\_{t}\\mid{x\\_{t}}\\right)}\\right)$ and $c\\_{i} = \\text{min}\\left(\\bar{c}, \\frac{\\pi\\left(a\\_{t}\\mid{x\\_{t}}\\right)}{\\mu\\left(a\\_{t}\\mid{x\\_{t}}\\right)}\\right)$ are truncated importance sampling weights. We assume that the truncation levels are such that $\\bar{\\rho} \\geq \\bar{c}$.""" ;
    skos:prefLabel "V-trace" .

:VAE a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1312.6114v10> ;
    rdfs:seeAlso <https://github.com/AntixK/PyTorch-VAE/blob/8700d245a9735640dda458db4cf40708caf2e77f/models/vanilla_vae.py#L8> ;
    skos:altLabel "Variational Autoencoder" ;
    skos:definition "A **Variational Autoencoder** is a type of likelihood-based generative model. It consists of an encoder, that takes in data $x$ as input and transforms this into a latent representation $z$,  and a decoder, that takes a latent representation $z$ and returns a reconstruction $\\hat{x}$. Inference is performed via variational inference to approximate the posterior of the model." ;
    skos:prefLabel "VAE" .

:VATT a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2104.11178v3> ;
    skos:definition """**Video-Audio-Text Transformer**, or **VATT**, is a framework for learning multimodal representations from unlabeled data using [convolution](https://paperswithcode.com/method/convolution)-free [Transformer](https://paperswithcode.com/method/transformer) architectures. Specifically, it takes raw signals as inputs and extracts multidimensional representations that are rich enough to benefit a variety of downstream tasks. VATT borrows the exact architecture from [BERT](https://paperswithcode.com/method/bert) and [ViT](https://paperswithcode.com/method/vision-transformer) except the layer of tokenization and linear projection reserved for each modality separately. The design follows the same spirit as ViT that makes the minimal changes to the architecture so that the learned model can transfer its weights to various frameworks and tasks.\r
\r
VATT linearly projects each modality into a feature vector and feeds it into a Transformer encoder. A semantically hierarchical common space is defined to account for the granularity of different modalities and noise contrastive estimation is employed to train the model.""" ;
    skos:prefLabel "VATT" .

:VCR-CNN a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2002.12204v3> ;
    skos:altLabel "Visual Commonsense Region-based Convolutional Neural Network" ;
    skos:definition "**VC R-CNN** is an unsupervised feature representation learning method, which uses Region-based Convolutional Neural Network ([R-CNN](https://paperswithcode.com/method/r-cnn)) as the visual backbone, and the causal intervention as the training objective. Given a set of detected object regions in an image (e.g., using [Faster R-CNN](https://paperswithcode.com/method/faster-r-cnn)), like any other unsupervised feature learning methods (e.g., word2vec), the proxy training objective of VC R-CNN is to predict the contextual objects of a region. However, they are fundamentally different: the prediction of VC R-CNN is by using causal intervention: P(Y|do(X)), while others are by using the conventional likelihood: P(Y|X). This is also the core reason why VC R-CNN can learn \"sense-making\" knowledge like chair can be sat -- while not just \"common\" co-occurrences such as the chair is likely to exist if table is observed." ;
    skos:prefLabel "VC R-CNN" .

:VDO-SLAM a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2005.11052v3> ;
    skos:definition "**VDO-SLAM** is a feature-based stereo/RGB-D dynamic SLAM system that leverages image-based semantic information to simultaneously localise the robot, map the static and dynamic structure, and track motions of rigid objects in the scene. Input images are first pre-processed to generate instance-level object segmentation and dense optical flow. These are then used to track features on static background structure and dynamic objects. Camera poses and object motions estimated from feature tracks are then refined in a global batch optimisation, and a local map is maintained and updated with every new frame." ;
    skos:prefLabel "VDO-SLAM" .

:VEGA a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2011.01507v4> ;
    skos:definition "**VEGA** is an AutoML framework that is compatible and optimized for multiple hardware platforms. It integrates various modules of AutoML, including [Neural Architecture Search](https://paperswithcode.com/method/neural-architecture-search) (NAS), Hyperparameter Optimization (HPO), Auto Data Augmentation, Model Compression, and Fully Train. To support a variety of search algorithms and tasks, it involves a fine-grained search space and a description language to enable easy adaptation to different search algorithms and tasks." ;
    skos:prefLabel "VEGA" .

:VERSE a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1803.04742v1> ;
    skos:altLabel "VERtex Similarity Embeddings" ;
    skos:definition """VERtex Similarity Embeddings (VERSE) is a simple, versatile, and memory-efficient method that derives graph embeddings explicitly calibrated to preserve the distributions of a selected vertex-to-vertex similarity measure. VERSE learns such embeddings by training a single-layer neural network.\r
\r
Source: [Tsitsulin et al.](https://arxiv.org/pdf/1803.04742v1.pdf)\r
\r
Image source: [Tsitsulin et al.](https://arxiv.org/pdf/1803.04742v1.pdf)""" ;
    skos:prefLabel "VERSE" .

:VFNet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2008.13367v2> ;
    skos:altLabel "VarifocalNet" ;
    skos:definition "**VarifocalNet** is a method aimed at accurately ranking a huge number of candidate detections in object detection. It consists of a new loss function, named [Varifocal Loss](https://paperswithcode.com/method/varifocal-loss), for training a dense object detector to predict the IACS, and a new efficient star-shaped bounding box feature representation for estimating the IACS and refining coarse bounding boxes. Combining these two new components and a bounding box refinement branch, results in a dense object detector on the [FCOS](https://paperswithcode.com/method/fcos) architecture, what the authors call VarifocalNet or VFNet for short." ;
    skos:prefLabel "VFNet" .

:VGAE a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1611.07308v1> ;
    skos:altLabel "Variational Graph Auto Encoder" ;
    skos:definition "" ;
    skos:prefLabel "VGAE" .

:VGG a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1409.1556v6> ;
    rdfs:seeAlso <https://github.com/pytorch/vision/blob/6db1569c89094cf23f3bc41f79275c45e9fcb3f3/torchvision/models/vgg.py#L24> ;
    skos:definition """**VGG** is a classical convolutional neural network architecture. It was based on an analysis of how to increase the depth of such networks. The network utilises small 3 x 3 filters. Otherwise the network is characterized by its simplicity: the only other components being pooling layers and a fully connected layer.\r
\r
Image: [Davi Frossard](https://www.cs.toronto.edu/frossard/post/vgg16/)""" ;
    skos:prefLabel "VGG" .

:VGG-16 a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1409.1556v6> ;
    skos:definition "" ;
    skos:prefLabel "VGG-16" .

:VGG-19 a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1409.1556v6> ;
    skos:altLabel "Visual Geometry Group 19 Layer CNN" ;
    skos:definition "" ;
    skos:prefLabel "VGG-19" .

:VGGLoss a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1609.04802v5> ;
    rdfs:seeAlso <https://github.com/tensorlayer/srgan/blob/9721adce6a2ee51a56aaf0b319b89d35836f57e6/train.py#L124> ;
    skos:definition """**VGG Loss** is a type of content loss intorduced in the [Perceptual Losses for Real-Time Style Transfer and Super-Resolution](https://paperswithcode.com/paper/perceptual-losses-for-real-time-style) super-resolution and style transfer framework. It is an alternative to pixel-wise losses; VGG Loss attempts to be closer to perceptual similarity. The [VGG](https://paperswithcode.com/method/vgg) loss is based on the [ReLU](https://paperswithcode.com/method/relu) activation layers of the pre-trained 19 layer VGG network. With $\\phi\\_{i,j}$ we indicate the feature map obtained by the $j$-th [convolution](https://paperswithcode.com/method/convolution) (after activation) before the $i$-th maxpooling layer within the VGG19 network, which we consider given. We then define the VGG loss as the euclidean distance between the feature representations of a reconstructed image $G\\_{\\theta\\_{G}}\\left(I^{LR}\\right)$ and the reference image $I^{HR}$:\r
\r
$$ l\\_{VGG/i.j} = \\frac{1}{W\\_{i,j}H\\_{i,j}}\\sum\\_{x=1}^{W\\_{i,j}}\\sum\\_{y=1}^{H\\_{i,j}}\\left(\\phi\\_{i,j}\\left(I^{HR}\\right)\\_{x, y} - \\phi\\_{i,j}\\left(G\\_{\\theta\\_{G}}\\left(I^{LR}\\right)\\right)\\_{x, y}\\right)^{2}$$ \r
\r
Here $W\\_{i,j}$ and $H\\_{i,j}$ describe the dimensions of the respective feature maps within the VGG network.""" ;
    skos:prefLabel "VGG Loss" .

:VGGoptiVMD a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2210.13067v1> ;
    skos:altLabel "VGG and variational Model Decomposition" ;
    skos:definition "" ;
    skos:prefLabel "VGGoptiVMD" .

:VIME a skos:Concept ;
    dcterms:source <http://proceedings.neurips.cc/paper/2020/hash/7d97667a3e056acab9aaf653807b4a03-Abstract.html> ;
    skos:altLabel "Value Imputation and Mask Estimation" ;
    skos:definition "**VIME **, or **Value Imputation and Mask Estimation**, is a self- and semi-supervised learning framework for tabular data. It consists of a pretext task of estimating mask vectors from corrupted tabular data in addition to the reconstruction pretext task for self-supervised learning." ;
    skos:prefLabel "VIME" .

:VL-BERT a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1908.08530v4> ;
    skos:altLabel "Visual-Linguistic BERT" ;
    skos:definition "VL-BERT is pre-trained on a large-scale image-captions dataset together with text-only corpus. The input to the model are either words from the input sentences or regions-of-interest (RoI) from input images. It can be fine-tuned to fit most visual-linguistic downstream tasks. Its backbone is a multi-layer bidirectional Transformer encoder, modified to accommodate visual contents, and new type of visual feature embedding to the input feature embeddings. VL-BERT takes both visual and linguistic elements as input, represented as RoIs in images and subwords in input sentences. Four different types of embeddings are used to represent each input: token embedding, visual feature embedding, segment embedding, and sequence position embedding. VL-BERT is pre-trained using Conceptual Captions and text-only datasets. Two pre-training tasks are used: masked language modeling with visual clues, and masked RoI classification with linguistic clues." ;
    skos:prefLabel "VL-BERT" .

:VL-T5 a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2102.02779v2> ;
    skos:definition "VL-T5 is a unified framework that learns different tasks in a single architecture with the same language modeling objective, i.e., multimodal conditional text generation. The model learns to generate labels in text based on the visual and textual inputs. In contrast to other existing methods, the framework unifies tasks as generating text labels conditioned on multimodal inputs. This allows the model to tackle vision-and-language tasks with unified text generation objective. The models use text prefixes to adapt to different tasks." ;
    skos:prefLabel "VL-T5" .

:VLG-Net a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2011.10132v2> ;
    skos:altLabel "Video Language Graph Matching Network" ;
    skos:definition "VLG-Net leverages recent advantages in Graph Neural Networks (GCNs) and leverages a novel multi-modality graph-based fusion method for the task of natural language video grounding." ;
    skos:prefLabel "VLG-Net" .

:VLMo a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2111.02358v2> ;
    skos:altLabel "Vision-Language pretrained Model" ;
    skos:definition "VLMo is a unified vision-language pre-trained model that jointly learns a dual encoder and a fusion encoder with a modular Transformer network. A Mixture-of-Modality-Experts (MOME) transformer is introduced to encode different modalities which helps it to capture modality-specific information by modality experts, and align content of different modalities by the self-attention module shared across modalities. The model parameters are shared across image-text contrastive learning, masked language modeling, and image-text matching tasks. During fine-tuning, the flexible modeling allows for VLMO to be used as either a dual encoder (i.e., separately encode images and text for retrieval tasks) or a fusion encoder (i.e., jointly encode image-text pairs for better interaction across modalities) Stage-wise pretraining on image-only and text-only data improved the vision-language pre-trained model. The model can be used for classification tasks and fine-tuned as a dual encoder for retrieval tasks." ;
    skos:prefLabel "VLMo" .

:VOS a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2003.00908v2> ;
    skos:definition "**VOS** is a type of video object segmentation model consisting of two network components. The target appearance model consists of a light-weight module, which is learned during the inference stage using fast optimization techniques to predict a coarse but robust target segmentation. The segmentation model is exclusively trained offline, designed to process the coarse scores into high quality segmentation masks." ;
    skos:prefLabel "VOS" .

:VPSNet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2006.11339v1> ;
    skos:altLabel "Video Panoptic Segmentation Network" ;
    skos:definition """**Video Panoptic Segmentation Network**, or **VPSNet**, is a model for video panoptic segmentation. On top of UPSNet, which is a method for image panoptic segmentation, VPSNet is designed to take an additional frame as the reference to correlate time information at two levels: pixel-level fusion and object-level tracking. To pick up the complementary feature points in the reference frame, a flow-based feature map alignment module is introduced along with an asymmetric attention block that computes similarities between the target and reference features to fuse them into one-frame shape. Additionally, to associate object instances across time, \r
 an object track head is added which learns the correspondence between the instances in the target and reference frames based\r
on their RoI feature similarity.""" ;
    skos:prefLabel "VPSNet" .

:VQ-VAE a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1711.00937v2> ;
    skos:definition "**VQ-VAE** is a type of variational autoencoder that uses vector quantisation to obtain a discrete latent representation. It differs from [VAEs](https://paperswithcode.com/method/vae) in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, ideas from vector quantisation (VQ) are incorporated. Using the VQ method allows the model to circumvent issues of posterior collapse - where the latents are ignored when they are paired with a powerful autoregressive decoder - typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes." ;
    skos:prefLabel "VQ-VAE" .

:VQ-VAE-2 a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1906.00446v1> ;
    rdfs:seeAlso <https://github.com/rosinality/vq-vae-2-pytorch> ;
    skos:definition "**VQ-VAE-2** is a type of variational autoencoder that combines a a two-level hierarchical VQ-[VAE](https://paperswithcode.com/method/vae) with a self-attention autoregressive model ([PixelCNN](https://paperswithcode.com/method/pixelcnn)) as a prior. The encoder and decoder architectures are kept simple and light-weight as in the original [VQ-VAE](https://paperswithcode.com/method/vq-vae), with the only difference that hierarchical multi-scale latent maps are used for increased resolution." ;
    skos:prefLabel "VQ-VAE-2" .

:VQSVD a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2006.02336v3> ;
    skos:definition "**Variational Quantum Singular Value Decomposition** is a variational quantum algorithm for singular value decomposition (VQSVD). By exploiting the variational principles for singular values and the Ky Fan Theorem, a novel loss function is designed such that two quantum neural networks (or parameterized quantum circuits) could be trained to learn the singular vectors and output the corresponding singular values." ;
    skos:prefLabel "VQSVD" .

:VSF a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2003.09044v3> ;
    skos:altLabel "VisuoSpatial Foresight" ;
    skos:definition "**VisuoSpatial Foresight** is a method for robotic fabric manipulation that leverages a combination of RGB and depth information to learn goal conditioned fabric manipulation policies for a variety of long horizon tasks." ;
    skos:prefLabel "VSF" .

:VSGNet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2003.05541v1> ;
    skos:altLabel "Visual-Spatial-Graph Network" ;
    skos:definition "**Visual-Spatial-Graph Network** (VSGNet) is a network for human-object interaction detection. It extracts visual features from the image representing the human-object pair, refines the features with spatial configurations of the pair, and utilizes the structural connections between the pair via graph convolutions." ;
    skos:prefLabel "VSGNet" .

:VTDE a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2012.05768v3> ;
    skos:altLabel "Variational Trace Distance Estimation" ;
    skos:definition "**Variational Trace Distance Estimation**, or **VTDE**, is a variational algorithm for trace norm estimation that only involves one ancillary qubit. Notably, the cost function in VTDE gathers information from a single-qubit observable and thus could avoid the barren plateau issue with logarithmic depth parameterized circuits." ;
    skos:prefLabel "VTDE" .

:VarImpVIANN a skos:Concept ;
    dcterms:source <https://link.springer.com/chapter/10.1007/978-3-030-33778-0_24> ;
    skos:altLabel "Variance-based Feature Importance of Artificial Neural Networks" ;
    skos:definition "" ;
    skos:prefLabel "VarImpVIANN" .

:VariationalDropout a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1512.05287v5> ;
    rdfs:seeAlso <https://github.com/salesforce/awd-lstm-lm/blob/32fcb42562aeb5c7e6c9dec3f2a3baaaf68a5cb5/weight_drop.py#L5> ;
    skos:definition "**Variational Dropout** is a regularization technique based on [dropout](https://paperswithcode.com/method/dropout), but uses a variational inference grounded approach. In Variational Dropout, we repeat the same dropout mask at each time step for both inputs, outputs, and recurrent layers (drop the same network units at each time step). This is in contrast to ordinary Dropout where different dropout masks are sampled at each time step for the inputs and outputs alone." ;
    skos:prefLabel "Variational Dropout" .

:VariationalEntanglementDetection a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2012.14311v1> ;
    skos:definition "**Variational Entanglement Detection** is a variational quantum algorithm which uses criteria based on positive maps as a bridge and works as follows. Given an unknown target bipartite quantum state, it firstly decomposes the chosen positive map into a linear combination of NISQ implementable quantum operations. Then, it variationally estimates the minimal eigenvalue of the final state, obtained by executing these quantum operations on the target state and averaging the output states. Deterministic and probabilistic methods are proposed to compute the average. At last, it asserts that the target state is entangled if the optimized minimal eigenvalue is negative. VLNE builds upon a linear decomposition of the transpose map into Pauli terms and the recently proposed trace distance estimation algorithm. It variationally estimates the well-known logarithmic negativity entanglement measure and could be applied to quantify entanglement on near-term quantum devices." ;
    skos:prefLabel "Variational Entanglement Detection" .

:VariationalInference a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2012.03715v1> ;
    skos:definition "" ;
    skos:prefLabel "Variational Inference" .

:VarifocalLoss a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2008.13367v2> ;
    skos:definition """**Varifocal Loss** is a loss function for training a dense object detector to predict the IACS, inspired by [focal loss](https://paperswithcode.com/method/focal-loss). Unlike the focal loss that deals with positives and negatives equally, Varifocal Loss treats them asymmetrically.\r
\r
$$ VFL\\left(p, q\\right) = −q\\left(q\\log\\left(p\\right) + \\left(1 − q\\right)\\log\\left(1 − p\\right)\\right) \\text{ if } q > 0 $$\r
\r
$$ VFL\\left(p, q\\right) = −\\alpha{p^{\\gamma}}\\log\\left(1-p\\right) $$\r
\r
where $p$ is the predicted IACS and $q$ is the target IoU score.\r
\r
For a positive training example, $q$ is set as the IoU between the generated bounding box and the ground-truth one\r
(gt IoU), whereas for a negative training example, the training target $q$ for all classes is $0$.""" ;
    skos:prefLabel "Varifocal Loss" .

:ViLBERT a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1908.02265v1> ;
    skos:altLabel "Vision-and-Language BERT" ;
    skos:definition "**Vision-and-Language BERT** (**ViLBERT**) is a [BERT](https://paperswithcode.com/method/bert)-based model for learning task-agnostic joint representations of image content and natural language. ViLBERT extend the popular BERT architecture to a multi-modal two-stream model, processing both visual and textual inputs in separate streams that interact through co-attentional [transformer](https://paperswithcode.com/method/transformer) layers." ;
    skos:prefLabel "ViLBERT" .

:ViLT a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2102.03334v2> ;
    skos:altLabel "Vision-and-Langauge Transformer" ;
    skos:definition "ViLT is a minimal vision-and-language pre-training transformer model where processing of visual inputs is simplified to just the same convolution-free manner that text inputs are processed. The model-specific components of ViLT require less computation than the transformer component for multimodal interactions. ViLTThe model is pre-trained on the following objectives: image text matching, masked language modeling, and word patch alignment." ;
    skos:prefLabel "ViLT" .

:ViP-DeepLab a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2012.05258v1> ;
    skos:definition "**ViP-DeepLab** is a model for depth-aware video panoptic segmentation. It extends Panoptic-[DeepLab](https://paperswithcode.com/method/deeplab) by adding a depth prediction head to perform monocular depth estimation and a next-frame instance branch which regresses to the object centers in frame $t$ for frame $t + 1$.  This allows the model to jointly perform video panoptic segmentation and monocular depth estimation." ;
    skos:prefLabel "ViP-DeepLab" .

:VideoBERT a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1904.01766v2> ;
    skos:definition "VideoBERT adapts the powerful [BERT](https://paperswithcode.com/method/bert) model to learn a joint visual-linguistic representation for video. It is used in numerous tasks, including action classification and video captioning." ;
    skos:prefLabel "VideoBERT" .

:ViewmakerNetwork a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2010.07432v2> ;
    skos:definition """**Viewmaker Network** is a type of generative model that learns to produce input-dependent views for contrastive learning. This network is trained jointly with an encoder network. The viewmaker network is trained adversarially to create views which increase the contrastive loss of the encoder network. Rather than directly outputting views for an image, the viewmaker instead outputs a stochastic perturbation that is added to the input. This perturbation is projected onto an $\\mathcal{l}\\_{p}$ sphere, controlling the effective strength of the view, similar to methods in adversarial robustness. This constrained adversarial training method enables the model to reduce the mutual information between different views while preserving useful input features for the encoder to learn from.\r
\r
Specifically, the encoder and viewmaker are optimized in alternating steps to minimize and maximize $\\mathcal{L}$, respectively. An image-to-image neural network is used as the viewmaker network, with an architecture adapted from work on style transfer. This network ingests the input image and outputs a perturbation that is constrained to an $\\ell_{1}$ sphere. The sphere's radius is determined by the volume of the input tensor times a hyperparameter $\\epsilon$, the distortion budget, which determines the strength of the applied perturbation. This perturbation is added to the input image and optionally clamped in the case of images to ensure all pixels are in $[0,1]$.""" ;
    skos:prefLabel "Viewmaker Network" .

:VirTex a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2006.06666v3> ;
    skos:definition "**VirText**, or **Visual representations from Textual annotations** is a pretraining approach using semantically dense captions to learn visual representations. First a ConvNet and [Transformer](https://paperswithcode.com/method/transformer) are jointly trained from scratch to generate natural language captions for images. Then, the learned features are transferred to downstream visual recognition tasks." ;
    skos:prefLabel "VirTex" .

:VirtualBatchNormalization a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1606.03498v1> ;
    rdfs:seeAlso <https://github.com/santi-pdp/segan/blob/c88a08d3299fe6b3627550a4fdb036b179a6537a/bnorm.py#L4> ;
    skos:definition "**Virtual Batch Normalization** is a normalization method used for training generative adversarial networks that extends batch normalization. Regular [batch normalization](https://paperswithcode.com/method/batch-normalization) causes the output of a neural network for an input example $\\mathbf{x}$ to be highly dependent on several other inputs $\\mathbf{x}'$ in the same minibatch. To avoid this problem in virtual batch normalization (VBN), each example $\\mathbf{x}$ is normalized based on the statistics collected on a reference batch of examples that are chosen once and fixed at the start of training, and on $\\mathbf{x}$ itself. The reference batch is normalized using only its own statistics. VBN is computationally expensive because it requires running forward propagation on two minibatches of data, so the authors use it only in the generator network." ;
    skos:prefLabel "Virtual Batch Normalization" .

:VirtualDataAugmentation a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2109.05793v1> ;
    skos:definition "**Virtual Data Augmentation**, or **VDA**, is a framework for robustly fine-tuning pre-trained language model. Based on the original token embeddings, a multinomial mixture for augmenting virtual data is constructed, where a masked language model guarantees the semantic relevance and the Gaussian noise provides the augmentation diversity. Furthermore, a regularized training strategy is proposed to balance the two aspects." ;
    skos:prefLabel "Virtual Data Augmentation" .

:VisTR a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2011.14503v5> ;
    skos:definition "**VisTR** is a [Transformer](https://paperswithcode.com/method/transformer) based video instance segmentation model. It views video instance segmentation as a direct end-to-end parallel sequence decoding/prediction problem. Given a video clip consisting of multiple image frames as input, VisTR outputs the sequence of masks for each instance in the video in order directly. At the core is a new, effective instance sequence matching and segmentation strategy, which supervises and segments instances at the sequence level as a whole. VisTR frames the instance segmentation and tracking in the same perspective of similarity learning, thus considerably simplifying the overall pipeline and is significantly different from existing approaches." ;
    skos:prefLabel "VisTR" .

:Visformer a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2104.12533v4> ;
    skos:definition "**Visformer**, or **Vision-friendly Transformer**, is an architecture that combines [Transformer](https://paperswithcode.com/methods/category/transformers)-based architectural features with those from [convolutional neural network](https://paperswithcode.com/methods/category/convolutional-neural-networks) architectures. Visformer adopts the stage-wise design for higher base performance. But [self-attentions](https://paperswithcode.com/method/multi-head-attention) are only utilized in the last two stages, considering that self-attention in the high-resolution stage is relatively inefficient even when the FLOPs are balanced. Visformer employs [bottleneck blocks](https://paperswithcode.com/method/bottleneck-residual-block) in the first stage and utilizes [group 3 × 3 convolutions](https://paperswithcode.com/method/grouped-convolution) in bottleneck blocks inspired by [ResNeXt](https://paperswithcode.com/method/resnext). It also introduces [BatchNorm](https://paperswithcode.com/method/batch-normalization) to patch embedding modules as in CNNs." ;
    skos:prefLabel "Visformer" .

:Vision-aidedGAN a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2112.09130v3> ;
    rdfs:seeAlso <https://github.com/nupurkmr9/vision-aided-gan> ;
    skos:definition "Vision-aided GAN training involves using pretrained computer vision models in an ensemble of discriminators to improve GAN performance. Linear separability between real and fake samples in pretrained model embeddings is used as a measure to choose the most accurate pretrained models for a dataset." ;
    skos:prefLabel "Vision-aided GAN" .

:VisionTransformer a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2010.11929v2> ;
    skos:definition "The **Vision Transformer**, or **ViT**, is a model for image classification that employs a [Transformer](https://paperswithcode.com/method/transformer)-like architecture over patches of the image.  An image is split into fixed-size patches, each of them are then linearly embedded, position embeddings are added, and the resulting sequence of vectors is fed to a standard [Transformer](https://paperswithcode.com/method/transformer) encoder. In order to perform classification, the standard approach of adding an extra learnable “classification token” to the sequence is used." ;
    skos:prefLabel "Vision Transformer" .

:VisualAttention a skos:Concept ;
    dcterms:source <https://journals.flvc.org/FLAIRS/article/view/128538> ;
    skos:definition "" ;
    skos:prefLabel "Visual Attention" .

:VisualBERT a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1908.03557v1> ;
    skos:definition "VisualBERT aims to reuse self-attention to implicitly align elements of the input text and regions in the input image. Visual embeddings are used to model images where the representations are represented by a bounding region in an image obtained from an object detector. These visual embeddings are constructed by summing three embeddings: 1) visual feature representation, 2) a segment embedding indicate whether it is an image embedding, and 3) position embedding. Essentially, image regions and language are combined with a Transformer to allow self-attention to discover implicit alignments between language and vision. VisualBERT is trained using COCO, which consists of images paired with captions. It is pre-trained using two objectives: masked language modeling objective and sentence-image prediction task. It can then be fine-tuned on different downstream tasks." ;
    skos:prefLabel "VisualBERT" .

:VisualParsing a skos:Concept ;
    dcterms:source <https://openreview.net/forum?id=e0nZIFEpmYh> ;
    skos:definition """Visual Parsing is a vision and language pretrained model that adopts self-attention for visual feature learning where each visual token is an approximate weighted mixture of all tokens. Thus, visual parsing provides the dependencies of each visual token pair.  It helps better learning of visual relation with the language and promote inter modal alignment. The model is composed of a vision Transformer that takes an image as input and outputs the visual tokens and a multimodal Transformer. \r
It applies a linear layer and a Layer Normalization to embed the vision tokens. It follows BERT to get word embeddings. Vision and language tokens are concatenated to form the input sequences. A multi-modal Transformer is used to fuse the vision and language modality. A metric named Inter-Modality Flow (IMF) is used to quantify the interactions between two modalities.\r
Three pretraining tasks are adopted: Masked Language Modeling (MLM), Image-Text Matching (ITM), and Masked Feature Regression (MFR). MFR is a novel task that is included to mask visual tokens with similar or correlated semantics in this framework.""" ;
    skos:prefLabel "Visual Parsing" .

:VoTr a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2109.02497v2> ;
    skos:altLabel "Voxel Transformer" ;
    skos:definition """**VoTr** is a [Transformer](https://paperswithcode.com/method/transformer)-based 3D backbone for 3D object detection from point clouds. It contains a series of sparse and submanifold voxel modules. Submanifold voxel modules perform multi-head self-attention strictly on the non-empty voxels, while sparse voxel modules can extract voxel features at empty locations. Long-range relationships between voxels are captured via self-attention.\r
\r
Given the fact that non-empty voxels are naturally sparse but numerous, directly applying standard Transformer on voxels is non-trivial. To this end, VoTr uses a sparse voxel module and a submanifold voxel module, which can operate on the empty and non-empty voxel positions effectively. To further enlarge the attention range while maintaining comparable computational overhead to the convolutional counterparts, two attention mechanisms are used for [multi-head attention](https://paperswithcode.com/method/multi-head-attention) in those two modules: Local Attention and Dilated Attention. Furthermore [Fast Voxel Query](https://paperswithcode.com/method/fast-voxel-query) is used to accelerate the querying process in multi-head attention.""" ;
    skos:prefLabel "VoTr" .

:VoVNet a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1904.09730v1> ;
    rdfs:seeAlso <https://github.com/osmr/imgclsmob/blob/3197ca90e0270c01e553e4091fc37104718ad822/pytorch/pytorchcv/models/vovnet.py#L110> ;
    skos:definition "**VoVNet** is a convolutional neural network that seeks to make [DenseNet](https://paperswithcode.com/method/densenet) more efficient by concatenating all features only once in the last feature map, which makes input size constant and enables enlarging new output channel. In the Figure to the right, $F$ represents a [convolution](https://paperswithcode.com/method/convolution) layer and $\\otimes$ indicates concatenation." ;
    skos:prefLabel "VoVNet" .

:VoVNetV2 a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1911.06667v6> ;
    rdfs:seeAlso <https://github.com/youngwanLEE/CenterMask/blob/2a46f03f2bfdae4e4de3758bd7d04d938e7d91ec/maskrcnn_benchmark/modeling/backbone/vovnet.py#L226> ;
    skos:definition "**VoVNetV2** is a convolutional neural network that improves upon [VoVNet](https://paperswithcode.com/method/vovnet) with two effective strategies: (1) [residual connection](https://paperswithcode.com/method/residual-connection) for alleviating the optimization problem of larger VoVNets and (2) effective Squeeze-Excitation (eSE) dealing with the channel information loss problem of the original squeeze-and-excitation module." ;
    skos:prefLabel "VoVNetV2" .

:VocGAN a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2007.15256v1> ;
    skos:definition "Please enter a description about the method here" ;
    skos:prefLabel "VocGAN" .

:VoiceFilter-Lite a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2009.04323v1> ;
    skos:definition "**VoiceFilter-Lite** is a single-channel source separation model that runs on the device to preserve only the speech signals from a target user, as part of a streaming speech recognition system. In this architecture, the voice filtering model operates as a frame-by-frame frontend signal processor to enhance the features consumed by the speech recognizer, without reconstructing audio signals from the features. The key contributions are (1) A system to perform speech separation directly on ASR input features; (2) An asymmetric loss function to penalize oversuppression during training, to make the model harmless under various acoustic environments, (3) An adaptive suppression strength mechanism to adapt to different noise conditions." ;
    skos:prefLabel "VoiceFilter-Lite" .

:Vokenization a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2010.06775v1> ;
    skos:definition """**Vokenization** is an approach for extrapolating multimodal alignments to language-only data by contextually mapping language tokens to their related images ("vokens") by retrieval. Instead of directly supervising the language model with visually grounded language datasets (e.g., MS COCO) these relative small datasets are used to train the vokenization processor (i.e. the vokenizer). Vokens are generated for large language corpora (e.g., English Wikipedia), and the visually-supervised language model takes the\r
input supervision from these large datasets, thus bridging the gap between different data sources.""" ;
    skos:prefLabel "Vokenization" .

:VoxelR-CNN a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2012.15712v2> ;
    skos:definition """**Voxel R-CNN** is a voxel-based two stage framework for 3D object detection. It consists of a 3D backbone network, a 2D bird-eye-view (BEV) Region Proposal Network and a detect head. Voxel RoI Pooling is devised to extract RoI features directly from raw features for further refinement. \r
\r
End-to-end, the point clouds are first divided into regular voxels and fed into the 3D backbone network for feature extraction. Then, the 3D feature volumes are converted into BEV representation, on which the 2D backbone and [RPN](https://paperswithcode.com/method/rpn) are applied for region proposal generation. Subsequently, [Voxel RoI Pooling](https://paperswithcode.com/method/voxel-roi-pooling) directly extracts RoI features from the 3D feature volumes. Finally the RoI features are exploited in the detect head for further box refinement.""" ;
    skos:prefLabel "Voxel R-CNN" .

:VoxelRoIPooling a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2012.15712v2> ;
    skos:definition """**Voxel RoI Pooling** is a RoI feature extractor extracts RoI features directly from voxel features for further refinement. It starts by dividing a region proposal into $G \\times G \\times G$ regular sub-voxels. The center point is taken as the grid point of the corresponding sub-voxel. Since $3 D$ feature volumes are extremely sparse (non-empty voxels account for $<3 \\%$ spaces), we cannot directly utilize max pooling over features of each sub-voxel. Instead, features are integrated from neighboring voxels into the grid points for feature extraction. Specifically, given a grid point $g\\_{i}$, we first exploit voxel query to group a set of neighboring voxels $\\Gamma\\_{i}=\\left\\(\\mathbf{v}\\_{i}^{1}, \\mathbf{v}\\_{i}^{2}, \\cdots, \\mathbf{v}\\_{i}^{K}\\right\\) .$ Then, we aggregate the neighboring voxel features with a [PointNet](https://paperswithcode.com/method/pointnet) module $\\mathrm{a}$ as:\r
\r
$$\r
\\mathbf{\\eta}\\_{i}=\\max _{k=1,2, \\cdots, K}\\left\\(\\Psi\\left(\\left[\\mathbf{v}\\_{i}^{k}-\\mathbf{g}\\_{i} ; \\mathbf{\\phi}\\_{i}^{k}\\right]\\right)\\right\\)\r
$$\r
\r
where $\\mathbf{v}\\_{i}-\\mathbf{g}\\_{i}$ represents the relative coordinates, $\\mathbf{\\phi}\\_{i}^{k}$ is the voxel feature of $\\mathbf{v}\\_{i}^{k}$, and $\\Psi(\\cdot)$ indicates an MLP. The [max pooling](https://paperswithcode.com/method/max-pooling) operation $\\max (\\cdot)$ is performed along the channels to obtain the aggregated feature vector $\\eta_{i} .$ Particularly, Voxel RoI pooling is exploited to extract voxel features from the 3D feature volumes out of the last two stages in the $3 \\mathrm{D}$ backbone network. And for each stage, two Manhattan distance thresholds are set to group voxels with multiple scales. Then, we concatenate the aggregated features pooled from different stages and scales to obtain the RoI features.""" ;
    skos:prefLabel "Voxel RoI Pooling" .

:WEGL a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2006.09430v2> ;
    skos:altLabel "Wasserstein Embedding for Graph Learning" ;
    skos:definition "Please enter a description here" ;
    skos:prefLabel "WEGL" .

:WFST a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2104.05055v2> ;
    skos:altLabel "weighted finite state transducer" ;
    skos:definition "" ;
    skos:prefLabel "WFST" .

:WGAN a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1701.07875v3> ;
    rdfs:seeAlso <https://github.com/daheyinyin/wgan> ;
    skos:altLabel "Wasserstein GAN" ;
    skos:definition "**Wasserstein GAN**, or **WGAN**, is a type of generative adversarial network that minimizes an approximation of the Earth-Mover's distance (EM) rather than the Jensen-Shannon divergence as in the original [GAN](https://paperswithcode.com/method/gan) formulation. It leads to more stable training than original GANs with less evidence of mode collapse, as well as meaningful curves that can be used for debugging and searching hyperparameters." ;
    skos:prefLabel "WGAN" .

:WGAN-GPLoss a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1704.00028v3> ;
    rdfs:seeAlso <https://github.com/eriklindernoren/PyTorch-GAN/blob/a163b82beff3d01688d8315a3fd39080400e7c01/implementations/wgan_gp/wgan_gp.py#L171> ;
    skos:definition """**Wasserstein Gradient Penalty Loss**, or **WGAN-GP Loss**, is a loss used for generative adversarial networks that augments the Wasserstein loss with a gradient norm penalty for random samples $\\mathbf{\\hat{x}} \\sim \\mathbb{P}\\_{\\hat{\\mathbf{x}}}$ to achieve Lipschitz continuity:\r
\r
$$ L = \\mathbb{E}\\_{\\mathbf{\\hat{x}} \\sim \\mathbb{P}\\_{g}}\\left[D\\left(\\tilde{\\mathbf{x}}\\right)\\right] - \\mathbb{E}\\_{\\mathbf{x} \\sim \\mathbb{P}\\_{r}}\\left[D\\left(\\mathbf{x}\\right)\\right] + \\lambda\\mathbb{E}\\_{\\mathbf{\\hat{x}} \\sim \\mathbb{P}\\_{\\hat{\\mathbf{x}}}}\\left[\\left(||\\nabla\\_{\\tilde{\\mathbf{x}}}D\\left(\\mathbf{\\tilde{x}}\\right)||\\_{2}-1\\right)^{2}\\right]$$\r
\r
It was introduced as part of the [WGAN-GP](https://paperswithcode.com/method/wgan-gp) overall model.""" ;
    skos:prefLabel "WGAN-GP Loss" .

:WGANGP a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1704.00028v3> ;
    rdfs:seeAlso <https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/wgan_gp/wgan_gp.py> ;
    skos:altLabel "Wasserstein GAN (Gradient Penalty)" ;
    skos:definition """**Wasserstein GAN + Gradient Penalty**, or **WGAN-GP**, is a generative adversarial network that uses the Wasserstein loss formulation plus a gradient norm penalty to achieve Lipschitz continuity.\r
\r
The original [WGAN](https://paperswithcode.com/method/wgan) uses weight clipping to achieve 1-Lipschitz functions, but this can lead to undesirable behaviour by creating pathological value surfaces and capacity underuse, as well as gradient explosion/vanishing without careful tuning of the weight clipping parameter $c$.\r
\r
A Gradient Penalty is a soft version of the Lipschitz constraint, which follows from the fact that functions are 1-Lipschitz iff the gradients are of norm at most 1 everywhere. The squared difference from norm 1 is used as the gradient penalty.""" ;
    skos:prefLabel "WGAN GP" .

:WIPA a skos:Concept ;
    rdfs:seeAlso <https://github.com/hamidreza-dastmalchi/WIPA-Face-Super-Resolution> ;
    skos:altLabel "Wavelet-integrated Identity Preserving Adversarial Network for face super-resolution" ;
    skos:definition """# WIPA: Wavelet-integrated, Identity Preserving, Adversarial network for Face Super-resolution\r
Pytorch implementation of WIPA: Super-resolution of very low-resolution face images with a **W**avelet Integrated, **I**dentity **P**reserving, **A**dversarial Network. \r
# Paper:\r
[Super-resolution of very low-resolution face images with a Wavelet Integrated, Identity Preserving, Adversarial Network](https://www.sciencedirect.com/science/article/abs/pii/S0923596522000753?dgcid=coauthor).\r
You can download the pre-proof version of the article [here](https://drive.google.com/file/d/1GHWiCcScPF1PK4xozoRf-88Rytom-kvl/view?usp=sharing) but  please refer to the origital manuscript for citation.\r
## Citation\r
If you find this work useful for your research, please consider citing our paper:\r
```\r
@article{DASTMALCHI2022116755,\r
title = {Super-resolution of very low-resolution face images with a wavelet integrated, identity preserving, adversarial network},\r
journal = {Signal Processing: Image Communication},\r
volume = {107},\r
pages = {116755},\r
year = {2022},\r
issn = {0923-5965},\r
doi = {https://doi.org/10.1016/j.image.2022.116755},\r
url = {https://www.sciencedirect.com/science/article/pii/S0923596522000753},\r
author = {Hamidreza Dastmalchi and Hassan Aghaeinia},\r
keywords = {Super-resolution, Wavelet prediction, Generative Adversarial Networks, Face Hallucination, Identity preserving, Perceptual quality},\r
```\r
## Linkdin Profile:\r
**Hamidreza Dastmalchi linkdin profile:**\r
https://www.linkedin.com/in/hamidreza-dastmalchi-80bb4574/\r
## WIPA Algorithm\r
we present **Wavelet\r
Prediction blocks** attached to a **Baseline CNN network** to predict wavelet missing details of facial images. The\r
extracted wavelet coefficients are concatenated with original feature maps in different scales to recover fine\r
details. Unlike other wavelet-based FH methods, this algorithm exploits the wavelet-enriched feature maps as\r
complementary information to facilitate the hallucination task. We introduce a **wavelet prediction loss** to push\r
the network to generate wavelet coefficients. In addition to the wavelet-domain cost function, a combination of\r
**perceptual**, **adversarial**, and **identity loss** functions has been utilized to achieve low-distortion and perceptually\r
high-quality images while maintaining identity. The training scheme of the Wavelet-Integrated network with the combination of five loss terms is shown as below:\r
<p align="center">\r
  <img width="500" src="./block-diagram/WIPA-Training-Scheme.jpg">\r
</p>\r
\r
## Datasets\r
The [Celebrity dataset](https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html) used for training the proposed FH algorithm. The database contains more than 200 K different face images under significant pose, illumination, and expression variations. In our experiment, two distinct groups of 20 thousand images are randomly selected from the CelebA dataset as our train and test dataset. In order to test the generalizing capacity of the method, we have further evaluated the performance of the proposed approach on [FW](http://vis-www.cs.umass.edu/lfw/) and [Helen dataset](http://www.ifp.illinois.edu/~vuongle2/helen/) too. All the testing and training images are roughly aligned using similarity transformation with landmarks detected by the well-known MTCNN network. The images are rescaled to the size of 128 × 128. The corresponding LR images are also constructed by down-sampling the HR images using bicubic interpolation. The experiments are accomplished in two different **scaling** factors of 8X and 16X with LR images of size 16 × 16 and 8 × 8, respectively.\r
 **Before starting to train or test the network**, you must put the training images in the corresponding folders:\r
- Put training images in “.\\data\\train” directory.\r
- Put celeba test images in “.\\data\\test\\celeba” , lfw test images in “.\\data\\test\\lfw” and helen test images in “.\\data\\test\\helen”.\r
\r
## Pretrained Weights\r
The pretrained weights can be downloaded [here](https://drive.google.com/drive/folders/18V1kPDHW6F05L0xOOODNHZHO566SA6iC?usp=sharing).\r
\r
## Code\r
The codes are consisted of two main files: the **main.py** file for training the network and the **test.py** file for evaluating the algorithm with different metrics like PSNR, SSIM and verification rate.\r
### Training \r
To train the network, simply run this code in Anaconda terminal:\r
```\r
>>python main.py\r
```\r
We designed different input arguments for controlling the training procedure. Please use --help command to see the available input arguments. \r
\r
#### Example: \r
For example, to train the wavelet-integrated network through GPU with scale factor of 8, without having pre-trained model coefficients, with learning rate of 5e-5, you can simply run the following code in the terminal:\r
```\r
python main.py –scale 8 –wi_net “” –disc_net “” –wavelet_integrated True –lr 0.00005\r
```\r
\r
### Testing\r
for evaluating (testing), simply run the following code in terminal:\r
```\r
>>python test.py\r
```\r
We have also developed different options as input arguments to control the testing procedure. You can evaluate psnr, ssim, fid score and also verification rate by the “test.py” file. To do this, you have to put the test images in the corresponding folders in data root at first.\r
\r
#### Example: \r
For example, to evaluate the psnr and ssim of a wavelet-integrated pretrained model in scale of 8 and save the super-resolved results in folder of “./results/celeba”, you can write the following code in the command window:\r
```\r
>> test.py --wavelet_integrated True --scale 8 --wi_net gen_net_8x --save_flag True --save_folder ./results/celeba --metrics psnr ssim\r
```\r
To estimate the fid score, you have to produce the super-resolved test images first. Therefore, if you have not generated the super-resolved images, you have to call –metrics psnr ssim with fid simultaneously. You can also add the acc option to the metrics to evaluate the verification rate of the model:\r
```\r
>>python test.py --wavelet_integrated True --scale 8 --wi_net gen_net_8x --save_flag True --save_folder ./results/celeba --metrics psnr ssim fid acc\r
```\r
### Demo \r
In addition, we have developed a “demo.py” python file to demonstrate the results of some sample images in the “./sample_images/gt” directory. To run the demo file, simply write the following code in the terminal:\r
```\r
>>python demo.py\r
```\r
By default, the images of “./sample_images/gt” folder will be super-resolved by the wavelet-integrated network by a scale factor of 8 and the results will be saved in the “./sample_images/sr” folder. To change the scaling factor, one must alter not only the –scale option but also the corresponding –wi_net argument to import the relevant pretrained state dictionary.""" ;
    skos:prefLabel "WIPA" .

:WRQE a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2003.01966v7> ;
    skos:altLabel "Weighted Recurrent Quality Enhancement" ;
    skos:definition "**Weighted Recurrent Quality Enhancement**, or **WRQE**, is a recurrent quality enhancement network for video compression that takes both compressed frames and the bit stream as inputs. In the recurrent cell of WRQE, the memory and update signal are weighted by quality features to reasonably leverage multi-frame information for enhancement." ;
    skos:prefLabel "WRQE" .

:WYS a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1710.09599v2> ;
    skos:altLabel "Watch Your Step" ;
    skos:definition "" ;
    skos:prefLabel "WYS" .

:WaveGAN a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1802.04208v3> ;
    skos:definition """**WaveGAN** is a generative adversarial network for unsupervised synthesis of raw-waveform audio (as opposed to image-like spectrograms). \r
\r
The WaveGAN architecture is based off [DCGAN](https://paperswithcode.com/method/dcgan). The DCGAN generator uses the [transposed convolution](https://paperswithcode.com/method/transposed-convolution) operation to iteratively upsample low-resolution feature maps into a high-resolution image. WaveGAN modifies this transposed [convolution](https://paperswithcode.com/method/convolution) operation to widen its receptive field, using a longer one-dimensional filters of length 25 instead of two-dimensional filters of size 5x5, and upsampling by a factor of 4 instead of 2 at each layer. The discriminator is modified in a similar way, using length-25 filters in one dimension and increasing stride\r
from 2 to 4. These changes result in WaveGAN having the same number of parameters, numerical\r
operations, and output dimensionality as DCGAN. An additional layer is added afterwards to allow for more audio samples. Further changes include:\r
\r
1. Flattening 2D convolutions into 1D (e.g. 5x5 2D conv becomes length-25 1D).\r
2. Increasing the stride factor for all convolutions (e.g. stride 2x2 becomes stride 4).\r
3. Removing [batch normalization](https://paperswithcode.com/method/batch-normalization) from the generator and discriminator.\r
4. Training using the [WGAN](https://paperswithcode.com/method/wgan)-GP strategy.""" ;
    skos:prefLabel "WaveGAN" .

:WaveGlow a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1811.00002v1> ;
    skos:definition "**WaveGlow** is a flow-based generative model that generates audio by sampling from a distribution. Specifically samples are taken from a zero mean spherical Gaussian with the same number of dimensions as our desired output, and those samples are put through a series of layers that transforms the simple distribution to one which has the desired distribution." ;
    skos:prefLabel "WaveGlow" .

:WaveGrad a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2009.00713v2> ;
    rdfs:seeAlso <https://github.com/lmnt-com/wavegrad/blob/master/src/wavegrad/model.py> ;
    skos:definition "**WaveGrad** is a conditional model for waveform generation through estimating gradients of the data density. This model is built on the prior work on score matching and diffusion probabilistic models. It starts from Gaussian white noise and iteratively refines the signal via a gradient-based sampler conditioned on the mel-spectrogram. WaveGrad is non-autoregressive, and requires only a constant number of generation steps during inference. It can use as few as 6 iterations to generate high fidelity audio samples." ;
    skos:prefLabel "WaveGrad" .

:WaveGradDBlock a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2009.00713v2> ;
    skos:definition "**WaveGrad DBlocks** are used to downsample the temporal dimension of noisy waveform in [WaveGrad](https://paperswithcode.com/method/wavegrad). They are similar to UBlocks except that only one [residual block](https://paperswithcode.com/method/residual-block) is included. The dilation factors are 1, 2, 4 in the main branch. Orthogonal initialization is used." ;
    skos:prefLabel "WaveGrad DBlock" .

:WaveGradUBlock a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2009.00713v2> ;
    skos:definition "The **WaveGrad UBlock** is used for upsampling in [WaveGrad](https://paperswithcode.com/method/wavegrad). Neural audio generation models often use large receptive field. Dilation factors of four convolutional layers are 1, 2, 1, 2 for the first two UBlocks and 1, 2, 4, 8 for the rest. Orthogonal initialization is used." ;
    skos:prefLabel "WaveGrad UBlock" .

:WaveNet a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1609.03499v2> ;
    skos:definition """**WaveNet** is an audio generative model based on the [PixelCNN](https://paperswithcode.com/method/pixelcnn) architecture. In order to deal with long-range temporal dependencies needed for raw audio generation, architectures are developed based on dilated causal convolutions, which exhibit very large receptive fields.\r
\r
The joint probability of a waveform $\\vec{x} = \\{ x_1, \\dots, x_T \\}$ is factorised as a product of conditional probabilities as follows:\r
\r
$$p\\left(\\vec{x}\\right) = \\prod_{t=1}^{T} p\\left(x_t \\mid x_1, \\dots ,x_{t-1}\\right)$$\r
\r
Each audio sample $x_t$ is therefore conditioned on the samples at all previous timesteps.""" ;
    skos:prefLabel "WaveNet" .

:WaveRNN a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1802.08435v2> ;
    skos:definition """**WaveRNN** is a single-layer recurrent neural network for audio generation that is designed efficiently predict 16-bit raw audio samples.\r
\r
The overall computation in the WaveRNN is as follows (biases omitted for brevity):\r
\r
$$ \\mathbf{x}\\_{t} = \\left[\\mathbf{c}\\_{t−1},\\mathbf{f}\\_{t−1}, \\mathbf{c}\\_{t}\\right] $$\r
\r
$$ \\mathbf{u}\\_{t} = \\sigma\\left(\\mathbf{R}\\_{u}\\mathbf{h}\\_{t-1} + \\mathbf{I}^{*}\\_{u}\\mathbf{x}\\_{t}\\right) $$\r
\r
$$ \\mathbf{r}\\_{t} = \\sigma\\left(\\mathbf{R}\\_{r}\\mathbf{h}\\_{t-1} + \\mathbf{I}^{*}\\_{r}\\mathbf{x}\\_{t}\\right) $$\r
\r
$$ \\mathbf{e}\\_{t} = \\tau\\left(\\mathbf{r}\\_{t} \\odot \\left(\\mathbf{R}\\_{e}\\mathbf{h}\\_{t-1}\\right) + \\mathbf{I}^{*}\\_{e}\\mathbf{x}\\_{t} \\right) $$\r
\r
$$ \\mathbf{h}\\_{t} = \\mathbf{u}\\_{t} \\cdot \\mathbf{h}\\_{t-1} + \\left(1-\\mathbf{u}\\_{t}\\right) \\cdot \\mathbf{e}\\_{t} $$\r
\r
$$ \\mathbf{y}\\_{c}, \\mathbf{y}\\_{f} = \\text{split}\\left(\\mathbf{h}\\_{t}\\right) $$\r
\r
$$ P\\left(\\mathbf{c}\\_{t}\\right) = \\text{softmax}\\left(\\mathbf{O}\\_{2}\\text{relu}\\left(\\mathbf{O}\\_{1}\\mathbf{y}\\_{c}\\right)\\right) $$\r
\r
$$ P\\left(\\mathbf{f}\\_{t}\\right) = \\text{softmax}\\left(\\mathbf{O}\\_{4}\\text{relu}\\left(\\mathbf{O}\\_{3}\\mathbf{y}\\_{f}\\right)\\right) $$\r
\r
where the $*$ indicates a masked matrix whereby the last coarse input $\\mathbf{c}\\_{t}$ is only connected to the fine part of the states $\\mathbf{u}\\_{t}$, $\\mathbf{r}\\_{t}$, $\\mathbf{e}\\_{t}$ and $\\mathbf{h}\\_{t}$ and thus only affects the fine output $\\mathbf{y}\\_{f}$. The coarse and fine parts $\\mathbf{c}\\_{t}$ and $\\mathbf{f}\\_{t}$ are encoded as scalars in $\\left[0, 255\\right]$ and scaled to the interval $\\left[−1, 1\\right]$. The matrix $\\mathbf{R}$ formed from the matrices $\\mathbf{R}\\_{u}$, $\\mathbf{R}\\_{r}$, $\\mathbf{R}\\_{e}$ is computed as a single matrix-vector product to produce the contributions to all three gates $\\mathbf{u}\\_{t}$, $mathbf{r}\\_{t}$ and $\\mathbf{e}\\_{t}$ (a variant of the [GRU cell](https://paperswithcode.com/method/gru). $\\sigma$ and $\\tau$ are the standard sigmoid and tanh non-linearities.\r
\r
Each part feeds into a [softmax](https://paperswithcode.com/method/softmax) layer over the corresponding 8 bits and the prediction of the 8 fine bits is conditioned on the 8 coarse bits. The resulting Dual Softmax layer allows for efficient prediction of 16-bit samples using two small output spaces (2 8 values each) instead of a single large output space (with 2 16 values).""" ;
    skos:prefLabel "WaveRNN" .

:WaveTTS a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2002.00417v3> ;
    skos:definition """**WaveTTS** is a [Tacotron](https://paperswithcode.com/method/tacotron)-based text-to-speech architecture that has two loss functions: 1) time-domain loss, denoted as the waveform loss, that measures the distortion between the natural and generated waveform; and 2) frequency-domain loss, that measures the Mel-scale acoustic feature loss between the natural and generated acoustic features.\r
\r
The motivation arises from [Tacotron 2](https://paperswithcode.com/method/tacotron-2). Here its feature prediction network is trained independently of the [WaveNet](https://paperswithcode.com/method/wavenet) vocoder. At run-time, the feature prediction network and WaveNet vocoder are artificially joined together. As a result, the framework suffers from the mismatch between frequency-domain acoustic features and time-domain waveform. To overcome such mismatch, WaveTTS uses a joint time-frequency domain loss for TTS that effectively improves the synthesized voice quality.""" ;
    skos:prefLabel "WaveTTS" .

:WaveVAE a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1905.08459v3> ;
    skos:definition """**WaveVAE** is a generative audio model that can be used as a vocoder in text-to-speech systems. It is a [VAE](https://paperswithcode.com/method/vae) based model that can be trained from scratch by jointly optimizing the encoder $q\\_{\\phi}\\left(\\mathbf{z}|\\mathbf{x}, \\mathbf{c}\\right)$ and decoder $p\\_{\\theta}\\left(\\mathbf{x}|\\mathbf{z}, \\mathbf{c}\\right)$, where $\\mathbf{z}$ is latent variables and $\\mathbf{c}$ is the mel spectrogram conditioner. \r
\r
The encoder of WaveVAE $q\\_{\\phi}\\left(\\mathbf{z}|\\mathbf{x}\\right)$ is parameterized by a Gaussian autoregressive [WaveNet](https://paperswithcode.com/method/wavenet) that maps the ground truth audio x into the same length latent representation $\\mathbf{z}$. The decoder $p\\_{\\theta}\\left(\\mathbf{x}|\\mathbf{z}\\right)$ is parameterized by the one-step ahead predictions from an inverse autoregressive flow.\r
\r
The training objective is the ELBO for the observed $\\mathbf{x}$ in the VAE.""" ;
    skos:prefLabel "WaveVAE" .

:WaveletDistributedTraining a skos:Concept ;
    skos:definition "**Wavelet** is an asynchronous data parallel approach that interleaves waves of training tasks on the same group of GPUs, such that tasks belong to one wave can leverage on-device memory from tasks in another wave during their memory valley period, thus boost-up the training throughput. As shown in the Figure, Wavelet divides dataparallel training tasks into two waves, namely tick-wave and tock-wave. The task launching offset is achieved by delaying the launch time of tock-wave tasks for half of a whole forward-backward training cycle. Therefore, the tock-wave tasks can directly leverage GPU memory valley period of tick-wave tasks (e.g. 0.4s-0.6s in Figure 2(a)), since backward propagation of tick-wave tasks is compute-heavy but memory is often unused. Similarly, tick-wave tasks can leverage memory valley period of tock-wave tasks in the same way." ;
    skos:prefLabel "Wavelet Distributed Training" .

:WeightDecay a skos:Concept ;
    skos:definition """**Weight Decay**, or **$L_{2}$ Regularization**, is a regularization technique applied to the weights of a neural network. We minimize a loss function compromising both the primary loss function and a penalty on the $L\\_{2}$ Norm of the weights:\r
\r
$$L\\_{new}\\left(w\\right) = L\\_{original}\\left(w\\right) + \\lambda{w^{T}w}$$\r
\r
where $\\lambda$ is a value determining the strength of the penalty (encouraging smaller weights). \r
\r
Weight decay can be incorporated directly into the weight update rule, rather than just implicitly by defining it through to objective function. Often weight decay refers to the implementation where we specify it directly in the weight update rule (whereas L2 regularization is usually the implementation which is specified in the objective function).\r
\r
Image Source: Deep Learning, Goodfellow et al""" ;
    skos:prefLabel "Weight Decay" .

:WeightDemodulation a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1912.04958v2> ;
    rdfs:seeAlso <https://github.com/NVlabs/stylegan2/blob/4874628c7dfffaae01f89558c476842b475f54d5/metrics/perceptual_path_length.py#L18> ;
    skos:definition """**Weight Modulation** is an alternative to [adaptive instance normalization](https://paperswithcode.com/method/adaptive-instance-normalization) for use in generative adversarial networks, specifically it is introduced in [StyleGAN2](https://paperswithcode.com/method/stylegan2). The purpose of [instance normalization](https://paperswithcode.com/method/instance-normalization) is to remove the effect of $s$ - the scales of the features maps - from the statistics of the [convolution](https://paperswithcode.com/method/convolution)’s output feature maps. Weight modulation tries to achieve this goal more directly. Assuming that input activations are i.i.d. random variables with unit standard deviation. After modulation and convolution, the output activations have standard deviation of:\r
\r
$$ \\sigma\\_{j} = \\sqrt{{\\sum\\_{i,k}w\\_{ijk}'}^{2}} $$\r
\r
i.e., the outputs are scaled by the $L\\_{2}$ norm of the corresponding weights. The subsequent normalization aims to restore the outputs back to unit standard deviation. This can be achieved if we scale (“demodulate”) each output feature map $j$ by $1/\\sigma\\_{j}$ . Alternatively, we can again bake this into the convolution weights:\r
\r
$$ w''\\_{ijk} = w'\\_{ijk} / \\sqrt{{\\sum\\_{i, k}w'\\_{ijk}}^{2} + \\epsilon} $$\r
\r
where $\\epsilon$ is a small constant to avoid numerical issues.""" ;
    skos:prefLabel "Weight Demodulation" .

:WeightNormalization a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1602.07868v3> ;
    rdfs:seeAlso <https://github.com/pytorch/pytorch/blob/1c5c289b6218eb1026dcb5fd9738231401cfccea/torch/nn/utils/weight_norm.py#L8> ;
    skos:definition """**Weight Normalization** is a normalization method for training neural networks. It is inspired by [batch normalization](https://paperswithcode.com/method/batch-normalization), but it is a deterministic method that does not share batch normalization's property of adding noise to the gradients. It reparameterizes each $k$-dimentional weight vector $\\textbf{w}$ in terms of a parameter vector $\\textbf{v}$ and a scalar parameter $g$ and to perform stochastic gradient descent with respect to those parameters instead. Weight vectors are expressed in terms of the new parameters using:\r
\r
$$ \\textbf{w} = \\frac{g}{\\Vert\\\\textbf{v}\\Vert}\\textbf{v}$$\r
\r
where $\\textbf{v}$ is a $k$-dimensional vector, $g$ is a scalar, and $\\Vert\\textbf{v}\\Vert$ denotes the Euclidean norm of $\\textbf{v}$. This reparameterization has the effect of fixing the Euclidean norm of the weight vector $\\textbf{w}$: we now have $\\Vert\\textbf{w}\\Vert = g$, independent of the parameters $\\textbf{v}$.""" ;
    skos:prefLabel "Weight Normalization" .

:WeightStandardization a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1903.10520v2> ;
    skos:definition """**Weight Standardization** is a normalization technique that smooths the loss landscape by standardizing the weights in convolutional layers. Different from the previous normalization methods that focus on *activations*, WS considers the smoothing effects of *weights* more than just length-direction decoupling. Theoretically, WS reduces the Lipschitz constants of the loss and the gradients.\r
Hence, WS smooths the loss landscape and improves training.\r
\r
In Weight Standardization, instead of directly optimizing the loss $\\mathcal{L}$ on the original weights $\\hat{W}$, we reparameterize the weights $\\hat{W}$ as a function of $W$, i.e. $\\hat{W}=\\text{WS}(W)$, and optimize the loss $\\mathcal{L}$ on $W$ by [SGD](https://paperswithcode.com/method/sgd):\r
\r
$$\r
    \\hat{W} = \\Big[ \\hat{W}\\_{i,j}~\\big|~ \\hat{W}\\_{i,j} = \\dfrac{W\\_{i,j} - \\mu\\_{W\\_{i,\\cdot}}}{\\sigma\\_{W\\_{i,\\cdot}+\\epsilon}}\\Big]\r
$$\r
\r
$$\r
    y = \\hat{W}*x\r
$$\r
\r
where\r
\r
$$\r
    \\mu_{W\\_{i,\\cdot}} = \\dfrac{1}{I}\\sum\\_{j=1}^{I}W\\_{i, j},~~\\sigma\\_{W\\_{i,\\cdot}}=\\sqrt{\\dfrac{1}{I}\\sum\\_{i=1}^I(W\\_{i,j} - \\mu\\_{W\\_{i,\\cdot}})^2}\r
$$\r
\r
Similar to [Batch Normalization](https://paperswithcode.com/method/batch-normalization), WS controls the first and second moments of the weights of each output channel individually in convolutional layers. Note that many initialization methods also initialize the weights in some similar ways. Different from those methods, WS standardizes the weights in a differentiable way which aims to normalize gradients during back-propagation. Note that we do not have any affine transformation on $\\hat{W}$. This is because we assume that normalization layers such as BN or [GN](https://paperswithcode.com/method/group-normalization) will normalize this convolutional layer again.""" ;
    skos:prefLabel "Weight Standardization" .

:WeightTying a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1608.05859v3> ;
    skos:definition """**Weight Tying** improves the performance of language models by tying (sharing) the weights of the embedding and [softmax](https://paperswithcode.com/method/softmax) layers. This method also massively reduces the total number of parameters in the language models that it is applied to. \r
\r
Language models are typically comprised of an embedding layer, followed by a number of [Transformer](https://paperswithcode.com/method/transformer) or [LSTM](https://paperswithcode.com/method/lstm) layers, which are finally followed by a softmax layer. Embedding layers learn word representations, such that similar words (in meaning) are represented by vectors that are near each other (in cosine distance). [Press & Wolf, 2016] showed that the softmax matrix, in which every word also has a vector representation, also exhibits this property. This leads them to propose to share the softmax and embedding matrices, which is done today in nearly all language models.  \r
\r
This method was independently introduced by [Press & Wolf, 2016](https://paperswithcode.com/paper/using-the-output-embedding-to-improve) and [Inan et al, 2016](https://paperswithcode.com/paper/tying-word-vectors-and-word-classifiers-a).\r
\r
Additionally, the Press & Wolf paper proposes Three-way Weight Tying, a method for NMT models in which the embedding matrix for the source language, the embedding matrix for the target language, and the softmax matrix for the target language are all tied. That method has been adopted by the Attention Is All You Need model and many other neural machine translation models.""" ;
    skos:prefLabel "Weight Tying" .

:Weightexcitation a skos:Concept ;
    dcterms:source <https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/7039_ECCV_2020_paper.php> ;
    skos:definition "A novel built-in attention mechanism, that is complementary to all other prior attention mechanisms (e.g. squeeze and excitation, transformers) that are external (i.e., not built-in - please read paper for more details)" ;
    skos:prefLabel "Weight excitation" .

:WeightsReset a skos:Concept ;
    dcterms:source <https://www.mdpi.com/2079-3197/11/8/148> ;
    skos:definition """Weight Reset is an implicit regularization procedure that periodically resets a randomly selected portion of layer weights during the training process, according to predefined probability distributions.\r
\r
To delineate the Weight Reset procedure, a straightforward formulation is introduced. Assume $\\mathcal{B}(p)$ as a multivariate Bernoulli distribution with parameter $p$, and let's propose that $\\mathcal{D}$ is an arbitrary distribution used for initializing model weights. At specified intervals (after a certain number of training iterations, except for the last one), a random portion of the weights $W={w^l}$ from selected layers in the neural network undergoes a reset utilizing the following method:\r
$$\r
\\tilde{w}^l = w^l\\cdot (1-m) + \\xi\\cdot m,\r
$$\r
where $\\cdot$ operation is an element-wise hadamar type multiplication, $w^l$ are current weights for layer $l$, $\\tilde{w}^l$ are reset weights for this layer, $m \\sim \\mathcal{B}(p^l)$ is a resetting mask, $p^l$ is a resetting rate for a layer $l$, $\\xi \\sim \\mathcal{D}$ are new random weights.\r
\r
Evidence has indicated that Weight Reset can compete with, and in some instances, surpass traditional regularization techniques.\r
\r
Given the observable effects of the Weight Reset technique on an increasing number of weights in a model, there's a plausible hypothesis suggesting its potential association with the Double Descent phenomenon.""" ;
    skos:prefLabel "Weights Reset" .

:WenLan a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2103.06561v6> ;
    skos:definition "Proposes a two-tower pre-training model called BriVL within the cross-modal contrastive learning framework. A cross-modal pre-training model is defined based on the image-text retrieval task. The main goal is thus to learn two encoders that can embed image and text samples into the same space for effective image-text retrieval. To enforce such cross-modal embedding learning, we introduce contrastive learning with the InfoNCE loss into the BriVL model. Given text embedding, the learning objective aims to find the best image embedding from a batch of image embeddings. Similarly, for a given image embedding, the learning objective is to find the best text embedding from a batch of text embeddings. The pre-training model learns a cross-modal embedding space by jointly training the image and text encoders to maximize the cosine similarity of the image and text embeddings of the true pair for each sample in the batch while minimizing the cosine similarity of the embeddings of the other incorrect pairs." ;
    skos:prefLabel "WenLan" .

<http://w3id.org/mlso/vocab/ml_algorithm/Wide&Deep> a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1606.07792v1> ;
    skos:definition "**Wide&Deep** jointly trains wide linear models and deep neural networks to combine the benefits of memorization and generalization for real-world recommender systems. In summary, the wide component is a generalized linear model. The deep component is a feed-forward neural network. The deep and wide components are combined using a weighted sum of their output log odds as the prediction. This is then fed to a logistic loss function for joint training, which is done by back-propagating the gradients from the output to both the wide and deep part of the model simultaneously using mini-batch stochastic optimization. The AdaGrad optimizer is used for the wider part. The combined model is illustrated in the figure (center)." ;
    skos:prefLabel "Wide&Deep" .

:WideResNet a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1605.07146v4> ;
    rdfs:seeAlso <https://github.com/osmr/imgclsmob/blob/c03fa67de3c9e454e9b6d35fe9cbb6b15c28fda7/pytorch/pytorchcv/models/wrn.py#L239> ;
    skos:definition "**Wide Residual Networks** are a variant on [ResNets](https://paperswithcode.com/method/resnet) where we decrease depth and increase the width of residual networks. This is achieved through the use of wide residual blocks." ;
    skos:prefLabel "WideResNet" .

:WideResidualBlock a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1605.07146v4> ;
    rdfs:seeAlso <https://github.com/pytorch/vision/blob/1aef87d01eec2c0989458387fa04baebcc86ea7b/torchvision/models/resnet.py#L75> ;
    skos:definition "A **Wide Residual Block** is a type of [residual block](https://paperswithcode.com/method/residual-block) that utilises two conv 3x3 layers (with [dropout](https://paperswithcode.com/method/dropout)). This is wider than other variants of residual blocks (for instance [bottleneck residual blocks](https://paperswithcode.com/method/bottleneck-residual-block)). It was proposed as part of the [WideResNet](https://paperswithcode.com/method/wideresnet) CNN architecture." ;
    skos:prefLabel "Wide Residual Block" .

:Window-basedDiscriminator a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1910.06711v3> ;
    rdfs:seeAlso <https://github.com/descriptinc/melgan-neurips/blob/8d7533e2d958cf11c9c625259bec67c0ab60b724/mel2wav/modules.py#L182> ;
    skos:definition "A **Window-based Discriminator** is a type of discriminator for generative adversarial networks. It is analogous to a [PatchGAN](https://paperswithcode.com/method/patchgan) but designed for audio. While a standard [GAN](https://paperswithcode.com/method/gan) discriminator learns to classify between distributions of entire audio sequences, window-based discriminator learns to classify between distribution of small audio chunks. Since the discriminator loss is computed over the overlapping windows where each window is very large (equal to the receptive field of the discriminator), the model learns to maintain coherence across patches." ;
    skos:prefLabel "Window-based Discriminator" .

:Wizard a skos:Concept ;
    skos:altLabel "Wizard: Unsupervised goats tracking algorithm" ;
    skos:definition "Computer vision is an interesting tool for animal behavior monitoring, mainly because it limits animal handling and it can be used to record various traits using only one sensor. From previous studies, this technic has shown to be suitable for various species and behavior. However it remains challenging to collect individual information, i.e. not only to detect animals and behavior on the video frames, but also to identify them. Animal identification is a prerequisite to gather individual information in order to characterize individuals and compare them. A common solution to this problem, known as multiple objects tracking, consists in detecting the animals on each video frame, and then associate detections to a unique animal ID. Association of detections between two consecutive frames are generally made to maintain coherence of the detection locations and appearances. To extract appearance information, a common solution is to use a convolutional neural network (CNN), trained on a large dataset before running the tracking algorithm. For farmed animals, designing such network is challenging as far as large training dataset are still lacking. In this article, we proposed an innovative solution, where the CNN used to extract appearance information is parameterized using offline unsupervised training. The algorithm, named Wizard, was evaluated for the purpose of goats monitoring in outdoor conditions. 17 annotated videos were used, for a total of 4H30, with various number of animals on the video (from 3 to 8) and different level of color differences between animals. First, the ability of the algorithm to track the detected animals was evaluated. When animals were detected, the algorithm found the correct animal ID in 94.82% of the frames. When tracking and detection were evaluated together, we found that Wizard found the correct animal ID in 86.18% of the video length. In situations where the animal detection rate could be high, Wizard seems to be a suitable solution for individual behavior analysis experiments based on computer vision." ;
    skos:prefLabel "Wizard" .

:WordPiece a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1609.08144v2> ;
    skos:definition """**WordPiece** is a subword segmentation algorithm used in natural language processing.  The vocabulary is initialized with individual characters in the language, then the most frequent combinations of symbols in the vocabulary are iteratively added to the vocabulary. The process is:\r
\r
1. Initialize the word unit inventory with all the characters in the text.\r
2. Build a language model on the training data using the inventory from 1.\r
3. Generate a new word unit by combining two units out of the current word inventory to increment the word unit inventory by one. Choose the new word unit out of all the possible ones that increases the likelihood on the training data the most when added to the model.\r
4. Goto 2 until a predefined limit of word units is reached or the likelihood increase falls below a certain threshold.\r
\r
Text: [Source](https://stackoverflow.com/questions/55382596/how-is-wordpiece-tokenization-helpful-to-effectively-deal-with-rare-words-proble/55416944#55416944)\r
\r
Image: WordPiece as used in [BERT](https://paperswithcode.com/method/bert)""" ;
    skos:prefLabel "WordPiece" .

:XCiT a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2106.09681v2> ;
    skos:definition """**Cross-Covariance Image Transformers**, or **XCiT**, is a type of [vision transformer](https://paperswithcode.com/methods/category/vision-transformer) that aims to combine the accuracy of [conventional transformers](https://paperswithcode.com/methods/category/transformers) with the scalability of [convolutional architectures](https://paperswithcode.com/methods/category/convolutional-neural-networks). \r
\r
The [self-attention operation](https://paperswithcode.com/method/scaled) underlying transformers yields global interactions between all tokens, i.e. words or image patches, and enables flexible modelling of image data beyond the local interactions of convolutions. This flexibility, however, comes with a quadratic complexity in time and memory, hindering application to long sequences and high-resolution images. The authors propose a “transposed” version of self-attention called [cross-covariance attention](https://paperswithcode.com/method/cross-covariance-attention) that operates across feature channels rather than tokens, where the interactions are based on the cross-covariances matrix between keys and queries.""" ;
    skos:prefLabel "XCiT" .

:XCiTLayer a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2106.09681v2> ;
    skos:definition "An **XCiT Layer** is the main building block of the [XCiT](https://paperswithcode.com/method/xcit) architecture which uses a [cross-covariance attention]() operator as its principal operation. The XCiT layer consists of three main blocks, each preceded by [LayerNorm](https://paperswithcode.com/method/layer-normalization) and followed by a [residual connection](https://paperswithcode.com/method/residual-connection): (i) the core [cross-covariance attention](https://paperswithcode.com/method/cross-covariance-attention) (XCA) operation, (ii) the [local patch interaction](https://paperswithcode.com/method/local-patch-interaction) (LPI) module, and (iii) a [feed-forward network](https://paperswithcode.com/method/feedforward-network) (FFN). By transposing the query-key interaction, the computational complexity of XCA is linear in the number of data elements N, rather than quadratic as in conventional self-attention." ;
    skos:prefLabel "XCiT Layer" .

:XGPT a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2003.01473v2> ;
    skos:definition "XGPT is a method of cross-modal generative pre-training for image captioning designed to pre-train text-to-image caption generators through three novel generation tasks, including image-conditioned masked language modeling (IMLM), image-conditioned denoising autoencoding (IDA), and text-conditioned image feature generation (TIGF). The pre-trained XGPT can be fine-tuned without any task-specific architecture modifications and build strong image captioning models." ;
    skos:prefLabel "XGPT" .

:XGrad-CAM a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2008.02312v4> ;
    skos:definition "**XGrad-CAM**, or **Axiom-based Grad-CAM**, is a class-discriminative visualization method and able to highlight the regions belonging to the objects of interest. Two axiomatic properties are introduced in the derivation of XGrad-CAM: Sensitivity and Conservation. In particular, the proposed XGrad-CAM is still a linear combination of feature maps, but able to meet the constraints of those two axioms." ;
    skos:prefLabel "XGrad-CAM" .

:XLM a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1901.07291v1> ;
    skos:definition """**XLM** is a [Transformer](https://paperswithcode.com/method/transformer) based architecture that is pre-trained using one of three language modelling objectives:\r
\r
1. Causal Language Modeling - models the probability of a word given the previous words in a sentence.\r
2. Masked Language Modeling - the masked language modeling objective of [BERT](https://paperswithcode.com/method/bert).\r
3. Translation Language Modeling - a (new) translation language modeling objective for improving cross-lingual pre-training.\r
\r
The authors find that both the CLM and MLM approaches provide strong cross-lingual features that can be used for pretraining models.""" ;
    skos:prefLabel "XLM" .

:XLM-R a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1911.02116v2> ;
    skos:definition "XLM-R" ;
    skos:prefLabel "XLM-R" .

:XLNet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1906.08237v2> ;
    skos:definition """**XLNet** is an autoregressive [Transformer](https://paperswithcode.com/method/transformer) that leverages the best of both autoregressive language modeling and autoencoding while attempting to avoid their limitations. Instead of using a fixed forward or backward factorization order as in conventional autoregressive models, XLNet maximizes the expected log likelihood of a sequence w.r.t. all possible permutations of the factorization order. Thanks to the permutation operation, the context for each position can consist of tokens from both left and right. In expectation, each position learns to utilize contextual information from all positions, i.e., capturing bidirectional context.\r
\r
Additionally, inspired by the latest advancements in autogressive language modeling, XLNet integrates the segment recurrence mechanism and relative encoding scheme of [Transformer-XL](https://paperswithcode.com/method/transformer-xl) into pretraining, which empirically improves the performance especially for tasks involving a longer text sequence.""" ;
    skos:prefLabel "XLNet" .

:XLSR a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2006.13979v2> ;
    skos:definition "**XLSR** is a multilingual speech recognition model built on wav2vec 2.0 which is trained by solving a contrastive task over masked latent speech representations and jointly learns a quantization of the latents shared across languages. The model is fine-tuned on labeled data and experiments show that cross-lingual pretraining significantly outperforms monolingual pretraining. A shared quantization module over feature encoder representations produces multilingual quantized speech units whose embeddings are then used as targets for a [Transformer](https://paperswithcode.com/method/transformer) trained by contrastive learning. The model learns to share discrete tokens across languages, creating bridges across languages." ;
    skos:prefLabel "XLSR" .

:XavierInitialization a skos:Concept ;
    rdfs:seeAlso <https://github.com/pytorch/pytorch/blob/0adb5843766092fba584791af76383125fd0d01c/torch/nn/init.py#L289> ;
    skos:definition """**Xavier Initialization**, or **Glorot Initialization**, is an initialization scheme for neural networks. Biases are initialized be 0 and the weights $W\\_{ij}$ at each layer are initialized as:\r
\r
$$ W\\_{ij} \\sim U\\left[-\\frac{\\sqrt{6}}{\\sqrt{fan_{in} + fan_{out}}}, \\frac{\\sqrt{6}}{\\sqrt{fan_{in} + fan_{out}}}\\right] $$\r
\r
Where $U$ is a uniform distribution and $fan_{in}$ is the size of the previous layer (number of columns in $W$) and $fan_{out}$ is the size of the current layer.""" ;
    skos:prefLabel "Xavier Initialization" .

:Xception a skos:Concept ;
    dcterms:source <http://openaccess.thecvf.com/content_cvpr_2017/html/Chollet_Xception_Deep_Learning_CVPR_2017_paper.html> ;
    rdfs:seeAlso <https://github.com/keras-team/keras-applications/blob/bc89834ed36935ab4a4994446e34ff81c0d8e1b7/keras_applications/xception.py#L40> ;
    skos:definition "**Xception** is a convolutional neural network architecture that relies solely on [depthwise separable convolution](https://paperswithcode.com/method/depthwise-separable-convolution) layers." ;
    skos:prefLabel "Xception" .

:YOHO a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2109.00182v2> ;
    skos:altLabel "You Only Hypothesize Once" ;
    skos:definition "**You Only Hypothesize Once** is a local descriptor-based framework for the registration of two unaligned point clouds. The proposed descriptor achieves the rotation invariance by recent technologies of group equivariant feature learning, which brings more robustness to point density and noise. The descriptor in YOHO also has a rotation-equivariant part, which enables the estimation the registration from just one correspondence hypothesis." ;
    skos:prefLabel "YOHO" .

:YOLOP a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2108.11250v7> ;
    skos:definition """**YOLOP** is a panoptic driving perception network for handling traffic object detection, drivable area segmentation and lane detection simultaneously. It is composed of one encoder for feature extraction and three decoders to handle the specific tasks. It can be thought of a lightweight version of Tesla's HydraNet model for self-driving cars.\r
\r
A lightweight CNN, from Scaled-yolov4, is used as the encoder to extract features from the image. Then these feature maps are fed to three decoders to complete their respective tasks. The detection decoder is based on the current best-performing single-stage detection network, [YOLOv4](https://paperswithcode.com/method/yolov4),  for two main reasons: (1) The single-stage detection network is faster than the two-stage detection network. (2) The grid-based prediction mechanism of the single-stage detector is more related to the other two semantic segmentation tasks, while instance segmentation is usually combined with the region based detector as in [Mask R-CNN](https://paperswithcode.com/method/mask-r-cnn). The feature map output by the encoder incorporates semantic features of different levels and scales, and our segmentation branch can use these feature maps to complete pixel-wise semantic prediction.""" ;
    skos:prefLabel "YOLOP" .

:YOLOX a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2107.08430v2> ;
    rdfs:seeAlso <https://github.com/Megvii-BaseDetection/YOLOX> ;
    skos:definition """**YOLOX** is a single-stage object detector that makes several modifications to [YOLOv3](https://paperswithcode.com/method/yolov3) with a  [DarkNet53](https://www.paperswithcode.com/method/darknet53) backbone. Specifically, YOLO’s head is replaced with a decoupled one. For each level of [FPN](https://paperswithcode.com/method/fpn) feature, we first adopt a 1 × 1 conv layer to reduce the feature channel to 256 and then add two parallel branches with two 3 × 3 conv layers each for classification and regression tasks respectively.\r
\r
Additional changes include adding Mosaic and [MixUp](https://paperswithcode.com/method/mixup) into the augmentation strategies to boost YOLOX’s performance. The anchor mechanism is also removed so YOLOX is anchor-free. Lastly, SimOTA for label assignment  -- where label assignment is formulated as an optimal transport problem via a top-k strategy.""" ;
    skos:prefLabel "YOLOX" .

:YOLOv1 a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1506.02640v5> ;
    rdfs:seeAlso <https://github.com/pjreddie/darknet> ;
    skos:definition """**YOLOv1** is a single-stage object detection model. Object detection is framed as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. \r
\r
The network uses features from the entire image to predict each bounding box. It also predicts all bounding boxes across all classes for an image simultaneously. This means the network reasons globally about the full image and all the objects in the image.""" ;
    skos:prefLabel "YOLOv1" .

:YOLOv2 a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1612.08242v1> ;
    rdfs:seeAlso <https://github.com/pjreddie/darknet> ;
    skos:definition "**YOLOv2**, or [**YOLO9000**](https://www.youtube.com/watch?v=QsDDXSmGJZA), is a single-stage real-time object detection model. It improves upon [YOLOv1](https://paperswithcode.com/method/yolov1) in several ways, including the use of [Darknet-19](https://paperswithcode.com/method/darknet-19) as a backbone, [batch normalization](https://paperswithcode.com/method/batch-normalization), use of a high-resolution classifier, and the use of anchor boxes to predict bounding boxes, and more." ;
    skos:prefLabel "YOLOv2" .

:YOLOv3 a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1804.02767v1> ;
    rdfs:seeAlso <https://github.com/pjreddie/darknet> ;
    skos:definition "**YOLOv3** is a real-time, single-stage object detection model that builds on [YOLOv2](https://paperswithcode.com/method/yolov2) with several improvements. Improvements include the use of a new backbone network, [Darknet-53](https://paperswithcode.com/method/darknet-53) that utilises residual connections, or in the words of the author, \"those newfangled residual network stuff\", as well as some improvements to the bounding box prediction step, and use of three different scales from which to extract features (similar to an [FPN](https://paperswithcode.com/method/fpn))." ;
    skos:prefLabel "YOLOv3" .

:YOLOv4 a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2004.10934v1> ;
    rdfs:seeAlso <https://github.com/AlexeyAB/darknet> ;
    skos:definition "**YOLOv4** is a one-stage object detection model that improves on [YOLOv3](https://paperswithcode.com/method/yolov3) with several bags of tricks and modules introduced in the literature. The components section below details the tricks and modules used." ;
    skos:prefLabel "YOLOv4" .

:YellowFin a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1706.03471v2> ;
    rdfs:seeAlso <https://github.com/JianGoForIt/YellowFin/blob/847d9948a3707dde501318ba3d31db2c4c7e3e61/tuner_utils/yellowfin.py#L20> ;
    skos:definition "**YellowFin** is a learning rate and momentum tuner motivated by robustness properties and analysis of quadratic objectives. It stems from a known but obscure fact: the momentum operator's spectral radius is constant in a large subset of the hyperparameter space. For quadratic objectives, the optimizer tunes both the learning rate and the momentum to keep the hyperparameters within a region in which the convergence rate is a constant rate equal to the root momentum. This notion is extended empirically to non-convex objectives. On every iteration, YellowFin optimizes the hyperparameters to minimize a local quadratic optimization." ;
    skos:prefLabel "YellowFin" .

:Z-PNN a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2111.08334v3> ;
    skos:altLabel "Pansharpening by convolutional neural networks in the full resolution framework" ;
    skos:definition """In recent years, there has been a growing interest on deep learning-based pansharpening.\r
Research has mainly focused on architectures.\r
However, lacking a ground truth, model training is also a major issue.\r
A popular approach is to train networks in a reduced resolution domain, using the original data as ground truths.\r
The trained networks are then used on full resolution data, relying on an implicit scale invariance hypothesis.\r
Results are generally good at reduced resolution, but more questionable at full resolution.\r
\r
Here, we propose a full-resolution training framework for deep learning-based pansharpening.\r
Training takes place in the high resolution domain, relying only on the original data, with no loss of information.\r
To ensure spectral and spatial fidelity, suitable losses are defined,\r
which force the pansharpened output to be consistent with the available panchromatic and multispectral input.\r
Experiments carried out on WorldView-3, WorldView-2, and GeoEye-1 images show that methods trained with the proposed framework\r
guarantee an excellent performance in terms of both full-resolution numerical indexes and visual quality.\r
The framework is fully general, and can be used to train and fine-tune any deep learning-based pansharpening network.""" ;
    skos:prefLabel "Z-PNN" .

:ZCAWhitening a skos:Concept ;
    skos:definition """**ZCA Whitening** is an image preprocessing method that leads to a transformation of data such that the covariance matrix $\\Sigma$ is the identity matrix, leading to decorrelated features.\r
\r
Image Source: [Alex Krizhevsky](http://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf)""" ;
    skos:prefLabel "ZCA Whitening" .

:ZFNet a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1311.2901v3> ;
    rdfs:seeAlso <https://github.com/osmr/imgclsmob/blob/bf135f4d635cdc94d89ed5d8bce1f2761fb8593d/pytorch/pytorchcv/models/zfnet.py#L12> ;
    skos:definition "**ZFNet** is a classic convolutional neural network. The design was motivated by visualizing intermediate feature layers and the operation of the classifier. Compared to [AlexNet](https://paperswithcode.com/method/alexnet), the filter sizes are reduced and the stride of the convolutions are reduced." ;
    skos:prefLabel "ZFNet" .

:ZeRO a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1910.02054v3> ;
    skos:definition "**Zero Redundancy Optimizer (ZeRO)** is a sharded data parallel method for distributed training. ZeRODP removes the memory state redundancies across data-parallel processes by partitioning the model states instead of replicating them, and it retains the compute/communication efficiency by retaining the computational granularity and communication volume of DP using a dynamic communication schedule during training." ;
    skos:prefLabel "ZeRO" .

:ZeRO-Infinity a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2104.07857v1> ;
    skos:definition "**ZeRO-Infinity** is a sharded data parallel system that extends [ZeRO](https://paperswithcode.com/method/zero) with new innovations in heterogeneous memory access called the infinity offload engine. This allows ZeRO-Infinity to support massive model sizes on limited GPU resources by exploiting CPU and NVMe memory simultaneously. In addition, ZeRO-Infinity also introduces a novel GPU memory optimization technique called memory-centric tiling to support extremely large individual layers that would otherwise not fit in GPU memory even one layer at a time." ;
    skos:prefLabel "ZeRO-Infinity" .

:ZeRO-Offload a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2101.06840v1> ;
    skos:definition "ZeRO-Offload is a sharded data parallel method for distributed training. It exploits both CPU memory and compute for offloading, while offering a clear path towards efficiently scaling on multiple GPUs by working with [ZeRO-powered data parallelism](https://www.paperswithcode.com/method/zero). The symbiosis allows ZeRO-Offload to maintain a single copy of the optimizer states on the CPU memory regardless of the data parallel degree. Furthermore, it keeps the aggregate communication volume between GPU and CPU, as well as the aggregate CPU computation a constant regardless of data parallelism, allowing ZeRO-Offload to effectively utilize the linear increase in CPU compute with the increase in the data parallelism degree." ;
    skos:prefLabel "ZeRO-Offload" .

:Zero-paddedShortcutConnection a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1610.02915v4> ;
    rdfs:seeAlso <https://github.com/dyhan0920/PyramidNet-PyTorch/blob/5a0b32f43d79024a0d9cd2d1851f07e6355daea2/PyramidNet.py#L50> ;
    skos:definition "A **Zero-padded Shortcut Connection** is a type of [residual connection](https://paperswithcode.com/method/residual-connection) used in the [PyramidNet](https://paperswithcode.com/method/pyramidnet) architecture. For PyramidNets, identity mapping alone cannot be used for a shortcut because the feature map dimension differs among individual residual units. Therefore, only a zero-padded shortcut or projection shortcut can be used for all the residual units. However,  a projection shortcut can hamper information propagation and lead to optimization problems, especially for very deep networks. On the other hand, the zero-padded shortcut avoids the overfitting problem because no additional parameters exist." ;
    skos:prefLabel "Zero-padded Shortcut Connection" .

:Zoneout a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1606.01305v4> ;
    rdfs:seeAlso <https://github.com/WelkinYang/Zoneout-Pytorch/blob/23c85cce75c17e2648acae84f215bfe384d84a62/ZoneoutRNN.py#L6> ;
    skos:definition """**Zoneout** is a  method for regularizing [RNNs](https://paperswithcode.com/methods/category/recurrent-neural-networks). At each timestep, zoneout stochastically forces some hidden units to maintain their previous values. Like [dropout](https://paperswithcode.com/method/dropout), zoneout uses random noise to train a pseudo-ensemble, improving generalization.\r
But by preserving instead of dropping hidden units, gradient information and state information are more readily propagated through time, as in feedforward [stochastic depth](https://paperswithcode.com/method/stochastic-depth) networks.""" ;
    skos:prefLabel "Zoneout" .

:ZoomNet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2007.11858v1> ;
    skos:definition "**ZoomNet** is a 2D human whole-body pose estimation technique. It aims to localize dense landmarks on the entire human body including face, hands, body, and feet. ZoomNet follows the top-down paradigm. Given a human bounding box of each person, ZoomNet first localizes the easy-to-detect body keypoints and estimates the rough position of hands and face. Then it zooms in to focus on the hand/face areas and predicts keypoints using features with higher resolution for accurate localization. Unlike previous approaches which usually assemble multiple networks, ZoomNet has a single network that is end-to-end trainable. It unifies five network heads including the human body pose estimator, hand and face detectors, and hand and face pose estimators into a single network with shared low-level features." ;
    skos:prefLabel "ZoomNet" .

:b2btransferlearning a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2301.10663v2> ;
    skos:altLabel "building to building transfer learning" ;
    skos:definition "using transfer learning to transfer knowledge from one building to predict the energy consumption of another building with scarce data" ;
    skos:prefLabel "b2b transfer learning" .

:bilayerdecoupling a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2103.12340v1> ;
    skos:altLabel "bilayer convolutional neural network" ;
    skos:definition "" ;
    skos:prefLabel "bilayer decoupling" .

:cVAE a skos:Concept ;
    dcterms:source <http://papers.nips.cc/paper/5775-learning-structured-output-representation-using-deep-conditional-generative-models> ;
    skos:altLabel "Conditional Variational Auto Encoder" ;
    skos:definition "" ;
    skos:prefLabel "cVAE" .

:classifier-guidance a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2105.05233v4> ;
    skos:definition "" ;
    skos:prefLabel "classifier-guidance" .

:context2vec a skos:Concept ;
    dcterms:source <https://aclanthology.org/K16-1006> ;
    skos:definition """**context2vec** is an unsupervised model for learning generic context embedding of wide sentential contexts, using a bidirectional [LSTM](https://paperswithcode.com/method/lstm). A large plain text corpora is trained on to learn a neural model that embeds entire sentential contexts and target words in the same low-dimensional space, which\r
is optimized to reflect inter-dependencies between targets and their entire sentential context as a whole. \r
\r
In contrast to word2vec that use context modeling mostly internally and considers the target word embeddings as their main output, the focus of context2vec is the context representation. context2vec achieves its objective by assigning similar embeddings to sentential contexts and their associated target words.""" ;
    skos:prefLabel "context2vec" .

:fastText a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1607.04606v2> ;
    skos:definition "**fastText** embeddings exploit subword information to construct word embeddings. Representations are learnt of character $n$-grams, and words represented as the sum of the $n$-gram vectors. This extends the word2vec type models with subword information. This helps the embeddings understand suffixes and prefixes. Once a word is represented using character $n$-grams, a skipgram model is trained to learn the embeddings." ;
    skos:prefLabel "fastText" .

:gCANS a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2108.10434v1> ;
    skos:altLabel "Global Coupled Adaptive Number of Shots" ;
    skos:definition "**gCANS**, or **Global Coupled Adaptive Number of Shots**, is a variational quantum algorithm for stochastic gradient descent. It adaptively allocates shots for the measurement of each gradient component at each iteration. The optimizer uses a criterion for allocating shots that incorporates information about the overall scale of the shot cost for the iteration." ;
    skos:prefLabel "gCANS" .

:gMLP a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2105.08050v2> ;
    skos:definition """**gMLP** is an [MLP](https://paperswithcode.com/methods/category/feedforward-networks)-based alternative to [Transformers](https://paperswithcode.com/methods/category/vision-transformer) without [self-attention](https://paperswithcode.com/method/scaled), which simply consists of channel projections and spatial projections with static parameterization. It is built out of basic MLP layers with gating. The model consists of a stack of $L$ blocks with identical size and structure. Let $X \\in \\mathbb{R}^{n \\times d}$ be the token representations with sequence length $n$ and dimension $d$. Each block is defined as:\r
\r
$$\r
Z=\\sigma(X U), \\quad \\tilde{Z}=s(Z), \\quad Y=\\tilde{Z} V\r
$$\r
\r
where $\\sigma$ is an activation function such as [GeLU](https://paperswithcode.com/method/gelu). $U$ and $V$ define linear projections along the channel dimension - the same as those in the FFNs of Transformers (e.g., their shapes are $768 \\times 3072$ and $3072 \\times 768$ for $\\text{BERT}_{\\text {base }}$).\r
\r
A key ingredient is $s(\\cdot)$, a layer which captures spatial interactions. When $s$ is an identity mapping, the above transformation degenerates to a regular FFN, where individual tokens are processed independently without any cross-token communication. One of the major focuses is therefore to design a good $s$ capable of capturing complex spatial interactions across tokens. This leads to the use of a [Spatial Gating Unit](https://www.paperswithcode.com/method/spatial-gating-unit) which involves a modified linear gating.\r
\r
The overall block layout is inspired by [inverted bottlenecks](https://paperswithcode.com/method/inverted-residual-block), which define $s(\\cdot)$ as a [spatial depthwise convolution](https://paperswithcode.com/method/depthwise-separable-convolution). Note, unlike Transformers, gMLP does not require position embeddings because such information will be captured in $s(\\cdot)$.""" ;
    skos:prefLabel "gMLP" .

:gSDE a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2005.05719v2> ;
    skos:altLabel "Generalized State-Dependent Exploration" ;
    skos:definition """**Generalized State-Dependent Exploration**, or **gSDE**, is an exploration method for reinforcement learning that uses more general features and re-sampling the noise periodically. \r
\r
State-Dependent Exploration (SDE) is an intermediate solution for exploration that consists in adding noise as a function of the state $s\\_{t}$, to the deterministic action $\\mu\\left(\\mathbf{s}\\_{t}\\right)$. At the beginning of an episode, the parameters $\\theta\\_{\\epsilon}$ of that exploration function are drawn from a Gaussian distribution. The resulting action $\\mathbf{a}\\_{t}$ is as follows:\r
\r
$$\r
\\mathbf{a}\\_{t}=\\mu\\left(\\mathbf{s}\\_{t} ; \\theta\\_{\\mu}\\right)+\\epsilon\\left(\\mathbf{s}\\_{t} ; \\theta\\_{\\epsilon}\\right), \\quad \\theta\\_{\\epsilon} \\sim \\mathcal{N}\\left(0, \\sigma^{2}\\right)\r
$$\r
\r
This episode-based exploration is smoother and more consistent than the unstructured step-based exploration. Thus, during one episode, instead of oscillating around a mean value, the action a for a given state $s$ will be the same.\r
\r
In the case of a linear exploration function $\\epsilon\\left(\\mathbf{s} ; \\theta\\_{\\epsilon}\\right)=\\theta\\_{\\epsilon} \\mathbf{s}$, by operation on Gaussian distributions, Rückstieß et al. show that the action element $\\mathbf{a}\\_{j}$ is normally distributed:\r
\r
$$\r
\\pi]_{j}\\left(\\mathbf{a}\\_{j} \\mid \\mathbf{s}\\right) \\sim \\mathcal{N}\\left(\\mu\\_{j}(\\mathbf{s}), \\hat{\\sigma\\_{j}}^{2}\\right)\r
$$\r
\r
where $\\hat{\\sigma}$ is a diagonal matrix with elements $\\hat{\\sigma}\\_{j}=\\sqrt{\\sum\\_{i}\\left(\\sigma\\_{i j} \\mathbf{s}\\_{i}\\right)^{2}}$.\r
\r
Because we know the policy distribution, we can obtain the derivative of the log-likelihood $\\log \\pi(\\mathbf{a} \\mid \\mathbf{s})$ with respect to the variance $\\sigma$ :\r
\r
$$\r
\\frac{\\partial \\log \\pi(\\mathbf{a} \\mid \\mathbf{s})}{\\partial \\sigma_{i j}}=\\frac{\\left(\\mathbf{a}\\_{j}-\\mu\\_{j}\\right)^{2}-\\hat{\\sigma\\_{j}}^{2}}{\\hat{\\sigma}\\_{j}^{3}} \\frac{\\mathbf{s}\\_{i}^{2} \\sigma\\_{i j}}{\\hat{\\sigma_{j}}}\r
$$\r
\r
This can be easily plugged into the likelihood ratio gradient estimator, which allows to adapt $\\sigma$ during training. SDE is therefore compatible with standard policy gradient methods, while addressing most shortcomings of the unstructured exploration.\r
\r
For gSDE, two improvements are suggested:\r
\r
1. We sample the parameters $\\theta\\_{\\epsilon}$ of the exploration function every $n$ steps instead of every episode.\r
2. Instead of the state s, we can in fact use any features. We chose policy features $\\mathbf{z}\\_{\\mu}\\left(\\mathbf{s} ; \\theta\\_{\\mathbf{z}\\_{\\mu}}\\right)$ (last layer before the deterministic output $\\left.\\mu(\\mathbf{s})=\\theta\\_{\\mu} \\mathbf{z}\\_{\\mu}\\left(\\mathbf{s} ; \\theta_{\\mathbf{z}\\_{\\mu}}\\right)\\right)$ as input to the noise function $\\epsilon\\left(\\mathbf{s} ; \\theta\\_{\\epsilon}\\right)=\\theta\\_{\\epsilon} \\mathbf{z}\\_{\\mu}(\\mathbf{s})$""" ;
    skos:prefLabel "gSDE" .

:hdxresnet a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2009.12318v2> ;
    skos:altLabel "Hybrid-deconvolution" ;
    skos:definition "A resnet-like architecture with deconvolution feature normalization (Ye et al. 2020, ICLR) layers in the first few layers for sparse low-level feature identification, and batch normalization layers in the later layers." ;
    skos:prefLabel "hdxresnet" .

:imGHUM a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2108.10842v1> ;
    skos:definition """**imGHUM** is a generative model of 3D human shape and articulated pose, represented as a signed distance function. The full body is modeled implicitly as a function zero-level-set and without the use of an explicit template mesh. We compute the signed distance $s = S\\left(\\rho, \\alpha\\right)$ and the semantics $c = C\\left(\\rho, \\alpha\\right)$ of a spatial point $\\rho$ to the surface of an articulated human shape defined by the generative latent code $\\alpha$. Using an explicit skeleton, we transform the point $\\rho$ into the normalized coordinate frames as {$\\tilde{\\rho}^{j}$} for $N = 4$ sub-part networks, modeling body, hands, and head. Each sub-model {$S^{j}$} represents a semantic signed-distance function. The sub-models are finally combined consistently using an MLP U to compute the outputs $s$ and $c$ for the full body. The multi-part pipeline builds a full body model as well as sub-part models for head and hands, jointly, in a consistent training loop. \r
\r
On the right of the Figure, we visualize the zero-level-set body surface extracted with marching cubes and the implicit correspondences to a canonical instance given by the output semantics. The semantics allows e.g. for surface coloring or texturing.""" ;
    skos:prefLabel "imGHUM" .

:k-MeansClustering a skos:Concept ;
    rdfs:seeAlso <https://cryptoabout.info> ;
    skos:definition """**k-Means Clustering** is a clustering algorithm that divides a training set into $k$ different clusters of examples that are near each other. It works by initializing $k$ different centroids {$\\mu\\left(1\\right),\\ldots,\\mu\\left(k\\right)$} to different values, then alternating between two steps until convergence:\r
\r
(i) each training example is assigned to cluster $i$ where $i$ is the index of the nearest centroid $\\mu^{(i)}$\r
\r
(ii) each centroid $\\mu^{(i)}$ is updated to the mean of all training examples $x^{(j)}$ assigned to cluster $i$.\r
\r
Text Source: Deep Learning, Goodfellow et al\r
\r
Image Source: [scikit-learn](https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html)""" ;
    skos:prefLabel "k-Means Clustering" .

:k-NN a skos:Concept ;
    skos:altLabel "k-Nearest Neighbors" ;
    skos:definition """**$k$-Nearest Neighbors** is a clustering-based algorithm for classification and regression. It is a a type of instance-based learning as it does not attempt to construct a general internal model, but simply stores instances of the training data. Prediction is computed from a simple majority vote of the nearest neighbors of each point: a query point is assigned the data class which has the most representatives within the nearest neighbors of the point.\r
\r
Source of Description and Image: [scikit-learn](https://scikit-learn.org/stable/modules/neighbors.html#classification)""" ;
    skos:prefLabel "k-NN" .

:k-SparseAutoencoder a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1312.5663v2> ;
    rdfs:seeAlso <https://github.com/snooky23/K-Sparse-AutoEncoder> ;
    skos:definition "**k-Sparse Autoencoders** are autoencoders with linear activation function, where in hidden layers only the $k$ highest activities are kept. This achieves exact sparsity in the hidden representation. Backpropagation only goes through the the top $k$ activated units. This can be achieved with a [ReLU](https://paperswithcode.com/method/relu) layer with an adjustable threshold." ;
    skos:prefLabel "k-Sparse Autoencoder" .

:lda2vec a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1605.02019v1> ;
    skos:definition """**lda2vec** builds representations over both words and documents by mixing word2vec’s skipgram architecture with Dirichlet-optimized sparse topic mixtures. \r
\r
The Skipgram Negative-Sampling (SGNS) objective of word2vec is modified to utilize document-wide feature vectors while simultaneously learning continuous document weights loading onto topic vectors. The total loss term $L$ is the sum of the Skipgram Negative Sampling Loss (SGNS) $L^{neg}\\_{ij}$ with the addition of a Dirichlet-likelihood term over document weights, $L\\_{d}$. The loss is conducted using a context vector, $\\overrightarrow{c\\_{j}}$ , pivot word vector $\\overrightarrow{w\\_{j}}$, target word vector $\\overrightarrow{w\\_{i}}$, and negatively-sampled word vector $\\overrightarrow{w\\_{l}}$:\r
\r
$$ L = L^{d} + \\Sigma\\_{ij}L^{neg}\\_{ij} $$\r
\r
$$L^{neg}\\_{ij} = \\log\\sigma\\left(c\\_{j}\\cdot\\overrightarrow{w\\_{i}}\\right) + \\sum^{n}\\_{l=0}\\sigma\\left(-\\overrightarrow{c\\_{j}}\\cdot\\overrightarrow{w\\_{l}}\\right)$$""" ;
    skos:prefLabel "lda2vec" .

:m-arcsinh a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2009.07530v1> ;
    skos:altLabel "modified arcsinh" ;
    skos:definition "" ;
    skos:prefLabel "m-arcsinh" .

:mBART a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2001.08210v2> ;
    skos:definition "**mBART** is a sequence-to-sequence denoising auto-encoder pre-trained on large-scale monolingual corpora in many languages using the [BART objective](https://paperswithcode.com/method/bart). The input texts are noised by masking phrases and permuting sentences, and a single [Transformer model](https://paperswithcode.com/method/transformer) is learned to recover the texts. Different from other pre-training approaches for machine translation, mBART pre-trains a complete autoregressive [Seq2Seq](https://paperswithcode.com/method/seq2seq) model. mBART is trained once for all languages, providing a set of parameters that can be fine-tuned for any of the language pairs in both supervised and unsupervised settings, without any task-specific or language-specific modifications or initialization schemes." ;
    skos:prefLabel "mBART" .

:mBARTHez a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2010.12321v2> ;
    skos:definition "**BARThez** is a self-supervised transfer learning model for the French language based on [BART](https://paperswithcode.com/method/bart). Compared to existing [BERT](https://paperswithcode.com/method/bert)-based French language models such as [CamemBERT](https://paperswithcode.com/paper/camembert-a-tasty-french-language-model) and [FlauBERT](https://paperswithcode.com/paper/flaubert-unsupervised-language-model-pre), BARThez is well-suited for generative tasks, since not only its encoder but also its decoder is pretrained." ;
    skos:prefLabel "mBARTHez" .

:mBERT a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1810.04805v2> ;
    skos:definition "mBERT" ;
    skos:prefLabel "mBERT" .

:mLSTM a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1609.07959v3> ;
    skos:altLabel "Multiplicative LSTM" ;
    skos:definition "A **Multiplicative LSTM (mLSTM)** is a  recurrent neural network architecture for sequence modelling that combines the long short-term memory ([LSTM](https://paperswithcode.com/method/lstm)) and multiplicative recurrent neural network ([mRNN](https://paperswithcode.com/method/mrnn)) architectures. The mRNN and LSTM architectures can be combined by adding connections from the mRNN’s intermediate state $m\\_{t}$ to each gating units in the LSTM." ;
    skos:prefLabel "mLSTM" .

:mRNN a skos:Concept ;
    skos:altLabel "Multiplicative RNN" ;
    skos:definition """A **Multiplicative RNN (mRNN)** is a type of recurrent neural network with multiplicative connections. In a standard RNN, the current input $x\\_{t}$ is first transformed via the visible-to-hidden weight matrix $W\\_{hx}$ and then contributes additively to the input for the current hidden state. An mRNN allows the current input (a character in the original example) to affect the hidden state dynamics by determining the entire hidden-to-hidden matrix (which defines the non-linear dynamics) in addition to providing an additive bias.\r
\r
To achieve this goal, the authors modify the RNN so that its hidden-to-hidden weight matrix is a (learned) function of the current input $x\\_{t}$:\r
\r
$$ h\\_{t} = \\tanh\\left(W\\_{hx}x\\_{t} + W\\_{hh}^{\\left(x\\_{y}\\right)}h\\_{t-1} + b\\_{h}\\right)$$\r
\r
$$ o\\_{t} = W\\_{oh}h\\_{t} + b\\_{o} $$\r
\r
This is the same as the equations for a standard RNN, except that $W\\_{hh}$ is replaced with $W^{(xt)}\\_{hh}$. allowing each input (character) to specify a different hidden-to-hidden weight matrix.""" ;
    skos:prefLabel "mRNN" .

:mT0 a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2211.01786v2> ;
    skos:definition "**mT0** is a Multitask prompted finetuning (MTF) variant of mT5." ;
    skos:prefLabel "mT0" .

:mT5 a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2010.11934v3> ;
    skos:definition "**mt5** is a multilingual variant of [T5](https://paperswithcode.com/method/t5) that was pre-trained on a new Common Crawl-based dataset covering $101$ languages." ;
    skos:prefLabel "mT5" .

:modReLU a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1511.06464v4> ;
    skos:definition """**modReLU** is an activation that is a modification of a [ReLU](https://paperswithcode.com/method/relu). It is a pointwise nonlinearity, $\\sigma\\_{modReLU}\\left(z\\right) : C \\rightarrow C$, which affects only the absolute value of a complex number, defined as:\r
\r
$$ \\sigma\\_{modReLU}\\left(z\\right) = \\left(|z| + b\\right)\\frac{z}{|z|} \\text{ if } |z| + b \\geq 0 $$\r
$$ \\sigma\\_{modReLU}\\left(z\\right) = 0 \\text{ if } |z| + b \\leq 0 $$\r
\r
where $b \\in \\mathbb{R}$ is a bias parameter of the nonlinearity. For a $n\\_{h}$ dimensional hidden space we learn $n\\_{h}$ nonlinearity bias parameters, one per dimension.""" ;
    skos:prefLabel "modReLU" .

:myGym a skos:Concept ;
    rdfs:seeAlso <https://github.com/incognite-lab/myGym> ;
    skos:altLabel "MyGym: Modular Toolkit for Visuomotor Robotic Tasks" ;
    skos:definition "We introduce myGym, a toolkit suitable for fast prototyping of neural networks in the area of robotic manipulation and navigation. Our toolbox is fully modular, enabling users to train their algorithms on different robots, environments, and tasks. We also include pretrained neural network modules for the real-time vision that allows training visuomotor tasks with sim2real transfer. The visual modules can be easily retrained using the dataset generation pipeline with domain augmentation and randomization. Moreover, myGym provides automatic evaluation methods and baselines that help the user to directly compare their trained model with the state-of-the-art algorithms. We additionally present a novel metric, called learnability, to compare the general learning capability of algorithms in different settings, where the complexity of the environment, robot, and the task is systematically manipulated. The learnability score tracks differences between the performance of algorithms in increasingly challenging setup conditions, and thus allows the user to compare different models in a more systematic fashion. The code is accessible at https://github.com/incognite-lab/myGym" ;
    skos:prefLabel "myGym" .

:nnFormer a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2109.03201v6> ;
    skos:definition "**nnFormer**, or **not-another transFormer**, is a semantic segmentation model with an interleaved architecture based on empirical combination of self-attention and [convolution](https://paperswithcode.com/method/convolution). Firstly, a light-weight convolutional embedding layer ahead is used ahead of [transformer](https://paperswithcode.com/method/transformer) blocks. In comparison to directly flattening raw pixels and applying 1D pre-processing, the convolutional embedding layer encodes precise (i.e., pixel-level) spatial information and provide low-level yet high-resolution 3D features. After the embedding block, transformer and convolutional down-sampling blocks are interleaved to fully entangle long-term dependencies with high-level and hierarchical object concepts at various scales, which helps improve the generalization ability and robustness of learned representations." ;
    skos:prefLabel "nnFormer" .

:node2vec a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1607.00653v1> ;
    skos:definition """**node2vec** is a framework for learning graph embeddings for nodes in graphs. Node2vec maximizes a likelihood objective over mappings which preserve neighbourhood distances in higher dimensional spaces. From an algorithm design perspective, node2vec exploits the freedom to define neighbourhoods for nodes and provide an explanation for the effect of the choice of neighborhood on the learned representations. \r
\r
For each node, node2vec simulates biased random walks based on an efficient network-aware search strategy and the nodes appearing in the random walk define neighbourhoods. The search strategy accounts for the relative influence nodes exert in a network. It also generalizes prior work alluding to naive search strategies by providing flexibility in exploring neighborhoods.""" ;
    skos:prefLabel "node2vec" .

:pixel2style2pixel a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2008.00951v2> ;
    skos:definition "**Pixel2Style2Pixel**, or **pSp**, is an image-to-image translation framework that is based on a novel encoder that directly generates a series of style vectors which are fed into a pretrained [StyleGAN](https://paperswithcode.com/method/stylegan) generator, forming the extended $\\mathcal{W+}$ latent space. Feature maps are first extracted using a standard feature pyramid over a [ResNet](https://paperswithcode.com/method/resnet) backbone. Then, for each of $18$ target styles, a small mapping network is trained to extract the learned styles from the corresponding feature map, where styles $(0-2)$ are generated from the small feature map, $(3-6)$ from the medium feature map, and $(7-18)$ from the largest feature map. The mapping network, map2style, is a small fully convolutional network, which gradually reduces spatial size using a set of 2-strided convolutions followed by [LeakyReLU](https://paperswithcode.com/method/leaky-relu) activations. Each generated 512 vector, is fed into [StyleGAN](https://paperswithcode.com/method/stylegan), starting from its matching affine transformation, $A$." ;
    skos:prefLabel "pixel2style2pixel" .

:rTPNN a skos:Concept ;
    dcterms:source <https://ieeexplore.ieee.org/document/9451553> ;
    skos:altLabel "Recurrent Trend Predictive Neural Network" ;
    skos:definition "A neural network model to automatically capture trends in time-series data for improved prediction/forecasting performance" ;
    skos:prefLabel "rTPNN" .

:reSGLD a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2008.05367v3> ;
    rdfs:seeAlso <https://github.com/gaoliyao/Replica_Exchange_Stochastic_Gradient_MCMC> ;
    skos:altLabel "Replica exchange stochastic gradient Langevin Dynamics" ;
    skos:definition "reSGLD proposes to simulate a high-temperature particle for exploration and a low-temperature particle for exploitation and allows them to swap simultaneously. Moreover, a correction term is included to avoid biases." ;
    skos:prefLabel "reSGLD" .

:rnnDrop a skos:Concept ;
    skos:definition """**rnnDrop** is a [dropout](https://paperswithcode.com/method/dropout) based regularization technique for [recurrent neural networks](https://paperswithcode.com/methods/category/recurrent-neural-networks). It amounts to using the same dropout mask at every timestep. It drops both the non-recurrent and recurrent connections. A simple figure to explain the idea is shown to the right. The figure shows an RNN being trained with rnnDrop for three frames $\\left(t-1, t, t+1\\right)$ on two different training sequences in the data (denoted as ‘sequence1’ and ‘sequence2’). The black circles denote the randomly omitted hidden nodes during training, and the dotted arrows stand for the model weights connected to those omitted nodes.\r
\r
*From: RnnDrop: A Novel Dropout for RNNs in ASR by Moon et al*""" ;
    skos:prefLabel "rnnDrop" .

:scSE a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1808.08127v1> ;
    skos:altLabel "Spatial and Channel SE Blocks" ;
    skos:definition """To aggregate global spatial information,\r
an SE block applies global pooling to the feature map.\r
However, it ignores pixel-wise spatial information,\r
which is important in dense prediction tasks.\r
Therefore, Roy et al. proposed\r
spatial and channel SE blocks (scSE). \r
Like BAM, spatial SE blocks are used, complementing SE blocks, \r
to provide spatial attention weights to focus on important regions.\r
\r
Given the input feature map $X$, two parallel modules, spatial SE and channel SE, are applied to feature maps to encode spatial and channel information respectively. The channel SE module is an ordinary SE block, while the spatial SE module adopts $1\\times 1$ convolution for spatial squeezing. The outputs from the two modules are fused. The overall process can be written as\r
\\begin{align}\r
    s_c & = \\sigma (W_{2} \\delta (W_{1}\\text{GAP}(X)))\r
\\end{align}\r
\\begin{align}\r
    X_\\text{chn} & = s_c  X \r
\\end{align}\r
\\begin{align}\r
    s_s &= \\sigma(\\text{Conv}^{1\\times 1}(X))\r
\\end{align}\r
\\begin{align}\r
    X_\\text{spa} & = s_s  X\r
\\end{align}\r
\\begin{align}\r
    Y &= f(X_\\text{spa},X_\\text{chn})  \r
\\end{align}\r
\r
where $f$ denotes the fusion function, which can be  maximum, addition, multiplication or concatenation. \r
\r
The proposed scSE block combines channel and spatial attention to\r
enhance features as well as \r
capturing pixel-wise spatial information.\r
Segmentation tasks are greatly benefited as a result.\r
The integration of an scSE block in F-CNNs makes a consistent improvement in semantic segmentation at negligible extra cost.""" ;
    skos:prefLabel "scSE" .

:srBTAW\(BTW\) a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2104.15090v8> ;
    skos:altLabel "Self-regularizing Boundary Time and Amplitude Warping" ;
    skos:definition "" ;
    skos:prefLabel "srBTAW (BTW)" .

:uNetXST a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2005.04078v1> ;
    skos:definition "uNet neural network architecture which takes multiple (X) tensors as input and contains [Spatial Transformer](https://paperswithcode.com/method/spatial-transformer) units (ST)" ;
    skos:prefLabel "uNetXST" .

:uPIT a skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1607.00325v2> ;
    skos:altLabel "utterance level permutation invariant training" ;
    skos:definition "" ;
    skos:prefLabel "uPIT" .

:wav2vec-U a skos:Concept ;
    dcterms:source <https://arxiv.org/abs/2105.11084v3> ;
    skos:altLabel "wav2vec Unsupervised" ;
    skos:definition """**wav2vec-U** is an unsupervised method to train speech recognition models without any labeled data. It leverages self-supervised speech representations to segment unlabeled language and learn a mapping from these representations to phonemes via adversarial training. \r
\r
Specifically, we learn self-supervised representations with wav2vec 2.0 on unlabeled speech audio, then identify clusters in the representations with k-means to segment the audio data. Next, we build segment representations by mean pooling the wav2vec 2.0 representations, performing [PCA](https://paperswithcode.com/method/pca) and a second mean pooling step between adjacent segments. This is input to the generator which outputs a phoneme sequence that is fed to the discriminator, similar to phonemized unlabeled text to perform adversarial training.""" ;
    skos:prefLabel "wav2vec-U" .

skos:altLabel a owl:AnnotationProperty .

skos:broader a owl:AnnotationProperty .

skos:member a owl:AnnotationProperty .

skos:narrower a owl:AnnotationProperty .

skos:prefLabel a owl:AnnotationProperty .

:AIS a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :AssociationRuleLearningAlgorithm ;
    skos:prefLabel "AIS" .

:AQ a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :RuleBased ;
    skos:prefLabel "AQ" .

:AdaBoost a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :EnsembleAlgorithm ;
    skos:prefLabel "AdaBoost" .

:AdaMax a owl:NamedIndividual,
        skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1412.6980v9> ;
    rdfs:seeAlso <https://github.com/pytorch/pytorch/blob/b7bda236d18815052378c88081f64935427d7716/torch/optim/adamax.py#L5> ;
    skos:broader :Gradient_Descent ;
    skos:definition """**AdaMax** is a generalisation of [Adam](https://paperswithcode.com/method/adam) from the $l\\_{2}$ norm to the $l\\_{\\infty}$ norm. Define:\r
\r
$$ u\\_{t} = \\beta^{\\infty}\\_{2}v\\_{t-1} + \\left(1-\\beta^{\\infty}\\_{2}\\right)|g\\_{t}|^{\\infty}$$\r
\r
$$ = \\max\\left(\\beta\\_{2}\\cdot{v}\\_{t-1}, |g\\_{t}|\\right)$$\r
\r
We can plug into the Adam update equation by replacing $\\sqrt{\\hat{v}_{t} + \\epsilon}$ with $u\\_{t}$ to obtain the AdaMax update rule:\r
\r
$$ \\theta\\_{t+1} = \\theta\\_{t} - \\frac{\\eta}{u\\_{t}}\\hat{m}\\_{t} $$\r
\r
Common default values are $\\eta = 0.002$ and $\\beta\\_{1}=0.9$ and $\\beta\\_{2}=0.999$.""" ;
    skos:prefLabel "AdaMax" .

:Adadelta a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :Gradient_Descent ;
    skos:prefLabel "Adadelta" .

:Adagrad a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :Gradient_Descent ;
    skos:prefLabel "Adagrad" .

:Adam a owl:NamedIndividual,
        skos:Concept ;
    dcterms:source <http://arxiv.org/abs/1412.6980v9> ;
    rdfs:seeAlso <https://github.com/pytorch/pytorch/blob/b7bda236d18815052378c88081f64935427d7716/torch/optim/adam.py#L6> ;
    skos:broader :Gradient_Descent ;
    skos:definition """**Adam** is an adaptive learning rate optimization algorithm that utilises both momentum and scaling, combining the benefits of [RMSProp](https://paperswithcode.com/method/rmsprop) and [SGD w/th Momentum](https://paperswithcode.com/method/sgd-with-momentum). The optimizer is designed to be appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. \r
\r
The weight updates are performed as:\r
\r
$$ w_{t} = w_{t-1} - \\eta\\frac{\\hat{m}\\_{t}}{\\sqrt{\\hat{v}\\_{t}} + \\epsilon}  $$\r
\r
with\r
\r
$$ \\hat{m}\\_{t} = \\frac{m_{t}}{1-\\beta^{t}_{1}} $$\r
\r
$$ \\hat{v}\\_{t} = \\frac{v_{t}}{1-\\beta^{t}_{2}} $$\r
\r
$$ m_{t} = \\beta_{1}m_{t-1} + (1-\\beta_{1})g_{t} $$\r
\r
$$ v_{t} = \\beta_{2}v_{t-1} + (1-\\beta_{2})g_{t}^{2}  $$\r
\r
\r
$ \\eta $ is the step size/learning rate, around 1e-3 in the original paper. $ \\epsilon $ is a small number, typically 1e-8 or 1e-10, to prevent dividing by zero. $ \\beta_{1} $ and $ \\beta_{2} $ are forgetting parameters, with typical values 0.9 and 0.999, respectively.""" ;
    skos:prefLabel "Adam" .

:Advantage_Actor-Critic a owl:NamedIndividual,
        skos:Concept ;
    skos:altLabel "A3C" ;
    skos:broader :Actor-Critic ;
    skos:prefLabel "Advantage Actor-Critic" .

:AffinityPropagation a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :ClusteringAlgorithm ;
    skos:prefLabel "Affinity Propagation" .

:Apriori-TID a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :Apriori ;
    skos:prefLabel "Apriori-TID" .

:Apriori_Hybrid a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :Apriori ;
    skos:prefLabel "Apriori Hybrid" .

:Autoencoder a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :DeepLearningAlgorithm ;
    skos:prefLabel "Autoencoder" .

:BIRCH a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :HierarchicalClustering ;
    skos:prefLabel "BIRCH" .

:Back_Propagation a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :ArtificialNeuralNetwork ;
    skos:prefLabel "Back Propagation" .

:Batch_Gradient_Descent a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :Gradient_Descent ;
    skos:prefLabel "Batch Gradient Descent" .

:Batch_Normalization a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :RegularizationAlgorithm ;
    skos:prefLabel "Batch Normalization" .

:Bayesian_Belief_Network a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :Bayesian_Network ;
    skos:prefLabel "Bayesian Belief Network" .

:Boosted_Regression_Tree a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :RegressionAlgorithm ;
    skos:prefLabel "Boosted Regression Tree" .

:C45 a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :DecisionTree ;
    skos:prefLabel "C4.5" .

:C50 a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :DecisionTree ;
    skos:prefLabel "C5.0" .

:CHAMELEON a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :HierarchicalClustering ;
    skos:prefLabel "CHAMELEON" .

:CLARA a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :PartitionalClustering ;
    skos:prefLabel "CLARA" .

:CLARANS a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :PartitionalClustering ;
    skos:prefLabel "CLARANS" .

:CLIQUE a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :PartitionalClustering ;
    skos:prefLabel "CLIQUE" .

:CN2 a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :RuleBased ;
    skos:prefLabel "CN2" .

:CNN a owl:NamedIndividual,
        skos:Concept ;
    skos:altLabel "CNN" ;
    skos:broader :DeepLearningAlgorithm ;
    skos:prefLabel "Convolution Neural Network" .

:CRUISE a owl:NamedIndividual,
        skos:Concept ;
    skos:altLabel "CRUISE" ;
    skos:broader :DecisionTree ;
    skos:prefLabel "Classification Rule with Unbiased Interaction Selection and Estimation" .

:CTREE a owl:NamedIndividual,
        skos:Concept ;
    skos:altLabel "CTREE" ;
    skos:broader :DecisionTree ;
    skos:prefLabel "Conditional Inference Trees" .

:CURE a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :HierarchicalClustering ;
    skos:prefLabel "CURE" .

:Canonical_Correlation_Analysis a owl:NamedIndividual,
        skos:Concept ;
    skos:altLabel "CCA" ;
    skos:broader :DimensionalityReductionAlgorithm ;
    skos:prefLabel "Canonical Correlation Analysis" .

:Capped_L1 a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :RegularizationAlgorithm ;
    skos:prefLabel "Capped L1 Regularization" .

:Chi-squared_Automatic_Interaction_Detection a owl:NamedIndividual,
        skos:Concept ;
    skos:altLabel "CHAID" ;
    skos:broader :DecisionTree ;
    skos:prefLabel "Chi-squared Automatic Interaction Detection" .

:Classification_and_Regression_Trees a owl:NamedIndividual,
        skos:Concept ;
    skos:altLabel "CART" ;
    skos:broader :DecisionTree ;
    skos:prefLabel "Classification and Regression Trees" .

:DBSCAN a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :PartitionalClustering ;
    skos:prefLabel "DBSCAN" .

:Data_Augmentation a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :RegularizationAlgorithm ;
    skos:prefLabel "Data Augmentation" .

:Decision_Stump a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :DecisionTree ;
    skos:prefLabel "Decision Stump" .

:Deep_Belief_Network a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :DeepLearningAlgorithm ;
    skos:prefLabel "Deep Belief Network" .

:Deep_Recurrent_Q-Network a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :Deep_Q-Network ;
    skos:prefLabel "Deep Recurrent Q-Network" .

:Deterministic_Policy_Gradients_Algorithm a owl:NamedIndividual,
        skos:Concept ;
    skos:altLabel "DPG" ;
    skos:broader :Actor-Critic ;
    skos:prefLabel "Deterministic Policy Gradients Algorithm" .

:Double_and_Dueling_Deep_Q-Network a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :Deep_Q-Network ;
    skos:prefLabel "Double and Dueling Deep Q-Network" .

:DropConnect a owl:NamedIndividual,
        skos:Concept ;
    dcterms:source <http://cds.nyu.edu/projects/regularization-neural-networks-using-dropconnect/> ;
    rdfs:seeAlso <https://github.com/teelinsan/KerasDropconnect/blob/0b203b56b2564686358fe0907b89316e3fce4014/ddrop/layers.py#L24> ;
    skos:broader :RegularizationAlgorithm ;
    skos:definition """**DropConnect** generalizes [Dropout](https://paperswithcode.com/method/dropout) by randomly dropping the weights rather than the activations with probability $1-p$. DropConnect is similar to Dropout as it introduces dynamic sparsity within the model, but differs in that the sparsity is on the weights $W$, rather than the output vectors of a layer. In other words, the fully connected layer with DropConnect becomes a sparsely connected layer in which the connections are chosen at random during the training stage. Note that this is not equivalent to setting $W$ to be a fixed sparse matrix during training.\r
\r
For a DropConnect layer, the output is given as:\r
\r
$$ r = a \\left(\\left(M * W\\right){v}\\right)$$\r
\r
Here $r$ is the output of a layer, $v$ is the input to a layer, $W$ are weight parameters, and $M$ is a binary matrix encoding the connection information where $M\\_{ij} \\sim \\text{Bernoulli}\\left(p\\right)$. Each element of the mask $M$ is drawn independently for each example during training, essentially instantiating a different connectivity for each example seen. Additionally, the biases are also masked out during training.""" ;
    skos:prefLabel "DropConnect" .

:Dropout a owl:NamedIndividual,
        skos:Concept ;
    dcterms:source <http://jmlr.org/papers/v15/srivastava14a.html> ;
    rdfs:seeAlso <https://github.com/google/jax/blob/7f3078b70d0ed9bea6228efa420879c56f72ef69/jax/experimental/stax.py#L271-L275> ;
    skos:broader :RegularizationAlgorithm ;
    skos:definition """**Dropout** is a regularization technique for neural networks that drops a unit (along with connections) at training time with a specified probability $p$ (a common value is $p=0.5$). At test time, all units are present, but with weights scaled by $p$ (i.e. $w$ becomes $pw$).\r
\r
The idea is to prevent co-adaptation, where the neural network becomes too reliant on particular connections, as this could be symptomatic of overfitting. Intuitively, dropout can be thought of as creating an implicit ensemble of neural networks.""" ;
    skos:prefLabel "Dropout" .

:Dyna-Q a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :ReinforcementLearningAlgorithm ;
    skos:prefLabel "Dyna-Q" .

:ECLAT a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :AssociationRuleLearningAlgorithm ;
    skos:prefLabel "ECLAT" .

:Early_Stopping a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :RegularizationAlgorithm ;
    skos:prefLabel "Early Stopping" .

:ExpectationMaximization a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :PartitionalClustering ;
    skos:prefLabel "Expectation - Maximization" .

:Extremely_Randomized_Trees a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :EnsembleAlgorithm ;
    skos:prefLabel "Extremely Randomized Trees" .

:FOIL a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :RuleBased ;
    skos:narrower :FOIL ;
    skos:prefLabel "FOIL" .

:Factor_Analysis a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :DimensionalityReductionAlgorithm ;
    skos:prefLabel "Factor Analysis" .

:Forest-augmented_Naive_Bayes a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :One-Dependence_Bayesian_Estimator ;
    skos:prefLabel "Forest-augmented Naive Bayes" .

:Forward_Propagation a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :ArtificialNeuralNetwork ;
    skos:prefLabel "Forward Propagation" .

:Fp-Growth a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :AssociationRuleLearningAlgorithm ;
    skos:prefLabel "Fp-Growth" .

:Fractional_Pooling a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :RegularizationAlgorithm ;
    skos:prefLabel "Fractional Pooling" .

:FuzzyC-Means a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :FuzzyClustering ;
    skos:prefLabel "Fuzzy c-means" .

:FuzzyK-Modes a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :FuzzyClustering ;
    skos:prefLabel "Fuzzy k-modes" .

:GAN a owl:NamedIndividual,
        skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1406.2661v1> ;
    rdfs:seeAlso <https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/gan/gan.py> ;
    skos:altLabel "GAN",
        "Generative Adversarial Network" ;
    skos:broader :DeepLearningAlgorithm ;
    skos:definition """A **GAN**, or **Generative Adversarial Network**, is a generative model that simultaneously trains\r
two models: a generative model $G$ that captures the data distribution, and a discriminative model $D$ that estimates the\r
probability that a sample came from the training data rather than $G$.\r
\r
The training procedure for $G$ is to maximize the probability of $D$ making\r
a mistake. This framework corresponds to a minimax two-player game. In the\r
space of arbitrary functions $G$ and $D$, a unique solution exists, with $G$\r
recovering the training data distribution and $D$ equal to $\\frac{1}{2}$\r
everywhere. In the case where $G$ and $D$ are defined by multilayer perceptrons,\r
the entire system can be trained with backpropagation. \r
\r
(Image Source: [here](http://www.kdnuggets.com/2017/01/generative-adversarial-networks-hot-topic-machine-learning.html))""" ;
    skos:prefLabel "GAN",
        "Generative Adversarial Network" .

:GUIDE a owl:NamedIndividual,
        skos:Concept ;
    skos:altLabel "GUIDE" ;
    skos:broader :DecisionTree ;
    skos:prefLabel "Generalized Unbiased Interaction Detection and Estimation" .

:Gaussian_Naive_Bayes a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :Naive_Bayes ;
    skos:prefLabel "Gaussian Naive Bayes" .

:Gaussian_Regression a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :RegressionAlgorithm ;
    skos:prefLabel "Gaussian Regression" .

:Generalized_Minimax_Concave a owl:NamedIndividual,
        skos:Concept ;
    skos:altLabel "GMC" ;
    skos:broader :RegularizationAlgorithm ;
    skos:prefLabel "Generalized Minimax Concave" .

:Generative_Topographic_Mapping a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :DimensionalityReductionAlgorithm ;
    skos:prefLabel "Generative Topographic Mapping" .

:Huber_Regression a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :RegressionAlgorithm ;
    skos:prefLabel "Huber Regression" .

:ICA a owl:NamedIndividual,
        skos:Concept ;
    skos:altLabel "ICA",
        "Independent Component Analysis" ;
    skos:broader :DimensionalityReductionAlgorithm ;
    skos:definition """_**Independent component analysis** (ICA) is a statistical and computational technique for revealing hidden factors that underlie sets of random variables, measurements, or signals._\r
\r
_ICA defines a generative model for the observed multivariate data, which is typically given as a large database of samples. In the model, the data variables are assumed to be linear mixtures of some unknown latent variables, and the mixing system is also unknown. The latent variables are assumed nongaussian and mutually independent, and they are called the independent components of the observed data. These independent components, also called sources or factors, can be found by ICA._\r
\r
_ICA is superficially related to principal component analysis and factor analysis. ICA is a much more powerful technique, however, capable of finding the underlying factors or sources when these classic methods fail completely._\r
\r
\r
Extracted from (https://www.cs.helsinki.fi/u/ahyvarin/whatisica.shtml)\r
\r
**Source papers**:\r
\r
[Blind separation of sources, part I: An adaptive algorithm based on neuromimetic architecture](https://doi.org/10.1016/0165-1684(91)90079-X)\r
\r
[Independent component analysis, A new concept?](https://doi.org/10.1016/0165-1684(94)90029-9)\r
\r
[Independent component analysis: algorithms and applications](https://doi.org/10.1016/S0893-6080(00)00026-5)""" ;
    skos:prefLabel "ICA",
        "Independent Component Analysis" .

:ID3 a owl:NamedIndividual,
        skos:Concept ;
    skos:altLabel "ID3" ;
    skos:broader :DecisionTree ;
    skos:prefLabel "Iterative Dichotomiser 3" .

:ISOMAP a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :Manifold_Learning ;
    skos:prefLabel "ISOMAP" .

:Importance_Weighted_Actor-Learner_Architecture a owl:NamedIndividual,
        skos:Concept ;
    skos:altLabel "IM-PALA" ;
    skos:broader :Actor-Critic ;
    skos:prefLabel "Importance Weighted Actor-Learner Architecture" .

:Improved_Bagging_Algorithm a owl:NamedIndividual,
        skos:Concept ;
    skos:altLabel "IBA" ;
    skos:broader :Bagging ;
    skos:prefLabel "Improved Bagging Algorithm" .

:K-Dependence_Bayesian_Classifier a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :Bayesian ;
    skos:prefLabel "K-Dependence Bayesian Classifier" .

:K-Medians a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :PartitionalClustering ;
    skos:prefLabel "K-medians" .

:KECA a owl:NamedIndividual,
        skos:Concept ;
    skos:altLabel "KECA" ;
    skos:broader :DimensionalityReductionAlgorithm ;
    skos:prefLabel "Kernel Entropy Component Analysis" .

:L-BFGS a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :Neural_Network_Optimization ;
    skos:prefLabel "L-BFGS" .

:LDA a owl:NamedIndividual,
        skos:Concept ;
    skos:altLabel "LDA",
        "Linear Discriminant Analysis" ;
    skos:broader :DimensionalityReductionAlgorithm ;
    skos:definition """**Linear discriminant analysis** (LDA), normal discriminant analysis (NDA), or discriminant function analysis is a generalization of Fisher's linear discriminant, a method used in statistics, pattern recognition, and machine learning to find a linear combination of features that characterizes or separates two or more classes of objects or events. The resulting combination may be used as a linear classifier, or, more commonly, for dimensionality reduction before later classification.\r
\r
Extracted from [Wikipedia](https://en.wikipedia.org/wiki/Linear_discriminant_analysis)\r
\r
**Source**:\r
\r
Paper: [Linear Discriminant Analysis: A Detailed Tutorial](https://dx.doi.org/10.3233/AIC-170729)\r
\r
Public version: [Linear Discriminant Analysis: A Detailed Tutorial](https://usir.salford.ac.uk/id/eprint/52074/)""" ;
    skos:prefLabel "LDA",
        "Linear Discriminant Analysis" .

:LSTM a owl:NamedIndividual,
        skos:Concept ;
    skos:altLabel "LSTM",
        "Long Short-Term Memory" ;
    skos:broader :RNN ;
    skos:definition """An **LSTM** is a type of [recurrent neural network](https://paperswithcode.com/methods/category/recurrent-neural-networks) that addresses the vanishing gradient problem in vanilla RNNs through additional cells, input and output gates. Intuitively, vanishing gradients are solved through additional *additive* components, and forget gate activations, that allow the gradients to flow through the network without vanishing as quickly.\r
\r
(Image Source [here](https://medium.com/datadriveninvestor/how-do-lstm-networks-solve-the-problem-of-vanishing-gradients-a6784971a577))\r
\r
(Introduced by Hochreiter and Schmidhuber)""" ;
    skos:prefLabel "LSTM",
        "Long Short-Term Memory" .

:Label_Smoothing a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :RegularizationAlgorithm ;
    skos:prefLabel "Label Smoothing" .

:Layer_Normalization a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :RegularizationAlgorithm ;
    skos:prefLabel "Layer Normalization" .

:Lazy_SVM_Classification a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :InstanceBasedAlgorithm ;
    skos:prefLabel "Lazy SVM Classification" .

:Leaky_Capped_L1_Norm_Regularizer a owl:NamedIndividual,
        skos:Concept ;
    skos:altLabel "LCNR" ;
    skos:broader :RegularizationAlgorithm ;
    skos:prefLabel "Leaky Capped L1 Norm Regularizer" .

:Levenberg-Marquardt a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :Neural_Network_Optimization ;
    skos:prefLabel "Levenberg-Marquardt" .

:LightGBM a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :Gradient_Boosting_Machine ;
    skos:prefLabel "LightGBM" .

:Linear_Regression a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :RegressionAlgorithm ;
    skos:prefLabel "Linear Regression" .

:Local_Linear_Embedding a owl:NamedIndividual,
        skos:Concept ;
    skos:altLabel "LLE" ;
    skos:broader :Manifold_Learning ;
    skos:prefLabel "Local Linear Embedding" .

:Locally_Preserved_Projection a owl:NamedIndividual,
        skos:Concept ;
    skos:altLabel "LPP" ;
    skos:broader :Manifold_Learning ;
    skos:prefLabel "Locally Preserved Projection" .

:Locally_Weighted_Learning a owl:NamedIndividual,
        skos:Concept ;
    skos:altLabel "LWL" ;
    skos:broader :InstanceBasedAlgorithm ;
    skos:prefLabel "Locally Weighted Learning" .

:MARS a owl:NamedIndividual,
        skos:Concept ;
    skos:altLabel "MARS" ;
    skos:broader :DecisionTree ;
    skos:prefLabel "Multivariate Adaptive Regression Splines" .

:MDA a owl:NamedIndividual,
        skos:Concept ;
    skos:altLabel "MDA" ;
    skos:broader :DimensionalityReductionAlgorithm ;
    skos:prefLabel "Mixture Discriminant Analysis" .

:MDS a owl:NamedIndividual,
        skos:Concept ;
    skos:altLabel "MDS" ;
    skos:broader :DimensionalityReductionAlgorithm ;
    skos:prefLabel "Multidimensional Scaling" .

:Maximum_Autocorrelation_Factors a owl:NamedIndividual,
        skos:Concept ;
    skos:altLabel "MAF" ;
    skos:broader :DimensionalityReductionAlgorithm ;
    skos:prefLabel "Maximum Autocorrelation Factors" .

:MeanShift a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :PartitionalClustering ;
    skos:prefLabel "Mean-Shift" .

:Mini-Batch_Gradient_Descent a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :Gradient_Descent ;
    skos:prefLabel "Mini-Batch Gradient Descent" .

:MiniBatchK-Means a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :K-Means ;
    skos:prefLabel "Mini batch k-means" .

:Minimax_Concave_Penalty a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :RegularizationAlgorithm ;
    skos:prefLabel "Minimax Concave Penalty" .

:MinimumSpanningTree a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :ClusteringAlgorithm ;
    skos:prefLabel "Minimum spanning tree" .

:Monte_Carlo_Policy_Evaluation a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :Policy_Evaluation ;
    skos:prefLabel "Monte Carlo Policy Evaluation" .

:Multinomial_Naive_Bayes a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :Naive_Bayes ;
    skos:prefLabel "Multinomial Naive Bayes" .

:NMF a owl:NamedIndividual,
        skos:Concept ;
    skos:altLabel "NMF" ;
    skos:broader :DimensionalityReductionAlgorithm ;
    skos:prefLabel "Non-Negative Matrix Factorization" .

:Nadam a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :Gradient_Descent ;
    skos:prefLabel "Nadam" .

:Nesterov_Accelerated_Gradient a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :Gradient_Descent ;
    skos:prefLabel "Nesterov Accelerated Gradient" .

:Neural_Network_Regression a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :RegressionAlgorithm ;
    skos:prefLabel "Neural Network Regression" .

:Normalized_Advantage_Function_Algorithm a owl:NamedIndividual,
        skos:Concept ;
    skos:altLabel "NAF" ;
    skos:broader :ReinforcementLearningAlgorithm ;
    skos:prefLabel "Normalized Advantage Function Algorithm" .

:OPTICS a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :PartitionalClustering ;
    skos:prefLabel "OPTICS" .

:OPUS a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :RuleBased ;
    skos:prefLabel "OPUS" .

:Ordinal_Regression a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :RegressionAlgorithm ;
    skos:prefLabel "Ordinal Regression" .

:Ordinary_Least_Squares a owl:NamedIndividual,
        skos:Concept ;
    skos:altLabel "OLS" ;
    skos:broader :RegressionAlgorithm ;
    skos:prefLabel "Ordinary Least Squares" .

:PAM a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :PartitionalClustering ;
    skos:prefLabel "PAM" .

:PCR a owl:NamedIndividual,
        skos:Concept ;
    skos:altLabel "PCR" ;
    skos:broader :DimensionalityReductionAlgorithm ;
    skos:prefLabel "Principal Component Regression" .

:PEGASUS a owl:NamedIndividual,
        skos:Concept ;
    dcterms:source <https://arxiv.org/abs/1912.08777v2> ;
    skos:broader :ReinforcementLearningAlgorithm ;
    skos:definition "**PEGASUS** proposes a transformer-based model for abstractive summarization. It uses a special self-supervised pre-training objective called gap-sentences generation (GSG) that's designed to perform well on summarization-related downstream tasks. As reported in the paper, \"both GSG and MLM are applied simultaneously to this example as pre-training objectives. Originally there are three sentences. One sentence is masked with [MASK1] and used as target generation text (GSG). The other two sentences remain in the input, but some tokens are randomly masked by [MASK2].\"" ;
    skos:prefLabel "PEGASUS" .

:PLSDA a owl:NamedIndividual,
        skos:Concept ;
    skos:altLabel "PLSDA" ;
    skos:broader :DimensionalityReductionAlgorithm ;
    skos:prefLabel "Partial Least Squares Discriminant Analvsis" .

:PLSR a owl:NamedIndividual,
        skos:Concept ;
    skos:altLabel "PLSR" ;
    skos:broader :DimensionalityReductionAlgorithm ;
    skos:prefLabel "Partial Least Squares Regression" .

:PPCA a owl:NamedIndividual,
        skos:Concept ;
    skos:altLabel "PPCA" ;
    skos:broader :PCA ;
    skos:prefLabel "Probabilistic Principal Component Analysis" .

:Pareto_optimization a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :Neural_Network_Optimization ;
    skos:prefLabel "Pareto optimization" .

:Particle_Swarm_optimization a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :Neural_Network_Optimization ;
    skos:prefLabel "Particle Swarm optimization" .

:Perceptron a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :ArtificialNeuralNetwork ;
    skos:prefLabel "Perceptron" .

:Polynomial_Regression a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :RegressionAlgorithm ;
    skos:prefLabel "Polynomial Regression" .

:Projection_Operation_Regularization a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :RegularizationAlgorithm ;
    skos:prefLabel "Projection Operation Regularization" .

:Projection_Pursuit a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :DimensionalityReductionAlgorithm ;
    skos:prefLabel "Projection Pursuit" .

:Q-Learning a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :ReinforcementLearningAlgorithm ;
    skos:definition """**Q-Learning** is an off-policy temporal difference control algorithm:\r
\r
$$Q\\left(S\\_{t}, A\\_{t}\\right) \\leftarrow Q\\left(S\\_{t}, A\\_{t}\\right) + \\alpha\\left[R_{t+1} + \\gamma\\max\\_{a}Q\\left(S\\_{t+1}, a\\right) - Q\\left(S\\_{t}, A\\_{t}\\right)\\right] $$\r
\r
The learned action-value function $Q$ directly approximates $q\\_{*}$, the optimal action-value function, independent of the policy being followed.\r
\r
Source: Sutton and Barto, Reinforcement Learning, 2nd Edition""" ;
    skos:prefLabel "Q-Learning" .

:QDA a owl:NamedIndividual,
        skos:Concept ;
    skos:altLabel "QDA" ;
    skos:broader :DimensionalityReductionAlgorithm ;
    skos:prefLabel "Quadratic Discriminant Analysis" .

:QUEST a owl:NamedIndividual,
        skos:Concept ;
    skos:altLabel "QUEST" ;
    skos:broader :DecisionTree ;
    skos:prefLabel "Quick Unbiased and Efficient Statistical Tree" .

:Quantile_Regression a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :RegressionAlgorithm ;
    skos:prefLabel "Quantile Regression" .

:RANSAC_Regression a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :RegressionAlgorithm ;
    skos:prefLabel "RANSAC Regression" .

:RBFN a owl:NamedIndividual,
        skos:Concept ;
    skos:altLabel "RBFN" ;
    skos:broader :DeepLearningAlgorithm ;
    skos:prefLabel "Radial Basis Function Network" .

:RBM a owl:NamedIndividual,
        skos:Concept ;
    skos:altLabel "RBM" ;
    skos:broader :DeepLearningAlgorithm ;
    skos:prefLabel "Restricted Boltzmann Machine" .

:REINFORCE a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :ReinforcementLearningAlgorithm ;
    skos:definition """**REINFORCE** is a Monte Carlo variant of a policy gradient algorithm in reinforcement learning. The agent collects samples of an episode using its current policy, and uses it to update the policy parameter $\\theta$. Since one full trajectory must be completed to construct a sample space, it is updated as an off-policy algorithm.\r
\r
$$ \\nabla\\_{\\theta}J\\left(\\theta\\right) = \\mathbb{E}\\_{\\pi}\\left[G\\_{t}\\nabla\\_{\\theta}\\ln\\pi\\_{\\theta}\\left(A\\_{t}\\mid{S\\_{t}}\\right)\\right]$$\r
\r
Image Credit: [Tingwu Wang](http://www.cs.toronto.edu/~tingwuwang/REINFORCE.pdf)""" ;
    skos:prefLabel "REINFORCE" .

:RIPPER a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :RuleBased ;
    skos:prefLabel "RIPPER" .

:RMSprop a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :Gradient_Descent ;
    skos:prefLabel "RMSprop" .

:ROCK a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :HierarchicalClustering ;
    skos:prefLabel "ROCK" .

:Residual_Network a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :DeepLearningAlgorithm ;
    skos:prefLabel "Residual Network" .

:Rotation_Forest a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :EnsembleAlgorithm ;
    skos:prefLabel "Rotation Forest" .

:RvNN a owl:NamedIndividual,
        skos:Concept ;
    skos:altLabel "RvNN" ;
    skos:broader :DeepLearningAlgorithm ;
    skos:prefLabel "Recursive Neural Network" .

:SETM a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :ArtificialNeuralNetwork ;
    skos:prefLabel "SETM" .

:SVD a owl:NamedIndividual,
        skos:Concept ;
    skos:altLabel "SVD" ;
    skos:broader :DimensionalityReductionAlgorithm ;
    skos:prefLabel "Singular Value Decomposition" .

:Sammon_Mapping a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :DimensionalityReductionAlgorithm ;
    skos:prefLabel "Sammon Mapping" .

:Selective_Naive_Bayes a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :Naive_Bayes ;
    skos:prefLabel "Selective Naive Bayes" .

:Self-Organizing_Map a owl:NamedIndividual,
        skos:Concept ;
    skos:altLabel "SOM" ;
    skos:broader :InstanceBasedAlgorithm ;
    skos:prefLabel "Self-Organizing Map" .

:Semi-Naive_Bayes a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :Naive_Bayes ;
    skos:prefLabel "Semi-Naive Bayes" .

:Sequential_Covering a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :RuleBased ;
    skos:prefLabel "Sequential Covering" .

:Simulated_Annealing a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :Neural_Network_Optimization ;
    skos:prefLabel "Simulated Annealing" .

:Slow_Feature_Analysis a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :DimensionalityReductionAlgorithm ;
    skos:prefLabel "Slow Feature Analysis" .

:Smoothly_Clipped_Absolute_Deviation a owl:NamedIndividual,
        skos:Concept ;
    skos:altLabel "SCAD" ;
    skos:broader :RegularizationAlgorithm ;
    skos:prefLabel "Smoothly Clipped Absolute Deviation" .

:Soft_Actor-Critic a owl:NamedIndividual,
        skos:Concept ;
    skos:altLabel "SAC" ;
    skos:broader :Actor-Critic ;
    skos:prefLabel "Soft Actor-Critic" .

:Sparse_PCA a owl:NamedIndividual,
        skos:Concept ;
    skos:altLabel "SPCA" ;
    skos:broader :PCA ;
    skos:prefLabel "Sparse Principal Component Analysis" .

:SpectralClustering a owl:NamedIndividual,
        skos:Concept ;
    dcterms:source <http://arxiv.org/abs/0711.0189v1> ;
    skos:broader :ClusteringAlgorithm ;
    skos:definition """Spectral clustering has attracted increasing attention due to\r
the promising ability in dealing with nonlinearly separable datasets [15], [16]. In spectral clustering, the spectrum of the graph Laplacian is used to reveal the cluster structure. The spectral clustering algorithm mainly consists of two steps: 1) constructs the low dimensional embedded representation of the data based on the eigenvectors of the graph Laplacian, 2) applies k-means on the constructed low dimensional data to obtain the clustering result. Thus,""" ;
    skos:prefLabel "Spectral Clustering" .

:Stacking a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :EnsembleAlgorithm ;
    skos:prefLabel "Stacking" .

:Stein_Variational_Policy_Gradient a owl:NamedIndividual,
        skos:Concept ;
    skos:altLabel "SVPG" ;
    skos:broader :ReinforcementLearningAlgorithm ;
    skos:prefLabel "Stein Variational Policy Gradient" .

:Stohastic_Gradient_Descent_Momentum a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :Stohastic_Gradient_Descent ;
    skos:prefLabel "Stohastic Gradient Descent Momentum" .

:Support_Vector_Machine a owl:NamedIndividual,
        skos:Concept ;
    skos:altLabel "SVM" ;
    skos:broader :RegressionAlgorithm ;
    skos:prefLabel "Support Vector Machine" .

:Temporal_Difference a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :ReinforcementLearningAlgorithm ;
    skos:prefLabel "Temporal Difference" .

:Tertius_Apriori a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :Apriori ;
    skos:prefLabel "Tertius Apriori" .

:Theil-Sen_Regression a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :RegressionAlgorithm ;
    skos:prefLabel "Theil-Sen Regression" .

:Tree-Augmented_Naive_Bayes a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :One-Dependence_Bayesian_Estimator ;
    skos:prefLabel "Tree-Augmented Naive Bayes" .

:Trust_Region_Policy_Optimization a owl:NamedIndividual,
        skos:Concept ;
    skos:altLabel "TRPO" ;
    skos:broader :ReinforcementLearningAlgorithm ;
    skos:prefLabel "Trust Region Policy Optimization" .

:Weight_Decay a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :RegularizationAlgorithm ;
    skos:prefLabel "Weight Decay" .

:XGBoost a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :Gradient_Boosting_Machine ;
    skos:prefLabel "XGBoost" .

skos:Collection a owl:Class .

:Bagging a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :EnsembleAlgorithm ;
    skos:narrower :Improved_Bagging_Algorithm ;
    skos:prefLabel "Bagging" .

:Bayesian_Network a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :Bayesian ;
    skos:narrower :Bayesian_Belief_Network ;
    skos:prefLabel "Bayesian Network" .

:Bayesian_Regression a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :Bayesian,
        :RegressionAlgorithm ;
    skos:prefLabel "Bayesian Regression" .

:Cubist a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :DecisionTree,
        :RuleBased ;
    skos:prefLabel "Cubist" .

:Decision_Tree_Regression a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :DecisionTree,
        :RegressionAlgorithm ;
    skos:prefLabel "Decision Tree Regression" .

:Deep_SARSA a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :Deep_Q-Network,
        :SARSA ;
    skos:prefLabel "Deep SARSA" .

:Elastic_Net_Regression a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :RegressionAlgorithm,
        :RegularizationAlgorithm ;
    skos:prefLabel "Elastic Net Regression" .

:K-Means a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :PartitionalClustering ;
    skos:narrower :MiniBatchK-Means ;
    skos:prefLabel "K-means" .

:K-Nearest_Neighbor a owl:NamedIndividual,
        skos:Concept ;
    skos:altLabel "KNN" ;
    skos:broader :InstanceBasedAlgorithm,
        :RegressionAlgorithm ;
    skos:prefLabel "K-Nearest Neighbor" .

:Kohonen_Self_Organizing_Neural_Network a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :DeepLearningAlgorithm,
        :DimensionalityReductionAlgorithm ;
    skos:prefLabel "Kohonen Self Organizing Neural Network" .

:Lazy_Naive_Bayes a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :InstanceBasedAlgorithm,
        :Naive_Bayes ;
    skos:prefLabel "Lazy Naive Bayes" .

:Least_Angle_Regression a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :RegressionAlgorithm,
        :RegularizationAlgorithm ;
    skos:prefLabel "Least Angle Regression" .

:M5 a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :DecisionTree,
        :RuleBased ;
    skos:prefLabel "M5" .

:Policy_Evaluation a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :ReinforcementLearningAlgorithm ;
    skos:narrower :Monte_Carlo_Policy_Evaluation ;
    skos:prefLabel "Policy Evaluation" .

:RNN a owl:NamedIndividual,
        skos:Concept ;
    skos:altLabel "RNN" ;
    skos:broader :DeepLearningAlgorithm ;
    skos:narrower :LSTM ;
    skos:prefLabel "Recurrent Neural Network" .

:Random_Forest a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :EnsembleAlgorithm,
        :RegressionAlgorithm ;
    skos:prefLabel "Random Forest" .

:Ridge_Regression a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :RegressionAlgorithm,
        :RegularizationAlgorithm ;
    skos:prefLabel "Ridge Regression" .

:SARSA a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :ReinforcementLearningAlgorithm ;
    skos:narrower :Deep_SARSA ;
    skos:prefLabel "SARSA" .

:Stohastic_Gradient_Descent a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :Gradient_Descent ;
    skos:narrower :Stohastic_Gradient_Descent_Momentum ;
    skos:prefLabel "Stohastic Gradient Descent" .

:Vector_Quantization a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :DimensionalityReductionAlgorithm,
        :InstanceBasedAlgorithm ;
    skos:prefLabel "Vector Quantization" .

:FuzzyClustering a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :PartitionalClustering ;
    skos:narrower :FuzzyC-Means,
        :FuzzyK-Modes ;
    skos:prefLabel "Fuzzy Clustering" .

:Gradient_Boosting_Machine a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :EnsembleAlgorithm ;
    skos:narrower :LightGBM,
        :XGBoost ;
    skos:prefLabel "Gradient Boosting Machine" .

:LASSO a owl:NamedIndividual,
        skos:Concept ;
    skos:altLabel "Lasso" ;
    skos:broader :DimensionalityReductionAlgorithm,
        :RegressionAlgorithm,
        :RegularizationAlgorithm ;
    skos:prefLabel "Least Absolute Shrinkage and Selection Operator" .

:One-Dependence_Bayesian_Estimator a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :Bayesian ;
    skos:narrower :Forest-augmented_Naive_Bayes,
        :Tree-Augmented_Naive_Bayes ;
    skos:prefLabel "One-Dependence Bayesian Estimator" .

:PCA a owl:NamedIndividual,
        skos:Concept ;
    skos:altLabel "PCA",
        "Principal Components Analysis" ;
    skos:broader :DimensionalityReductionAlgorithm ;
    skos:definition """**Principle Components Analysis (PCA)** is an unsupervised method primary used for dimensionality reduction within machine learning.  PCA is calculated via a singular value decomposition (SVD) of the design matrix, or alternatively, by calculating the covariance matrix of the data and performing eigenvalue decomposition on the covariance matrix. The results of PCA provide a low-dimensional picture of the structure of the data and the leading (uncorrelated) latent factors determining variation in the data.\r
\r
Image Source: [Wikipedia](https://en.wikipedia.org/wiki/Principal_component_analysis#/media/File:GaussianScatterPCA.svg)""" ;
    skos:narrower :PPCA,
        :Sparse_PCA ;
    skos:prefLabel "PCA",
        "Principal Component Analysis" .

:Apriori a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :AssociationRuleLearningAlgorithm ;
    skos:narrower :Apriori-TID,
        :Apriori_Hybrid,
        :Tertius_Apriori ;
    skos:prefLabel "Apriori" .

:Deep_Q-Network a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :ReinforcementLearningAlgorithm ;
    skos:narrower :Deep_Recurrent_Q-Network,
        :Deep_SARSA,
        :Double_and_Dueling_Deep_Q-Network ;
    skos:prefLabel "Deep Q-Network" .

:Manifold_Learning a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :DimensionalityReductionAlgorithm ;
    skos:narrower :ISOMAP,
        :Local_Linear_Embedding,
        :Locally_Preserved_Projection ;
    skos:prefLabel "Manifold Learning" .

:Actor-Critic a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :ReinforcementLearningAlgorithm ;
    skos:narrower :Advantage_Actor-Critic,
        :Deterministic_Policy_Gradients_Algorithm,
        :Importance_Weighted_Actor-Learner_Architecture,
        :Soft_Actor-Critic ;
    skos:prefLabel "Actor-Critic" .

:AssociationRuleLearningAlgorithm a owl:NamedIndividual,
        skos:Concept ;
    skos:narrower :AIS,
        :Apriori,
        :ECLAT,
        :Fp-Growth,
        :SETM ;
    skos:prefLabel "Association Rule Learning Algorithm" .

:HierarchicalClustering a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :ClusteringAlgorithm ;
    skos:narrower :BIRCH,
        :CHAMELEON,
        :CURE,
        :ROCK ;
    skos:prefLabel "Hierarchical Clustering" .

:ArtificialNeuralNetwork a owl:NamedIndividual,
        skos:Concept ;
    skos:narrower :Back_Propagation,
        :Forward_Propagation,
        :Neural_Network_Optimization,
        :Perceptron ;
    skos:prefLabel "Artificial Neural Network Algorithm" .

:Bayesian a owl:NamedIndividual,
        skos:Concept ;
    skos:narrower :Bayesian_Network,
        :Bayesian_Regression,
        :K-Dependence_Bayesian_Classifier,
        :Naive_Bayes,
        :One-Dependence_Bayesian_Estimator ;
    skos:prefLabel "Bayesian Algorithm" .

:ClusteringAlgorithm a owl:NamedIndividual,
        skos:Concept ;
    skos:narrower :AffinityPropagation,
        :HierarchicalClustering,
        :MinimumSpanningTree,
        :PartitionalClustering,
        :SpectralClustering ;
    skos:prefLabel "Clustering Algorithm" .

:Naive_Bayes a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :Bayesian,
        :Selective_Naive_Bayes ;
    skos:narrower :Gaussian_Naive_Bayes,
        :Lazy_Naive_Bayes,
        :Multinomial_Naive_Bayes,
        :Semi-Naive_Bayes ;
    skos:prefLabel "Naive Bayes" .

:InstanceBasedAlgorithm a owl:NamedIndividual,
        skos:Concept ;
    skos:narrower :K-Nearest_Neighbor,
        :Lazy_Naive_Bayes,
        :Lazy_SVM_Classification,
        :Locally_Weighted_Learning,
        :Self-Organizing_Map,
        :Vector_Quantization ;
    skos:prefLabel "Instance Based Algorithm" .

:EnsembleAlgorithm a owl:NamedIndividual,
        skos:Concept ;
    skos:narrower :AdaBoost,
        :Bagging,
        :Extremely_Randomized_Trees,
        :Gradient_Boosting_Machine,
        :Random_Forest,
        :Rotation_Forest,
        :Stacking ;
    skos:prefLabel "Ensemble Algorithm" .

:Neural_Network_Optimization a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :ArtificialNeuralNetwork,
        :DeepLearningAlgorithm ;
    skos:narrower :Gradient_Descent,
        :L-BFGS,
        :Levenberg-Marquardt,
        :Pareto_optimization,
        :Particle_Swarm_optimization,
        :Simulated_Annealing ;
    skos:prefLabel "Neural Network Optimization" .

:RuleBased a owl:NamedIndividual,
        skos:Concept ;
    skos:narrower :AQ,
        :CN2,
        :Cubist,
        :M5,
        :OPUS,
        :RIPPER,
        :Sequential_Covering ;
    skos:prefLabel "Rule Based Algorithm" .

:Gradient_Descent a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :Neural_Network_Optimization ;
    skos:narrower :AdaMax,
        :Adadelta,
        :Adagrad,
        :Adam,
        :Batch_Gradient_Descent,
        :Mini-Batch_Gradient_Descent,
        :Nadam,
        :Nesterov_Accelerated_Gradient,
        :RMSprop,
        :Stohastic_Gradient_Descent ;
    skos:prefLabel "Gradient Descent" .

:DeepLearningAlgorithm a owl:NamedIndividual,
        skos:Concept ;
    skos:narrower :Autoencoder,
        :CNN,
        :Deep_Belief_Network,
        :GAN,
        :Kohonen_Self_Organizing_Neural_Network,
        :Neural_Network_Optimization,
        :RBFN,
        :RBM,
        :RNN,
        :Residual_Network,
        :RvNN ;
    skos:prefLabel "Deep Learning Algorithm" .

:PartitionalClustering a owl:NamedIndividual,
        skos:Concept ;
    skos:broader :ClusteringAlgorithm ;
    skos:narrower :CLARA,
        :CLARANS,
        :CLIQUE,
        :DBSCAN,
        :ExpectationMaximization,
        :FuzzyClustering,
        :K-Means,
        :K-Medians,
        :MeanShift,
        :OPTICS,
        :PAM ;
    skos:prefLabel "Partitional Clustering" .

:ReinforcementLearningAlgorithm a owl:NamedIndividual,
        skos:Concept ;
    skos:narrower :Actor-Critic,
        :Deep_Q-Network,
        :Dyna-Q,
        :Normalized_Advantage_Function_Algorithm,
        :PEGASUS,
        :Policy_Evaluation,
        :Q-Learning,
        :REINFORCE,
        :SARSA,
        :Stein_Variational_Policy_Gradient,
        :Temporal_Difference,
        :Trust_Region_Policy_Optimization ;
    skos:prefLabel "Reinforcement Learning Algorithm" .

:DecisionTree a owl:NamedIndividual,
        skos:Concept ;
    skos:narrower :C45,
        :C50,
        :CRUISE,
        :CTREE,
        :Chi-squared_Automatic_Interaction_Detection,
        :Classification_and_Regression_Trees,
        :Cubist,
        :Decision_Stump,
        :Decision_Tree_Regression,
        :GUIDE,
        :ID3,
        :M5,
        :MARS,
        :QUEST ;
    skos:prefLabel "Decision Tree Algorithm" .

:RegularizationAlgorithm a owl:NamedIndividual,
        skos:Concept ;
    skos:narrower :Batch_Normalization,
        :Capped_L1,
        :Data_Augmentation,
        :DropConnect,
        :Dropout,
        :Early_Stopping,
        :Elastic_Net_Regression,
        :Fractional_Pooling,
        :Generalized_Minimax_Concave,
        :LASSO,
        :Label_Smoothing,
        :Layer_Normalization,
        :Leaky_Capped_L1_Norm_Regularizer,
        :Least_Angle_Regression,
        :Minimax_Concave_Penalty,
        :Projection_Operation_Regularization,
        :Ridge_Regression,
        :Smoothly_Clipped_Absolute_Deviation,
        :Weight_Decay ;
    skos:prefLabel "Regularization Algorithm" .

:RegressionAlgorithm a owl:NamedIndividual,
        skos:Concept ;
    skos:narrower :Bayesian_Regression,
        :Boosted_Regression_Tree,
        :Decision_Tree_Regression,
        :Elastic_Net_Regression,
        :Gaussian_Regression,
        :Huber_Regression,
        :K-Nearest_Neighbor,
        :LASSO,
        :Least_Angle_Regression,
        :Linear_Regression,
        :Neural_Network_Regression,
        :Ordinal_Regression,
        :Ordinary_Least_Squares,
        :Polynomial_Regression,
        :Quantile_Regression,
        :RANSAC_Regression,
        :Random_Forest,
        :Ridge_Regression,
        :Support_Vector_Machine,
        :Theil-Sen_Regression ;
    skos:prefLabel "Regression Algorithm" .

:DimensionalityReductionAlgorithm a owl:NamedIndividual,
        skos:Concept ;
    skos:narrower :Canonical_Correlation_Analysis,
        :Factor_Analysis,
        :Generative_Topographic_Mapping,
        :ICA,
        :KECA,
        :Kohonen_Self_Organizing_Neural_Network,
        :LASSO,
        :LDA,
        :MDA,
        :MDS,
        :Manifold_Learning,
        :Maximum_Autocorrelation_Factors,
        :NMF,
        :PCA,
        :PCR,
        :PLSDA,
        :PLSR,
        :Projection_Pursuit,
        :QDA,
        :SVD,
        :Sammon_Mapping,
        :Slow_Feature_Analysis,
        :Vector_Quantization ;
    skos:prefLabel "Dimensionality Reduction Algorithm" .

skos:Concept a owl:Class .

